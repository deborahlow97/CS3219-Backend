Submission ID,Passcode,Title,Authors,Track,Track Chairs,Acceptance Status,Conditions,Track Recommendation,Abstract,Submission Date,Submission Type,Subject Area,Keywords,1: First Name,1: Last Name,1: Email,1: Affiliation,1: Presenter,2: First Name,2: Last Name,2: Email,2: Affiliation,2: Presenter,3: First Name,3: Last Name,3: Email,3: Affiliation,3: Presenter,4: First Name,4: Last Name,4: Email,4: Affiliation,4: Presenter,5: First Name,5: Last Name,5: Email,5: Affiliation,5: Presenter,6: First Name,6: Last Name,6: Email,6: Affiliation,6: Presenter,7: First Name,7: Last Name,7: Email,7: Affiliation,7: Presenter,8: First Name,8: Last Name,8: Email,8: Affiliation,8: Presenter,9: First Name,9: Last Name,9: Email,9: Affiliation,9: Presenter,10: First Name,10: Last Name,10: Email,10: Affiliation,10: Presenter,Main Contact Title,Main Contact Firstname,Main Contact Lastname,Main Contact Affiliation,Main Contact Affiliation Dpt,Main Contact Job Function,Main Contact Phone,Main Contact Mobile,Main Contact Fax,Main Contact Email,Main Contact Street Address,Main Contact City,Main Contact State/Province,Main Contact Zipcode,Main Contact Country,Main Contact Biography,Authors with Affiliations,All Author Emails,procTitle,procShortTitle,paperNumPages,copyrightSig,jobTitle,orgNameAddress,Implementation?,Data?,Dataset_Author,Final Attachments OK,Final Tags,Final Notes
1,1X-J8F4H7P4F5,Table-to-Text: Describing Table Region with Natural Language,Juxxxx Bxx;Duxx Taxx;Nxx Duxx;Zhxx Yxx;Zhxxxx Zhxxx;Mixx Zhxx and Tixxxx Zxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"In this paper, we present a generative model to generate a natural language
sentence describing a table region, e.g. a row. The model maps a row from a
table to a continuous vector, and then generates a natural language sentence by
leveraging the semantics of a table. To deal with the rare words appeared in a
table, we develop a flexible copy mechanism that selectively replicates
contents from the table in the output sequence. Extensive experiments
demonstrate the accuracy of the model and the power of the copy mechanism. On
two synthetic datasets, WIKIBIO and SIMPLEQUESTIONS, our model improves the
current state-of-the-art BLEU-4 score from 34.70 to 40.26 and from 33.32 to
34.28, respectively. Furthermore, we introduce an open-domain dataset
WIKITABLETEXT, which includes 13,318 explanatory sentences for 4,962 tables.
Our model achieves 35.25 BLEU-4 score, which is significantly better than a
neural language model baseline.",7 Feb 2017 07:32:33 GMT,Empirical/Data-Driven,Generation,language generation,Juxxxx,Bxx,xxxxxxxxxxx1@gmail.com,Harbin Institute of Technology,No,Duxx,Taxx,xxxxxxxxxxrosoft.com,Microsoft Research Asia,No,Nxx,Duxx,xxxxxxxxxxcrosoft.com,Microsoft Research Asia,No,Zhxx,Yxx,xxxxxxxxxuaa.edu.cn,Beihang university,No,Zhxxxx,Zhxxx,xxxxxxxxxgmail.com,University of Science and Technology of China,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,Tixxxx,Zhxx,xxxxxxxxit.edu.cn,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,Duxx,Taxx,Microsoft Research Asia,,,1831xxxxxxx,,,xxxxxxxxxxxhotmail.com,,,,,China,,Juxxxx Bxx;Duxx Taxx;Nxx Duxx;Zhxx Yxx;Zhxxxx Zhxxx;Mixx Zhxx;Tixxxx Zhxx,xxxxxxxxxxx1@gmail.com;xxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxuaa.edu.cn;xxxxxxxxx@gmail.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxit.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
2,2X-H3D7D4D3B6,KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm,Hxx Xixx;Mixxxx Huxxx;Lixx Mexx;Daxxx Lx and xiaxxxx zx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Knowledge representation is an important, long-history topic in AI, and there
have been a large amount of work for knowledge graph embedding which projects
symbolic entities and relations into low-dimensional, real-valued vector space.
However, most embedding methods merely concentrate on data fitting and ignore
the explicit semantic expression, leading to uninterpretable representations.
Thus, traditional embedding methods have limited potentials for many
applications such as question answering, and entity classification. To this
end, this paper proposes a semantic representation  method for knowledge graph
\textbf{(KSR)}, which imposes a two-level hierarchical generative process that
globally extracts many aspects and then locally assigns a specific category in
each aspect for every triple. Since both aspects and categories are
semantics-relevant, the collection of categories in each aspect is treated as
the semantic representation of this triple. Extensive experiments justify our
model outperforms other state-of-the-art baselines substantially.",30 Nov 2016 06:59:47 GMT,Empirical/Data-Driven,Semantics,ontological semantics;  semantic knowledge induction,Hxx,Xixx,xxxxxxxxxip.163.com,Tsinghua University,No,Mixxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,Lixx,Mexx,xxxxxxxxxoxmail.com,Tsinghua University,No,Daxxx,Lx,xxxxxxxxxo@163.com,Tsinghua University,No,xiaxxxx,zxx,xxxxxxxxxxxnghua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Hxx,Xixx,Tsinghua University,,,+(86)xxxxxxxxxxx,,,xxxxxxxxxip.163.com,,Beijing,Beijing,,China,,Hxx Xixx;Mixxxx Huxxx;Lixx Mexx;Daxxx Lx;xiaxxxx zxx,xxxxxxxxxip.163.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxxoxmail.com;xxxxxxxxxao@163.com;xxxxxxxxxxxxnghua.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
5,5X-H9A7F2B4G4,Building Visual Scene Relation Bank for Event Relation Recognition,Yx Hoxx;Sixxxx Dixx;Lixxx Yxx;Hexx Jx and Guoxxxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We study a visual scene analysis method for recognizing multi-type event-event
relations. The essential goal is to infer what type of relation a pair of
events hold. We propose to use similar events and their relations as prior
knowledge to support the inference. We apply linguistic approaches and
retrieval techniques for knowledge acquisition. A visual scene relation bank
(VSRB) is built to help organize knowledge, which interlinks visual scenes of
events using their a priori relations. Based on deep visual-semantic alignment,
we fetch mirror images in VSRB for target events. Supervised by relations of
the mirrors, our recognizer determines the relation of the targets by maximum
likelihood. Experiments on a benchmark dataset show that our recognizer
achieves a F-score of 43%.",7 Feb 2017 09:18:50 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction,Yx,Hoxx,xxxxxxxxxx@gmail.com,Soochow University,No,Sixxxx,Dixx,xxxxxxxxxgmail.com,Soochow University,No,Lixxx,Yxx,xxxxxxxxx@gmail.com,Soochow University,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yx,Hoxx,Soochow University,,,+1-51xxxxxxxxxx,,,xxxxxxxxxx@gmail.com,,Suzhou City,,,China,,Yx Hoxx;Sixxxx Dixx;Lixxx Yxx;Hexx Jx;Guoxxxx Zhxx,xxxxxxxxxx@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxxx@gmail.com;xxxxxxpi.edu;xxxxxxxxxuda.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
7,7X-E6P2P2A2G2,Topical Behavior Prediction with Spatial-Temporal Learning,Shixxxxxxx Sx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"In this paper, we study the topical behavior in a large scale. Both the
temporal and the spatial relationships of the behavior are explored with the
deep learning architectures combing the recurrent neural network (RNN) and the
convolutional neural network (CNN). To make the behavioral data appropriate for
the spatial learning in the CNN, several reduction steps are taken in forming
the topical metrics and placing them homogeneously like pixels in the images.
The experimental result shows both temporal and spatial gains when compared
against a multilayer perceptron (MLP) network. A new learning framework called
the
spatially connected convolutional networks (SCCN) is introduced to predict the
topical metrics more efficiently.",31 Jan 2017 01:34:17 GMT,Empirical/Data-Driven,Machine learning,NLP applications;  information extraction;  user studies;  reinforcement learning;  temporal/spatial information extraction,Shixxxxxxx,Sx,xxxxxxxxgmail.com,Qualcomm,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shixxxxxxx,Sx,Qualcomm,,,,,,xxxxxxxxgmail.com,,,,,United States,,Shixxxxxxx Sx,xxxxxxxxgmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
8,8X-J3J4J6J9H6,Topical Summarization and Perception of Logged Behavior,Shixxxxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present a system that organized the massive behavioral data into highly
interactable analytics, where behavioral drift can be easily detected and
observed. The system generates topics based on the corpus of activity logs. The
split-diffuse (SD) algorithm is proposed to distributes the data points
uniformly across the visualization space. The result, called the topic grids,
is a set of grids each representing a topic. 

Log entries are then related to the topics and aggregated toward the entity of
interest. The topical behavior of the entity is quantified and visualized at
the corresponding grid location. Topical analysis, comparison and interaction
can be performed on the topic grids in a more perceivable way. The performance
of the SD algorithm is analyzed on different topic distributions over various
topic grids layouts.",4 Jan 2017 18:37:55 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",interactive IR;  document summarization;  document mining;  document clustering,Shixxxxxxx,Sx,xxxxxxxxgmail.com,Qualcomm,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shixxxxxxx,Sx,Qualcomm,,,,,,xxxxxxxxgmail.com,,,,,United States,,Shixxxxxxx Sx,xxxxxxxxgmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
9,9X-C7H6A4A7P2,Structural Regularities in Text-based Entity Vector Spaces,Chrxxxxxxx Vxx;Maaxxxx dx and Evaxxxxxx Kanxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Entity retrieval is the task of finding entities such as people or products in
response to a query, based solely on the textual documents they are associated
with. Recent semantic entity retrieval algorithms represent queries and experts
in finite-dimensional vector spaces, where both are constructed from text
sequences.

We investigate entity vector spaces and the degree to which they capture
structural regularities. Such vector spaces are constructed in an unsupervised
manner without explicit information about structural aspects. For concreteness,
we address these questions for a specific type of entity: experts in the
context of expert finding. We discover how clusterings of experts correspond to
committees in organizations, the ability of expert representations to encode
the co-author graph, and the degree to which they encode academic rank. We
compare latent, continuous representations created using methods based on
distributional semantics (LSI), topic models (LDA) and neural networks
(word2vec, doc2vec, SERT). Vector spaces created using neural methods, such as
doc2vec and SERT, systematically perform better at clustering than LSI, LDA and
word2vec. When it comes to encoding entity relations, SERT performs best.",6 Feb 2017 18:26:48 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",information retrieval;  document clustering,Chrxxxxxxx,Vanxxxxxx,xxxxxxxxl@uva.nl,University of Amsterdam,No,Maaxxxx,de xxxxx,xxxxxxx@uva.nl,University of Amsterdam,No,Evaxxxxxx,Kanxxxxx,xxxxxxxxas@uva.nl,University of Amsterdam,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chrxxxxxxx,Vanxxxxxx,University of Amsterdam,,,,,,xxxxxxxxl@uva.nl,,,,,Netherlands,,Chrxxxxxxx Vxx;Maaxxxx dx;Evaxxxxxx Kanxxxxx,xxxxxxxxl@uva.nl;xxxxxxxe@uva.nl;xxxxxxxxxas@uva.nl,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
11,11X-G8D7F9P4A6,Contextual Recurrent Units for Sequence Modeling in Natural Language Processing,Yixxxx Cxx;Zhixxxx Chxx;Weixxxx Zhxxx;Tixx Lxx;Shxxxx Waxx and Guoxxxx H,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Recurrent Neural Networks (RNN) are known as powerful models for handling
sequential data, and especially widely utilized in various NLP tasks.
In this paper, we propose a Contextual Recurrent Units (CRU) for sequence
modeling, which injects convolutional neural networks (CNN) into the recurrent
units to
enhance the ability of modeling the local context and reducing word
ambiguities. We mainly tested our CRU model on two representative NLP tasks:
sentiment
classification and cloze-style reading comprehension. Experimental results show
that the proposed CRU model could give significant improvements over
traditional CNN or RNN models, as well as various
state-of-the-art systems on both tasks, and show its promising future of
extensibility to other NLP tasks as well.",7 Feb 2017 06:10:38 GMT,Empirical/Data-Driven,Machine learning,experimental evaluation/comparison of ML methods;  text classification;  question answering in restricted domains,Yixxxx,Cxx,xxxxxxxxlytek.com,iFLYTEK Research,No,Zhixxxx,Chxx,xxxxxxxxxlytek.com,iFLYTEK Research,No,Weixxxx,Zhxxx,xxxxxxxxxx.hit.edu.cn,Harbin Institute of Technology,No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,Shxxxx,Waxx,xxxxxxxxxflytek.com,iFLYTEK Research,No,Guoxxxx,Hx,xxxxxxxxytek.com,iFLYTEK Research,No,,,,,,,,,,,,,,,,,,,,,,Yixxxx,Cxx,iFLYTEK Research,,,,,,xxxxxxxxlytek.com,,Beijing,,,China,,Yixxxx Cxx;Zhixxxx Chxx;Weixxxx Zhxxx;Tixx Lxx;Shxxxx Waxx;Guoxxxx Hx,xxxxxxxxlytek.com;xxxxxxxxxflytek.com;xxxxxxxxxxx.hit.edu.cn;xxxxxxxxxip.126.com;xxxxxxxxxxflytek.com;xxxxxxxxlytek.com,,,,,,,,on,"Yes, include my submission even if the paper is rejected.",No,None,None
12,12X-J4B6B3J3G4,Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules,Xiaxxxx Zhxxx;Aixxx Sxx and Erxx Caxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"Extracting time expressions from free text is a fundamental task for many
applications. We analyze the time expressions from four datasets and find that
only a small group of words are used to express time information, and the words
in time expressions demonstrate similar syntactic behaviour. Based on the
findings, we propose a type-based approach, named SynTime, to recognize time
expressions. Specifically, we define three main syntactic token types, namely
time token, modifier, and numeral, to group time-related regular expressions
over tokens. On the types we design general heuristic rules to
recognize time expressions. In recognition, SynTime first identifies the time
tokens from raw text, then searches their surroundings for modifiers and
numerals to form time segments, and finally merges the time segments to time
expressions. As a light-weight rule-based tagger, SynTime runs in real time,
and can be easily expanded by simply adding keywords for the text of different
types and of different domains. Experiment on benchmark datasets and tweets
data shows that SynTime outperforms state-of-the-art methods.",23 Apr 2017 10:57:56 GMT,Theoretical,"Information extraction, text mining, and question answering",,Xiaxxxx,Zhxxx,xxxxxxxxxxxi@gmail.com,Nanyang Technological University,No,Aixxx,Sxx,xxxxxxxxu.edu.sg,Nanyang Technological University,No,Erxx,Camxxxx,xxxxxxxxxxdia.mit.edu,Nanyang Technological University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Zhxxx,Nanyang Technological University,,,820xxxxx,,,xxxxxxxxxtu.edu.sg,,Singapore,,,Singapore,,Xiaxxxx Zhxxx;Aixxx Sxx;Erxx Camxxxx,xxxxxxxxxxxi@gmail.com;xxxxxxxxtu.edu.sg;xxxxxxxxxxxdia.mit.edu,Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules,Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules,10,Xiaoshi Zhong,PhD student,Nanyang Technological University. Singapore,on,on,Only include my submission if it is accepted.,No,None,None
14,14X-D6D6A4D9D8,Inter-Weighted Alignment Network for Sentence Pair Modeling,Gexxx Shxx;Yuxxxx Yaxx and Zhixxxxx Dxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we propose a model to measure the similarity of a sentence pair
focusing on the interaction information. We utilize the word level similarity
matrix to discover fine-grained alignment of two sentences. It should be
emphasized that each word in a sentence has a different importance from the
perspective of semantic composition, so we exploit a novel and efficient
strategy to calculate a weight for each word. Although the proposed model only
use a sequential LSTM for sentence modeling without any external resource such
as syntactic parser tree and additional lexicon features, experimental results
show that our model achieves state-of-the-art performance on three datasets of
two tasks.",6 Feb 2017 04:14:53 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  information retrieval;  textual entailment and paraphrasing;  text mining;  text classification;  open-domain question answering;  alignment,Gexxx,Shxx,xxxxxxxxxxxe@pku.edu.cn,Peking University,No,Yuxxxx,Yaxx,xxxxxxxxxxxxxlun@pku.edu.cn,Peking University,No,Zhixxxxx,Dexx,xxxxxxxxku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gexxx,Shxx,Peking University,,,,,,xxxxxxxxxxxe@pku.edu.cn,,,,,China,,Gexxx Shxx;Yuxxxx Yaxx;Zhixxxxx Dexx,xxxxxxxxxxxe@pku.edu.cn;xxxxxxxxxxxxxxlun@pku.edu.cn;xxxxxxxxxku.edu.cn,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
16,16X-E5D3G2P8A3,Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms,Shxxxx Lxx;Yuxx Chxx;Kaxx Lxx and Jxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"This paper tackles the task of event detection (ED), which involves identifying
and categorizing events. We argue that arguments provide significant clues to
this task, but they are either completely ignored or exploited in an indirect
manner in existing detection approaches. In this work, we propose to exploit
argument information explicitly for ED via supervised attention mechanisms. In
specific, we systematically investigate the proposed model under the
supervision of different attention strategies. Experimental results show that
our approach advances state-of-the-arts and achieves the best F1 score on
ACE 2005 dataset.",22 Apr 2017 00:54:38 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Shxxxx,Lxx,xxxxxxxxxxxxlpr.ia.ac.cn,Chinese Academy of Sciences,No,Yuxx,Chxx,xxxxxxxxxxxlpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Kaxx,Lxx,xxxxxxxxx.ia.ac.cn,Chinese Academy of Sciences,No,Jxx,Zhxx,xxxxxxxxxr.ia.ac.cn,Chinese Academy of Sciences,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Lxx,Chinese Academy of Sciences,,,,,,xxxxxxxxxxxxlpr.ia.ac.cn,,,,,China,,Shxxxx Lxx;Yuxx Chxx;Kaxx Lxx;Jxx Zhxx,xxxxxxxxxxxxlpr.ia.ac.cn;xxxxxxxxxxxxlpr.ia.ac.cn;xxxxxxxxxr.ia.ac.cn;xxxxxxxxxxr.ia.ac.cn,Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms,Exploiting Argument Information to Improve Event Detection,10,Shulin Liu,,"Institute of Automation Chinese Academy of Sciences
Address: 95 Zhongguancun East Road, 100190, BEIJING, CHINA",,on,Only include my submission if it is accepted.,No,None,None
17,17X-E3B4G8C7F6,Context Generative Inference for Commonsense Machine Comprehension,Binxxxxx Waxx;Kaxx Lxx and Jxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Recently proposed Story Cloze Test (Mostafazadeh et al., 2016) is a commonsense
machine comprehension dataset to deal with natural language understanding
problem. In this dataset, we must choose the right sentence from two candidates
that can be inferred from previous 4 sentences. However, in the training set,
only the right target sentence is provided that makes a discriminative
classifier unable to apply. In this paper, we treat the discrimination problem
as a generation problem and propose a generative context inference model that
generate the target sentence based on the context. Our proposed generative
model outperforms all previous state of the art models. In addition, when
pre-trained in external large unlabeled book corpus, more improvement is
achieved. Our experiment result reveals the advantage of the generative model
and achieves the state-of-the-arts result on Story Cloze Test",7 Feb 2017 01:00:49 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",context-aware question answering;  open-domain question answering,Binxxxxx,Waxx,xxxxxxxxxxxxx@nlpr.ia.ac.cn,"National Laboratory of Pattern Recognition,Institute of Automation, Chinese Academy of Sciences",No,Kaxx,Lxx,xxxxxxxxx.ia.ac.cn,Chinese Academy of Sciences,No,Jxx,Zhxx,xxxxxxxxxr.ia.ac.cn,Chinese Academy of Sciences,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Binxxxxx,Waxx,"National Laboratory of Pattern Recognition,Institute of Automation, Chinese Academy of Sciences",,,,,,xxxxxxxxxxxxx@nlpr.ia.ac.cn,,Beijing,,,China,,Binxxxxx Waxx;Kaxx Lxx;Jxx Zhxx,xxxxxxxxxxxxx@nlpr.ia.ac.cn;xxxxxxxxxr.ia.ac.cn;xxxxxxxxxxr.ia.ac.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
18,18X-P8P4F7G3C7,Attention-over-Attention Neural Networks for Reading Comprehension,Yixxxx Cxx;Zhixxxx Chxx;sx wxx;Shxxxx Waxx;Tixx Lxx and Guoxxxx H,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"Cloze-style reading comprehension is a representative problem in mining
relationship between document and query.
In this paper, we present a simple but novel model called
attention-over-attention reader for better solving cloze-style reading
comprehension task.
The proposed model aims to place another attention mechanism over the
document-level attention and induces ``attended attention'' for final answer
predictions.
One advantage of our model is that it is simpler than related works while
giving excellent performance.
In addition to the primary model, we also propose an N-best re-ranking strategy
to double check the validity of the candidates and further improve the
performance.
Experimental results show that the proposed methods significantly outperform
various state-of-the-art systems by a large margin in public datasets, such as
CNN and Children's Book Test.",23 Apr 2017 07:44:30 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Yixxxx,Cxx,xxxxxxxxlytek.com,iFLYTEK Research,No,Zhixxxx,Chxx,xxxxxxxxxlytek.com,iFLYTEK Research,No,sx,wxx,xxxxxxxxlytek.com,,No,Shxxxx,Waxx,xxxxxxxxxflytek.com,iFLYTEK Research,No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,Guoxxxx,Hx,xxxxxxxxytek.com,iFlyTEK Research,No,,,,,,,,,,,,,,,,,,,,,,Yixxxx,Cxx,iFLYTEK Research,,,,,,xxxxxxxxlytek.com,,Beijing,,,China,,Yixxxx Cxx;Zhixxxx Chxx;sx wxx;Shxxxx Waxx;Tixx Lxx;Guoxxxx Hx,xxxxxxxxlytek.com;xxxxxxxxxflytek.com;xxxxxxxxxlytek.com;xxxxxxxxxxflytek.com;xxxxxxxxxip.126.com;xxxxxxxxlytek.com,Attention-over-Attention Neural Networks for Reading Comprehension,Attention-over-Attention Neural Networks for Reading Comprehension,10,Yiming Cui,,"iFLYTEK Research. Floor 5, Xinzonghe Building, No.15 Xueyuan Road, Haidian District, Beijing, China",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
19,19X-P3B6A5G6F9,Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution,Tixx Lxx;Yixxxx Cxx;Qixxxx Yxx;Weixxxx Zhxxx;Shxxxx Waxx and Guoxxxx H,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Oral Monday,,Undecided (Discourse Pragmatics),"Most existing approaches for zero pronoun resolution are heavily relying on
annotated data, which is often released by shared task organizers.
Therefore, the lack of annotated data becomes a major obstacle in the progress
of zero pronoun resolution task. 
Also, it is expensive to spend manpower on labeling the data for better
performance.
To alleviate the problem above, in this paper, we propose a simple but novel
approach to automatically generate large-scale pseudo training data for zero
pronoun resolution.
Furthermore, we successfully transfer the cloze-style reading comprehension
neural network model into zero pronoun resolution task and propose a two-step
training mechanism to overcome the gap between the pseudo training data and the
real one.
Experimental results show that the proposed approach significantly outperforms
the state-of-the-art systems with an absolute improvements of 3.1\% F-score on
OntoNotes 5.0 data.",23 Apr 2017 07:40:57 GMT,Empirical/Data-Driven,Discourse and pragmatics,,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,Yixxxx,Cxx,xxxxxxxxlytek.com,iFLYTEK Research,No,Qixxxx,Yxx,xxxxxxqq.com,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China",No,Weixxxx,Zhxxx,xxxxxxxxxx.hit.edu.cn,Harbin Institute of Technology,No,Shxxxx,Waxx,xxxxxxxxxflytek.com,iFLYTEK Research,No,Guoxxxx,Hx,xxxxxxxxytek.com,iFlyTEK Research,No,,,,,,,,,,,,,,,,,,,,,,Yixxxx,Cxx,iFLYTEK Research,,,,,,xxxxxxxxlytek.com,,Beijing,,,China,,Tixx Lxx;Yixxxx Cxx;Qixxxx Yxx;Weixxxx Zhxxx;Shxxxx Waxx;Guoxxxx Hx,xxxxxxxxxp.126.com;xxxxxxxxxlytek.com;xxxxxx@qq.com;xxxxxxxxxxx.hit.edu.cn;xxxxxxxxxxflytek.com;xxxxxxxxlytek.com,Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution,Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution,10,Yiming Cui,,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin City, Heilongjiang Province, China",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
20,20X-D3P3F2F6H6,An Extension of Budgeted Maximum Coverage Model and Multi-Document Summarization,Jiaxxxx Waxx;Jxx Wxx;Qixxxx Mx and Guxxxx Wx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"The budgeted maximum coverage model (abbr. BMC) is widely used in the field of
automatic text summarization. To improve the expressive power of the BMC model,
we present an extended maximum coverage model under a budget constraint (abbr.
EBMC), and a factor of $(1-1/ \sqrt{e})$ approximation algorithm. To coordinate
with the application of the EBMC model to text summarization, we propose a new
sentence-level textual feature, named \textit{the sentence coverage}, and then
treat text summarization  as an EBMC problem.  The experimental results on
DUC2001-DUC2004 datasets show that EBMC together with the sentence coverage
feature is significantly superior to BMC, especially for ROUGE-2 scores.",7 Feb 2017 06:51:58 GMT,Theoretical,Summarization,unsupervised and semi-supervised learning;  theoretical aspects of machine learning;  multi-document summarization,Jiaxxxx,Waxx,xxxxxxxxxut.edu.cn,"School of Computer Science and Engineering, South China University of Technology",No,Jxx,Wxx,xxxxxxxxxut.edu.cn,"School of Computer Science and Engineering, South China University of Technology",No,Qixxxx,Mx,xxxxxxxxxxcut.edu.cn,"School of Computer Science and Engineering, South China University of Technology",No,Guxxxx,Wxx,xxxxxxxxxcut.edu.cn,"School of Computer Science and Engineering, South China University of Technology",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jiaxxxx,Waxx,"School of Computer Science and Engineering, South China University of Technology",,,,,,xxxxxxxxxut.edu.cn,,Guangzhou,Guangdong,,China,,Jiaxxxx Waxx;Jxx Wxx;Qixxxx Mx;Guxxxx Wxx and Engixxxxxxx Soxxx,xxxxxxxxxut.edu.cn;xxxxxxxxxcut.edu.cn;xxxxxxxxxxscut.edu.cn;xxxxxxxxxxcut.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
21,21X-B3F3A4D6B3,Transductive Non-linear Learning for Chinese Hypernym Prediction,Chexxxx Waxx;Juxxxx Yxx;Aoxxxx Zhxx and Xiaxxxxx H,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"Finding the correct hypernyms for entities is essential for taxonomy learning,
fine-grained entity categorization, query understanding, etc. Due to the
flexibility of the Chinese language, it is challenging to identify hypernyms
in Chinese accurately. Rather than extracting hypernyms from texts, in this
paper, we present a transductive learning approach to establish mappings from
entities to hypernyms in the embedding space directly. It combines linear and
non-linear embedding projection models, with the capacity of
encoding arbitrary language-specific rules. Experiments on real-world datasets
illustrate that our approach outperforms previous methods for Chinese hypernym
prediction.",11 Apr 2017 05:02:20 GMT,Applications/Tools,"Information extraction, text mining, and question answering",,Chexxxx,Waxx,xxxxxxxxxx3@gmail.com,East China Normal University,No,Juxxxx,Yxx,xxxxxxxx3@163.com,IBM Research - China,No,Aoxxxx,Zhxx,xxxxxxxxxxxecnu.edu.cn,East China Normal University,No,Xiaxxxxx,Hx,xxxxxxxxxxcnu.edu.cn,East China Normal University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chexxxx,Waxx,East China Normal University,,,,,,xxxxxxxxxx3@gmail.com,,,,,China,,Chexxxx Waxx;Juxxxx Yxx;Aoxxxx Zhxx;Xiaxxxxx Hx,xxxxxxxxxx3@gmail.com;xxxxxxxxx3@163.com;xxxxxxxxxxx.ecnu.edu.cn;xxxxxxxxxxecnu.edu.cn,Transductive Non-linear Learning for Chinese Hypernym Prediction,Transductive Non-linear Learning for Chinese Hypernym Prediction,11,Chengyu Wang,,"East China Normal University
3663 North Zhongshan Road, Shanghai, China",on,on,Only include my submission if it is accepted.,No,None,None
23,23X-C7E9D9A3H4,RecSA: Recommendation Semantic Analysis for Rating Matrix Completion,Hxx Xixx;Mixxxx Huxxx and xiaxxxx zx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Recommendation system is an Internet necessity for daily life and there are two
technique branches for this task: matrix completion and social information
integration. However, most matrix completion methods focus on a theoretically
mathematic formulation concerning nearly none of the background semantics,
which degrades the prediction accuracy. While social information integration
methods need extra corpus, which are much noisy and hardly achievable. To this
end, this paper proposes a semantic analysis method \textbf{(RecSA)} for
recommendation system, which imposes a two-level hierarchical generative
process that globally resolves many semantic properties and then locally works
out a specific category in each property for every user and item. In this way,
RecSA extracts semantic information merely from ratings, which complements the
traditional methods in the manner of semantic matching. Extensive experiments
demonstrate RecSA outperforms other state-of-the-art baselines substantially.",4 Feb 2017 01:02:10 GMT,Empirical/Data-Driven,Semantics,filtering and recommendation;  ontological semantics,Hxx,Xixx,xxxxxxxxxip.163.com,Tsinghua University,No,Mixxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,xiaxxxx,zxx,xxxxxxxxxxxnghua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hxx,Xixx,Tsinghua University,,,+(86)xxxxxxxxxxx,,,xxxxxxxxxip.163.com,,Beijing,Beijing,,China,,Hxx Xixx;Mixxxx Huxxx;xiaxxxx zxx,xxxxxxxxxip.163.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxxxxnghua.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
25,25X-P4G3F5H9D4,Content-Based Table Retrieval for Web Queries,Zhxx Yxx;Duxx Taxx;Nxx Duxx;Juxxxx Bxx;Yuaxxxx Lx;Mixx Zhxx and Zhoxxxx L,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Understanding the connections between unstructured text and semi-structured
table is an important yet neglected problem in natural language processing. In
this work, we focus on content-based table retrieval. Given a query, the task
is to find the most relevant table from a collection of tables. Further
progress towards improving this area requires powerful models of semantic
matching and richer training and evaluation resources. To remedy this, we
present a ranking based approach, and implement both carefully designed
features and neural network architectures to measure the relevance between a
query and the content of a table. Furthermore, we release an open-domain
dataset that includes 21,113 web queries for 273,816 tables. We conduct
comprehensive experiments on both real world and synthetic datasets. Results
verify the effectiveness of our approach and present the challenges for this
task.",7 Feb 2017 02:29:18 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",information retrieval,Zhxx,Yxx,xxxxxxxxxuaa.edu.cn,Beihang university,No,Duxx,Taxx,xxxxxxxxxxrosoft.com,Microsoft Research Asia,No,Nxx,Duxx,xxxxxxxxxxcrosoft.com,Microsoft Research Asia,No,Juxxxx,Bxx,xxxxxxxxxxx1@gmail.com,Harbin Institute of Technology,No,Yuaxxxx,Lx,xxxxxxxxxxxcrosoft.com,Microsoft,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,Zhoxxxx,Lx,xxxxxxxxa.edu.cn,Beihang University,No,,,,,,,,,,,,,,,,,Duxx,Taxx,Microsoft Research Asia,,,1831xxxxxxx,,,xxxxxxxxxxxhotmail.com,,,,,China,,Zhxx Yxx;Duxx Taxx;Nxx Duxx;Juxxxx Bxx;Yuaxxxx Lx;Mixx Zhxx;Zhoxxxx Lx,xxxxxxxxxuaa.edu.cn;xxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxx01@gmail.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxaa.edu.cn,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
26,26X-C3C3D8D3P3,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,Yanxxxx Hxx;Yuaxxxx Zhxxx;Kaxx Lxx;Shxxxx Hx;Zhxxxx Lxx;Hxx Wx and Jxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"With the rapid growth of knowledge bases (KBs) on the web, how to take full
advantage of them becomes increasingly important. Question answering over
knowledge base (KB-QA) is one of the  promising approaches to access the
substantial knowledge. Meanwhile, as the neural network-based (NN-based)
methods develop, NN-based KB-QA has already achieved impressive results.
However, previous work did not put more emphasis on question representation,
and the question is converted into a fixed vector regardless of its candidate
answers. This simple representation strategy is not easy to express the proper
information in the question. Hence, we present an end-to-end neural network
model to represent the questions and their corresponding scores dynamically
according to the various candidate answer aspects via cross-attention
mechanism. In addition, we leverage the global knowledge inside the underlying
KB, aiming at integrating the rich KB information into the representation of
the answers. As a result, it could alleviates the out-of-vocabulary (OOV)
problem, which helps the cross-attention model to represent the question more
precisely. The experimental results on WebQuestions demonstrate the
effectiveness of the proposed approach.",21 Apr 2017 15:58:58 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Yanxxxx,Hxx,xxxxxxxxxxxxnlpr.ia.ac.cn,"Institute of Automation,Chinese Academy of Sciences",No,Yuaxxxx,Zhxxx,xxxxxxxxxxpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Kaxx,Lxx,xxxxxxxxx.ia.ac.cn,Chinese Academy of Sciences,No,Shxxxx,Hx,xxxxxxxxxxxlpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Zhxxxx,Lxx,xxxxxxxxx@baidu.com,Baidu,No,Hxx,Wx,xxxxxxxxaidu.com,Baidu,No,Jxx,Zhxx,xxxxxxxxxr.ia.ac.cn,Chinese Academy of Sciences,No,,,,,,,,,,,,,,,,,Yanxxxx,Hxx,"Institute of Automation,Chinese Academy of Sciences",,,,,,xxxxxxxxxxxxnlpr.ia.ac.cn,,,,,China,,Yanxxxx Hxx;Yuaxxxx Zhxxx;Kaxx Lxx;Shxxxx Hx;Zhxxxx Lxx;Hxx Wx;Jxx Zhxx,xxxxxxxxxxxxnlpr.ia.ac.cn;xxxxxxxxxxxpr.ia.ac.cn;xxxxxxxxxr.ia.ac.cn;xxxxxxxxxxxxlpr.ia.ac.cn;xxxxxxxxxx@baidu.com;xxxxxxxxbaidu.com;xxxxxxxxxxr.ia.ac.cn,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,11,Yanchao Hao,,"National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",,on,Only include my submission if it is accepted.,No,None,None
27,27X-F3E3G3C4B5,cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information,Shaxxxxxx Cxx;Wxx Lx;Jxx Zhxx and Xiaxxxxx L,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"We propose a novel method for learning Chinese word representations (a.k.a.
word embeddings). Existing approaches explored character or radical level
information for capturing semantics associated with Chinese words. In this
work, we make an observation that the stroke level information is crucial and
can be exploited for improved learning of Chinese word representations. Unlike
prior works, our proposed method is able to incorporate semantically useful
Chinese morphological information during the learning process via the unique
stroke n-gram information  the model captures. Through qualitative analysis, we
demonstrate that our model is able to extract semantic information that  cannot
be captured by existing methods. Empirical results on the word similarity and
word analogy tasks show that the proposed approach outperforms state-of-the-art
approaches such as word2vec, GloVe and CWE.",7 Feb 2017 11:35:01 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",unsupervised and semi-supervised learning;  morphology,Shaxxxxxx,Cxx,xxxxxxxxxxxxs@antfin.com,Ant Financial Services Group,No,Wxx,Lx,xxxxxxxxxtd.edu.sg,Singapore University of Technology and Design,No,Jxx,Zhxx,xxxxxxxxxxx@antfin.com,Ant Financial Services Group,No,Xiaxxxxx,Lx,xxxxxxxxtfin.com,Ant Financial Services Group,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shaxxxxxx,Cxx,Ant Financial Services Group,,,(86)1xxxxxxxxxx,,,xxxxxxxxxxxxs@antfin.com,,Hangzhou,Zhejiang,,China,,Shaxxxxxx Cxx;Wxx Lx;Jxx Zhxx;Xiaxxxxx Lx,xxxxxxxxxxxxs@antfin.com;xxxxxxxxxutd.edu.sg;xxxxxxxxxxxn@antfin.com;xxxxxxxxntfin.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
31,31X-C3D9H3B6C7,Event Factuality Identification via Deep Neural Networks,Zhxxx Qixx;Peixxxx Lx;Guoxxxx Zhxx and Qiaxxxxx Zx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Event factuality identification plays an important role in deep NLP
applications. In this paper, we propose a deep learning framework for this task
which first extracts essential information from raw texts as the inputs and
then identifies the factuality of events via a deep neural network with a
proper combination of Bidirectional Long Short-Term Memory (BiLSTM) neural
network and Convolutional Neural Network (CNN). The experimental results on
FactBank show that our framework significantly outperforms several
state-of-the-art baselines.",7 Feb 2017 11:37:50 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP applications;  information extraction;  text mining,Zhxxx,Qixx,xxxxxxxxxqz@163.com,Soochow University,No,Peixxxx,Lx,xxxxxxxxa.edu.cn,Soochow University,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,Qiaxxxxx,Zxx,xxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhxxx,Qixx,Soochow University,,,,,,xxxxxxxxxqz@163.com,,,,,China,,Zhxxx Qixx;Peixxxx Lx;Guoxxxx Zhxx;Qiaxxxxx Zxx,xxxxxxxxxqz@163.com;xxxxxxxxda.edu.cn;xxxxxxxxxuda.edu.cn;xxxxxxxxxda.edu.cn,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
32,32X-J7C8J6C4P3,Effective Incorporation of Linguistic Knowledge into Neural Machine Translation,Qixxx Lx;Toxx Xixx and Jixxxx Zx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation has the problems of producing translations with
meaningful word missing and incorrect sentence structure. Source input
sentences with linguistic knowledge that includes part-of-speech tag, named
entity, chunk, and syntactic dependency tree can effectively address these
problems. In this work, we incorporate linguistic knowledge into neural machine
translation in multi-source encoder-decoder architecture at the word-level. The
core idea behind is to transform linguistic knowledge into sequence that is fed
to an additional encoder. Finally, we observe significant improvements from 1.0
to 2.0 BLEU points on Chinese↔English translations with our proposed
approach.",7 Feb 2017 09:06:40 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation,Qixxx,Lx,xxxxxxxxxx@gmail.com,"Natural Language Processing Lab, Northeastern University, China",No,Toxx,Xixx,xxxxxxxxxxxxl.neu.edu.cn,Northestern University,No,Jixxxx,Zxx,xxxxxxxxxxxxil.neu.edu.cn,Northeastern University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixxx,Lx,"Natural Language Processing Lab, Northeastern University, China",,,1331xxxxxxx,,,xxxxxxxxxx@gmail.com,,,,,China,,Qixxx Lx;Toxx Xixx;Jixxxx Zxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxxxil.neu.edu.cn;xxxxxxxxxxxxxil.neu.edu.cn,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
33,33X-G6P9G9G4D8,Linguistically Regularized LSTM for Sentiment Classification,Qixx Qixx;Mixxxx Huxxx;Jixxxx Lxx and xiaxxxx zx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Poster Monday,,Undecided (Sentiment Analysis Opinion Mining),"This paper deals with sentence-level sentiment classification. Though a variety
of neural network models have been proposed recently, however, previous models
either depend on expensive phrase-level annotation, most of which has
remarkably degraded performance when trained with only sentence-level
annotation; or do not fully employ linguistic resources (e.g., sentiment
lexicons, negation words, intensity words). In this paper, we propose simple
models trained with sentence-level annotation, but also attempt to model the
linguistic role of sentiment lexicons, negation words, and intensity words.
Results show that our models are able to capture the linguistic role of
sentiment words, negation words, and intensity words in sentiment expression.",23 Apr 2017 06:51:22 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Qixx,Qixx,xxxxxxxxxxxxxber29@126.com,Tsinghua University,No,Mixxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,Jixxxx,Lxx,xxxxxxxxgmail.com,Tsinghua University,No,xiaxxxx,zxx,xxxxxxxxxxxnghua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixx,Qixx,Tsinghua University,,,,,,xxxxxxxxxxxxxber29@126.com,,,,,China,,Qixx Qixx;Mixxxx Huxxx;Jixxxx Lxx;xiaxxxx zxx,xxxxxxxxxxxxxber29@126.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxgmail.com;xxxxxxxxxxxxnghua.edu.cn,Linguistically Regularized LSTM for Sentiment Classification,Linguistically Regularized LSTM,11,qianqiao,,"State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China",on,on,Only include my submission if it is accepted.,No,None,None
34,34X-B3F6H3C7G6,Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory,Hxx Zhxx;Mixxxx Huxxx;Tiaxxxxx Zhxxx;xiaxxxx zxx and Bixx Lx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Emotional intelligence is one of the key factors to the success of dialogue
systems or conversational agents. In this paper, we propose Emotional Chatting
Machine (ECM) which generates responses that are appropriate not only at the
logic level (relevant and grammatical) but also at the emotion level
(consistent emotional expression). To the best of our knowledge, this is the
first work that addresses the emotion factor in large-scale conversation
generation. ECM addresses the factor in three ways: modeling high-level
abstraction of emotion expression by embedding emotion categories, changing of
implicit internal emotion states, and using explicit emotion expressions with
an external emotion vocabulary. Experiments show that our model can generate
responses appropriate not only in logic but also in emotion.",7 Feb 2017 05:12:57 GMT,Empirical/Data-Driven,Generation,language generation;  dialogue;  spoken language  generation,Hxx,Zhxx,xxxxxxxxgmail.com,Tsinghua University,No,Mixxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,Tiaxxxxx,Zhxxx,xxxxxxxxxxxxty@gmail.com,Tsinghua University,No,xiaxxxx,zxx,xxxxxxxxxxxnghua.edu.cn,,No,Bixx,Lxx,xxxxxxic.edu,University of Illinois at Chicago,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Hxx,Zhxx,Tsinghua University,,,,,,xxxxxxxxgmail.com,,,,,China,,Hxx Zhxx;Mixxxx Huxxx;Tiaxxxxx Zhxxx;xiaxxxx zxx;Bixx Lxx,xxxxxxxxgmail.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxxxxzty@gmail.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxuic.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
37,37X-H8D6G7E3H9,Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,Yx Wx;wxx wx;Chxx Xixx;Mixx Zhxx and Zhoxxxx L,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Monday,,Undecided (Dialog Interactive Systems),"We study response selection for multi-turn conversation in retrieval based
chatbots. Existing work either concatenates utterances in context or matches a
response with a highly abstract context vector finally, which may lose
relationships among the utterances or important information in the context. We
propose a sequential matching network (SMN) to address both problems. SMN first
matches a response with each utterance in the context on multiple levels of
granularity, and distills important matching information from each pair as a
vector with convolution and pooling operations. The vectors are then
accumulated in a chronological order through a recurrent neural network (RNN)
which models relationships among the utterances. The final matching score is
calculated with the hidden states of the RNN. Empirical study on two public
data sets shows that SMN can significantly outperform state-of-the-art methods
for response selection in multi-turn conversation.",21 Apr 2017 02:25:52 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Yx,Wx,xxxxxxx126.com,Beihang,No,wxx,wx,xxxxxxxxxrosoft.com,,No,Chxx,Xixx,xxxxxxxxxxxcrosoft.com,Nankai University,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Zhoxxxx,Lx,xxxxxxxxa.edu.cn,Beihang University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yx,Wx,Beihang,,,,,,xxxxxxx126.com,,,,,China,"I am a fourth year Ph.D. student of Beihang University under the supervision of Prof.Ming Zhou and Prof. Zhoujun Li. Until now, I have published over 10 top conference papers, which study on the response generation and matching models for chatbots. I am a subreviewer of WSDM, SIGIR and AAAI.",Yx Wx;wxx wx;Chxx Xixx;Mixx Zhxx;Zhoxxxx Lx,xxxxxxx126.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxaa.edu.cn,Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,10,Yu Wu,,"Beihang Univeristy
Microsoft Research Asia",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
38,38X-J5B6G6J6G8,Sequence-to-Dependency Neural Machine Translation,Shuxxxxxx Wx;Donxxxxx Zhxxx;Nxx Yaxx;Mx Lx and Mixx Zxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Oral Tuesday,,Undecided (Machine Translation),"Nowadays a typical Neural Machine Translation (NMT) model generates
translations from left to right as a linear sequence, during which latent
syntactic structures of the target sentences are not explicitly concerned.
Inspired by the success of using syntactic knowledge of target language for
improving statistical machine translation,
in this paper we propose a novel Sequence-to-Dependency Neural Machine
Translation (SD-NMT) method, in which the target word sequence and its
corresponding dependency structure are jointly constructed and modeled, and
this structure is used as context to facilitate word generations. Experimental
results show that the proposed method significantly outperforms
state-of-the-art baselines on Chinese-English and Japanese-English translation
tasks.",20 Apr 2017 05:22:42 GMT,Empirical/Data-Driven,Machine translation,,Shuxxxxxx,Wx,xxxxxxxxxxxcrosoft.com,Harbin Institute of Technology,No,Donxxxxx,Zhxxx,xxxxxxxxxxcrosoft.com,Microsoft Research Asia,No,Nxx,Yaxx,xxxxxxxxxrosoft.com,Microsoft Research Asia,No,Mx,Lx,xxxxxxxxxosoft.com,Microsoft Research Asia,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Shuxxxxxx,Wx,Harbin Institute of Technology,,,1355xxxxxxx,,,xxxxxxxxxxxcrosoft.com,,,,,China,,Shuxxxxxx Wx;Donxxxxx Zhxxx;Nxx Yaxx;Mx Lx;Mixx Zhxx,xxxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com,Sequence-to-Dependency Neural Machine Translation,Sequence-to-Dependency Neural Machine Translation,10,Shuangzhi Wu,,"Harbin Institute of Technology, No.92, Xidazhi Street, Nangang District, Harbin City, Heilongjiang Province, China",,,No. Do not include my submission in this dataset.,No,None,None
39,39X-F3H7F3E3G2,Hierarchical Recurrent Attention Network for Response Generation,Chxx Xixx;Wxx Wx;Yx Wx;Mixx Zhxx;Weixxxxx Mx and Yaxxx Huxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"We study multi-turn response generation in chatbots where a response is
generated according to a conversation context. Existing work has modeled the
hierarchy of the context, but does not pay enough attention to the fact that
words and utterances in the context are differentially important. As a result,
they may lose important information in context and generate irrelevant
responses. We propose a hierarchical recurrent attention network (HRAN) to
model both aspects in a unified framework. In HRAN, a hierarchical attention
mechanism attends to important parts within and among utterances with word
level attention and utterance level attention respectively. With the word level
attention, hidden vectors of a word level encoder are synthesized as utterance
vectors and fed to an utterance level encoder to construct hidden
representations of the context. The hidden vectors of the context are then
processed by the utterance level attention and formed as context vectors for
decoding the response. Empirical studies on both automatic evaluation and human
judgment show that HRAN can significantly outperform state-of-the-art
models for multi-turn response generation.",6 Feb 2017 03:47:26 GMT,Empirical/Data-Driven,Dialog and interactive systems,dialogue;  contex modeling for dialogues,Chxx,Xixx,xxxxxxxxxxxcrosoft.com,Nankai University,No,Wxx,Wx,xxxxxxxxxrosoft.com,Microsoft Research,No,Yx,Wx,xxxxxxxxxxrosoft.com,Beihang University,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Weixxxxx,Mx,xxxxxxxxxosoft.com,Microsoft Research,No,Yaxxx,Huxxx,xxxxxxxxxxnkai.edu.cn,Nankai University,No,,,,,,,,,,,,,,,,,,,,,,Chxx,Xixx,Nankai University,,,,,,xxxxxxxxxn@163.com,,,,,China,,Chxx Xixx;Wxx Wx;Yx Wx;Mixx Zhxx;Weixxxxx Mx;Yaxxx Huxxx,xxxxxxxxxxxcrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxcrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxrosoft.com;xxxxxxxxxxxnkai.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
40,40X-A4G3G6F5B4,TSTR: Topic-Specific reTweet count Ranking for Weibo,Haxxxx Mxx;Yaxx Xixx;Jiaxxxx Waxx;Yuxx Waxx;Weixxxx Kx and Zhxx Xxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Retweet behavior is the key mechanism to diffuse information on micro-blogging
services. Researchers have put much effort to predict tweet popular level as
measured by future retweet count. However, previous studies ignore the topic
entity information, and require intensive manual feature engineering.
Accordingly, we investigate *topic-specific* retweet count prediction problem,
and use neural networks to generate most features automatically. A
Topic-Specific reTweet count Ranking (TSTR) framework is proposed to address
this problem from a ranking perspective. We evaluate TSTR as well as several
baselines using five widely adopted ranking metrics. All results of extensive
experiments on large Weibo data show the effectiveness and flexibility of our
method.",6 Feb 2017 04:50:18 GMT,Applications/Tools,Social media,NLP applications;  Web mining;  NLP on noisy unstructured text;  text mining;  NLP in social networking media;  document mining;  social network,Haxxxx,Mxx,xxxxxxxxku.edu.cn,Peking University,No,Yaxx,Xixx,xxxxxxxxxxx@pku.edu.cn,Peking University,No,Jiaxxxx,Waxx,xxxxxxxxxpku.edu.cn,Peking University,No,Yuxx,Waxx,xxxxxxxxxxf@gmail.com,Peking University,No,Weixxxx,Kx,xxxxxxxxxxo@gmail.com,Peking University,No,Zhxx,Xixx,xxxxxxxxmail.com,Peking University,No,,,,,,,,,,,,,,,,,,,,,,Haxxxx,Mxx,Peking University,,,,,,xxxxxxxxxx@gmail.com,,,,,China,,Haxxxx Mxx;Yaxx Xixx;Jiaxxxx Waxx;Yuxx Waxx;Weixxxx Kx;Zhxx Xixx,xxxxxxxxku.edu.cn;xxxxxxxxxxxu@pku.edu.cn;xxxxxxxxxxpku.edu.cn;xxxxxxxxxxxf@gmail.com;xxxxxxxxxxxo@gmail.com;xxxxxxxxgmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
41,41X-F7F4J8J5P3,Enhancing Word Embedding by Implicitly Incorporating Morphological Information,Yaxx Xx;Jixxxx Lxx;Wxx Yaxx and Liuxxxxx Huxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Traditional word embedding methods learn semantic information from word-level
while ignore the useful internal structures of words like morphemes. The
explicit models, which directly utilize morphemes to improve word embedding,
suffer from the similar problem that neglect the meanings of morphemes. In this
paper, we propose three novel models to enhance word embedding by implicitly
using morphological information. Experiments on word similarity and syntactic
analogy show that the implicit models are superior to traditional explicit
ones. Our models outperform all state-of-the-art baselines and significantly
improve the performance on both tasks. Parameter analysis indicates that the
implicit models can supplement semantic information during the word embedding
training process. Moreover, our performance on the smallest corpus is similar
to the performance of CBOW on the corpus which is five times the size of ours.",7 Feb 2017 07:15:28 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",lexical semantics;  morphology;  distributional similarity;  NLP on Wikipedia and other collaboratively constructed resources,Yaxx,Xx,xxxxxxxxxxxxl.ustc.edu.cn,University of Science and Technology of China,No,Jixxxx,Lxx,xxxxxxxxxxxx.ustc.edu.cn,University of Science and Technology of China,No,Wxx,Yaxx,xxxxxxxxtc.edu.cn,University of Science and Technology of China,No,Liuxxxxx,Huxxx,xxxxxxxxxstc.edu.cn,University of Science and Technology of China,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Xx,University of Science and Technology of China,,,,,,xxxxxxxxxxxxl.ustc.edu.cn,,,,,China,,Yaxx Xx;Jixxxx Lxx;Wxx Yaxx;Liuxxxxx Huxxx and Tecxxxxxxx ox,xxxxxxxxxxxxl.ustc.edu.cn;xxxxxxxxxxxxl.ustc.edu.cn;xxxxxxxxxtc.edu.cn;xxxxxxxxxxstc.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
42,42X-A3G3E3J2J8,Online Word Embedding: Theory and Experiment for Skip-gram Model with Negative Sampling,Nobxxxxx Kaxx and Haxxxx Kobxxxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"This paper explores training the skip-gram model with negative sampling (SGNS)
in an online setting for incrementally updating word embeddings. A new online
variant of SGNS and its efficient implementation are proposed. In addition, a
theoretical analysis is conducted to show the validity of the online method.
Empirical experiments demonstrated the correctness of the theoretical analysis
as well as the practical usefulness of the proposed online method.",7 Feb 2017 06:52:43 GMT,Empirical/Data-Driven,Machine learning,scalability/efficiency of ML methods;  on-line learning,Nobxxxxx,Kaxx,xxxxxxxxxoo-corp.jp,Yahoo Japan Corporation,No,Haxxxx,Kobxxxxxx,xxxxxxxxxxxxxshi@gmail.com,Yahoo Japan Corporation,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nobxxxxx,Kaxx,Yahoo Japan Corporation,,,,,,xxxxxxxxxoo-corp.jp,,,,,Japan,,Nobxxxxx Kaxx;Haxxxx Kobxxxxxx,xxxxxxxxxoo-corp.jp;xxxxxxxxxxxxxashi@gmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
43,43X-F9A8C3F2B2,Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks,Mrixxxxx Saxxxx and Erxx Xixx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Humans as well as animals are good at imitation. Inspired by this, the learning
by demonstration view of machine learning learns to perform a task from
detailed example demonstrations. In this work, we propose an approach that
learns to solve SAT style geometry problems using demonstrative solutions to
these problems available in textbooks. The approach jointly learns how to
interpret the demonstration and how to use this interpretation to solve
geometry problems. We collect a new dataset of demonstrative solutions of
geometry problems from various textbooks and show significant improvements over
best previously published results.",7 Feb 2017 10:41:52 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",educational applications;  question answering in restricted domains,Mrixxxxx,Saxxxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Erxx,Xixx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mrixxxxx,Saxxxx,Carnegie Mellon University,,,412xxxxxxx,,,xxxxxxxxxxxr@gmail.com,,,,,United States,,Mrixxxxx Saxxxx;Erxx Xixx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
44,44X-E4D3A3E2D3,Position-aware Factorization Machine for Jointly Learning Sentiment Word Representation and Interaction,Shxxx Waxx;Miaxxxx Zhxx;Gexx Fxx;Yx Chxxx and Bixx Lx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Sentiment analysis involves various tasks. Two fundamental components are
learning word representation and sentiment polarity. When the word
representation itself carries sentiment information, we name it sentiment word
representation. For fine-grained analysis at the snippet level (a phrase or
sentence), an additional issue is to distinguish different sentiment based on
different word-wise interactions, which we call sentiment interaction. Although
existing approaches have implicitly modeled the effect of word co-functioning,
few studies explicitly considered sentiment interaction in a well-interpretable
paradigm. We suggest that sentiment interaction can be encoded into sentiment
word representation to make it more interpretable. This also simplifies the
parameter learning/tuning and reduces the time cost. For that, this paper
proposes a general framework to jointly model them. Specifically, we develop
two new models. They are Contextual Factorization Machine (CFM) and
Position-aware Factorization Machine (PFM). Extensive experiments have been
conducted across three real-world datasets to show their effectiveness.",7 Feb 2017 07:35:51 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis,Shxxx,Waxx,xxxxxxxxxxi@gmail.com,University of Illinois at Chicago,No,Miaxxxx,Zhxx,xxxxxxxxmail.com,Yahoo Lab,No,Gexx,Fxx,xxxxxxuic.edu,University of Illinois at Chicago,No,Yx,Chxxx,xxxxxxx@acm.org,Huawei Research America,No,Bixx,Lxx,xxxxxxic.edu,University of Illinois at Chicago,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxx,Waxx,University of Illinois at Chicago,,,312xxxxxxx,,,xxxxxxxxxxi@gmail.com,,Chicago,IL,,United States,,Shxxx Waxx;Miaxxxx Zhxx;Gexx Fxx;Yx Chxxx;Bixx Lxx,xxxxxxxxxxi@gmail.com;xxxxxxxxgmail.com;xxxxxxxuic.edu;xxxxxxxx@acm.org;xxxxxxuic.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
47,47X-B4J6G8J5H3,Hybrid Model for Named Entity Recognition with a Large Number of Categories,Truxxxxxx Tx;Khxx Mxx;Thaxxxxxxx Phxx;Mixx Trxxx;Tuxx Dxx;Yosxxxxxx Nisxxxxxx;Takxxxx Egxxxx;Hidxxxxx Shixxxx;Ryxxxx Saxxxx and Satxxxx Sexxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In intelligent dialogue systems, it is essential to transform free text into
structured form of information. One of the technologies to achieve this goal is
information extraction (IE) technology, and named entity (NE) extraction is a
crucial component technology of IE. However, traditional NE is limited to a
small number of categories, which does not satisfy the needs of current
intelligent dialogue systems. For example, in the text ``I went to Stanley Park
in Vancouver last July. Have you ever been to the city?'', just recognizing
that Stanley Park and Vancouver are location names is not enough. Dialogue
systems are required to recognize that Stanley Park is a park name and
Vancouver is a city name.
  Therefore, in this paper, we tackle named entity recognition (NER) with a
large number of categories. Specifically, we focus on recognizing extended
named-entity (ENE) hierarchy for Japanese and present a system that combines
four base methods: Recurrent Neural Networks (RNNs), Wikification, cascade of
Conditional Random Fields (CRFs) and Support Vector Machines (SVMs), and
rule-based. Our experimental results show that we could exploit the advantages
of each method by properly combining them. Using a combination of four base
methods, we achieve a good performance of 71.95% F1 on a hierarchy of 200
categories.",7 Feb 2017 12:42:52 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  named entity recognition;  word segmentation;  chunking;  text mining;  mention detection;  named entity disambiguation,Trucxxxxxxxx,Ngxxxx,xxxxxxxxxxxuyen@alt.ai,Alt Inc.,No,Khxx,Mxx,xxxxxxxxxxn@gmail.com,Alt Inc.,No,Thaxxxxxxx,Phxx,xxxxxxxxxxxxx.hn@gmail.com,Alt Inc.,No,Minxxxxxxx,Ngxxxx,xxxxxxxxxxxxtrung@alt.ai,Alt Inc.,No,Tuaxxxxx,Ngxxxx,xxxxxxxxxxx.duc@alt.ai,Alt Inc.,No,Yosxxxxxx,Nisxxxxxx,xxxxxxxxxxxxxhimura@alt.ai,Alt Inc.,No,Takxxxx,Egxxxx,xxxxxxxxxxuchi@alt.ai,Alt Inc.,No,Hidxxxxx,Shixxxx,xxxxxxxxxxxxeis.ynu.ac.jp,Yokohama National University,No,Ryxxxx,Saxxxx,xxxxxxxxxxxitech.ac.jp,Tokyo Institute of Technology,No,Satxxxx,Sexxxx,xxxxxxxxs.nyu.edu,New York University,No,,Trucxxxxxxxx,Ngxxxx,Alt Inc.,,,,,,xxxxxxxxxxxxyen@gmail.com,,,,,Viet Nam,,Truxxxxxx Tx;Khxx Mxx;Thaxxxxxxx Phxx;Mixx Trxxx;Tuxx Dxx;Yosxxxxxx Nisxxxxxx;Takxxxx Egxxxx;Hidxxxxx Shixxxx;Ryxxxx Saxxxx;Satxxxx Sexxxx,xxxxxxxxxxxuyen@alt.ai;xxxxxxxxxxxn@gmail.com;xxxxxxxxxxxxxg.hn@gmail.com;xxxxxxxxxxxx.trung@alt.ai;xxxxxxxxxxxn.duc@alt.ai;xxxxxxxxxxxxxshimura@alt.ai;xxxxxxxxxxxuchi@alt.ai;xxxxxxxxxxxxxeis.ynu.ac.jp;xxxxxxxxxxxtitech.ac.jp;xxxxxxxxxs.nyu.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
48,48X-E2J3H4A7G6,Knowledge Enhanced Hybrid Neural Network for Text Matching,Yx Wx;Wxx Wx;Mixx Zhxx and Zhoxxxx L,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Long text brings a big challenge to neural network based text matching
approaches due to their complicated structures. To tackle the challenge, we
propose a knowledge enhanced hybrid neural network (KEHNN) that leverages prior
knowledge to identify useful information and filter out noise in long text and
performs matching from multiple perspectives. The model fuses prior knowledge
(e.g., tags and topics) into word representations by knowledge gates and
establishes three matching channels with words, sequential structures of text
given by Gated Recurrent Units (GRUs), and knowledge enhanced representations.
The three channels are processed by a convolutional neural network to generate
high level features for matching, and the features are synthesized as a
matching score by a multilayer perceptron.  Evaluation results from extensive
experiments on public data sets of question answering and conversation show
that KEHNN can significantly outperform state-of-the-art matching models and
particularly improve matching accuracy on pairs with long text.",6 Feb 2017 02:20:48 GMT,Empirical/Data-Driven,Semantics,distributional similarity,Yx,Wx,xxxxxxx126.com,Beihang,No,Wxx,Wx,xxxxxxxxxrosoft.com,Microsoft Research,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Zhoxxxx,Lx,xxxxxxxxa.edu.cn,Beihang University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yx,Wx,Beihang,,,,,,xxxxxxx126.com,,,,,China,"I am a fourth year Ph.D. student of Beihang University under the supervision of Prof.Ming Zhou and Prof. Zhoujun Li. Until now, I have published over 10 top conference papers, which study on the response generation and matching models for chatbots. I am a subreviewer of WSDM, SIGIR and AAAI.",Yx Wx;Wxx Wx;Mixx Zhxx;Zhoxxxx Lx,xxxxxxx126.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxaa.edu.cn,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
49,49X-G6J7F4D6C9,Chunk-based Decoder for Neural Machine Translation,Shoxxxxxx Ishxxxxxxx;Jinxxxx Yxx;Shxxxx Lxx;Mx Lx;Mixx Zhxx;Naxxx Yosxxxxxx;Maxxxx Kitsxxxxxxx and Wexxxx Jx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Tuesday,,Undecided (Machine Translation),"Chunks (or phrases) once played a pivotal role in machine translation. By using
a chunk rather than a word as the basic translation unit, local (intra-chunk)
and global (inter-chunk) word orders and dependencies can be easily modeled.
The chunk structure, despite its importance, has not been considered in the
decoders used for neural machine translation (NMT). In this paper, we propose
chunk-based decoders for (NMT), each of which consists of a chunk-level decoder
and a word-level decoder. The chunk-level decoder models global dependencies
while the word-level decoder decides the local word order in a chunk. To output
a target sentence, the chunk-level decoder generates a chunk representation
containing global information, which the word-level decoder then uses as a
basis to predict the words inside the chunk. Experimental results show that our
proposed decoders can significantly improve translation performance in a WAT
'16 English-to-Japanese translation task.",23 Apr 2017 11:49:37 GMT,Empirical/Data-Driven,Machine translation,,Shoxxxxxx,Ishxxxxxxx,xxxxxxxxxxxxxxxxis.u-tokyo.ac.jp,The University of Tokyo,No,Jinxxxx,Yxx,xxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong University,No,Shxxxx,Lxx,xxxxxxxxxxcrosoft.com,Microsoft Research Asia,No,Mx,Lx,xxxxxxxxxosoft.com,,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,Naxxx,Yosxxxxxx,xxxxxxxxxxxxx.u-tokyo.ac.jp,"Institute of Industrial Science, the University of Tokyo",No,Maxxxx,Kitsxxxxxxx,xxxxxxxxxxxxxxs.u-tokyo.ac.jp,"National Institute of Informatics, Japan",No,Wexxxx,Jxx,xxxxxxxxxxsjtu.edu.cn,Shanghai Jiao Tong University,No,,,,,,,,,,,,Shoxxxxxx,Ishxxxxxxx,The University of Tokyo,,,,,,xxxxxxxxxxxxxxxxis.u-tokyo.ac.jp,,,,,,,Shoxxxxxx Ishxxxxxxx;Jinxxxx Yxx;Shxxxx Lxx;Mx Lx;Mixx Zhxx;Naxxx Yosxxxxxx;Maxxxx Kitsxxxxxxx;Wexxxx Jxx,xxxxxxxxxxxxxxxxis.u-tokyo.ac.jp;xxxxxxxxxxjtu.edu.cn;xxxxxxxxxxxcrosoft.com;xxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxxxx.u-tokyo.ac.jp;xxxxxxxxxxxxxxxs.u-tokyo.ac.jp;xxxxxxxxxxxsjtu.edu.cn,Chunk-based Decoder for Neural Machine Translation,Chunk-based Decoder for Neural Machine Translation,12,Shonosuke Ishiwatari,,The University of Tokyo,,,Only include my submission if it is accepted.,No,None,None
50,50X-F8G2F9F2F3,Coarse-To-Fine Learning for Neural Machine Translation,Zhxxxx Zhxxx;Shxxxx Lxx;Mx Lx;Mixx Zhxx and Enxxxx Cxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"In this paper, we address the problem of learning better word representations
for neural machine translation (NMT). We propose a novel approach to NMT model
training based on coarse-to-fine learning paradigm, which is able to infer
better NMT model parameters for a wide range of less-frequent words in
vocabulary. To this end, our proposed method first groups source and target
words into a set of hierarchical clusters, then a sequence of NMT models are
learned based on it with growing cluster granularity. Each subsequent model
inherits model parameters from its previous one and refines them with
finer-grained word-cluster mapping. Experimental results on public data sets
demonstrate that our proposed method significantly outperforms baseline
attention-based NMT model on Chinese-English and English-French translation
tasks.",7 Feb 2017 08:33:01 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation,Zhxxxx,Zhxxx,xxxxxxxxxgmail.com,University of Science and Technology of China,No,Shxxxx,Lxx,xxxxxxxxxxcrosoft.com,"Microsoft Research Asia, Beijing, China",No,Mx,Lx,xxxxxxxxxosoft.com,,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,Enxxxx,Chxx,xxxxxxxxxtc.edu.cn,University of Science and Technology of China,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhxxxx,Zhxxx,University of Science and Technology of China,,,8618xxxxxxxxx,,,xxxxxxxxxgmail.com,,Beijing,Beijing,,China,,Zhxxxx Zhxxx;Shxxxx Lxx;Mx Lx;Mixx Zhxx;Enxxxx Chxx and Tecxxxxxxx ox,xxxxxxxxxgmail.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxstc.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
51,51X-J9C7F5P8H6,An LSTM-based Approach for User Attribute Extraction,Phxxxx Le-xxxx;Haxx Haxx;Thi-Mxxxxxxxxx Ngxxxx;Taxxxx Obxxx;Truxxxxxx Tx;Yosxxxxxx Nisxxxxxx;Hidxxxxx Shixxxx and Takxxxx Egxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present an efficient approach for extracting diverse user attributes from
free text. The approach combines statistical methods using Long Short-Term
Memory Networks (LSTMs) with rule-based methods to generate and predict
attribute types and their corresponding values from conversational texts. We
also present a novel search algorithm for finding values conforming to the
predicted attributes. Our hybrid model is capable of extracting many different
personal attributes with high accuracy. Experimental results on two data sets
of English and Japanese conversational texts show the efficiency of our
approach. We are able to achieve an $F_1$ score of 84.26% for English, and of
66.29% for Japanese. The paper concludes with a detailed discussion of future
directions for handling hundreds of personal attributes and their
correspondingly richer potential values.",7 Feb 2017 11:21:12 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",named entity recognition;  relation discovery;  semantic relations;  mention detection;  relation/event extraction,Phxxxx,Le-xxxx,xxxxxxxxxgmail.com,"Vietnam National University, Hanoi",No,Haxx,Haxx,xxxxxxxxb@alt.ai,Alt Inc.,No,Thi-Mxxxxxxxxx,Ngxxxx,xxxxxxxxxvnu.edu.vn,"Vietnam National University, Hanoi",No,Taxxxx,Obxxx,xxxxxxxxxata@alt.ai,Alt Inc.,No,Trucxxxxxxxx,Ngxxxx,xxxxxxxxxxxuyen@alt.ai,Alt Inc.,No,Yosxxxxxx,Nisxxxxxx,xxxxxxxxxxxxxhimura@alt.ai,Alt Inc.,No,Hidxxxxx,Shixxxx,xxxxxxxxxxxxeis.ynu.ac.jp,Yokohama National University,No,Takxxxx,Egxxxx,xxxxxxxxxxuchi@alt.ai,Alt Inc.,No,,,,,,,,,,,,Phxxxx,Le-xxxx,"Vietnam National University, Hanoi",,,,,,xxxxxxxxxgmail.com,,Hanoi,,,Viet Nam,http://mim.hus.vnu.edu.vn/mim/phuonglh,Phxxxx Le-xxxx;Haxx Haxx;Thi-Mxxxxxxxxx Ngxxxx;Taxxxx Obxxx;Truxxxxxx Tx;Yosxxxxxx Nisxxxxxx;Hidxxxxx Shixxxx;Takxxxx Egxxxx,xxxxxxxxxgmail.com;xxxxxxxxrb@alt.ai;xxxxxxxxxxvnu.edu.vn;xxxxxxxxxxata@alt.ai;xxxxxxxxxxxguyen@alt.ai;xxxxxxxxxxxxxshimura@alt.ai;xxxxxxxxxxxxxeis.ynu.ac.jp;xxxxxxxxxxxuchi@alt.ai,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
52,52X-J3B3B3B9G5,Multi-view Fusion Neural Network for Answer Selection,Lxx Sxx;Suxxxx Lx;Baxxxx Chxxx and Zhixxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In community question  answering, it is required to choose the most appropriate
answer to a given question, which is  important in many NLP applications. 
    In previous neural network-based methods, several different aspects of
information are taken into consideration by calculating an ``attention''
signal. However, different kinds of attentions are always simply added
together, which will cause severe information loss.
    %However, we still find that sometimes the model made a bad choice since it
is not very clear about what the question is asking about. 
    In this paper, we propose a Multi-View Fusion Neural Network, where each
attention component generates a ``view'' of the QA pair. Then  a fusion RNN 
integrates these views to form a more holistic view of the QA pair. In this
fusion RNN method, a filter gate  collects  important information of  input and
directly adds it to the output, which is an improvement of residual networks.
We conduct experiments on two datasets: WikiQA and SemEval-2016 CQA.
Experimental results demonstrate that our proposed model  outperforms the
state-of-the-art method.",4 Feb 2017 07:43:13 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",answer extraction;  open-domain question answering;  question answering in restricted domains,Lxx,Sxx,xxxxxxxxx@sina.com,Peking University,No,Suxxxx,Lx,xxxxxxxxxpku.edu.cn,Peking University,No,Baxxxx,Chxxx,xxxxxxxu.edu.cn,Peking University,No,Zhixxxx,Sxx,xxxxxxx.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lxx,Sxx,Peking University,,,1352xxxxxxx,,,xxxxxxxxx@sina.com,,,,,China,,Lxx Sxx;Suxxxx Lx;Baxxxx Chxxx;Zhixxxx Sxx,xxxxxxxxx@sina.com;xxxxxxxxxxpku.edu.cn;xxxxxxxxu.edu.cn;xxxxxxxu.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
53,53X-E8P6P7H6B8,Learning to Ask: Generating Question-Answer Pairs from Passages based on Frequently-Asked Question Patterns,Nxx Duxx;Duxx Taxx;Pexx Chxx;Juxxxx Bxx;Zhxx Yxx and Mixx Zxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"In this paper, we present an approach that aims to generate question-answer (or
QA) pairs from passages based on frequently-asked question patterns. First,
questions from a community QA (or CQA) website are grouped into question
clusters, each of which consists of questions that are related to each other;
Then, for each question cluster, a question topic is selected based on question
consensus, and removed from each question to form a question pattern; Next, a
question pattern prediction model is trained based on answer passage-question
pattern pairs as training data, and used to predict the most possible question
patterns for an input passage. After that, a set of question topics are
selected from the input passage and filled into predicted question patterns to
form a set of complete question candidates. Last, all generated questions are
ranked by features and the highest ranked question is used to form a QA pair,
where the answer is the input passage. For evaluation, a new dataset, QAGen, is
built for the QA pair generation task and used to measure the question
generation quality. We also integrate our approach into an end-to-end QA task,
i.e., the answer sentence selection task. By using generated questions as an
extra signal, significant improvement can be achieved.",3 Feb 2017 02:22:56 GMT,Empirical/Data-Driven,Generation,language generation,Nxx,Duxx,xxxxxxxxxxcrosoft.com,Microsoft Research Asia,No,Duxx,Taxx,xxxxxxxxxxrosoft.com,Microsoft Research Asia,No,Pexx,Chxx,xxxxxxxxxrosoft.com,Microsoft Xiaoice,No,Juxxxx,Bxx,xxxxxxxxxxx1@gmail.com,Harbin Institute of Technology,No,Zhxx,Yxx,xxxxxxxxxuaa.edu.cn,Beihang university,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,,,,,,,,,,,,,,,,,,,,,,Nxx,Duxx,Microsoft Research Asia,,,,,,xxxxxxxxxxcrosoft.com,,Beijing,Beijing,,China,,Nxx Duxx;Duxx Taxx;Pexx Chxx;Juxxxx Bxx;Zhxx Yxx;Mixx Zhxx,xxxxxxxxxxcrosoft.com;xxxxxxxxxxcrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxx01@gmail.com;xxxxxxxxxxuaa.edu.cn;xxxxxxxxxxxicrosoft.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
55,55X-G3E3H4B8J3,From Text Clustering to Classification: An End-to-End Neural Network Based Unified Framework,Jxx Zhxx;Jinxxxx Zhxxx and Wxx X,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"To obtain the text categorization information is one of the important needs and
has a wide range of influence on multiple natural language understanding tasks.
Both supervised and unsupervised methods have been developed to deal with
labeled and much more unlabeled text respectively. We propose a pure neural
framework for clustering the text in an end-to-end form. It jointly learns the
text representation and the clustering model. Our model works under a simple
and common assumption that sequences within a short distance are statistically
to have the same category. We have our method evaluated on two widely used text
categorization benchmarks: IMDB movie review and $20$-Newsgroup. Despite its
simplicity, experiments show the model outperforms previous clustering methods
by a large margin. Moreover, this clustering framework can be easily extended
to utilize the labeled data for classification tasks. Through exploiting both
unlabeled and labeled data, we achieve the state-of-the-art performance on IMDB
movie review benchmark.",7 Feb 2017 06:02:27 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",text classification;  document clustering,Jxx,Zhxx,xxxxxxxxx@baidu.com,Baidu Research,No,Jinxxxx,Zhxxx,xxxxxxxxxxxx01@baidu.com,Baidu Research - Institute of Deep Learning,No,Wxx,Xx,xxxxxxxxaidu.com,Baidu Research - Institute of Deep Learning,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jxx,Zhxx,Tencent Inc.,,,+86-1xxxxxxxxxx,,,xxxxxxxxxgmail.com,,,,,China,,Jxx Zhxx;Jinxxxx Zhxxx;Wxx Xx,xxxxxxxxx@baidu.com;xxxxxxxxxxxxo01@baidu.com;xxxxxxxxbaidu.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
56,56X-D8B3H3P3F7,Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics,Zxx Zhxx;Txx Lxx;Shxx Lx;Boxxxx Lx and Xiaxxxxx D,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"The existing word representation methods mostly limit their information source
to word co-occurrence statistics. In this paper, we introduce ngrams into four
representation methods: SGNS, GloVe, PPMI matrix and its SVD factorization.
Comprehensive experiments are conducted on word analogy and similarity tasks.
The results show that improved word representations are learned from ngram
co-occurrence statistics. We also demonstrate that the trained ngram
representations are useful in many aspects such as finding antonyms and
collocations. Besides, a novel approach of building co-occurrence matrix is
proposed to alleviate the hardware burden brought by ngrams.",7 Feb 2017 11:56:32 GMT,Empirical/Data-Driven,Semantics,language modeling for spoken language;  syntax;  distributional similarity;  relation discovery;  NLP on Wikipedia and other collaboratively constructed resources,Zxx,Zhxx,xxxxxxxxxx@ruc.edu.cn,Renmin University of China,No,Txx,Lxx,xxxxxxxc.edu.cn,Renmin University of China,No,Shxx,Lx,xxxxxxxxxxbnu.edu.cn,Beijing Normal University,No,Boxxxx,Lx,xxxxxxxxxruc.edu.cn,Renmin University of China,No,Xiaxxxxx,Dx,xxxxxxxxuc.edu.cn,Renmin University of China,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Zxx,Zhxx,Renmin University of China,,,,,,xxxxxxxxxx@ruc.edu.cn,,,,,China,,Zxx Zhxx;Txx Lxx;Shxx Lx;Boxxxx Lx;Xiaxxxxx Dx,xxxxxxxxxx@ruc.edu.cn;xxxxxxxxc.edu.cn;xxxxxxxxxx.bnu.edu.cn;xxxxxxxxxxruc.edu.cn;xxxxxxxxxuc.edu.cn,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
57,57X-G2H6E5A3E2,Predictive Linguistic Features of Schizophrenia,Efxxx Sarxxxxx;Glxx Coppxxxxxxx;Micxxxx Comxxxx;Luxx Pauxxxxx and Moxx Dxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"Schizophrenia is one of the most disabling and difficult to treat of all human
medical/health conditions, ranking in the top ten causes of disability
worldwide. Schizophrenia has been a puzzle in part due to difficulty in
identifying its basic, fundamental components. Several studies have shown that
some manifestations of schizophrenia (e.g., the negative symptoms that include
blunting of speech prosody, as well as the disorganization symptoms that lead
to disordered language) can be understood from the perspective of linguistics.
However, schizophrenia research has not kept pace with technologies in
computational linguistics, especially in semantics and pragmatics. As such, we
examine the writings of schizophrenia patients analyzing their syntax,
semantics and pragmatics. In addition, we analyze tweets of (self proclaimed)
schizophrenia patients who publicly discuss their diagnoses. For writing
samples dataset, syntactic features are found to be the most successful in
classification whereas for the less structured Twitter dataset, a combination
of features performed the best.",6 Feb 2017 22:55:57 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,sentiment analysis;  part-of-speech tagging;  NLP applications;  distributional similarity;  parsing;  text classification;  biomedical text mining;  pragmatics;  semantic role labelling,Efxxx,Sarixxxxxxxxx,xxxxxxgwu.edu,George Washington University,No,Glxx,Coppxxxxxxx,xxxxxxxtfy.com,Qntfy,No,Micxxxx,Comxxxx,xxxxxxxxxxxrthwell.edu,Columbia University,No,Luxx,Pauxxxxx,xxxxxxxxxxxxx.columbia.edu,Columbia University,No,Moxx,Dixx,xxxxxxxgwu.edu,GWU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Efxxx,Sarixxxxxxxxx,George Washington University,,,,,,xxxxxxgwu.edu,,Washington,DC,,United States,,Efxxx Sarxxxxx;Glxx Coppxxxxxxx;Micxxxx Comxxxx;Luxx Pauxxxxx;Moxx Dixx,xxxxxxgwu.edu;xxxxxxxntfy.com;xxxxxxxxxxxorthwell.edu;xxxxxxxxxxxxxi.columbia.edu;xxxxxxx@gwu.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
59,59X-D5A2J2F8J6,Exploring Easy-First Neural Dependency Parsing: Searching and Learning,Zhixxxx Zhxxx and Hxx Zhxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"In this work, we explore better searching and learning strategies for
easy-first dependency parsing with neural model, which differs from most recent
works that focus on model improvement. For searching, we apply dynamic
programming motivated state merging to beam search for enlarging reachable
space. For learning, we view the loss functions directly from the perspective
of gradients and propose a novel reordering training criterion, with structured
margin loss enhancement and ""correcting-in-time"" updating strategies.
Evaluations on standard English and Chinese Treebanks show the effectiveness of
the proposed schemes.",4 Feb 2017 11:32:28 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",structured input/output;  syntax;  parsing,Zhixxxx,Zhxxx,xxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong University,No,Hxx,Zhxx,xxxxxxxxxxxsjtu.edu.cn,Shanghai Jiao Tong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhixxxx,Zhxxx,Shanghai Jiao Tong University,,,8613xxxxxxxxx,,,xxxxxxxxxx7@gmail.com,,Shanghai,Shanghai,,China,I'm a graduate student from Shanghai Jiao Tong University and major in computer science.,Zhixxxx Zhxxx;Hxx Zhxx,xxxxxxxxxjtu.edu.cn;xxxxxxxxxxx.sjtu.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
60,60X-E7P6F9P2F3,Adaptive Spelling Error Correction Models for Learner English,Rxx Naxxxx;Hixxxx Takxxxxx and Grxxxx Nexxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),"Spelling errors are a characteristic of learner English and degrade the
performances of natural language processing systems targeting English learners.
This paper describes a method specially designed for automatically correcting
spelling errors in learner English that reduces the effects from noise (e.g.,
grammatical and spelling errors) by adaptively creating spelling error
correction models from raw learner corpora. An evaluation shows that the
proposed method outperforms previous edit-distance-based and
language-model-based methods. We also
report results of an investigation into what types of spelling errors English
learners tend to make, using the spelling error models created by the proposed
method as a tool for our analysis. We have now released spelling error models
for eight mother tongues to the public, which will facilitate the development
of
spellcheckers and further investigation of learner English spelling.",7 Feb 2017 05:28:00 GMT,Applications/Tools,Other,educational applications,Rxx,Naxxxx,xxxxxxxxxxxxyogo-u.ac.jp,Konan University,No,Hixxxx,Takxxxxx,xxxxxxxxxxxxtitech.ac.jp,Tokyo Institute of Technology,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rxx,Naxxxx,Konan University,,,,,,xxxxxxxxxxxxyogo-u.ac.jp,,,,,Japan,,Rxx Naxxxx;Hixxxx Takxxxxx;Grxxxx Nexxxx,xxxxxxxxxxxxyogo-u.ac.jp;xxxxxxxxxxxx.titech.ac.jp;xxxxxxxxxcs.cmu.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
61,61X-F2G4E3C7A8,Improving Event Extraction through Cross-media Integration,Tonxxxx Zhxxx;Spexxxx Whixxxxxx;Honxxxx Lx;Joxxxx Elxxx;Lixx Huxxx;Hexx Jx and Shixxxx Chxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"Methods that comprehensively adopt vision, language and other cross-media
techniques are becoming more and more prevalent in research and application
areas. In this paper, we propose a new approach to improve the performance of
textual event extraction by integrating external visual information into
extraction models. We utilize a deep visual pattern mining framework to
generate a visual repository which consists of visual clusters containing
common localized objects. Visual features are then dynamically extracted from
this visual repository, integrated with text-based features, and applied in
event extraction pipelines. Experiments demonstrate that this new method can
provide significantly better event extraction results: absolute 7.1% F-score
gain on trigger labeling and 8.5% F-score gain on argument labeling.",7 Feb 2017 12:27:14 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",information extraction;  multimodal representations and processing;  relation/event extraction,Tonxxxx,Zhxxx,xxxxxxxx@rpi.edu,Rensselaer Polytechnic Institute,No,Spexxxx,Whixxxxxx,xxxxxxx@rpi.edu,Rensselaer Polytechnic Institute,No,Honxxxx,Lx,xxxxxxxxxxxcolumbia.edu,Columbia University,No,Joxxxx,Elxxx,xxxxxxxxxxlumbia.edu,Columbia University,No,Lixx,Huxxx,xxxxxxxxxx@gmail.com,Rensselaer Polytechnic Institute,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,Shixxxx,Chxxx,xxxxxxxxxumbia.edu,Columbia University,No,,,,,,,,,,,,,,,,,Tonxxxx,Zhxxx,Rensselaer Polytechnic Institute,,,718xxxxxxx,,,xxxxxxxx@rpi.edu,,,,,United States,,Tonxxxx Zhxxx;Spexxxx Whixxxxxx;Honxxxx Lx;Joxxxx Elxxx;Lixx Huxxx;Hexx Jx;Shixxxx Chxxx,xxxxxxxx@rpi.edu;xxxxxxxx@rpi.edu;xxxxxxxxxxxxcolumbia.edu;xxxxxxxxxxolumbia.edu;xxxxxxxxxxu@gmail.com;xxxxxxpi.edu;xxxxxxxxxlumbia.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
63,63X-G7C6A3A3C4,Hierarchical Dirichlet Processes with Social Influence,Yexxx Goxx;Qx Zhxxx and Xuaxxxxx Huxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"The hierarchical Dirichlet process model has been successfully used for
extracting the topical or semantic content of documents and other kinds of
sparse count data. Along with the growth of social media, there have been
simultaneous increases in the amounts of textual information and social
structural information. To incorporate the information contained in these
structures, in this paper, we propose a novel non-parametric model, social
hierarchical Dirichlet process (sHDP), to solve the problem. We assume that the
topic distributions of documents are similar to each other if their authors
have relations in social networks. The proposed method is extended from the
hierarchical Dirichlet process model. We evaluate the utility of our method by
applying it to three data sets: papers from NIPS proceedings, a subset of
articles from Cora, and microblogs with social network. Experimental results
demonstrate that the proposed method can achieve better performance than
state-of-the-art methods in all three data sets.",2 Feb 2017 12:49:36 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",NLP in social networking media;  social network;  document clustering,Yexxx,Goxx,xxxxxxxx@163.com,Fudan University,No,Qx,Zhxxx,xxxxxxxn.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qx,Zhxxx,Fudan University,,,,,,xxxxxxxn.edu.cn,,,,,China,,Yexxx Goxx;Qx Zhxxx;Xuaxxxxx Huxxx,xxxxxxxx@163.com;xxxxxxxxn.edu.cn;xxxxxxxxxxudan.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
64,64X-P2J3H7B4P3,A Finite State and Rule-based Method for Word Prosody Computation in Hindi,Somxxxx Rxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Word prosody derivation from its underlying phonemic form in Hindi has an
intriguing sub-problem, which is well known in Hindi phonology as schwa
deletion. This paper proposes two finite state machines—one for the
syllabification and another for the syllable labeling. In addition to that, it
proposes a set of nonlinear phonological rules for foot formation and
prediction of stressed syllables in a word. The nonlinear phonological rules
are based on metrical phonology with the provision of recursive foot structure.
The proposed finite state machines and the nonlinear phonological rules
encompass solution to the sub-problem. A software module is implemented in
Python. The testing of the software for schwa deletion,syllabification and
stress prediction yield an accuracy of more than 99% on the lexicons of size
5817 and 22847 words.",1 Feb 2017 05:35:19 GMT,Applications/Tools,"Phonology, morphology, and word segmentation",phonology;  rule-based/symbolic learning methods;  finite-state methods,Somxxxx,Rxx,xxxxxxxxxxx6@gmail.com,"Jawaharlal Nehru University, Delhi",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Somxxxx,Rxx,Melvault Software Solutions Pvt Ltd,,,,,,xxxxxxxxxxxxmelvault.com,,,,,India,,Somxxxx Rxx,xxxxxxxxxxx6@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
65,65X-P4P6B3B2F6,Make Deep Neural Networks Robust to Noise by Feeding Linguistic Knowledge: A Case Study on Name Tagging for Low-resource Languages,Bolxxxx Zhxxx;Dx Lx;Xiaxxxx Pxx;Yixx Lxx;Hexx Jx and Kexxx Knxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Current name tagging approaches are inadequate for most low-resource languages
due to the lack of annotated data and actionable linguistic knowledge. All
supervised learning methods (including deep neural networks (DNN)) are
sensitive to noise and thus they are not quite portable without massive clean
annotations. We found that the F-scores of DNN based name taggers drop rapidly
(20%-30%) when we replace clean manual annotations with noisy annotations in
the training data. One possible solution is to incorporate many non-traditional
language universal resources which are readily available but rarely explored,
including the World Atlas of Linguistic Structure, CIA names, and survival
guides. We acquire and encode various types of linguistic resources into a DNN
name tagger. Experiments on three low-resource languages show that feeding
linguistic knowledge can make DNN significantly more robust to noise (8%-22%
absolute F-score gains). We will make all cleaned resources and converted
linguistic features publicly available.",6 Feb 2017 23:34:29 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  named entity recognition;  multilingual applications;  multilingual resources;  adaptation to noisy data,Bolxxxx,Zhxxx,xxxxxxxxxxxx85@gmail.com,Rensselaer Polytechnic Institue,No,Dx,Lx,xxxxxxpi.edu,Rensselaer Polytechnic Institute,No,Xiaxxxx,Pxx,xxxxxxrpi.edu,Rensselaer Polytechnic Institute,No,Yixx,Lxx,xxxxxxrpi.edu,Rensselaer Polytechnic Institute,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,Kexxx,Knxxxx,xxxxxxxisi.edu,USC/ISI,No,,,,,,,,,,,,,,,,,,,,,,Bolxxxx,Zhxxx,Rensselaer Polytechnic Institue,,,518xxxxxxx,,,xxxxxxxxxxxx85@gmail.com,,,,,United States,,Bolxxxx Zhxxx;Dx Lx;Xiaxxxx Pxx;Yixx Lxx;Hexx Jx;Kexxx Knxxxx,xxxxxxxxxxxx85@gmail.com;xxxxxxrpi.edu;xxxxxxxrpi.edu;xxxxxxxrpi.edu;xxxxxxpi.edu;xxxxxxx@isi.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
66,66X-C5J3D3E6D3,Generating Memorable Mnemonic Encodings of Numbers,Vinxxxx Fioxxxxxxx;Mexxx Shxx and Juxxx Mexxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"The major system is a mnemonic system that can be used to memorize sequences of
numbers. In this work, we present a method to automatically generate sentences
that encode a given number.
We propose several encoding models and compare the most promising ones in a
password memorability study. The results of the study show that a model
combining part-of-speech sentence templates with an n-gram language model
produces the most memorable password representations.",29 Jan 2017 02:16:09 GMT,Applications/Tools,Generation,NLP applications,Vinxxxx,Fioxxxxxxx,xxxxxxxxxni@hmc.edu,Harvey Mudd College,No,Mexxx,Shxx,xxxxxxhmc.edu,Harvey Mudd College,No,Juxxx,Mexxxx,xxxxxxxx.hmc.edu,Harvey Mudd College,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Juxxx,Mexxxx,Harvey Mudd College,,,,,,xxxxxxxx.hmc.edu,,,CA,,United States,,Vinxxxx Fioxxxxxxx;Mexxx Shxx;Juxxx Mexxxx,xxxxxxxxxni@hmc.edu;xxxxxxxhmc.edu;xxxxxxxxs.hmc.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
67,67X-G8P7A6E7H4,Constructing Semantic Hierarchies via Fusion Learning Architecture,Tiaxxxx Jixxx;Bixx Qxx and Tixx Lx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Semantic hierarchies construction means to build structure of concepts linked
by hypernym-hyponym (``is-a'') relations. A major challenge for this task is
the automatic discovery of hypernym-hyponym (``is-a'') relations. We propose a
fusion learning architecture based on word embeddings for constructing semantic
hierarchies, composed of discriminative generative fusion architecture and a
very simple lexical structure rule for assisting, getting an F1-score of 74.20%
with 91.60% precision-value, outperforming the state-of-the-art methods on a
manually labeled test dataset. Subsequently, combining our method with
manually-built hierarchies can further improve F1-score to 82.01%. Besides, the
fusion learning architecture is language-independent.",6 Feb 2017 08:44:50 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relation discovery;  semantic relations;  relation/event extraction;  ontological semantics,Tiaxxxx,Jixxx,xxxxxxxxxx.hit.edu.cn,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology",No,Bixx,Qxx,xxxxxxxxxit.edu.cn,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology",No,Tixx,Lxx,xxxxxxxxxit.edu.cn,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tiaxxxx,Jixxx,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology",,,,,,xxxxxxxxxx.hit.edu.cn,,,,,China,,Tiaxxxx Jixxx;Bixx Qxx;Tixx Lxx and Infoxxxxxxx Retxxxxxx,xxxxxxxxxx.hit.edu.cn;xxxxxxxxxhit.edu.cn;xxxxxxxxxhit.edu.cn,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
68,68X-B6E6D3A3F5,A New Formula for Vietnamese Text Readability Assessment,An-xxxx Luxxx;Ngxx Txx and Dixx Dxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Text readability has an important role in text drafting and document selecting.
Researches on the readability of the text have been made long ago for English
and some common languages. There are few researches in Vietnamese text
readability and most of them are performed from more than two decades ago on
very small corpora. In this paper, we build a new and larger corpus and use it
to create a newer formula to predict the difficulty of Vietnamese text. The
experimental results show that the new formula can predict the readability of
Vietnamese documents with over 80% accuracy.",29 Jan 2017 13:15:36 GMT,Resources/Evaluation,"Document analysis including text categorization, topic models, and retrieval",corpus development;  educational applications;  text classification,An-xxxx,Luxxx,xxxxxxxxxxg@gmail.com,Ho Chi Minh City University of Science,No,Ngoxxxxx,Lx,xxxxxxxxxx@gmail.com,Université du Québec à Montréal,No,Dixx,Dixx,xxxxxxxxxxxcmuns.edu.vn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,An-xxxx,Luxxx,Ho Chi Minh City University of Science,,,8493xxxxxxx,,,xxxxxxxxxxg@gmail.com,,Ho Chi Minh,,,Viet Nam,,An-xxxx Luxxx;Ngxx Txx;Dixx Dixx,xxxxxxxxxxg@gmail.com;xxxxxxxxxxn@gmail.com;xxxxxxxxxxxxcmuns.edu.vn,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
71,71X-C3B8F3H4J4,Cross-lingual Name Tagging and Linking for 282 Languages,Xiaxxxx Pxx;Bolxxxx Zhxxx;Jonxxxxx Mxx;Joxx Notxxxx;Kexxx Knxxxx and Hexx J,Multilingual,Omxx Abxxx;Moxx Dixx,Accept - Poster Tuesday,,Undecided (Multilingual),"The ambitious goal of this work is to develop a cross-lingual name tagging and
linking framework for 282 languages that exist in Wikipedia. Given a document
in any of these languages, our framework is able to identify name mentions,
assign a coarse-grained or fine-grained type to each mention, and link it to an
English Knowledge Base (KB) if it is linkable. We achieve this goal by
performing a series of new KB mining methods: generating ``silver-standard''
annotations by transferring annotations from English to other languages through
cross-lingual links and KB properties, refining annotations through
self-training and topic selection, deriving language-specific morphology
features from anchor links, and mining word translation pairs from
cross-lingual links. Both name tagging and linking results for 282 languages
are promising on Wikipedia data and on-Wikipedia data.",23 Apr 2017 08:25:43 GMT,Resources/Evaluation,Multilinguality,,Xiaxxxx,Pxx,xxxxxxrpi.edu,Rensselaer Polytechnic Institute,No,Bolxxxx,Zhxxx,xxxxxxxxxxxx85@gmail.com,Rensselaer Polytechnic Institue,No,Jonxxxxx,Mxx,xxxxxxxxmail.com,USC Information Sciences Institute,No,Joxx,Notxxxx,xxxxxxxxxxxn@gmail.com,University of Melbourne,No,Kexxx,Knxxxx,xxxxxxxisi.edu,USC/ISI,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Pxx,Rensselaer Polytechnic Institute,,,,,,xxxxxxrpi.edu,,,,,United States,,Xiaxxxx Pxx;Bolxxxx Zhxxx;Jonxxxxx Mxx;Joxx Notxxxx;Kexxx Knxxxx;Hexx Jx,xxxxxxrpi.edu;xxxxxxxxxxxxg85@gmail.com;xxxxxxxxgmail.com;xxxxxxxxxxxan@gmail.com;xxxxxxx@isi.edu;xxxxxxpi.edu,Cross-lingual Name Tagging and Linking for 282 Languages,Cross-lingual Name Tagging and Linking for 282 Languages,13,Xiaoman Pan,,"Computer Science Department, Rensselaer Polytechnic Institute
110 8th Street
Troy, NY 12180-3590",on,on,Only include my submission if it is accepted.,No,None,None
72,72X-E6B4A6E5E9,Referring Expression Generation under Uncertainty: Algorithm and Evaluation Framework,Txx Wilxxxxx and Matxxxxx Schxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"For situated agents to effectively engage in natural-language interactions with
humans, they must be able to refer to entities such as people, locations, and
objects. While classic referring expression generation (REG) algorithms like
the Incremental Algorithm (IA) assume perfect, complete, and accessible
knowledge of all referents, this is not always possible. In previous work, we
presented a consultant framework which facilitates reference resolution when
knowledge is uncertain, heterogeneous and distributed. In this work, we show
how this framework can be used to extend the IA to produce DIST-PIA, a
domain-independent algorithm for REG under uncertain, heterogeneous, and
distributed knowledge, and present a novel framework that can be used to
evaluate such REG algorithms without conflating the performance of the
algorithm with the performance of the classifiers it employs.",29 Jan 2017 22:25:17 GMT,Empirical/Data-Driven,Generation,language generation;  experimental evaluation/comparison of ML methods,Txx,Wilxxxxx,xxxxxxxxxxs.tufts.edu,Tufts University,No,Matxxxxx,Schxxxx,xxxxxxxxxxxxxutz@tufts.edu,Tufts University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Txx,Wilxxxxx,Tufts University,,,,,,xxxxxxxxxxs.tufts.edu,,,,,United States,,Txx Wilxxxxx;Matxxxxx Schxxxx,xxxxxxxxxxs.tufts.edu;xxxxxxxxxxxxxeutz@tufts.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
73,73X-G6E6C6B7P9,Extracting Bilingual Dictionary from Comparable Corpora using Multilingual Topic Model with Partial Synchronization,yucxxxx chxxx;Toxxxx Naxxxx;Isxxx Okxxx;Yosxxxxx Oixx and Maxxx Idxxxx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"A bilingual lexicon extraction framework that combines a multilingual topic
model and word alignment is effective, but it does not work well if the
assigned result of the word topic is in-correct. To reduce the incorrect topic
assignment, our proposed method uses bilingual word pairs as a constraint
condition that can derive the probability distribution of the word topics. Our
proposed method can be applied to the active learning method. Therefore, the
accuracy of bilingual lexicon extraction can be improved by using active
learning without additional resources or pre-processing. The results show that
after 23 iterations of active learning, the precision of the top 1000 word
pairs increased by 9.1%.",7 Feb 2017 11:48:01 GMT,Empirical/Data-Driven,Multilinguality,MT applications;  NLP applications;  cross-lingual approaches;  lexical semantics;  lexicon development;  multilingual resources;  ontology development;  ontological semantics,yucxxxx,chxxx,xxxxxxxxxxxxxxjp.fujitsu.com,FUJITSU LABORATORIES LTD.,No,Toxxxx,Naxxxx,xxxxxxxxxxxxxxjp.fujitsu.com,Fujitsu Laboratories Limited.,No,Isxxx,Okxxx,xxxxxxxxxxxxxxjp.fujitsu.com,Fujitsu Limited.,No,Yosxxxxx,Oixx,xxxxxxxxxxxxxxjp.fujitsu.com,Fujitsu Limited.,No,Maxxx,Idexxxx,xxxxxxxxxxxxxxjp.fujitsu.com,Fujitsu Limited.,No,,,,,,,,,,,,,,,,,,,,,,,,,,,yucxxxx,chxxx,FUJITSU LABORATORIES LTD.,,,,,,xxxxxxxxxxxxxxjp.fujitsu.com,,,,,Japan,,yucxxxx chxxx;Toxxxx Naxxxx;Isxxx Okxxx;Yosxxxxx Oixx;Maxxx Idexxxx,xxxxxxxxxxxxxxjp.fujitsu.com;xxxxxxxxxxxxxx@jp.fujitsu.com;xxxxxxxxxxxxxx@jp.fujitsu.com;xxxxxxxxxxxxxx@jp.fujitsu.com;xxxxxxxxxxxxxx@jp.fujitsu.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
74,74X-H5D8A9F7H8,Deep Learning in Semantic Kernel Spaces,Daxxxx Crxxx;Sixxxx Fixxxx;Giuxxxxx Castxxxxxxx and Robxxxx Baxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Monday,,Undecided (Machine Learning),"Kernel methods enable the direct usage of structured representations of textual
data during language learning and inference tasks. Expressive kernels, such as
Tree Kernels, achieve excellent performance in NLP. 
On the other side, deep neural networks have been demonstrated effective in
automatically learning feature representations during training. However, their
input is tensor data, i.e., they can not manage rich structured information.
In this paper, we show that expressive kernels and deep neural networks can be
combined in a common framework in order to (i) explicitly model structured
information and (ii) learn non-linear decision functions. We show that the
input layer of a deep architecture can be pre-trained through the application
of the Nystrom low-rank approximation of kernel spaces. 
The resulting ``kernelized"" neural network achieves state-of-the-art accuracy
in three different tasks.",20 Apr 2017 21:07:19 GMT,Empirical/Data-Driven,Machine learning,,Daxxxx,Crxxx,xxxxxxxxxxxuniroma2.it,"University of Roma, Tor Vergata",No,Sixxxx,Fixxxx,xxxxxxxxxxxne@gmail.com,"University of Roma ""Tor Vergata""",No,Giuxxxxx,Castxxxxxxx,xxxxxxxxxxxxxxxseppe@gmail.com,University of Roma Tor Vergata,No,Robxxxx,Baxxxx,xxxxxxxxxxx.uniroma2.it,"University of Roma, Tor Vergata",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxxx,Crxxx,"University of Roma, Tor Vergata",,,,,,xxxxxxxxxxxuniroma2.it,,,,,Italy,,Daxxxx Crxxx;Sixxxx Fixxxx;Giuxxxxx Castxxxxxxx;Robxxxx Baxxxx,xxxxxxxxxxxuniroma2.it;xxxxxxxxxxxxne@gmail.com;xxxxxxxxxxxxxxxuseppe@gmail.com;xxxxxxxxxxxx.uniroma2.it,Deep Learning in Semantic Kernel Spaces,Deep Learning in Semantic Kernel Spaces,10,Danilo Croce,,"Department of Enterprise Engineering,
University of Roma Tor Vergata,
Via del Politecnico 1, 00133, Rome, Italy",on,,No. Do not include my submission in this dataset.,No,None,None
75,75X-D6D2H4G6J3,Joint Extracting Event Trigger and Arguments Using Dependency Bridge Recurrent Tensor Network,Lxx Sxx;Baxxxx Chxxx;Suxxxx Lx and Zhixxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Event extraction plays an important role in natural language processing (NLP)
applications including question answering, information retrieval, etc. 
In recent works, traditional approaches mainly rely on elaborately designed
lexical and sentence-level features, but these features are too complicated and
not easy to generalize while suffering data sparsity problem. 
Deep neural network based approaches are easy to generalize, but the current
approaches cannot make full use of grammatical relations. Also, they consider 
candidate arguments separately when identifying and classifying arguments.  
In this paper, we propose a novel deep neural network architecture called
Dependency Bridge Recurrent Tensor Network (DBRTN) to extract events from raw
text. In LSTM-RNN architecture, we import dependency relation bridges which
carry grammatically related information  directly to the current word. In
addition, we use  a tensor layer to capture the relationship between candidate
arguments and identify / classify all arguments of an event simultaneously.
The experiment results show that our proposed approach achieves a competitive
result compared to the previous work.",7 Feb 2017 06:13:20 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  text mining,Lxx,Sxx,xxxxxxxxx@sina.com,Peking University,No,Baxxxx,Chxxx,xxxxxxxu.edu.cn,Peking University,No,Suxxxx,Lx,xxxxxxxxxpku.edu.cn,Peking University,No,Zhixxxx,Sxx,xxxxxxx.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lxx,Sxx,Peking University,,,1352xxxxxxx,,,xxxxxxxxx@sina.com,,,,,China,,Lxx Sxx;Baxxxx Chxxx;Suxxxx Lx;Zhixxxx Sxx,xxxxxxxxx@sina.com;xxxxxxxxu.edu.cn;xxxxxxxxxxpku.edu.cn;xxxxxxxu.edu.cn,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
76,76X-F8H3H9E5E9,Parsing for Grammatical Relations via Graph Merging,Wexxxx Sxx;Yaxxxx Dx and Xiaxxxx Wx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"This paper is concerned with building deep grammatical relation (GR) analysis
using data-driven approach. To deal with this problem, we propose graph
merging, a
new perspective, for building flexible dependency graphs: Constructing complex
graphs via constructing simple subgraphs. We discuss two key problems in this 
perspective: (1) how to decompose a complex graph into simple subgraphs, and
(2)
how to combine subgraphs into a coherent complex graph. Experiments demonstrate
the effectiveness of graph merging. Our parser reaches state-of-the-art
performance and is significantly better than two transition-based parsers.",6 Feb 2017 22:32:22 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",parsing,Wexxxx,Sxx,xxxxxx.edu.cn,Peking University,No,Yaxxxx,Dx,xxxxxxxxxpku.edu.cn,Institute of Computer Science and Technology of Peking University,No,Xiaxxxx,Wxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wexxxx,Sxx,Peking University,,,8601xxxxxxxxx,,,xxxxxx.edu.cn,,,,,China,,Wexxxx Sxx;Yaxxxx Dx;Xiaxxxx Wxx,xxxxxx.edu.cn;xxxxxxxxxxpku.edu.cn;xxxxxxxxxxx@pku.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
77,77X-H8D9P8F2A5,Single Image Narrative Generation,Anxxxx Shxx;Yuixxxxx Kixxxx;Kunxxxx Saxxx;Yosxxxxxx Usxxxx and Tatxxxx Haxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"It has been taken for granted that a single sentence of factual description
suffices for single image. Yet, images frequently provide more contents than
what can be described in a sentence, whether it is visual (further details or
sentiments), or non-visual (inference about the image). Incorporating such
elements can make image description more human-like. While visual storytelling
task aims to generate story-like text from the images, it requires a sequence
of images. Likewise, dense captioning generates multiple captions for single
image at region-level, but it is restricted to factual description of each
region and involves very expensive human annotation.  We introduce a novel task
of single image narrative generation, in which we attempt to generate
multiple-sentence description from a single image that consists of both visual
and non-visual elements. We note that visual question answering (VQA) datasets
cover a wider range of topics than caption datasets, and exploit them by
generating multiple questions about the image and collecting answers.
Experimental results demonstrate that our proposed model can generate image
narratives that are richer in contents and more human-like than the baseline
models.",6 Feb 2017 14:28:56 GMT,Applications/Tools,"Vision, robots, and other grounding",multimodal communication;  multimodal representations and processing;  question answering in restricted domains,Anxxxx,Shxx,xxxxxxxxxxxxu-tokyo.ac.jp,The University of Tokyo,No,Yuixxxxx,Kixxxx,xxxxxxxxxxxxu-tokyo.ac.jp,The University of Tokyo,No,Kunxxxx,Saxxx,xxxxxxxxxxxxxu-tokyo.ac.jp,The University of Tokyo,No,Yosxxxxxx,Usxxxx,xxxxxxxxxxxxu-tokyo.ac.jp,The University of Tokyo,No,Tatxxxx,Haxxxx,xxxxxxxxxxxxu-tokyo.ac.jp,The University of Tokyo,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxxxx,Shxx,The University of Tokyo,,,(81)08xxxxxxxxxxx,,,xxxxxxxxxxxxu-tokyo.ac.jp,,,,,Japan,,Anxxxx Shxx;Yuixxxxx Kixxxx;Kunxxxx Saxxx;Yosxxxxxx Usxxxx;Tatxxxx Haxxxx,xxxxxxxxxxxxu-tokyo.ac.jp;xxxxxxxxxxxxxu-tokyo.ac.jp;xxxxxxxxxxxxx.u-tokyo.ac.jp;xxxxxxxxxxxxxu-tokyo.ac.jp;xxxxxxxxxxxxxu-tokyo.ac.jp,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
78,78X-H5H3G9C9P2,Domain Adaptation for Named Entity Recognition in Online Media with Word Embeddings,Vixxx Kulxxxxx;Yaxxxx Mexxxx and Trxx Chexxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Content on the Internet is heterogeneous and arises from various domains like
News, Entertainment, Finance, and Technology. Understanding such content
requires identifying named entities (persons, places, and organizations) as one
of the key steps. Traditionally Named Entity Recognition (NER) systems have
been built using available annotated datasets (like CoNLL, MUC) and demonstrate
excellent performance. However, these models fail to generalize onto other
domains like Sports and Finance where conventions and language use can differ
significantly. Furthermore, several domains do not have large amounts of
annotated labeled data for training robust Named Entity Recognition models. A
key step towards this challenge is to adapt models learned on domains where
large amounts of annotated training data are available to domains with scarce
annotated data. 
In this paper, we propose methods to effectively adapt models learned on one
domain onto other domains using distributed word representations. First, we
analyze the linguistic variation present across domains to identify key
linguistic insights that can boost performance across domains. We propose
methods to capture domain-specific semantics of word usage in addition to
global semantics. We then demonstrate how to effectively use such domain
specific knowledge to learn NER models that outperform previous baselines in
the domain adaptation setting.",30 Jan 2017 17:37:34 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",NLP applications;  named entity recognition;  domain adaptation;  distributional similarity;  text mining,Vixxx,Kulxxxxx,xxxxxxxxxgmail.com,Stony Brook University,No,Yaxxxx,Mexxxx,xxxxxxxxgmail.com,Airbnb,No,Trxx,Chexxxxxx,xxxxxxxxxoo-inc.com,Yahoo!,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vixxx,Kulxxxxx,"University of California, Santa Barbara",,,,,,xxxxxxxxxgmail.com,,,,,United States,,Vixxx Kulxxxxx;Yaxxxx Mexxxx;Trxx Chexxxxxx,xxxxxxxxxgmail.com;xxxxxxxxxgmail.com;xxxxxxxxxxoo-inc.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
79,79X-P3D8F8H5J2,An Interpretable Knowledge Transfer Model for Knowledge Base Completion,Qixxx Xxx;Xuxxxx Mx;Zixxxx Dxx and Edxxxx Hxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Tuesday,,Undecided (Semantics),"Knowledge bases are important resources for a variety of natural language
processing tasks but suffer from incompleteness. We propose a novel embedding
model, ITransF, to perform knowledge base completion. Equipped with a
sparse attention mechanism, ITransF discovers hidden concepts of relations and
transfer statistical strength through the sharing of concepts. Moreover, the
learned associations between relations and concepts, which are represented by
sparse attention vectors, can be interpreted easily.
We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge
base completion and obtains improvements on both the mean rank and Hits@10
metrics, over all baselines that do not use additional information.",19 Apr 2017 19:15:30 GMT,Empirical/Data-Driven,Semantics,,Qixxx,Xxx,xxxxxxxx.cmu.edu,Carnegie Mellon University,No,Xuxxxx,Mx,xxxxxxxxxs.cmu.edu,"Language Technologies Institute, Carnegie Mellon University",No,Zixxxx,Dxx,xxxxxxxxxx@gmail.com,CMU,No,Edxxxx,Hoxx,xxxxxxmu.edu,CMU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixxx,Xxx,CMU,,,412xxxxxxx,,,xxxxxxxxmail.com,,Pittsburgh,PA,,United States,,Qixxx Xxx;Xuxxxx Mx;Zixxxx Dxx;Edxxxx Hoxx,xxxxxxxx.cmu.edu;xxxxxxxxxcs.cmu.edu;xxxxxxxxxxi@gmail.com;xxxxxxcmu.edu,An Interpretable Knowledge Transfer Model for Knowledge Base Completion,An Interpretable Knowledge Transfer Model for Knowledge Base Completion,13,Qizhe Xie,,"carnegie mellon university.  5000 Forbes Avenue Pittsburgh, PA 15213",,on,Only include my submission if it is accepted.,No,None,None
81,81X-E3A2J7G2H9,Controlling Output Style and Topic in Neural Encoder-Decoders based Language Generation,Dx Waxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"We propose flexible decoding methods that are capable of controlling output
style and topic in neural encoder-decoder based language generation.
This capability is desirable in a variety of applications, including
conversation systems, where successful agents need to produce language in a
specific style and generate responses influenced by a human puppeteer or
external
knowledge. 
Inspired by statistical machine translation, we decompose neural generation
process  into empirically easier sub-problems: faithfulness model and
selective-sample base decoding algorithm.
We also describe different training and sampling algorithms that can bias the
generation process with a specific language restriction (e.g. in terms of
style), or a topic restriction.
Human evaluation results show that our decoding methods produce outputs more
relevant than beam-search outputs, more fluent than traditional sampling, and 
have the capabilities to control style and topic without degrading output
quality in conversational tasks.",7 Feb 2017 13:11:17 GMT,Empirical/Data-Driven,Generation,discourse;  language generation;  domain adaptation;  dialogue control;  dialogue;  open-domain question answering;  adaptation to noisy data,Dx,Waxx,xxxxxxxxs.cmu.edu,"School of Computer Science, Carnegie Mellon University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dx,Waxx,"School of Computer Science, Carnegie Mellon University",,,412 xxxxxxxx,,,xxxxxxxxs.cmu.edu,,,,,United States,,Dx Waxx,xxxxxxxxs.cmu.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
82,82X-J2E8D9G6A7,Transfer Learning for Sentiment Classification by Leveraging Feature Marginal Difference and Feature Posterior Agreement,Lx Zhxx;Mixxxx Huxxx and xiaxxxx zx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Sentiment classification aims at classifying the polarity of a given document.
Since sentiment classification is highly sensitive to the domain from which the
training data is extracted, transfer learning is important if we want to adapt
a classifier trained in one domain to other domains. Most existing
feature-based transfer learning methods, such as SCL and SFA, aim to learn
feature representation based on domain-invariant words. In this paper, we
propose a novel feature-based approach for transfer learning in the context of
sentiment classification. Instead of learning feature representation, we
leverage feature marginal difference and  feature posterior agreement between
source domain and target domain. Experiments show that our model outperforms
the state-of-art baselines on most tasks.",31 Jan 2017 05:50:12 GMT,Empirical/Data-Driven,Machine learning,text mining,Lx,Zhxx,xxxxxxxxxxx113@126.com,"State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology",No,Mixxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,xiaxxxx,zxx,xxxxxxxxxxxnghua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lx,Zhxx,"State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology",,,1881xxxxxxx,,,xxxxxxxxxxx113@126.com,,,,,China,,Lx Zhxx;Mixxxx Huxxx;xiaxxxx zxx,xxxxxxxxxxx113@126.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxxxxnghua.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
83,83X-J8E9E8F5A9,Semi-supervised sequence learning with unlabeled dialog data,Toxx Shixxxx;Nobxxxxx Shixxxx and Haxxxx Kobxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Prior work on recurrent neural networks (RNNs), especially one known as long
short term memory (LSTMs), tells us that adding a simple pretraining step to
the training of LSTMs with unlabeled data improves predictive performance (Dai
and Le, 2015).                    As we know from the effectiveness of the Yarowsky
Algorithm
(Yarowsky, 1995), exploiting a specific property of unlabeled data could make
semi-supervised learning effective.  Thus in this paper we explore the effects
of linguistic structures available in unlabeled data and propose a
semi-supervised approach that effectively exploits the conversational aspect of
a large-scale unlabeled microblog corpus.  Our classifiers outperform recently
proposed state-of-the-art methods by notable margins.  We also examine how
these models' classification performances vary depending on the amount of
labeled data.  The results show that, under appropriate pretraining, LSTM-RNN
classifiers can achieve higher relative accuracy with a small amount of labeled
data than with a large amount, compared to baselines using linear classifiers
such as SVM and logistic regression.",7 Feb 2017 13:05:00 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  unsupervised and semi-supervised learning;  NLP in social networking media,Toxx,Shixxxx,xxxxxxxxxxxhoo-corp.jp,Yahoo Japan Corporation,No,Nobxxxxx,Shixxxx,xxxxxxxxxxxhoo-corp.jp,Yahoo Japan Corporation,No,Haxxxx,Kobxxxxxx,xxxxxxxxxxxxxshi@gmail.com,Yahoo Japan Corporation,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Toxx,Shixxxx,Yahoo Japan Corporation,,,,,,xxxxxxxxxxxhoo-corp.jp,,,,,Japan,,Toxx Shixxxx;Nobxxxxx Shixxxx;Haxxxx Kobxxxxxx,xxxxxxxxxxxhoo-corp.jp;xxxxxxxxxxxahoo-corp.jp;xxxxxxxxxxxxxashi@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
84,84X-C4A6F7G9J2,Re-investigating Reference Bias in Monolingual MT Evaluation,Qinxxxxx Mx;Yvxxxx Grxxxx;Timxxxx Balxxxx and Qxx Lx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Fair and reliable human evaluation of Machine Translation (MT) is not only
important for unbiased evaluation of systems, but also as a valid yardstick
against which automatic metrics can be assessed. In this paper, we investigate
the degree to which human assessors of MT may be biased when evaluation is
carried out in a monolingual setting by comparison with a reference
translation. Our re-investigation, contrary to prior reports, shows no evidence
of reference bias in monolingual human evaluation of MT.",7 Feb 2017 10:06:35 GMT,Resources/Evaluation,Machine translation,MT evaluations;  human judgments of MT,Qinxxxxx,Mx,xxxxxxxxxx@ict.ac.cn,Institute of Computing Technology Chinese Academy of Sciences,No,Yvxxxx,Grxxxx,xxxxxxxxxxxte@gmail.com,Dublin City University,No,Timxxxx,Balxxxx,xxxxxxin.net,The University of Melbourne,No,Qxx,Lxx,xxxxxxx@dcu.ie,Dublin City University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yvxxxx,Grxxxx,Dublin City University,,,,,,xxxxxxxxxxxte@gmail.com,,,,,Ireland,,Qinxxxxx Mx;Yvxxxx Grxxxx;Timxxxx Balxxxx;Qxx Lxx,xxxxxxxxxx@ict.ac.cn;xxxxxxxxxxxxte@gmail.com;xxxxxxwin.net;xxxxxxxu@dcu.ie,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
85,85X-A3C6B8G5E6,Sense-word Pairs in Topic Models for Word Sense Induction,Reixxxx Kxx;Seuxxxxxx Hwxxx and Mxx Sxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"In this paper, we propose a sense topic model which makes use of sense-word
pairs to answer the word sense induction (WSI) problem. Sense-word pairs are
pairs of a target word and a neighboring word. When incorporated to the sense
topic model, it generates the target word and its neighboring word
simultaneously during inference, explicitly modeling the word co-occurrence
patterns. This improves the model by building a richer model and by
automatically detecting latent features between the words. Specifically, we
introduce sense-word pair topic model (SPTM). We describe a method for
inference using collapsed Gibbs sampling. We show that our models achieve
significant improvements over previous methods on the SemEval 2013 WSI dataset.",7 Feb 2017 09:22:38 GMT,Applications/Tools,Semantics,unsupervised and semi-supervised learning;  word sense disambiguation;  word sense induction;  graphical models;  document clustering,Reinxxxxxxx,Ampxxxx,xxxxxxxxxxxyonsei.ac.kr,Yonsei University,No,Seuxxxxxx,Hwxxx,xxxxxxxxxxxonsei.ac.kr,Yonsei University,No,Mxx,Soxx,xxxxxxxxxxonsei.ac.kr,Yonsei University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Reinxxxxxxx,Ampxxxx,University of Edinburgh,,,,,,xxxxxxxxxxxyonsei.ac.kr,,,,,United Kingdom,,Reixxxx Kxx;Seuxxxxxx Hwxxx;Mxx Soxx,xxxxxxxxxxxyonsei.ac.kr;xxxxxxxxxxxyonsei.ac.kr;xxxxxxxxxxxonsei.ac.kr,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
86,86X-B9P3H3C3B3,A Syntactic Neural Model for General-Purpose Code Generation,Penxxxxxx Yxx and Grxxxx Nexxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Monday,,Undecided (Semantics),"We consider the problem of parsing natural language descriptions into source
code written in a general-purpose programming language like Python. Existing
data-driven methods treat this problem as a language generation task without
considering the underlying syntax of the target programming language. Informed
by previous work in semantic parsing, in this paper we propose a novel neural
architecture powered by a grammar model to explicitly capture the target syntax
as prior knowledge. Experiments find this an effective way to scale up to
generation of complex programs from natural language descriptions, achieving
state-of-the-art results that well outperform previous code generation and
semantic parsing approaches.",23 Apr 2017 04:56:29 GMT,Empirical/Data-Driven,Semantics,,Penxxxxxx,Yxx,xxxxxxxx.cmu.edu,Carnegie Mellon University,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Penxxxxxx,Yxx,Carnegie Mellon University,,,412xxxxxxx,,,xxxxxxxx.cmu.edu,,Pittsburgh,PA,,United States,,Penxxxxxx Yxx;Grxxxx Nexxxx,xxxxxxxx.cmu.edu;xxxxxxxxxcs.cmu.edu,A Syntactic Neural Model for General-Purpose Code Generation,A Syntactic Neural Model for General-Purpose Code Generation,11,YIN Pengcheng,,Carnegie Mellon University,on,on,Only include my submission if it is accepted.,No,None,None
87,87X-D4C6J4G2J2,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,Coxxxx Floxxxxx and Corxxxxx Carxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Tuesday,,Undecided (Generation Summarization),"The large and growing amounts of online scholarly data present both challenges
and opportunities to enhance knowledge discovery. One such challenge is to
automatically extract a small set of keyphrases from a document that can
accurately describe the document's content and can facilitate fast information
processing. In this paper, we propose PositionRank, an unsupervised 
model for keyphrase extraction from scholarly documents that incorporates
information from all positions of a word's occurrences into a biased PageRank.
Our model obtains remarkable improvements in performance over PageRank models
that do not take into account word positions as well as over strong baselines
for this task. 
Specifically, on several datasets of research papers, PositionRank achieves
improvements as high as $29.09\%$.",20 Apr 2017 18:09:12 GMT,Empirical/Data-Driven,Summarization,,Coxxxx,Floxxxxx,xxxxxxxxxxxxcu@my.unt.edu,University of North Texas,No,Corxxxxx,Carxxxx,xxxxxxxx@unt.edu,University of North Texas,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Coxxxx,Floxxxxx,University of North Texas,,,,,,xxxxxxxxxxxxcu@my.unt.edu,,,,,United States,,Coxxxx Floxxxxx;Corxxxxx Carxxxx,xxxxxxxxxxxxcu@my.unt.edu;xxxxxxxxa@unt.edu,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,11,Corina Florescu,,"Corina Florescu, University of North Texas",on,on,Only include my submission if it is accepted.,No,None,None
88,88X-H2A5D8P7E2,Decomposing Open-Domain Hypernyms into Disambiguated Constituents,Maxxxx Paxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Open-domain lexical class labels available as ambiguous strings in the lexical
space (""Netherlands national football team managers"") are decomposed into
grounded (i.e., disambiguated) constituents in the semantic space (e.g.,
""Netherlands"", ""Manager (association football)""). Based on evidence available
within Wikipedia articles of class members (""Louis van Gaal"", ""Guus Hiddink"")
of the class labels being decomposed, constituent topics are extracted at
encouraging levels of precision and recall for both Wikipedia categories and
automatically extracted class labels.",31 Jan 2017 22:56:20 GMT,Empirical/Data-Driven,Semantics,multiword semantics/compositionality;  information extraction;  lexical semantics;  NLP on Wikipedia and other collaboratively constructed resources;  semantic knowledge induction,Maxxxx,Paxxx,xxxxxxxogle.com,Google Inc.,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,Paxxx,Google,,,,,,xxxxxxxxxxxxxxxxmarius@gmail.com,,,CA,,United States,,Maxxxx Paxxx,xxxxxxxogle.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
89,89X-P9A3F6P9E6,Discovery of Rare Key Phrases,Yanxxxx Chxx;Pixx Chxx and Qinxxxx Zhxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Most existing methods to extract key phrases from free text are statistically
based, where frequency count is used to assess the importance or
informativeness of a phrase. The problem is that many non-frequent phrases
often contain important information and will not be identified by statistical
based methods. The main challenge to discover rare key phrases is the
limitation of context due to rare occurrences. In this paper, we take steps to
address the problem. A method is presented to discover key phrases that occurs
even only one time in a corpus. In our experiments, we show the effectiveness
of our method by comparing with the state of the art approaches. Moreover, a
relation extraction task is also conducted to evaluate the rare phrases for
supporting NLP applications. The result shows that rare phrases, often been
discarded at the beginning of data processing, are informative.",5 Feb 2017 05:51:30 GMT,Theoretical,"Information extraction, text mining, and question answering",information extraction;  term extraction,Yanxxxx,Chxx,xxxxxxxxmail.com,Guizhou University,No,Pixx,Chxx,xxxxxxxxn@umb.edu,University of Massachusetts Boston,No,Qinxxxx,Zhxxx,xxxxxxxxxxxx.xjtu.edu.cn,Xi'an Jiaotong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yanxxxx,Chxx,Guizhou University,,,,,,xxxxxxxxmail.com,,,,,China,,Yanxxxx Chxx;Pixx Chxx;Qinxxxx Zhxxx,xxxxxxxxmail.com;xxxxxxxxxn@umb.edu;xxxxxxxxxxxxl.xjtu.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
90,90X-B8J6H4A6A3,Acquiring Background Knowledge to Improve Moral Value Prediction,Yixx Lxx;Jxx Hoxxxx;Maxxxx Mooxxxxx;Morxxxx Dehxxxxx and Hexx J,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Traditional Information Extraction (IE) mainly focuses on extracting explicit
information, such as a user’s city of residence. However, many applications
ranging from social unrest event prediction to ""value-based"" marketing require
IE techniques to discover implicit information, such as an individual’s moral
concerns for more comprehensive entity profiling and behavior modeling. In this
paper, we aim to tackle a challenging problem of moral value prediction based
on tweet content analysis, following the Moral Foundations Theory (MFT)
proposed by (Graham et al., 2013). In addition to textual features, we acquire
rich background knowledge from an external knowledge base (KB) to improve moral
value prediction. Moreover, besides the bag-of-words model used in most
previous work, we develop a Deep Neural Networks (DNN)-based classifier, which
shows its superiority in understanding texts and inferring moral values.
Experiments show that the combination of background knowledge and DNN model can
provide up to 9.5% absolute F-score gain and our best model achieves comparable
performance to a human reader.",10 Feb 2017 03:50:13 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP in social networking media,Yixx,Lxx,xxxxxxrpi.edu,Rensselaer Polytechnic Institute,No,Jxx,Hoxxxx,xxxxxxxx@usc.edu,University of Southern California,No,Maxxxx,Mooxxxxx,xxxxxxxx@usc.edu,University of Southern California,No,Morxxxx,Dehxxxxx,xxxxxxxx@usc.edu,University of Southern California,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yixx,Lxx,Rensselaer Polytechnic Institute,,,,,,xxxxxxrpi.edu,,,,,United States,,Yixx Lxx;Jxx Hoxxxx;Maxxxx Mooxxxxx;Morxxxx Dehxxxxx;Hexx Jx,xxxxxxrpi.edu;xxxxxxxxr@usc.edu;xxxxxxxxn@usc.edu;xxxxxxxxn@usc.edu;xxxxxxpi.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
91,91X-F4G6D6D3F6,Zero-Shot Transfer Learning for Event Extraction,Lixx Huxxx;Hexx Jx;Clxxx Voxx;Taxxxx Casxxxx and Sebxxxxxx Rixxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Most previous event extraction studies heavily rely on features encoded from
annotated event mentions, thus cannot be applied to new event types without
annotation effort. Inspired by the success of zero-shot transfer learning on
visual object classification, we apply it to event extraction. We leverage
existing human-constructed event ontologies and manual annotations for a small
set of pre-defined event types, and transfer the knowledge of these existing
types to extract events of new types. Our basic hypothesis is that all event
mentions and types can be mapped to a shared semantic space using structural
and compositional neural networks, where the type of each event mention can be
determined by comparing the similarity with all candidate types. Experiments on
both the existing ACE event types and new event types demonstrate the
effectiveness of our transferable neural architecture. Without using any manual
annotations on new event types, our framework achieves comparable performance
as supervised model trained from about 400 event mentions.",7 Feb 2017 08:53:11 GMT,Applications/Tools,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction,Lixx,Huxxx,xxxxxxxxxx@gmail.com,Rensselaer Polytechnic Institute,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,Clxxx,Voxx,xxxxxxxxxxxx.civ@mail.mil,US Army Research Lab,No,Taxxxx,Casxxxx,xxxxxxxxxxxxy64@gmail.com,Language Technology Solutions LLC,No,Sebxxxxxx,Rixxxx,xxxxxxxxxxs.ucl.ac.uk,University College London,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Lixx,Huxxx,Rensselaer Polytechnic Institute,,,,,,xxxxxxxxxx@gmail.com,,,,,United States,,Lixx Huxxx;Hexx Jx;Clxxx Voxx;Taxxxx Casxxxx;Sebxxxxxx Rixxxx,xxxxxxxxxx@gmail.com;xxxxxxpi.edu;xxxxxxxxxxxxx.civ@mail.mil;xxxxxxxxxxxxxy64@gmail.com;xxxxxxxxxxxs.ucl.ac.uk,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
92,92X-E5H5G6P5P8,Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures,Lixx Huxxx;Avxxxx Sxx;Hexx Jx and Raxx Flxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Slot Filling (SF) aims to extract the values of certain types of attributes (or
slots, such as person:cities\_of\_residence) for a given entity from a large
collection of source documents. It is one of the few Information Extraction
tasks that Deep Neural Networks (DNN) has not successfully advanced
state-of-the-art yet. Limited previous attempts using DNN models considered SF
as a traditional within-sentence relation extraction problem as in ACE or
SemEval. In this paper we argue that SF is more challenging, and introduce two
approaches to make DNN effective for SF: (1). Take a regularized dependency
graph instead of a raw sentence as input to DNN, to compress the wide contexts
between query and candidate filler; (2). Incorporate two attention mechanisms:
local attention learned from query and candidate filler, and global attention
learned from external knowledge bases, to guide the model to better select
indicative contexts to determine slot type. Experiments show that this
framework outperforms state-of-the-art on both relation extraction (16%
absolute F-score gain) and slot filling validation for each single system (up
to 8.5% absolute F-score gain).",7 Feb 2017 08:53:59 GMT,Applications/Tools,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction,Lixx,Huxxx,xxxxxxxxxx@gmail.com,Rensselaer Polytechnic Institute,No,Avxxxx,Sxx,xxxxxxxibm.com,IBM T.J. Watson Research Center,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,Raxx,Floxxxx,xxxxxxxx.ibm.com,IBM Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lixx,Huxxx,Rensselaer Polytechnic Institute,,,,,,xxxxxxxxxx@gmail.com,,,,,United States,,Lixx Huxxx;Avxxxx Sxx;Hexx Jx;Raxx Floxxxx,xxxxxxxxxx@gmail.com;xxxxxxx.ibm.com;xxxxxxpi.edu;xxxxxxxxs.ibm.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
93,93X-P9D6A4G8A8,Neural Structural Correspondence Learning for Domain Adaptation,Yfxxx Zixxx and Rxx Reixxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"We introduce a neural network model that marries together ideas from two
prominent strands of research on domain adaptation through representation
learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and
autoencoder neural networks (NNs). Our model is a three-layer NN that learns to
encode the non-pivot features of an input example into a low dimensional
representation, so that the existence of pivot features (features that are
prominent in both domains and convey useful information for the NLP task) in
the example can be decoded from that representation. The low-dimensional
representation is then employed in a learning algorithm for the task. Moreover,
we show how to inject pre-trained word embeddings into our model in order to
improve generalization across examples with similar pivot features. We
experiment with the task of cross-domain sentiment classification on 16 domain
pairs and show substantial improvements over strong baselines.",6 Feb 2017 22:31:09 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  domain adaptation,Yfxxx,Zixxx,xxxxxxxxgmail.com,Technion,No,Rxx,Reixxxxx,xxxxxxxxxxt@gmail.com,Technion - Israel Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rxx,Reixxxxx,Technion - Israel Institute of Technology,,,,,,xxxxxxxxxxt@gmail.com,,,,,Israel,Assistant Professor,Yfxxx Zixxx;Rxx Reixxxxx,xxxxxxxxgmail.com;xxxxxxxxxxxt@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
94,94X-G9B3H8G2J7,A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing,Daxxxx Fernáxxxxxxxxxxxxxx and Caxxxx Gómezxxxxxxxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Monday,,Undecided (Tagging Chunking Syntax Parsing),"Restricted non-monotonicity has been shown beneficial for the projective
arc-eager dependency parser in previous research, as posterior decisions can
repair mistakes made in previous states due to the lack of information. In this
paper, we propose a novel, fully non-monotonic transition system based on the
non-projective Covington algorithm. As a non-monotonic system requires
exploration of erroneous actions during the training process, we develop
several non-monotonic variants of the recently defined dynamic oracle for the
Covington parser, based on tight approximations of the loss. Experiments on
datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic
dynamic oracle outperforms the monotonic version in the majority of languages.",14 Apr 2017 16:53:27 GMT,Theoretical,"Tagging, chunking, syntax, and parsing",,Daxxxx,Fernáxxxxxxxxxxxxxx,xxxxxxxuvigo.es,Universidade de Vigo,No,Caxxxx,Gómezxxxxxxxxxxx,xxxxxxx@udc.es,Universidade da Coruña,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxxx,Fernáxxxxxxxxxxxxxx,Universidade da Coruña,,,,,,xxxxxxxxxez@udc.es,,,,,Spain,,Daxxxx Fernáxxxxxxxxxxxxxx;Caxxxx Gómezxxxxxxxxxxx,xxxxxxxuvigo.es;xxxxxxxr@udc.es,A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing,A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing,11,Carlos Gómez-Rodríguez,,"Universidade da Coruña. Rúa da Maestranza, 9, 15001 A Coruña (Spain)",,,Only include my submission if it is accepted.,No,None,None
95,95X-E3J5E8E6A8,A Situation Semantics for Algebraic Specification Language of Organizational agent Behaviors,Yxx Zhxxx and Lexxxx Lixx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Organizational behaviors play an important
role in reorganization of organizationoriented
multi-agent system (OOMAS), especially
for simulation of in-depth situation
awareness to group attacks. Existing
methods of reorganization modules
depend on binding Finite State Automata
(FSA) of roles. Because we can
not directly give the recognition of roles
within attacking groups, Existing methods
of reorganization modules are inefficient
for real-time simulation of large-scale
mutable organization such as large-scale
cliques. For real-time simulation of organizational
behaviors, reorganization approaches
are not only derived from roles
assignment with top-down way, but also
agglomerated from individual behaviors
with bottom-up way. Only in bottom-up
way can OOMAS-based simulation be reorganized
according to observed attackers’
behaviors. Moreover, we believe that an
approach of bottom-up abstract data type
(ADT) construction is suitable to simulate
unknown, mutable and typed organizational
behaviors. However, for bottomup
reorganization, there has been no semantics
for constructing abstract data type
(ADT) of organizational behaviors.
To solve this issue, we propose situation
semantics for algebraic specification language
of organizational behaviors to support
bottom-up reorganization based on
ADTs. Thus, we first define extractedbehavior
functions to support type description
and situation semantics interpretation
functions of individual agents within
organizational behaviors. In addition,
we refer behavior algebras to setup behavioral
equations for algebraic specification
language of organizational behaviors.",7 Feb 2017 11:55:49 GMT,Theoretical,Semantics,semantic relations;  term extraction,Yxx,Zhxxx,xxxxxxxxxn@sina.com,Beijing Insititute of Technology,No,Lexxxx,Lixx,xxxxxxxxit.edu.cn,Beijng Insititute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yxx,Zhxxx,Beijing Insititute of Technology,,,,,,xxxxxxxxxn@sina.com,,,,,China,,Yxx Zhxxx;Lexxxx Lixx,xxxxxxxxxn@sina.com;xxxxxxxxxit.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
96,96X-G6D2E6C8C8,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,Loxxx Pexxx and Rxx Reixxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Poster Monday,,Undecided (Sentiment Analysis Opinion Mining),"Sarcasm is a form of speech in which speakers say the opposite of what they
truly mean in order to convey a strong sentiment. In other words, ”Sarcasm is
the giant chasm between what I say, and the person who doesn’t get it.”. In
this paper we present the novel task of sarcasm interpretation, defined as the
generation of a non-sarcastic utterance conveying the same message as the
original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets,
each interpreted by five human judges. Addressing the task as monolingual
machine translation (MT), we experiment with MT algorithms and evaluation
measures. We then present SIGN:
an MT based sarcasm interpretation algorithm that targets sentiment words, a
defining element of textual sarcasm. We show that while the scores of n-gram
based automatic measures are similar for all interpretation models, SIGN’s
interpretations are scored higher by humans for adequacy and sentiment
polarity. We conclude with a discussion on future research directions for our
new task.",20 Apr 2017 15:30:26 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Loxxx,Pexxx,xxxxxxxxxxxd@gmail.com,Technion,No,Rxx,Reixxxxx,xxxxxxxxxxt@gmail.com,Technion - Israel Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rxx,Reixxxxx,Technion - Israel Institute of Technology,,,,,,xxxxxxxxxxt@gmail.com,,Haifa,,,Israel,Assistant Professor,Loxxx Pexxx;Rxx Reixxxxx,xxxxxxxxxxxd@gmail.com;xxxxxxxxxxxt@gmail.com,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,11,Roi Reichart,Assistant Professor,"Technion, Israel Institute of Technology

Technion City, Haifa 3200003, Israel",on,on,Only include my submission if it is accepted.,No,None,None
97,97X-C5C8A8D9A7,AI-based Japanese Short-answer Scoring and Support System,Tsuxxxxxx Ishxxxx and Masxxxxx Kaxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We have developed an automated Japanese short-answer scoring and support
machine for new National Center written test exams. Our approach is based on
the fact that recognizing textual entailment and/or synonymy has been almost
impossible for several years. The system generates automated scores on the
basis of evaluation criteria or rubrics, and human raters revise them. The
system determines semantic similarity between the model answers and the actual
written answers as well as a certain degree of semantic identity and
implication. Owing to the need for the scoring results to be classified at
multiple levels, we use random forests to utilize many predictors effectively
rather than use support vector machines. An experimental prototype operates as
a web system on a Linux computer. We compared human scores with the automated
scores for a case in which 3--6 allotment points were placed in 8 categories of
a social studies test as a trial examination. The differences between the
scores were within one point for 70--90 percent of the data when high semantic
judgment was not needed.",1 Feb 2017 11:12:30 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",NLP applications;  textual entailment and paraphrasing;  educational applications;  evaluation metrics,Tsuxxxxxx,Ishxxxx,xxxxxxxxxxd.dnc.ac.jp,The Center for University Entrance Examinations,No,Masxxxxx,Kaxxxx,xxxxxxxxx@nifty.com,The Center for University Entrance Examinations,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tsuxxxxxx,Ishxxxx,The Center for University Entrance Examinations,,,,,,xxxxxxxxxxd.dnc.ac.jp,,,,,Japan,,Tsuxxxxxx Ishxxxx;Masxxxxx Kaxxxx,xxxxxxxxxxd.dnc.ac.jp;xxxxxxxxxx@nifty.com,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
98,98X-F6G5A9E9C9,Dynamically Generating Context-Sensitive Word Representations,Jiaxxxxx Fexx and Xiaxxxxx Zhxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Although having a dense, distributed representation of similarity between all
words was proved to be quite useful in many NLP tasks, the representation of
each word is usually assigned independently by looking up the pre-trained
embeddings, without considering their contexts (even though the word
representations are derived from their contexts at a separate pre-training
phase). It has been recognized that the meaning of a word is significantly
affected by the surrounding text, and the word's representation in a specific
sentence or text should reflect the influence of its context. This study
explores the feasibility of dynamically generating the word representations,
taking their contexts into account, to deal with the issue of polysemy. We show
that the context-sensitive word representations could be factorized into their
essential and context-dependent parts, and those two components can be well
learned jointly from unlabeled texts in an unsupervised way. We, in fact,
suggest that word meanings exist along a continuum which does not exhibit sharp
boundaries between them. The experimental results demonstrated that the
proposed approach boosts the performance of different neural network-based
systems on various NLP tasks across two languages by plugging such
context-sensitive word representations into the NLP systems.",6 Feb 2017 17:18:25 GMT,Empirical/Data-Driven,Machine learning,multiword semantics/compositionality;  unsupervised and semi-supervised learning;  lexical semantics;  distributional similarity;  word sense induction,Jiaxxxxx,Fexx,xxxxxxxxxxudan.edu.cn,Fudan University,No,Xiaxxxxx,Zhxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxxx,Zhxxx,Fudan University,,,,,,xxxxxxxxxxdan.edu.cn,,,,,China,,Jiaxxxxx Fexx;Xiaxxxxx Zhxxx,xxxxxxxxxxudan.edu.cn;xxxxxxxxxxudan.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
99,99X-G8D4B2F3E4,Uncovering the Influence of Sub-debates on Stance Classification,Micxxxx Wojxxxxx and Torxxxx Zexxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In any discussion, stance towards a target (e.g. death penalty) is often
expressed by discussing related but subordinated aspects (e.g. prison
overcrowding). While we argue that such sub-debates play a crucial role in
stance classification, their influence on classification has not been
systematically examined so far. Thus, we create a data set of YouTube comments
that we annotate with stance and sub-debates. When we experimentally control
for sub-debates, we find that stance classifiers mainly learn to classify
sub-debates. We also show that our neural classification model is highly
dependent on those sub-debates, while our non-neural model learns a shallower
but more robust model.",6 Feb 2017 09:17:48 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction,Micxxxx,Wojxxxxx,xxxxxxxxxxxxxzki@uni-due.de,"Language Technology Lab, University of Duisburg-Essen",No,Torxxxx,Zexxx,xxxxxxxxxxxxh@uni-due.de,"Language Technology Lab, University of Duisburg-Essen",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Micxxxx,Wojxxxxx,"Language Technology Lab, University of Duisburg-Essen",,,,,,xxxxxxxxxxxxxzki@uni-due.de,,Duisburg,,,Germany,,Micxxxx Wojxxxxx;Torxxxx Zexxx,xxxxxxxxxxxxxzki@uni-due.de;xxxxxxxxxxxxch@uni-due.de,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
101,101X-F5A5F3G7C6,A Constituent-Centric Neural Architecture for Reading Comprehension,Penxxxx Xxx and Erxx Xixx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"Reading comprehension (RC), aiming to understand natural texts and answer
questions therein, is a challenging task. In this paper, we study the RC
problem on the Stanford Question Answering Dataset (SQuAD). Observing from the
training set that most correct answers are centered around constituents in the
parse tree, we design a constituent-centric neural architecture where the
generation of candidate answers and their representation learning are both
based on constituents and guided by the parse tree. Under this architecture,
the search space of candidate answers can be greatly reduced without
sacrificing the coverage of correct answers and the syntactic, hierarchical and
compositional structure among constituents can be well captured, which
contributes to better representation learning of the candidate answers. On
SQuAD, our method achieves the state of the art performance and the ablation
study corroborates the effectiveness of individual modules.",22 Apr 2017 14:01:58 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Penxxxx,Xxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Erxx,Xixx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Penxxxx,Xxx,Carnegie Mellon University,,,,,,xxxxxxxxxcs.cmu.edu,,,,,United States,,Penxxxx Xxx;Erxx Xixx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu,A Constituent-Centric Neural Architecture for Reading Comprehension,A Constituent-Centric Neural Architecture for Reading Comprehension,10,Pengtao Xie,,"Petuum Inc. 
2555 Smallman Street, Pittsburgh, PA 15222",on,on,No. Do not include my submission in this dataset.,No,None,None
102,102X-H2A9G3A8B3,Logistic Normal Topic Models via Hessian-free Optimization,Kx Zhxx;Yuexxxx Hx and Jiaxxxx Cxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Latent Dirichlet allocation discovers topics
that best reconstruct the documents. However,
it does not explicitly model topic and
word correlations, which is crucial to improve
the topic quality. Existing models
focus on modeling only one type of correlation,
either on topic side or on word side
(but not both), to improve the extracted
topics. This paper proposes logistic normal
topic models, which use logistic normal
priors to capture both topic correlations
and word correlations. To overcome
computational cost, we propose variational
inference with Hessian-free optimization
to infer latent parameters. We show our
model performs faster than existing methods,
while discovering word correlations
and topic correlations, in addition to meaningful
topics, both qualitatively and quantitatively.
Furthermore, our model provides
an elegant and principled way to encode
correlations from external sources. We apply
word correlation prior learned from
WORD2VEC as an example to demonstrate
that our model is able to effectively incorporate
such prior information.",1 Feb 2017 20:06:36 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",text mining;  document mining;  document clustering,Kx,Zhxx,xxxxxxxxxx@gmail.com,Yahoo Labs,No,Yuexxxx,Hx,xxxxxxxxx@gmail.com,Yahoo Labs,No,Jiaxxxx,Chxx,xxxxxxxxxgmail.com,Yahoo Labs,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kx,Zhxx,Microsoft,,,,,,xxxxxxxxxx@gmail.com,,,CA,,United States,,Kx Zhxx;Yuexxxx Hx;Jiaxxxx Chxx,xxxxxxxxxx@gmail.com;xxxxxxxxxx@gmail.com;xxxxxxxxx@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
103,103X-P5J3G6C5H9,Employing Attraction-Repulsion Forces to Identify Syntactic Phrases with Limited Training Data,Gexxxx Tambxxxxxxxx and Maxxxx Vasxxxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"The present article investigates the creation of a phrasing model which splits
any sentence into syntactic phrases, based on the concept of attraction and
repulsion forces. This phrasing task is studied in a specific framework, where
only a constrained training set (of typically just two hundred sentences) is
available. From this set of phrases, an accurate model must be evolved for
splitting arbitrary sentences into their corresponding phrases. The performance
of the proposed phrasing model is examined when it is integrated with an
existing MT system. Experimental results show that this approach has a
performance competitive to other methods, when the parameters are optimised
with two variants of a particle swarm optimization (PSO) algorithm.",7 Feb 2017 07:42:34 GMT,Empirical/Data-Driven,Machine learning,machine-aided translation;  hybrid MT;  example-based MT;  chunking,Gexxxx,Tambxxxxxxxx,xxxxxxx@ilsp.gr,ILSP/Athena R.C.,No,Maxxxx,Vasxxxxxx,xxxxxlsp.gr,ILSP/Athena R.C.,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gexxxx,Tambxxxxxxxx,ILSP/Athena R.C.,,,3021xxxxxxxx,,,xxxxxxx@ilsp.gr,,,,,Greece,,Gexxxx Tambxxxxxxxx;Maxxxx Vasxxxxxx,xxxxxxx@ilsp.gr;xxxxxxlsp.gr,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
104,104X-E4H2H6E6B9,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,Yixxx Cxx;Lixx Huxxx;Hexx Jx;Xx Chxx and Juxxxx L,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"Integrating text and knowledge into a unified semantic space has attracted
significant research interests recently. However, the ambiguity in the common
space remains a challenge, namely that the same mention phrase usually refers
to various entities. In this paper, to deal with the ambiguity of entity
mentions, we propose a novel Multi-Prototype Mention Embedding model, which
learns multiple sense embeddings for each mention by jointly modeling words
from textual contexts and entities derived from a knowledge base. In addition,
we further design an efficient language model based approach to disambiguate
each mention to a specific sense. In experiments, both qualitative and
quantitative analysis demonstrate the high quality of the word, entity and
multi-prototype mention embeddings. Using entity linking as a study case, we
apply our disambiguation method as well as the multi-prototype mention
embeddings on the benchmark dataset, and achieve state-of-the-art performance.",23 Apr 2017 06:55:14 GMT,Empirical/Data-Driven,Semantics,,Yixxx,Cxx,xxxxxxxxxx09@163.com,Tsinghua University,No,Lixx,Huxxx,xxxxxxxxxx@gmail.com,Rensselaer Polytechnic Institute,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,Xx,Chxx,xxxxxxxxx@gmail.com,Tsinghua University,No,Juxxxx,Lx,xxxxxxxxxxx8@gmail.com,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yixxx,Cxx,Tsinghua University,,,,,,xxxxxxxxxx09@163.com,,,,,China,,Yixxx Cxx;Lixx Huxxx;Hexx Jx;Xx Chxx;Juxxxx Lx,xxxxxxxxxx09@163.com;xxxxxxxxxxu@gmail.com;xxxxxxpi.edu;xxxxxxxxxx@gmail.com;xxxxxxxxxxx08@gmail.com,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,11,yixin cao,,"Tsinghua University, Beijing, China",on,,Only include my submission if it is accepted.,No,None,None
105,105X-C4J7B3H8D6,Morphological Inflection Generation with Hard Monotonic Attention,Roxx Ahaxxxx and Yoxx Golxxxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Accept - Poster Tuesday,,,"We present a neural model for morphological inflection generation which employs
a hard attention mechanism, inspired by the nearly-monotonic alignment commonly
found between the characters in a word and the characters in its inflection. We
evaluate the model on three previously studied morphological inflection
generation datasets and show that it provides state of the art results in
various setups compared to previous neural and non-neural approaches. Finally
we present an analysis of the continuous representations learned by both the
hard and soft (Bahdanau, 2014) attention models for the task, shedding some
light on the features such models extract.",8 Apr 2017 14:04:46 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",,Roxx,Ahaxxxx,xxxxxxxxxxxi@gmail.com,Bar Ilan University,No,Yoxx,Golxxxxx,xxxxxxxxxxxrg@gmail.com,Bar Ilan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Roxx,Ahaxxxx,Bar Ilan University,,,9725xxxxxxxx,,,xxxxxxxxxxxi@gmail.com,,,,,Israel,,Roxx Ahaxxxx;Yoxx Golxxxxx,xxxxxxxxxxxi@gmail.com;xxxxxxxxxxxxrg@gmail.com,Morphological Inflection Generation with Hard Monotonic Attention,Morphological Inflection Generation with Hard Monotonic Attention,12,RA,,"Bar Ilan University, Ramat Gan, Israel",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
106,106X-P4B5J9A3E4,Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation,Naxxxx Mostxxxxxxxx;Chxxx Broxxxxx;Bixx Doxxx;Mixxxx Gaxxxx;Jiaxxxxx Gxx;Geoxxxxx Spitxxxxxxxx and Luxx V,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"The popularity of image sharing on social media reflects the important role
visual context plays in everyday conversation. In this paper, we present a
novel task, Image-Grounded Conversations (IGC), in which natural-sounding
conversations are generated about shared photographic images. We investigate
this task using training data derived from image-grounded conversations on
social media and introduce a new dataset of crowd-sourced conversations for
benchmarking progress. Experiments using deep neural network models trained on
social media data show that the combination of visual and textual context can
enhance the quality of generated conversational turns. In human evaluation, a
gap between human performance and that of both neural and retrieval
architectures suggests that IGC presents an interesting challenge for vision
and language research.",6 Feb 2017 19:17:50 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",NLP on noisy unstructured text;  dialogue;  multimodal representations and processing;  language generation;  multimodal communication,Naxxxx,Mostxxxxxxxx,xxxxxxxxxxxxxadeh@gmail.com,University of Rochester,No,Chxxx,Broxxxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Bixx,Doxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Mixxxx,Gaxxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,Microsoft Research,No,Geoxxxxx,Spitxxxxxxxx,xxxxxxxxxxxxxs@cs.ucl.ac.uk,"Department of Computer Science, University College London",No,Luxx,Vandxxxxxxx,xxxxxxxxxrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,Naxxxx,Mostxxxxxxxx,University of Rochester,,,585xxxxxxx,,,xxxxxxxxxxxxxadeh@gmail.com,,,,,United States,,Naxxxx Mostxxxxxxxx;Chxxx Broxxxxx;Bixx Doxxx;Mixxxx Gaxxxx;Jiaxxxxx Gxx;Geoxxxxx Spitxxxxxxxx;Luxx V and erwxxxx (Mixxxxxxx,xxxxxxxxxxxxxadeh@gmail.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxxxxs@cs.ucl.ac.uk;xxxxxxxxxxrosoft.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
107,107X-H2A9C6P6P6,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,Jixx Nx;Geoxxxxxx Dixx and Raxx Flxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"The state-of-the-art named entity recognition (NER) systems are supervised
machine learning models that require large amounts of manually annotated data
to achieve high accuracy. However, annotating NER data by human is expensive
and time-consuming, and can be quite difficult for a new language. In this
paper, we present two weakly supervised approaches for cross-lingual NER with
no human annotation in a target language. The first approach is to create
automatically labeled NER data for a target language via annotation projection
on comparable corpora, where we develop a heuristic scheme that effectively
selects good-quality projection-labeled data from noisy data. The second
approach is to project distributed representations of words (word embeddings)
from a target language to a source language, so that the source-language NER
system can be applied to the target language without re-training. We also
design two co-decoding schemes that effectively combine the outputs of the two
projection-based approaches. We evaluate the performance of the proposed
approaches on both in-house and open NER data for several target languages. The
results show that the combined systems outperform three other weakly supervised
approaches on the CoNLL data.",23 Apr 2017 05:28:46 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Jixx,Nx,xxxxxxxibm.com,IBM Research,No,Geoxxxxxx,Dixx,xxxxxxxx.ibm.com,IBM Watson,No,Raxx,Floxxxx,xxxxxxxx.ibm.com,IBM Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixx,Nx,IBM Research,,,,,,xxxxxxxibm.com,,Yorktown Heights,NY,,United States,,Jixx Nx;Geoxxxxxx Dixx;Raxx Floxxxx,xxxxxxxibm.com;xxxxxxxxs.ibm.com;xxxxxxxxs.ibm.com,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,11,Jian Ni,,"IBM Research
1101 Kitchawan Road, Yorktown Heights, NY 10598, USA",,,Only include my submission if it is accepted.,No,None,None
108,108X-J5B3P7C9A6,A Multigraph-based Model for Overlapping Entity Recognition,Aldxxxx Obxxx and Wxx Lx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we propose a new model for predicting overlapping entities based
on multigraphs, as opposed to simple graphs commonly used in graphical models
for structured predictions. Through experiments in standard datasets containing
overlapping and non-overlapping entities, we demonstrate that our model
outperforms previous models. We also present some analysis on the
differences between our model and the previous models and discuss the possible
implications of the differences. To the best of our knowledge, this is the
first structured prediction model utilizing multigraphs to predict overlapping
structures.",7 Feb 2017 08:18:10 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  named entity recognition;  mention detection;  structured input/output;  graphical models,Aldrxxxxxxxxx,Muxx,xxxxxxxxxxxx@sutd.edu.sg,Singapore University of Technology and Design,No,Wxx,Lx,xxxxxxxxxtd.edu.sg,Singapore University of Technology and Design,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Aldrxxxxxxxxx,Muxx,Carnegie Mellon University,,,,,,xxxxxxxxxxxx.m@gmail.com,,,,,United States,,Aldxxxx Obxxx;Wxx Lx and  Dexxxxx,xxxxxxxxxxxx@sutd.edu.sg;xxxxxxxxxutd.edu.sg,,,,,,,on,,"Yes, include my submission even if the paper is rejected.",No,None,None
109,109X-H6A4G4A2P3,Cascade LSTMs based Deep Reinforcement Learning for Goal-driven Dialogue,Yxx Mx and Xiaxxxx WAxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"This paper proposes a deep neural network model for jointly modeling Natural
Language Understanding(NLU) and Dialogue Management(DM) in goal-driven dialogue
systems. There are three parts in the model. A Long Short-Term Memory(LSTM) at
the bottom of the network encodes utterances in each dialogue turn into a turn
embedding. Dialogue embeddings are learned by a LSTM at the bottom of the
network, and updated with the feeding of new turn embeddings. The top part is a
forward Deep Neural Network(DNN) mapping dialogue embeddings to Q-values of
different dialogue actions. The cascade LSTMs based reinforcement learning
network is jointly optimized by making use of the rewards received at each
dialogue turn as only supervision information. There is no explicit NLU and
dialogue states in the network. Experimental results show our model outperforms
traditional MDP model with fully correct NLU and state tracking on several
meeting-room booking tasks. The model is also shown with the capability of
recovering from wrong actions and NLU errors. Visualization of dialogue
embeddings illustrates they keep the information of dialogue states.",7 Feb 2017 03:22:07 GMT,Empirical/Data-Driven,Dialog and interactive systems,dialogue control;  dialogue;  contex modeling for dialogues;  evaluation methods for dialogues,Yxx,Mx,xxxxxxxxt.edu.cn,Beijing University of Posts and Telecommunications,No,Xiaxxxx,WAxx,xxxxxxxxxpt.edu.cn,Beijing University of Posts and Telecommunications,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yxx,Mx,Beijing University of Posts and Telecommunications,,,8618xxxxxxxxx,,,xxxxxxxxt.edu.cn,,Beijing,Beijing,,China,Graduate student at Beijing University of Posts and Telecommunications,Yxx Mx;Xiaxxxx WAxx and  Telecxxxxxxxxxxxxxx,xxxxxxxxt.edu.cn;xxxxxxxxxupt.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
110,110X-F5J8D9C3J9,Domain Specific Feature Transfer for Hybrid Domain Adaptation,PENxxxx Wxx;Yixxxx Kx and Cxx Kexxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Heterogeneous domain adaptation needs supplementary information to link up
different domains. However, such supplementary information may not always be
available in real cases.
In this paper, a new problem setting called hybrid domain adaptation is
investigated.
It is a special case of heterogeneous domain adaptation, in which different
domains share some common features, but also have their own domain specific
features.
We leverage upon common features instead of supplementary information to
achieve effective adaptation.
We propose a domain specific feature transfer method, which can link up
different domains using common features and simultaneously reduce domain
divergences.
Extensive experiments verify the effectiveness of our proposed method.",7 Feb 2017 01:45:54 GMT,Empirical/Data-Driven,Machine learning,sentiment analysis;  domain adaptation;  text classification,PENxxxx,Wxx,xxxxxxxxxxntu.edu.sg,NTU,No,Yixxxx,Kx,xxxxxxxu.edu.sg,NTU,No,Chixxxxxx,Gxx,xxxxxxxxxxxxxxolls-Royce.com,"Rolls-Royce Advanced Technology Centre, Singapore",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PENxxxx,Wxx,NTU,,,,,,xxxxxxxxxxntu.edu.sg,,,,,Singapore,,PENxxxx Wxx;Yixxxx Kx;Cxx Kexxx,xxxxxxxxxxntu.edu.sg;xxxxxxxxu.edu.sg;xxxxxxxxxxxxxxRolls-Royce.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
111,111X-H3C4A6C6B3,Arch: A Probabilistic Model of Author-Based Sentiment Aspect Discovery,Zxx Zhxxx and Munxxxxx Sixxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In opinionated text, such as a review, not only does each author express
subjective sentiments, usually he or she expresses those sentiments with
respect to specific aspects. We propose Arch, an unsupervised probabilistic
model that discovers aspects and sentiments from opinionated texts and
associates them with authors.  Arch can thus capture the association of aspects
and sentiments with authors that previous approaches omit.  We show that Arch
discovers aspects associated with sentiments; generates valid author profiles;
and is effective in authorship prediction and sentiment classification.",6 Feb 2017 20:05:35 GMT,Applications/Tools,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction,Zxx,Zhxxx,xxxxxxxx@ncsu.edu,IBM Watson,No,Munxxxxx,Sixxx,xxxxxxxcsu.edu,North Carolina State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zxx,Zhxxx,IBM Watson,,,919xxxxxxx,,,xxxxxxxx@ncsu.edu,,Durham,NC,,United States,"Zhe Zhang received an MS in computer science from North Carolina State University in 2012 and obtained his doctoral degree in computer science at North Carolina State University in 2014. His research interests include text mining, sentiment analysis, and social networks.",Zxx Zhxxx;Munxxxxx Sixxx,xxxxxxxx@ncsu.edu;xxxxxxxncsu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
112,112X-D8A2E3P5D7,Neural Memory Networks for Hashtag Recommendation,Minxxxx Pexx;Qx Zhxxx and Xuaxxxxx Huxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"In this paper, we address the problem of recommending hashtags for microblogs
when users type in the hashtag symbol (#). Hashtags are widely used on social
media for indexing keywords or topics. Different people may have different
preferences when choosing hashtags. Moreover, these preferences may change over
time. To tackle these problems, in this paper, we propose a novel neural memory
network called User Adaptive Neural Memory Network (UANMN) with a readable and
writable memory module on the user level. All of the memory operations,
including reading and writing, are differentiable. The experimental results on
a dataset crawled from Twitter demonstrate the effectiveness of our proposed
model.",6 Feb 2017 14:58:11 GMT,Empirical/Data-Driven,Social media,filtering and recommendation;  NLP in social networking media,Minxxxx,Pexx,xxxxxxxxxxxxfudan.edu.cn,Fudan University,No,Qx,Zhxxx,xxxxxxxn.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qx,Zhxxx,Fudan University,,,,,,xxxxxxxn.edu.cn,,,,,China,,Minxxxx Pexx;Qx Zhxxx;Xuaxxxxx Huxxx,xxxxxxxxxxxxfudan.edu.cn;xxxxxxxxn.edu.cn;xxxxxxxxxxudan.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
113,113X-B5B6C4D2D2,FastQA: A Simple and Efficient Neural Architecture for Question Answering,Dixx Weisxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Recent development of large-scale question answering (QA) datasets triggered a
substantial amount of research into end-to-end neural architectures for QA.
Increasingly complex systems have been proposed without comparison to a simpler
neural baseline system that would justify their complexity. In this work, we
propose a simple heuristic that guided the development of FastQA, an efficient
end-to-end neural model for question answering that is competitive with
existing models. We further demonstrate, that an extended version (FastQAExt)
achieves state-of-the-art performance on recent benchmark datasets, namely
SQuAD, NewsQA and MsMARCO. However, we show that increasing the complexity of
FastQA to FastQAExt does not yield any systematic improvements. We argue that
the same holds true for existing systems which are similar to FastQAExt. A
manual analysis reveals that our proposed heuristic explains most predictions
of our model, which indicates that modeling a simple heuristic is enough to
achieve strong performance on extractive QA datasets.",6 Feb 2017 13:42:49 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP applications;  information retrieval;  context-aware question answering;  answer extraction;  open-domain question answering,Dixx,Weisxxxxxxx,xxxxxxxxxxxxborn@dfki.de,German Research Center for Artificial Intelligence (DFKI),No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dixx,Weisxxxxxxx,German Research Center for Artificial Intelligence (DFKI),,,,,,xxxxxxxxxxxxborn@dfki.de,,,,,Germany,,Dixx Weisxxxxxxx,xxxxxxxxxxxxborn@dfki.de,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
114,114X-D7J5C6C3D7,Learning Word and Sense Embeddings jointly using Semantic Networks,Massxxxxxxxx Manxxxx;Joxx Camacxxxxxxxxxxx;Ignxxxx Iacxxxxxx and Robxxxx Naxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Word embeddings are widely used in Natural Language Processing, mainly due to
their success in capturing semantic information from massive corpora. However,
their creation process does not allow to automatically separate the different
meanings of a word, conflating them into a single vector. We address this issue
by proposing a new model which learns word and sense embeddings jointly. Our
model exploits large corpora and knowledge obtained from semantic networks in
order to produce a unified vector space of words and senses. We evaluate the
main features of our approach qualitatively and quantitatively in various
tasks, highlighting the advantages of the proposed method with respect to
state-of-the-art word- and sense-based models.",7 Feb 2017 11:16:14 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  word sense disambiguation;  multilingual resources;  distributional similarity;  NLP on Wikipedia and other collaboratively constructed resources,Massxxxxxxxx,Manxxxx,xxxxxxxxxxx.uniroma1.it,Sapienza University of Rome,No,Joxx,Camacxxxxxxxxxxx,xxxxxxxxxxx.uniroma1.it,Sapienza University of Rome,No,Ignxxxx,Iacxxxxxx,xxxxxxxxxxxx.uniroma1.it,Sapienza University of Rome,No,Robxxxx,Navxxxx,xxxxxxxxxxxuniroma1.it,Sapienza University of Rome,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Joxx,Camacxxxxxxxxxxx,Cardiff University,,,,,,xxxxxxxxxxxxxxxj@cardiff.ac.uk,,,,,United Kingdom,,Massxxxxxxxx Manxxxx;Joxx Camacxxxxxxxxxxx;Ignxxxx Iacxxxxxx;Robxxxx Navxxxx,xxxxxxxxxxx.uniroma1.it;xxxxxxxxxxxx.uniroma1.it;xxxxxxxxxxxxi.uniroma1.it;xxxxxxxxxxx.uniroma1.it,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
115,115X-J3H5D7H2F3,Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition,Txx Gx;Qixx Dxx;Hexx Jx;Lxx Cxx;Baxxxx Chxxx;Zhixxxx Sxx and Mixx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper proposes to study fine-grained coordinated cross-lingual text stream
alignment through a novel information network decipherment paradigm. We use
Burst Information Networks as media to represent the text streams and present a
simple yet effective network decipherment algorithm with diverse clues to
decipher the networks for accurate text stream alignment. Extensive experiments
on Chinese-English coordinated news streams show our approach can harvest
high-quality alignments from large amounts of streaming data for endless
language knowledge mining.",7 Feb 2017 11:11:00 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",text mining;  cross-lingual approaches;  graph-based algorithms,Txx,Gx,xxxxxxxxu.edu.cn,"Key Laboratory of Computational Linguistics, Peking University",No,Qixx,Dxx,xxxxxxxxgmail.com,Information Sciences Institute,No,Hexx,Jx,xxxxxpi.edu,Rensselaer Polytechnic Institute,No,Lxx,Cxx,xxxxxxxxxosoft.com,Microsoft Research,No,Baxxxx,Chxxx,xxxxxxxu.edu.cn,Peking University,No,Zhixxxx,Sxx,xxxxxxx.edu.cn,,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,,,,,,,,,,,,,,,,,Txx,Gx,Microsoft Research Asia,,,,,,xxxxxxxxxosoft.com,,,,,China,,Txx Gx;Qixx Dxx;Hexx Jx;Lxx Cxx;Baxxxx Chxxx;Zhixxxx Sxx;Mixx Zhxx,xxxxxxxxu.edu.cn;xxxxxxxxxgmail.com;xxxxxxpi.edu;xxxxxxxxxrosoft.com;xxxxxxxxu.edu.cn;xxxxxxxu.edu.cn;xxxxxxxxxxxicrosoft.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
117,117X-C6J6F9G9G5,Improved Neural Relation Detection for Knowledge Base Question Answering,Mx Yx;Wenxxxx Yxx;Kaxx Saxxxx;Cixxxx dxx;Bixx Xixxx and Boxxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"Relation detection is a core component of many NLP applications including
Knowledge Base Question Answering (KBQA). In this paper, we propose a
hierarchical recurrent neural network enhanced by residual learning which
detects KB relations given an input question. Our method uses deep residual
bidirectional LSTMs to compare questions and relation names via different
levels of abstraction. Additionally, we propose a simple KBQA system that
integrates entity linking and our proposed relation detector to make the two
components enhance each other. Our experimental results show that our approach
not only achieves outstanding relation detection performance, but more
importantly, it helps our KBQA system achieve state-of-the-art accuracy for
both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA
benchmarks.",23 Apr 2017 00:28:30 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Mx,Yx,xxxxxxxxmail.com,IBM Research,No,Wenxxxx,Yxx,xxxxxxxxxxxng@gmail.com,University of Munich,No,Kazixxxxxxx,Haxxx,xxxxxxxxxgmail.com,"University of Texas at Dallas, IBM",No,Cixxxx,dosxxxxxxx,xxxxxxxxxus.ibm.com,IBM Watson,No,Bixx,Xixxx,xxxxxxxxxgmail.com,IBM,No,Boxxx,Zhxx,xxxxxxx.ibm.com,IBM Research,No,,,,,,,,,,,,,,,,,,,,,,Mx,Yx,IBM Research,,,314xxxxxxx,,,xxxxxxxxmail.com,,,,,United States,,Mx Yx;Wenxxxx Yxx;Kaxx Saxxxx;Cixxxx dxx;Bixx Xixxx;Boxxx Zhxx,xxxxxxxxmail.com;xxxxxxxxxxxxng@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxxxus.ibm.com;xxxxxxxxx@gmail.com;xxxxxxxx.ibm.com,Improved Neural Relation Detection for Knowledge Base Question Answering,Improved Neural Relation Detection for KBQA,11,Mo Yu,,"AI Foundations, IBM Research, USA",,on,Only include my submission if it is accepted.,No,None,None
118,118X-G6C7C3A2A4,Tracing a Loose Wordhood for Chinese Input Method Engine,Xixx Zhxxx;Hxx Zhxx and Cxx Wx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Chinese input methods are used to convert pinyin sequence or other Latin
encoding systems into Chinese character sentences. For more effective
pinyin-to-character conversion, typical Input Method Engines (IMEs) rely on a
predefined vocabulary that demands manually maintenance on schedule. For the
purpose of removing the inconvenient vocabulary setting, this work focuses on
automatic wordhood acquisition by fully considering that Chinese inputting is a
free human-computer interaction procedure. Instead of strictly defining words,
a loose word likelihood is introduced for measuring how likely a character
sequence can be a user-recognized word with respect to using IME. Then an
online algorithm is proposed to adjust the word likelihood or generate new
words by comparing user true choice for inputting and the algorithm prediction.
The experimental results show that the proposed solution can agilely adapt to
diverse typings and demonstrate performance approaching highly-optimized IME
with fixed vocabulary.",7 Feb 2017 10:56:47 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",word segmentation,Xixx,Zhxxx,xxxxxxxxxxs@gmail.com,Shanghai Jiao Tong University,No,Hxx,Zhxx,xxxxxxxxxxxsjtu.edu.cn,Shanghai Jiao Tong University,No,Cxx,Wxx,xxxxxxxxxxxx@sjtu.edu.cn,Shanghai Jiao Tong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixx,Zhxxx,Shanghai Jiao Tong University,,,,,,xxxxxxxxxxs@gmail.com,,,,,China,,Xixx Zhxxx;Hxx Zhxx;Cxx Wxx,xxxxxxxxxxs@gmail.com;xxxxxxxxxxx.sjtu.edu.cn;xxxxxxxxxxxx0@sjtu.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
120,120X-C5J8B3H7P8,Reconstruction of Word Embeddings from Sub-Word Parameters,Kaxx Strxxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Pre-trained word embeddings improve the performance of a neural model at the
cost of increasing the model size. We propose to benefit from this resource
without paying the cost by operating strictly at the sub-lexical level. Our
approach is quite simple: before task-specific training, we first optimize
sub-word parameters to reconstruct pre-trained word embeddings using various
distance measures. The method can be applied to any sub-word architecture. To
demonstrate this point, we apply it to a novel sub-character model for the
Korean language based on unicode decomposition. We show improvement on a
variety of tasks: word similarity, word analogy, English part-of-speech
tagging, and Korean dependency parsing.",6 Feb 2017 18:40:46 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",unsupervised and semi-supervised learning;  lexical semantics;  phonology;  morphology,Kaxx,Strxxxx,xxxxxxxxxratos.com,Bloomberg L.P.,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxx,Strxxxx,Toyota Technological Institute at Chicago,,,,,,xxxxxxxxxratos.com,,New York,NY,,United States,,Kaxx Strxxxx,xxxxxxxxxratos.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
122,122X-F8E5H7H3A6,Neural Belief Tracker: Data-Driven Dialogue State Tracking,Nixxxx Mrkxxxxx;Diaxxxxx �x;Tsunxxxxxxx Wxx;Blxxxx Thoxxxx and Stxxx Yoxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Poster Tuesday,,Undecided (Dialog Interactive Systems),"One of the core components of modern spoken dialogue systems is the belief
tracker, which estimates the user's goal at every step of the dialogue.
However, most current approaches have difficulty scaling to larger, more
complex dialogue domains. This is due to their dependency on either: a) Spoken
Language Understanding models that require large amounts of annotated training
data; or b) hand-crafted lexicons for capturing some of the linguistic
variation in users' language. We propose a novel Neural Belief Tracking (NBT)
framework which overcomes these problems by building on recent advances in
representation learning. NBT models reason over pre-trained word vectors,
learning to compose them into distributed representations of user utterances
and dialogue context. Our evaluation on two datasets shows that this approach
surpasses past limitations, matching the performance of state-of-the-art models
which rely on hand-crafted semantic lexicons and outperforming them when such
lexicons are not provided.",20 Apr 2017 17:17:35 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Nixxxx,Mrkxxxxx,xxxxxxxxxxxic@gmail.com,University of Cambridge,No,Diaxxxxx,Ó Sxxxxxxxx,xxxxxxxam.ac.uk,Apple,No,Tsunxxxxxxx,Wxx,xxxxxxxam.ac.uk,University of Cambridge,No,Blxxxx,Thoxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Stxxx,Yoxxx,xxxxxxxxcam.ac.uk,Cambridge University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Nixxxx,Mrkxxxxx,PolyAI,,,4478xxxxxxxx,,,xxxxxxxxxxxic@gmail.com,,Cambridge,,,United Kingdom,"Nikola Mrksic is the CEO and Co-Founder of PolyAI, a London-based machine learning startup developing the next generation machine learning platform for building conversational user interfaces. Our platform enables the development of machine learning powered chatbots or voice-based agents which perform tasks across many applications, in a wide array of world languages. 
  
Nikola completed his PhD at the University of Cambridge, working with Professor Steve Young. He also worked as a machine learning researcher with the Apple Siri team in Cambridge. Before that, he was the first technical hire at VocalIQ, a Cambridge-based dialogue systems startup acquired by Apple in 2015.",Nixxxx Mrkxxxxx;Diaxxxxx �x;Tsunxxxxxxx Wxx;Blxxxx Thoxxxx;Stxxx Yoxxx,xxxxxxxxxxxic@gmail.com;xxxxxxxxam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxxcam.ac.uk,Neural Belief Tracker: Data-Driven Dialogue State Tracking,Neural Belief Tracker: Data-Driven Dialogue State Tracking,12,Nikola Mrksic,,"Engineering Department, University of Cambridge
Trumpington Street, Cambridge CB2 1PZ
United Kingdom",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
123,123X-C6D6A3F2G6,Linguistic analysis of differences in portrayal of movie characters,Anxx Ramaxxxxxxx;Vixxxx Rx;Nikxxxxx Malaxxxxxxx;Kaxxx Sixxxx and Shrxxxxxx Narxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Poster Monday,,Undecided (Sentiment Analysis Opinion Mining),"We examine differences in portrayal of characters in movies using
psycholinguistic and graph theoretic measures computed directly from
screenplays. Differences are examined with respect to characters' gender, race,
age and other metadata. Psycholinguistic metrics are extrapolated to dialogues
in movies using a linear regression model built on a set of manually annotated
seed words. Interesting patterns are revealed about relationships between
genders of production team and the gender ratio of characters. Several
correlations are noted between gender, race, age of characters and the
linguistic metrics.",22 Apr 2017 21:39:19 GMT,Applications/Tools,Sentiment analysis and opinion mining,,Anxx,Ramaxxxxxxx,xxxxxxxx@usc.edu,University of Southern California,No,Vicxxxxxx,Marxxxxxx,xxxxxxxx@usc.edu,University of Southern California,No,Nikxxxxx,Malaxxxxxxx,xxxxxxxx@usc.edu,"Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA",No,Kaxxx,Sixxxx,xxxxxxx@usc.edu,"SAIL, University of Southern California",No,Shrxxxxxx,Narxxxxxx,xxxxxxxxi.usc.edu,University of Southern California,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxx,Ramaxxxxxxx,University of Southern California,,,1213xxxxxxx,,,xxxxxxxx@usc.edu,,Los Angeles,AL,,United States,,Anxx Ramaxxxxxxx;Vixxxx Rx;Nikxxxxx Malaxxxxxxx;Kaxxx Sixxxx;Shrxxxxxx Narxxxxxx,xxxxxxxx@usc.edu;xxxxxxxxm@usc.edu;xxxxxxxxa@usc.edu;xxxxxxxx@usc.edu;xxxxxxxxxi.usc.edu,Linguistic analysis of differences in portrayal of movie characters,Linguistic analysis of differences in portrayal of movie characters,10,Anil K Ramakrishna,,"University of Southern California, Los Angeles, CA",on,on,No. Do not include my submission in this dataset.,No,None,None
124,124X-E7C4G3J6C9,Chinese Zero Pronoun Resolution: A Chain to Chain Approach,Faxx Koxx and Guoxxxx Zhxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Chinese zero pronoun (ZP) resolution plays a critical role in discourse
analysis. Different from traditional mention to mention approaches, this paper
proposes a chain to chain approach to improve the performance of ZP resolution
from three aspects. Firstly, consecutive ZPs are clustered into coreferential
chains, each working as one independent anaphor as a whole. In this way, those
ZPs far away from their overt antecedents can be bridged via other consecutive
ZPs in the same coreferential chains and thus better resolved. Secondly, common
noun phrases (NPs) are automatically grouped into coreferential chains using
traditional approaches, each working as one independent antecedent candidate as
a whole. Then, ZP resolution is made between ZP coreferential chains and common
NP coreferential chains. In this way, the performance can be much improved due
to the effective reduction of search space by pruning singletons and negative
instances.
 Finally, additional features from ZP and common NP coreferential chains are
employed to better represent anaphors and their antecedent candidates,
respectively. Comprehensive experiments on the OntoNotes corpus show that our
chain to chain approach significantly outperforms the state-of-the-art mention
to mention approaches. To our knowledge, this is the first work to resolve zero
pronouns in a chain to chain way.",3 Feb 2017 07:46:40 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse;  coreference resolution,Faxx,Koxx,xxxxxxxxxxuda.edu.cn,Soochow University,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Faxx,Koxx,Soochow University,,,86-13xxxxxxxxx,,,xxxxxxxxxxuda.edu.cn,,Suzhou,Jiangsu,,China,,Faxx Koxx;Guoxxxx Zhxx,xxxxxxxxxxuda.edu.cn;xxxxxxxxxuda.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
125,125X-F9E2D3H3A6,Effectively Training the Neural Machine Translation Model with Monolingual Data,Zhxx Yaxx;Wxx Chxx;Fexx Waxx and Bx X,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Improving the neural machine translation model (NMT) with monolingual data has
aroused more and more interests in this area and back-translation for
monolingual data augmentation \cite{sennrich2015improving} has been taken as a
promising development recently. While the naive back-translation approach
improves the translation performance significantly, we notice that its usage
for the monolingual data is not so effective because the traditional NMT model
makes no distinction between the true parallel corpus and the back translated
synthetic parallel corpus. This paper proposes a \textbf{\emph{gate-enhanced}}
NMT model which takes advantage of the monolingual data more effectively. The
central idea is to separate the data flow of the monolingual data and parallel
data into different channels by the elegant designed gate, which enables the
model to perform different transformations according to the type of the input
sequence (monolingual data or parallel data). Experiments on the
Chinese-English translation tasks show our approach achieves substantial
improvements over strong baselines.",6 Feb 2017 14:57:46 GMT,Empirical/Data-Driven,Machine translation,unsupervised and semi-supervised learning;  learning with small datasets;  statistical machine translation,Zhxx,Yaxx,xxxxxxxxxx14@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Wxx,Chxx,xxxxxxxxxxxdia@ia.ac.cn,"Institute of Automaton, Chinese Academy of Sciences",No,Fexx,Waxx,xxxxxxxx@ia.ac.cn,"Institute of Automaton, Chinese Academy of Sciences",No,Bx,Xx,xxxxxxxxxxxxa@whu.edu.cn,"Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhxx,Yaxx,"Institute of Automation, Chinese Academy of Sciences",,,1368xxxxxxx,,,xxxxxxxxxx14@ia.ac.cn,,Beijing,,,China,,Zhxx Yaxx;Wxx Chxx;Fexx Waxx;Bx Xx,xxxxxxxxxx14@ia.ac.cn;xxxxxxxxxxxxdia@ia.ac.cn;xxxxxxxxx@ia.ac.cn;xxxxxxxxxxxxda@whu.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
126,126X-H2A2P7C3E6,Multi-Perspective Context Matching for Machine Comprehension,Zhxxxx Waxx;Haxxxx Mx;Waxx Haxxx and Raxx Flxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Previous machine comprehension (MC) datasets are either too small to train
end-to-end deep learning models, or not difficult enough to evaluate the
ability of current MC techniques. The newly released SQuAD dataset alleviates
these limitations, and gives us a chance to develop more realistic MC models. 
Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM)
model, which is an end-to-end system that directly predicts the answer
beginning and ending points in a passage. Our model first adjusts each
word-embedding vector in the passage by multiplying a relevancy weight 
computed against the question. Then, we encode the question and weighted
passage by using bi-directional LSTMs. For each point in the passage, our model
matches the context of this point against the encoded question from multiple
perspectives and produces a matching vector. Given those matched vectors, we
employ another bi-directional LSTM to aggregate all the information and predict
the beginning and ending points.Experimental result on the test set of SQuAD
shows that our model achieves the state-of-the-art performance on the
leaderboard.",3 Feb 2017 20:35:58 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",context-aware question answering;  answer extraction;  open-domain question answering,Zhxxxx,Waxx,xxxxxxxxxxxw@gmail.com,IBM Watson Research Center,No,Haxxxx,Mx,xxxxxxxxxxp@gmail.com,IBM Watson Research Center,No,Waxx,Haxxx,xxxxxxxxs.ibm.com,IBM Watson Research,No,Raxx,Floxxxx,xxxxxxxx.ibm.com,IBM Watson Research Center,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhxxxx,Waxx,IBM Watson Research Center,,,781xxxxxxx,,,xxxxxxxxxxxw@gmail.com,,,,,United States,,Zhxxxx Waxx;Haxxxx Mx;Waxx Haxxx;Raxx Floxxxx,xxxxxxxxxxxw@gmail.com;xxxxxxxxxxxp@gmail.com;xxxxxxxxxs.ibm.com;xxxxxxxxs.ibm.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
127,127X-H6H7F6H6J6,Automatic Prediction of Discourse Connectives,Erxx Maxxx;Danxxxx Pixxxx;Sebxxxxxx Krxxxx and Mikxxxx Kozxxxxxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Accurate prediction of suitable discourse connectives (however, furthermore,
etc.) is a  key component of any system aimed at building coherent and fluent
discourses from shorter sentences and passages. As an example, a dialog system
might assemble a long and informative answer by sampling passages extracted
from different documents retrieved from the web. We formulate the task of
discourse connective prediction and release a dataset of 2.9M sentence pairs
separated by discourse connectives for this task. Then, we evaluate the
hardness of the task for human raters, apply a recently proposed decomposable
attention (DA) model to this task and observe that the automatic predictor has
a higher F1 than human raters (32 vs. 30).  Nevertheless, under specific
conditions the raters still outperform the DA model, suggesting that there is
headroom for future improvements. Finally, we further demonstrate the
usefulness of the connectives dataset by showing that it improves implicit
discourse relation prediction when used for model pre-training.",3 Feb 2017 13:43:55 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse;  textual entailment and paraphrasing,Erxx,Maxxx,xxxxxxxxxi@aalto.fi,Aalto University,No,Danxxxx,Pixxxx,xxxxxxxxoogle.com,Google,No,Sebxxxxxx,Krxxxx,xxxxxxxxxxxxause@dfki.de,"Language Technology Lab, DFKI",No,Mikxxxx,Kozhxxxxxxx,xxxxxxxogle.com,Google,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Erxx,Maxxx,Aalto University,,,,,,xxxxxxxxxi@aalto.fi,,,,,Finland,,Erxx Maxxx;Danxxxx Pixxxx;Sebxxxxxx Krxxxx;Mikxxxx Kozhxxxxxxx,xxxxxxxxxi@aalto.fi;xxxxxxxxxoogle.com;xxxxxxxxxxxxrause@dfki.de;xxxxxxxxogle.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
128,128X-E2G3F9G2C6,Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks,Yunxxxxx Chxx;Dixxx Hakkxxxxxxx;Goxxxx Txx;Asxx Celixxxxxxx;Jiaxxxxx Gxx and Lx Dxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Natural language understanding (NLU) is a core component of a dialogue system.
Recently recurrent neural networks (RNN) obtained strong results on NLU due to
their superior ability of preserving sequential information over time. 
Traditionally, the NLU module tags semantic slots for utterances considering
their flat structures, as the underlying RNN structure is a linear chain.
However, natural language exhibits linguistic properties that provide rich,
structured information for better understanding.
This paper introduces a novel model, knowledge-guided structural attention
networks (K-SAN), a generalization of RNN to additionally incorporate non-flat
network topologies guided by prior knowledge. There are two characteristics: 1)
important substructures can be captured from small training data, allowing the
model to generalize to previously unseen test data; 2) the model automatically
figures out the salient substructures that are essential to predict the
semantic tags of the given sentences, so that the understanding performance can
be improved. 
The experiments on the benchmark ATIS data show that the proposed K-SAN
architecture can effectively extract salient knowledge from substructures with
an attention mechanism, and outperform the state-of-the-art neural network
based frameworks.",7 Feb 2017 11:35:48 GMT,Empirical/Data-Driven,Dialog and interactive systems,spoken language understanding,Yunxxxxx,Chxx,xxxxxxxx@ieee.org,National Taiwan University,No,Dixxx,Hakkxxxxxxx,xxxxxxxeee.org,Microsoft Research,No,Goxxxx,Txx,xxxxxxxxxr@ieee.org,Google,No,Asxx,Celixxxxxxx,xxxxxxxxlive.com,Microsoft Research,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,"Microsoft Research, Redmond",No,Lx,Dexx,xxxxxxxxxosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,Yunxxxxx,Chxx,National Taiwan University,,,,,,xxxxxxxx@ieee.org,,Taipei,,,Taiwan,,Yunxxxxx Chxx;Dixxx Hakkxxxxxxx;Goxxxx Txx;Asxx Celixxxxxxx;Jiaxxxxx Gxx;Lx Dexx,xxxxxxxx@ieee.org;xxxxxxxieee.org;xxxxxxxxxxr@ieee.org;xxxxxxxx@live.com;xxxxxxxxxxrosoft.com;xxxxxxxxxrosoft.com,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
129,129X-E3F3P6J6J3,Syntax-aware Neural Machine Translation Using CCG,Maxxx Nadxxxx;Sixx Rexxx;Rixx Senxxxxx;Toxxxx Dwxxxx;Maxxxx Junczxxxxxxxxxx;Phixxxx Koxxx and  Axxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation (NMT) models are able to partially learn syntactic
information from sequential lexical information. Still, some complex syntactic
phenomena such as prepositional phrase attachment are poorly modeled. 
This work aims to answer two questions: 1) Does explicitly modeling source or
target language syntax help NMT? 2) Is tight integration of words and syntax
better than multitask training?
We introduce syntactic information in the form of CCG supertags either in the
source as an extra feature in the embedding, or in the target, by interleaving
the target supertags with the word sequence. 
Our results on WMT data show that explicitly modeling syntax improves machine
translation quality for English-German, a high-resource pair, and for
English-Romanian, a low-resource pair and also several syntactic phenomena
including prepositional phrase attachment. Furthermore, a tight coupling of
words and syntax improves translation quality more than multitask training.",3 Feb 2017 19:43:49 GMT,Empirical/Data-Driven,Machine translation,syntax;  structured input/output;  statistical machine translation,Maxxx,Nadxxxx,xxxxxxxxxxxde@gmail.com,University of Edinburgh,No,Sixx,Rexxx,xxxxxxxxxy@ed.ac.uk,University of Edinburgh,No,Rixx,Senxxxxx,xxxxxxxxxxxch@ed.ac.uk,University of Edinburgh,No,Toxxxx,Dwxxxx,xxxxxxxxxamu.edu.pl,Adam Mickiewicz University,No,Maxxxx,Junczxxxxxxxxxx,xxxxxxxxxmu.edu.pl,"Adam Mickiewicz University, Poznań",No,Phixxxx,Koxxx,xxxxxhu.edu,Johns Hopkins University,No,Alexxxxxx,Bixxx,xxxxxxxxxx@gmail.com,University of Edinburgh,No,,,,,,,,,,,,,,,,,Maxxx,Nadxxxx,Grammarly Inc,,,4474xxxxxxxx,,,xxxxxxxxxxxde@gmail.com,,Edinburgh,,,United States,,Maxxx Nadxxxx;Sixx Rexxx;Rixx Senxxxxx;Toxxxx Dwxxxx;Maxxxx Junczxxxxxxxxxx;Phixxxx Koxxx; Axxx and rx Bixxx,xxxxxxxxxxxde@gmail.com;xxxxxxxxxxy@ed.ac.uk;xxxxxxxxxxxich@ed.ac.uk;xxxxxxxxxxamu.edu.pl;xxxxxxxxxamu.edu.pl;xxxxxxhu.edu;xxxxxxxxxxh@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
130,130X-P6G7F3A7C5,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,Leaxxxx Saxxxx;Edixxxx Ansxxxx;Osvxxxx Olixxxxx;Dixxx Amaxxxx;Letxxxxx Maxxxx and  x,Biomedical,Aurxxxxx Néxxxxx;Kaxxx Verxxxxx,Accept - Poster Monday,,Undecided (Biomedical),"Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose.
Linguistic features, mainly from parsers, have been used to detect MCI, but
this is not suitable for large-scale assessments. MCI disfluencies produce
non-grammatical speech that requires manual or high precision automatic
correction of transcripts.  In this paper, we modeled transcripts into complex
networks and enriched them with word embedding (CNE) to better represent short
texts produced in neuropsychological assessments. The network measurements were
applied with well-known classifiers to automatically identify MCI in
transcripts, in a binary classification task. A comparison was made with the
performance of traditional approaches using Bag of Words (BoW) and linguistic
features for three datasets: DementiaBank in English, and Cinderella and
Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using
only complex networks, while Support Vector Machine was superior to other
classifiers. CNE provided the highest accuracies for DementiaBank and
Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably
owing to its short narratives. The approach using linguistic features yielded
higher accuracy if the transcriptions of the Cinderella dataset were manually
revised. Taken together, the results indicate that complex networks enriched
with embedding is promising for detecting MCI in large-scale assessments.",22 Apr 2017 13:36:31 GMT,Applications/Tools,Biomedical,,Leaxxxx,Saxxxx,xxxxxxxxs@usp.br,Institute of Mathematics and Computer Science - University of São Paulo (USP),No,Edilsxxxxxxxxxx,Corr�xxxxxxxxxx,xxxxxxxxxjr@usp.br,Institute of Mathematics and Computer Science - University of São Paulo (USP),No,Osvxxxx,Olivxxxxxxx,xxxxxxxc.usp.br,"Institute of Physics at Sao Carlos, University of Sao Paulo (USP)",No,Dixxx,Amaxxxx,xxxxxxxxmc.usp.br,Institute of Mathematics and Computer Science - University of São Paulo (USP),No,Letxxxxx,Maxxxx,xxxxxxx@usp.br,"Department of Physiotherapy, Speech Pathology and Occupational Therapy Medical School - University of São Paulo (USP)",No,Saxxxx,Aluxxxxx,xxxxxxxxxmc.usp.br,University of São Paulo,No,,,,,,,,,,,,,,,,,,,,,,Leandxxxxxxxxx,dosxxxxxxx,Institute of Mathematics and Computer Science - University of São Paulo (USP),,,,,,xxxxxxxxs@usp.br,,São Carlos,São Paulo,,Brazil,,Leaxxxx Saxxxx;Edixxxx Ansxxxx;Osvxxxx Olixxxxx;Dixxx Amaxxxx;Letxxxxx Maxxxx; x and rx Aluxxxxx,xxxxxxxxs@usp.br;xxxxxxxxxcjr@usp.br;xxxxxxxxc.usp.br;xxxxxxxxxmc.usp.br;xxxxxxxr@usp.br;xxxxxxxxxcmc.usp.br,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,13,Leandro Borges dos Santos,PhD Student,"Institute of Mathematics and Computer Science, University of São Paulo, São Carlos, São Paulo, Brazil",,on,Only include my submission if it is accepted.,No,None,None
131,131X-A4P8G5P3B9,Wikipedia based subjective similarity between scientific papers,Bxx Xx and Hxx Zhxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Previous text similarity measures neglect that the similarity between
scientific pa-pers may change with people’s different background knowledge.
This paper pre-sents a new text similarity measure that can appropriately adapt
the change of background knowledge.",3 Feb 2017 16:13:34 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",domain adaptation;  subjectivity analysis;  text classification;  NLP on Wikipedia and other collaboratively constructed resources;  document clustering,Bxx,Xx,xxxxxxxxxpt.edu.cn,"Nanjing University of Posts and Telecommunications, China",No,Hxx,Zhxxx,xxxxxxxct.ac.cn,aston university,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bxx,Xx,"Nanjing University of Posts and Telecommunications, China",,,+86-1xxxxxxxxxx,,,xxxxxxxxxpt.edu.cn,,,,,China,,Bxx Xx;Hxx Zhxxx,xxxxxxxxxpt.edu.cn;xxxxxxxxct.ac.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
132,132X-G9A4A6B5D7,Unsupervised Learning of Morphology with Graph Sampling,Maxxxx Sumxxxxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"We introduce a language-independent, graph-based probabilistic model of
morphology, which uses transformation rules operating on whole words instead of
the traditional morphological segmentation. The morphological analysis of a set
of words is expressed through a graph having words as vertices 
and structural relationships between words as edges. We define a probability
distribution over such graphs and develop a sampler based on the
Metropolis-Hastings algorithm. The sampling is applied in order to determine
the strength of morphological relationships between words, filter out
accidental similarities and reduce the set of transformation rules. The model
is evaluated on the task of finding morphologically similar words, with
'morphological similarity' defined as edit distance on sequences of morphemes,
as well as on the task of lexicon expansion. An own handcrafted Polish dataset
is used for evaluation. The results are compared to a state-of-the-art
segmentation-based approach.",7 Feb 2017 11:08:32 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",generative models;  unsupervised and semi-supervised learning;  multilingual applications;  graph-based algorithms;  morphology;  Bayesian learning,Maxxxx,Sumxxxxxx,xxxxxxxxxxxxxxxxxtik.uni-leipzig.de,University of Leipzig,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,Sumxxxxxx,University of Leipzig,,,,,,xxxxxxxxxxxxxxxxxtik.uni-leipzig.de,,,,,Germany,born: Maciej Janicki,Maxxxx Sumxxxxxx,xxxxxxxxxxxxxxxxxtik.uni-leipzig.de,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
133,133X-G3P2F6C5J8,What’s this Movie about? A Neural Network Pipeline for Movie Content Analysis,Phxxxx Joxx and Mirxxxx Laxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This work takes a first step toward movie content analysis by automatically
identifying movie attributes and synthesizing them into a comprehensive natural
language overview. The attributes and overviews give a first impression of the
movie, describing aspects like its plot, mood, location, or style. We create a
large-scale dataset that consists of screenplays augmented with movie
attributes and overviews which we extract from an online database. We model
attribute identification as a multi-label classification task, using a neural
network architecture for structured prediction. Movie overviews are generated
with an LSTM model conditioned on the identified attributes. Automatic and
human evaluation show that predicted attributes and overviews provide
informative and faithful descriptions of the movie's content.",3 Feb 2017 18:55:55 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",NLP applications;  language generation;  user studies;  structured input/output;  text mining;  text classification;  document summarization;  NLP for expert domains,Philxxxxxxx,Gorxxxxx,xxxxxxxxxxki@ed.ac.uk,"School of Informatics, University of Edinburgh",No,Mirxxxx,Laxxxx,xxxxxxxx.ed.ac.uk,"School of Informatics, University of Edinburgh",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Philxxxxxxx,Gorxxxxx,"School of Informatics, University of Edinburgh",,,,,,xxxxxxxxxxki@ed.ac.uk,,,,,United Kingdom,,Phxxxx Joxx;Mirxxxx Laxxxx,xxxxxxxxxxki@ed.ac.uk;xxxxxxxxx.ed.ac.uk,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
134,134X-E3J3H3C6C6,Neural End-to-End Learning for Computational Argumentation Mining,Stexxxx Egxx;Johxxxxx Daxexxxxxxx and Irxxx Gurxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"We investigate neural techniques for end-to-end computational argumentation
mining (AM). We frame AM both as a token-based dependency parsing and as a
token-based sequence tagging problem, including a multi-task learning setup.
Contrary to models that operate on the argument component level, we find that
framing AM as dependency parsing leads to subpar performance results. In
contrast, less complex (local) tagging models based on BiLSTMs perform robustly
across classification scenarios, being able to catch long-range dependencies
inherent to the AM problem. Moreover, we find that jointly
learning `natural' subtasks, in a multi-task learning setup, improves
performance.",22 Apr 2017 12:16:17 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Stexxxx,Egxx,xxxxxxxxxxxn@gmail.com,"UKP Lab, Technische Universität Darmstadt",No,Johxxxxx,Daxexxxxxxx,xxxxxxxxxxxxxxxxxxxxxmatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,Irxxx,Gurxxxxx,xxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Stexxxx,Egxx,"UKP Lab, Technische Universität Darmstadt",,,,,,xxxxxxxxxxxn@gmail.com,,,,,Germany,"March 2016-present: Post-Doc NLP UKP Darmstadt
2014-Feb. 2016: Post-Doc NLP Goethe University Frankfurt am Main
2014: PhD in economics
2001-2013: Study of Mathematics, Economics, and NLP universities Heidelberg, Frankfurt am Main+

Publications:

Schnober, Carsten, and Eger, Steffen,  and Do Dinh, Erik-Lan and Gurevych, Iryna.  Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks. Coling 2016

Eger, Steffen, and Hoenen, Armin and Mehler, Alexander. Language Classification from Bilingual Word Embedding Graphs.  Coling 2016.

Eger, Steffen and Mehler, Alexander.  On the linearity of semantic change: Investigating meaning variation via dynamic graph models. ACL 2016

Eger, Steffen, and vor der Brück, Tim, and Mehler, Alexander.  A Comparison of Four Character-Level String-to-String Translation Models for (OCR) Spelling Error Correction.
The Prague Bulletin of Mathematical Linguistics, 105, 77-99, 2016. 

Eger, Steffen. On the number of many-to-many alignments of multiple sequences.
Journal of Automata, Languages and Combinatorics 20 (2015) 1, 53-65.

Eger, Steffen. Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in G2P.
EMNLP 2015.

Eger, Steffen. Improving G2P from Wiktionary and other (web) resources. Interspeech, 2015. 

Eger, Steffen. Multiple Many-To-Many Sequence Alignment for Combining String-Valued Variables: A G2P Experiment. ACL 2015.

Eger, Steffen. Designing and comparing G2P-type lemmatizers for a morphology-rich language.
In: Fourth International Workshop on Systems and Frameworks for Computational Morphology, 2015.",Stexxxx Egxx;Johxxxxx Daxexxxxxxx;Irxxx Gurxxxxx,xxxxxxxxxxxn@gmail.com;xxxxxxxxxxxxxxxxxxxxxrmatik.tu-darmstadt.de;xxxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,Neural End-to-End Learning for Computational Argumentation Mining,Neural End-to-End Learning for Computational Argumentation Mining,12,SE,,"TU Darmstadt - FB 20 
Hochschulstraße 10 
64289 Darmstadt
Germany",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
135,135X-F8E8B9C8D4,Action Languages and Question Answering,Yuxxxx Liexxxx;Danxxxx Incxxxxx and Micxxxx Gexxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper describes a methodology for designing Question Answering systems
that utilize an action language ALM to allow inferences based on complex
interactions of events described in texts. This methodology assumes the
extension of the Verbnet lexicon with interpretable semantic annotations in
ALM.",6 Feb 2017 20:16:38 GMT,Applications/Tools,"Information extraction, text mining, and question answering",discourse;  lexical semantics;  formal semantics and logic;  open-domain question answering;  semantic knowledge induction;  question answering in restricted domains,Yuxxxx,Liexxxx,xxxxxxxxxxnomaha.edu,University of Nebraska at Omaha,No,Danxxxx,Incxxxxx,xxxxxxxxxiamiOH.edu,Miami University,No,Micxxxx,Gelxxxx,xxxxxxxxxxxfond@ttu.edu,Texas Tech University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxxx,Liexxxx,University of Nebraska at Omaha,,,,,,xxxxxxxxxxnomaha.edu,,,,,United States,,Yuxxxx Liexxxx;Danxxxx Incxxxxx;Micxxxx Gelxxxx,xxxxxxxxxxnomaha.edu;xxxxxxxxxxiamiOH.edu;xxxxxxxxxxxxfond@ttu.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
136,136X-H9C6C4D8F3,Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification,Xixxx Chxx;Yx Sxx;Bxx Athixxxxxxxx;Clxxxx Caxxxx and Kixxxx Weixxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In recent years deep neural networks have achieved great success in sentiment
classification for English, thanks in part to the availability of copious
annotated resources. Unfortunately, most other languages do not enjoy such an
abundance of annotated data for sentiment analysis. To tackle this problem, we
propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment
knowledge learned from labeled English data to low-resource languages where
only unlabeled data exists. ADAN is a ""Y-shaped"" network with two
discriminative branches: a sentiment classifier and an adversarial language
predictor. Both branches take input from a feature extractor that aims to learn
hidden representations that  capture the underlying sentiment of the text and
are invariant across languages. Experiments on Chinese and Arabic sentiment
classification demonstrate that ADAN significantly outperforms several
baselines, including a strong pipeline approach that relies on state-of-the-art
Machine Translation.",6 Feb 2017 22:43:35 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  unsupervised and semi-supervised learning;  cross-lingual approaches,Xixxx,Chxx,xxxxxxxxxxcornell.edu,Cornell University,No,Yx,Sxx,xxxxxxxxrnell.edu,Cornell University,No,Bxx,Athixxxxxxxx,xxxxxxxxrnell.edu,Cornell University,No,Clxxxx,Caxxxx,xxxxxxxxxxcornell.edu,Cornell University,No,Kixxxx,Weixxxxxxx,xxxxxxxxnell.edu,Cornell,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixxx,Chxx,Cornell University,,,1607xxxxxxx,,,xxxxxxxxxxcornell.edu,,Ithaca,NY,,United States,,Xixxx Chxx;Yx Sxx;Bxx Athixxxxxxxx;Clxxxx Caxxxx;Kixxxx Weixxxxxxx,xxxxxxxxxxcornell.edu;xxxxxxxxxrnell.edu;xxxxxxxxxrnell.edu;xxxxxxxxxxxcornell.edu;xxxxxxxxrnell.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
137,137X-J3A6B6P6B6,Topic Evolution Models for Long-running MOOCs,Arxx Raxxxx and Lixx Gexxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In order to improve the quality of online courses, instructors need to actively
monitor and discern patterns in previous iterations of the course and mold the
course to suit the needs of the ever-changing student population. To enable
this, in this work, we develop models to track evolution of topics across
repeated MOOC offerings. We present topic evolution results on two successful
long-running MOOCs: i) a business course, and ii) a computer science course. We
leverage seeded topic models to perform a detailed analysis of the evolution of
fine-grained topics in online course discussion forums and draw important
insights on the nature of students, types of issues, and student satisfaction.
Our methods uncover interesting topic trends in both courses including the
decline of logistic issues in both courses as iterations unfold, decline in
grading related issues when automatic grading is adopted in the business
course, and prevalence of technical issues in the computer science course in
comparison to the business course. Our models and analysis are useful for
educators and instructors to model the progression of courses and understand
how to fine-tune courses to meet student expectations.",6 Feb 2017 20:52:17 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",NLP applications;  educational applications,Arxx,Raxxxx,xxxxxxxxxxhamton.edu,"State University of New York, Binghamton",No,Lixx,Gexxxx,xxxxxxxxxe.ucsc.edu,"University of California, Santa Cruz",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Arxx,Raxxxx,"State University of New York, Binghamton",,,,,,xxxxxxxxxxhamton.edu,,Binghamton,NY,,United States,,Arxx Raxxxx;Lixx Gexxxx,xxxxxxxxxxhamton.edu;xxxxxxxxxxe.ucsc.edu,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
138,138X-C6F8D7P4H5,In Other Words: Analyzing Lexical Variability in Event Coreference,Vexxx Shwxxxx;Gabxxxx Staxxxxxx and Ixx Daxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Event coreference models employ lexical and syntactic features to recognize
that two predicate mentions describe the same event. Their performance is
limited, while a simple baseline that links mentions based on head lemma
equality is considered strong on several datasets. The challenge is to
recognize coreference between lexically-divergent predicates. Hence, we provide
a systematic study of lexical variability in event coreference and the type of
knowledge needed to recognize it. To promote the development of models that
better address such lexical variability, we propose a methodology to
automatically extract lexically-divergent event coreferences from Twitter.",6 Feb 2017 13:20:28 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  lexical paraphrasing;  semantic relations;  coreference resolution,Vexxx,Shwxxxx,xxxxxxxxx@gmail.com,Bar-Ilan University,No,Gabxxxx,Staxxxxxx,xxxxxxxxxxxxxxvsky@gmail.com,Bar Ilan University,No,Ixx,Daxxx,xxxxxxxxxbiu.ac.il,Bar-Ilan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vexxx,Shwxxxx,Bar-Ilan University,,,+972-xxxxxxxxx,,,xxxxxxxxx@gmail.com,,,,,Israel,,Vexxx Shwxxxx;Gabxxxx Staxxxxxx;Ixx Daxxx,xxxxxxxxx@gmail.com;xxxxxxxxxxxxxxovsky@gmail.com;xxxxxxxxx.biu.ac.il,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
139,139X-A2H7D2C9C9,Dating Documents: A Domain Independent Approach to Predict Year of Authorship,Vixxx Kulxxxxx;Yinxxxx Tixx;Paxxx D and iwxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present two classes of computational models to date documents based on
linguistic markers.  
The first class infers global usage patterns of neologisms over time to assign
dates to texts, providing insights into temporal locality of language use. 
Our second class of models are based on a neural-based approach that exploits
deeper linguistic cues. 
We demonstrate that our models generalize across various domains like News,
Fiction and Non Fiction over a span of 150 years. Finally, we apply our model
to books written by authors over their literary careers which yields insights
into the temporal patterns of language used by authors.",6 Feb 2017 21:02:01 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information retrieval;  text mining;  text classification;  temporal/spatial information extraction;  document mining,Vixxx,Kulxxxxx,xxxxxxxxxgmail.com,Stony Brook University,No,Yinxxxx,Tixx,xxxxxxxxxxxxtonybrook.edu,Stony Brook University,No,Paxxx,Danxxxxxx,xxxxxxxxxxxxxxstonybrook.edu,Stony Brook University,No,Stxxxx,Skxxxx,xxxxxxxxxxxxonybrook.edu,Stony Brook University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vixxx,Kulxxxxx,"University of California, Santa Barbara",,,,,,xxxxxxxxxgmail.com,,,,,United States,,Vixxx Kulxxxxx;Yinxxxx Tixx;Paxxx Danxxxxxx;Stxxxx Skxxxx,xxxxxxxxxgmail.com;xxxxxxxxxxxxxtonybrook.edu;xxxxxxxxxxxxxx.stonybrook.edu;xxxxxxxxxxxxtonybrook.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
140,140X-P4E3A3C6E6,A Convolutional Encoder Model for Neural Machine Translation,Joxxx Gehxxxx;Micxxxx Auxx;Daxxx Graxxxxx and Yaxx Daxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Oral Monday,,Undecided (Machine Translation),"The prevalent approach to neural machine translation relies on bi-directional
LSTMs to encode the source sentence. We present a faster and simpler
architecture based on a succession of convolutional layers. This allows to
encode the source sentence simultaneously compared to recurrent networks for
which computation is constrained by temporal dependencies. On WMT'16
English-Romanian translation we achieve competitive accuracy to the
state-of-the-art and on WMT'15 English-German we outperform several recently
published results. Our models obtain almost the same accuracy as a very deep
LSTM setup on WMT'14 English-French translation. We speed up CPU decoding by
more than two times at the same or higher accuracy as a strong bi-directional
LSTM.",22 Apr 2017 20:09:01 GMT,Empirical/Data-Driven,Machine translation,,Joxxx,Gehxxxx,xxxxxxxxxxing@kit.edu,Karlsruhe Institute of Technology,No,Micxxxx,Auxx,xxxxxxxxxxxi@gmail.com,Facebook AI Research,No,Daxxx,Graxxxxx,xxxxxxxr@fb.com,Facebook,No,Yaxx,Dauxxxx,xxxxxb.com,Facebook,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxx,Graxxxxx,Facebook,,,,,,xxxxxxxxxngier.info,,,,,United States,,Joxxx Gehxxxx;Micxxxx Auxx;Daxxx Graxxxxx;Yaxx Dauxxxx,xxxxxxxxxxing@kit.edu;xxxxxxxxxxxli@gmail.com;xxxxxxxxr@fb.com;xxxxxfb.com,A Convolutional Encoder Model for Neural Machine Translation,A Convolutional Encoder Model for Neural Machine Translation,13,Michael Auli,,Facebook AI Research,,,No. Do not include my submission in this dataset.,No,None,None
142,142X-H5H9J8C3A2,Opinion Target Understanding in Event-level Sentiment Analysis,Suxxxx Zxx;Shoxxxxx Lx and Guoxxxx Zxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In this paper, we focus on a critical subtask in event-level sentiment
analysis, namely opinion target understanding, with the goal to determine which
opinion target a comment talks about in an event description. Unlike
traditional aspect-level sentiment analysis, opinion target understanding needs
to not only recognize the opinion target in a comment, but also align the
target to the corresponding opinion target in an event description. To address
this problem, we propose a neural 2-sequences-to-1-sequence framework to
jointly leverage both texts in an event description and an comment, and apply a
word-by-word attention mechanism to capture the alignment between the two
texts. Experimental results justify the effectiveness of the proposed approach
to opinion target understanding in event-level sentiment analysis.",7 Feb 2017 08:14:35 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction,Suxxxx,Zxx,xxxxxxxxxxsuda.edu.cn,Soochow University,No,Shoxxxxx,Lx,xxxxxxxxxxxsuda.edu.cn,Soochow University,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Suxxxx,Zxx,Soochow University,,,,,,xxxxxxxxxxsuda.edu.cn,,Suzhou,Jiangsu,,China,,Suxxxx Zxx;Shoxxxxx Lx;Guoxxxx Zhxx,xxxxxxxxxxsuda.edu.cn;xxxxxxxxxxx@suda.edu.cn;xxxxxxxxxuda.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
143,143X-F3J8B3D7B6,Adversarial Training for Unsupervised Bilingual Lexicon Induction,Mexx Zhxxx;Yaxx Lxx;Huxxxx Luxx and Maoxxxx Sx,Multilingual,Omxx Abxxx;Moxx Dixx,Accept - Poster Tuesday,,Undecided (Multilingual),"Word embeddings are well known to capture linguistic regularities of the
language on which they are trained. Researchers also observe that these
regularities can transfer across languages. However, previous endeavors to
connect separate monolingual word embeddings typically require cross-lingual
signals as supervision, either in the form of parallel corpus or seed lexicon.
In this work, we show that such cross-lingual connection can actually be
established without any form of supervision. We achieve this end by formulating
the problem as a natural adversarial game, and investigating techniques that
are crucial to successful training. We carry out evaluation on the unsupervised
bilingual lexicon induction task. Even though this task appears intrinsically
cross-lingual, we are able to demonstrate encouraging performance without any
cross-lingual clues.",22 Apr 2017 15:01:55 GMT,Empirical/Data-Driven,Multilinguality,,Mexx,Zhxxx,xxxxxxxxxoxmail.com,Tsinghua University,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Huxxxx,Luxx,xxxxxxxxxx@gmail.com,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mexx,Zhxxx,Tsinghua University,,,,,,xxxxxxxxxoxmail.com,,,,,China,,Mexx Zhxxx;Yaxx Lxx;Huxxxx Luxx;Maoxxxx Sxx,xxxxxxxxxoxmail.com;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxo@gmail.com;xxxxxxxxxxhua.edu.cn,Adversarial Training for Unsupervised Bilingual Lexicon Induction,Adversarial Training for Unsupervised Bilingual Lexicon Induction,12,Meng Zhang,,"Tsinghua University, Beijing, China",,,No. Do not include my submission in this dataset.,No,None,None
144,144X-J6J6C3F6B9,Semantic Dependency Parsing via Book Embedding,Wexxxx Sxx;Juxxxx Cxx and Xiaxxxx Wx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Tuesday,,Undecided (Tagging Chunking Syntax Parsing),"We model a dependency graph as a book, a particular kind of topological space,
for
semantic dependency parsing. The spine of the book is made up of a sequence of
words, and each page contains a subset of noncrossing arcs. To build a semantic
graph for a given sentence, we design new Maximum Subgraph algorithms to
generate noncrossing graphs on each page, and a Lagrangian Relaxation-based
algorithm tocombine pages into a book. Experiments demonstrate the
effectiveness of the bookembedding framework across a wide range of conditions.
Our parser obtains
comparable results with a state-of-the-art
transition-based parser.",22 Apr 2017 14:41:07 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Wexxxx,Sxx,xxxxxx.edu.cn,Peking University,No,Juxxxx,Cxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,Xiaxxxx,Wxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wexxxx,Sxx,Peking University,,,8601xxxxxxxxx,,,xxxxxx.edu.cn,,,,,China,,Wexxxx Sxx;Juxxxx Cxx;Xiaxxxx Wxx,xxxxxx.edu.cn;xxxxxxxxxxx@pku.edu.cn;xxxxxxxxxxx@pku.edu.cn,Semantic Dependency Parsing via Book Embedding,Semantic Dependency Parsing via Book Embedding,11,Weiwei Sun,,"Institute of Computer Science and Technology, Peking University.
Zhongguancun North Street 128, Haidian District, Beijing, P. R. China",on,,No. Do not include my submission in this dataset.,No,None,None
145,145X-H3D6J5C4E2,Multimodal Word Distributions,Bxx Athixxxxxxxx and Anxxxx Wixxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"Word embeddings provide point representations of words containing useful
semantic information. 
We introduce multimodal word distributions formed from Gaussian mixtures, for
multiple word meanings, entailment, and rich uncertainty information.  To learn
these distributions, we propose an energy-based max-margin objective. We show
that the resulting approach captures uniquely  expressive semantic information,
and outperforms alternatives, such as word2vec skip-grams, and Gaussian
embeddings, on benchmark datasets such as word similarity and entailment.",30 Apr 2017 22:53:31 GMT,Empirical/Data-Driven,Semantics,,Bxx,Athixxxxxxxx,xxxxxxxxrnell.edu,Cornell University,No,Anxxxx,Wixxxx,xxxxxxxxx@gmail.com,Cornell University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bxx,Athixxxxxxxx,Cornell University,,,,,,xxxxxxxxrnell.edu,,,,,United States,,Bxx Athixxxxxxxx;Anxxxx Wixxxx,xxxxxxxxrnell.edu;xxxxxxxxxx@gmail.com,Multimodal Word Distributions,Multimodal Word Distributions,12,Ben Athiwaratkun,,Cornell University,on,on,Only include my submission if it is accepted.,No,None,None
146,146X-J9P2J9H9B5,Learning Contextually Informed Representations for Linear-Time Discourse Parsing,Yaxx Lxx and Mirxxxx Laxxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Recent advances in RST discourse parsing have focused on two modeling
paradigms: (a)~high order parsers which jointly predict the tree structure of
the discourse and the relations it encodes; or (b)~linear-time parsers which
are efficient but mostly based on local features.  In this work, we propose a
linear-time parser with a novel way of representing discourse constituents
based on neural networks which takes into account global contextual information
and is able to capture long-distance dependencies. Experimental results show
that our parser obtains state-of-the art performance on benchmark datasets,
while being efficient (with time complexity linear in the number of sentences
in the document) and requiring minimal feature engineering.",5 Feb 2017 23:33:36 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse,Yaxx,Lxx,xxxxxxxxx@ed.ac.uk,University of Edinburgh,No,Mirxxxx,Laxxxx,xxxxxxxx.ed.ac.uk,"School of Informatics, University of Edinburgh",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Lxx,University of Edinburgh,,,,,,xxxxxxxxx@ed.ac.uk,,,,,United Kingdom,,Yaxx Lxx;Mirxxxx Laxxxx,xxxxxxxxx@ed.ac.uk;xxxxxxxxx.ed.ac.uk,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
147,147X-J7H3G5D3C2,Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification,Liaxxxx Qxx;Zhixxxx Zhxxx;Hxx Zhxx;Zhixxxx Hx and Erxx Xxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Oral Tuesday,,Undecided (Discourse Pragmatics),"Implicit discourse relation classification is of great challenge due to the
lack of connectives as strong linguistic cues, which motivates the use of
annotated implicit connectives to improve the recognition. We propose a feature
imitation framework in which an implicit relation network is driven to learn
from another neural network with access to connectives, and thus encouraged to
extract similarly salient features for accurate classification. We develop an
adversarial model to enable an adaptive imitation scheme through competition
between the implicit network and a rival feature discriminator. Our method
effectively transfers discriminability of connectives to the implicit features,
and achieves state-of-the-art performance on the PDTB benchmark.",21 Apr 2017 15:42:35 GMT,Empirical/Data-Driven,Discourse and pragmatics,,Liaxxxx,Qxx,xxxxxxxxxxxsjtu.edu.cn,"Department of Computer Science and Engineering, Shanghai Jiao Tong University",No,Zhixxxx,Zhxxx,xxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong University,No,Hxx,Zhxx,xxxxxxxxxxxsjtu.edu.cn,Shanghai Jiao Tong University,No,Zhixxxx,Hx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Erxx,Xixx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Liaxxxx,Qxx,"Department of Computer Science and Engineering, Shanghai Jiao Tong University",,,,,,xxxxxxxxxx9@gmail.com,,,,,China,,Liaxxxx Qxx;Zhixxxx Zhxxx;Hxx Zhxx;Zhixxxx Hx;Erxx Xixx,xxxxxxxxxxxsjtu.edu.cn;xxxxxxxxxxjtu.edu.cn;xxxxxxxxxxx.sjtu.edu.cn;xxxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu,Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification,Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification,12,Zhiting Hu,,"Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA 15213",on,,No. Do not include my submission in this dataset.,No,None,None
148,148X-G9D6F8B4J4,Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability,Saxx Sugxxxxx;Yuxxxx Kixx;Hixxxx Yoxxxx and Akxxx Aixxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Accept - Oral Tuesday,,Undecided (Resources Evaluation),"Knowing the quality of reading comprehension (RC) datasets is important for the
development of natural-language understanding systems.
  In this study, two classes of metrics were adopted for evaluating RC
datasets: prerequisite skills and readability. We applied these classes to six
existing datasets, including MCTest and SQuAD, and highlighted the
characteristics of the datasets according to each metric and the correlation
between the two classes.
  Our dataset analysis suggests that the readability of RC datasets does not
directly affect the question difficulty and that it is possible to create an RC
dataset that is easy to read but difficult to answer.",23 Apr 2017 11:48:19 GMT,Resources/Evaluation,Resources and evaluation,,Saxx,Sugxxxxx,xxxxxxxi.ac.jp,The University of Tokyo,No,Yuxxxx,Kixx,xxxxxxxxxx@gmail.com,The University of Tokyo,No,Hixxxx,Yoxxxx,xxxxxxxxxxxxxxjp.fujitsu.com,Fujitsu Laboratories Ltd.,No,Akxxx,Aixxxx,xxxxxxxxii.ac.jp,National Institute of Informatics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Saxx,Sugxxxxx,The University of Tokyo,,,,,,xxxxxxxi.ac.jp,,,,,Japan,,Saxx Sugxxxxx;Yuxxxx Kixx;Hixxxx Yoxxxx;Akxxx Aixxxx,xxxxxxxi.ac.jp;xxxxxxxxxxd@gmail.com;xxxxxxxxxxxxxx@jp.fujitsu.com;xxxxxxxxnii.ac.jp,Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability,Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability,12,Saku Sugawara,,,,on,Only include my submission if it is accepted.,No,None,None
149,149X-E3J2C5F5C2,LSTMs with X-Skip Semantic Relevance for Automatic Text Scoring,Yx Txx;Mixx Cx;Axx Tuxx and Sxx Chxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Deep learning has demonstrated tremendous potential for Automatic Text Scoring
(ATS) tasks. In this paper, we describe a new deep learning architecture based
on two core motivations. Firstly, essays are typically long sequences. As such,
the memorization capability of the long short-term memory (LSTM) network may be
insufficient. Secondly and intuitively, a logically and semantically coherent
flow of writing is a critical feature to good writing. In this case, modeling
semantic relevance across an essay can be used as a suitable approximate.
Hence, we propose X-Skip Semantic LSTM (XS-LSTM), a novel extension of the LSTM
neural network that concurrently models semantic relevance while reading text.
Namely, we model the semantic relationships between the outputs of the LSTM
using parameterized tensor compositions. Overall, we present a unified deep
learning architecture that is trained end-to-end without requiring any feature
engineering. Our approach demonstrates state-of-the-art performance on the
benchmark ASAP dataset, defeating not only feature engineering baselines but
also other deep learning models.",4 Feb 2017 17:21:14 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",NLP applications;  educational applications;  text classification,Yx,Txx,xxxxxxxxxxntu.edu.sg,Nanyang Technological University,No,Minxxxx,Phxx,xxxxxxxxxx.ntu.edu.sg,Nanyang Technological University,No,Anhxxxxx,Lxx,xxxxxxxxxxxx-star.edu.sg,Institute for Infocomm Research,No,Siuxxxxxxx,Hxx,xxxxxxxxxtu.edu.sg,Nanyang Technological University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yx,Txx,Nanyang Technological University,,,,,,xxxxxxxxxxntu.edu.sg,,,,,Singapore,,Yx Txx;Mixx Cx;Axx Tuxx;Sxx Chxxxx,xxxxxxxxxxntu.edu.sg;xxxxxxxxxxx.ntu.edu.sg;xxxxxxxxxxxxa-star.edu.sg;xxxxxxxxxntu.edu.sg,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
150,150X-P8H5E7J2G3,Deep Character-Level Neural Machine Translation By Learning Morphology,Shexxxxx Zhxx and Zhxxxx Zhxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation aims at building a single large neural network that
can be trained to maximize translation performance. The encoder-decoder
architecture with an attention mechanism achieves a translation performance
comparable to the existing phrase-based systems. However, the use of large
vocabulary becomes the bottleneck in both training and improving the
performance. In this paper, we propose a novel architecture which learns
morphology by using two recurrent networks and a hierarchical decoder which
translates at character level. This gives rise to a deep character-level model
consisting of six recurrent networks. Such a deep model has two major
advantages. It avoids the large vocabulary issue radically; at the same time,
it is more efficient in training than word-based models and conventional
character-based models. Our model obtains a higher BLEU score than the
bpe-based model after training for one epoch on En-Fr and En-Cs translation
tasks. Moreover, the final BLEU score of our model is comparable to the
state-of-the-art systems. Further analyses show that our model is able to learn
morphology.",7 Feb 2017 11:53:56 GMT,Applications/Tools,Machine translation,MT applications;  hybrid MT;  morphology;  word segmentation,Shexxxxx,Zhxx,xxxxxxxxxx@gmail.com,Shanghai Jiao Tong University,No,Zhxxxx,Zhxxx,xxxxxxxxxxxh.pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shexxxxx,Zhxx,Shanghai Jiao Tong University,,,,,,xxxxxxxxxx@gmail.com,,,,,China,,Shexxxxx Zhxx;Zhxxxx Zhxxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxxxh.pku.edu.cn,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
151,151X-C7D2C3G7A5,Learning Intrinsic Sentiment-Expression Discrepancies Between Languages for Cross-Lingual Sentiment Analysis,Qixxx Chxx;Wexxxx Lx;Xuxx Lxx;Yx Lxx;Chxxxx Lxx and Yanxxxxx H,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Different languages express sentiments in different patterns. Discovering
correlations between cross-lingual sentiment expression patterns is the key to
cross-lingual sentiment analysis. This paper explores robust bilingual polarity
correlations (RBPCs) which are defined as intrinsic sentiment expression
discrepancies between languages. We aim to learn RBPCs by modeling the
discrepancies over labeled training data and use the learned RBPCs to identify
the polarity of test data. We propose two relation-based bilingual sentiment
translation models to learn RBPCs, both translating parallel sentiments across
languages. Experiments on a benchmark dataset demonstrate the advantages of the
proposed models.",6 Feb 2017 14:21:58 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  cross-lingual approaches;  domain adaptation;  cross-language information extraction;  text classification,Qixxx,Chxx,xxxxxxxxu.edu.cn,"School of Computer Science, Wuhan University",No,Wexxxx,Lx,xxxxxxxxxxxxpolyu.edu.hk,The Hong Kong Polytechnic University,No,Xuxx,Lxx,xxxxxxxxxhu.edu.cn,"School of Computer Science, Wuhan University",No,Yx,Lxx,xxxxxxxxxxxxpolyu.edu.hk,"Department of Computing, The Hong Kong Polytechnic University",No,Chxxxx,Lxx,xxxxxxxxxxwhu.edu.cn,"School of Computer Science, Wuhan University",No,Yanxxxxx,Hx,xxxxxxxu.edu.cn,"School of Computer Science, Wuhan University",No,,,,,,,,,,,,,,,,,,,,,,Qixxx,Chxx,Tencent AI Lab,,,+86 1xxxxxxxxxx,,,xxxxxxxxu.edu.cn,,Wuhan,Hubei,,China,"Main research interests:
Language Modelling,
Cross-lingual Sentiment Analysis,
Relation Embedding,
Text Mining,
Deep Learning Applications, etc.",Qixxx Chxx;Wexxxx Lx;Xuxx Lxx;Yx Lxx;Chxxxx Lxx;Yanxxxxx Hx,xxxxxxxxu.edu.cn;xxxxxxxxxxxx.polyu.edu.hk;xxxxxxxxxwhu.edu.cn;xxxxxxxxxxxx.polyu.edu.hk;xxxxxxxxxx@whu.edu.cn;xxxxxxxxu.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
152,152X-C6P7B3F9G7,FREERL:Fusion Relation Embodied Representation Learning Framework for Aspect Extraction,Jixx Lixx;Suxx Waxx;Dexx Lx and Xixxxx L,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Opinion object and attribute extraction is one of the core tasks of
fine-grained sentiment analysis. In this paper, we propose the FREERL
framework, which aims at embedding both the semantic structure and language
expression feature information into the representation of an opinion object and
its corresponding attribute simultaneously. Using this framework, we can freely
combine any type of language expression feature measurements as weights with a
structure-based embedding, and we can learn the fusion relation representation
of each entity. The trained representations of the aspect entities and
relations can be used to align the object-attribute pairs and predict new pairs
in a zero-shot scenario. Experiments on the datasets show that our best method
achieve significant improvements compared with the baseline.",4 Feb 2017 09:08:12 GMT,Applications/Tools,"Information extraction, text mining, and question answering",opinion mining and extraction;  semantic relations,Jixx,Lixx,xxxxxxxxxxter@163.com,"School of Computer & Information Technology, Shanxi University",No,Suxx,Waxx,xxxxxxx.edu.cn,"School of Computer & Information Technology, Shanxi University;Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University",No,Dexx,Lx,xxxxxxxu.edu.cn,"School of Computer & Information Technology, Shanxi University;Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University",No,Xixxxx,Lx,xxxxxxxxxxxstar.edu.sg,"Institute for Infocomm Research, A*STAR, Singapore",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixx,Lixx,"School of Computer & Information Technology, Shanxi University",,,,,,xxxxxxxxxxter@163.com,,Taiyuan,Shanxi,,China,,Jixx Lixx;Suxx Waxx;Kxx Labxxxxxxx;Dexx Lx;Kxx Labxxxxxxx;Xixxxx Lx,xxxxxxxxxxter@163.com;xxxxxxxu.edu.cn;xxxxxxxxu.edu.cn;xxxxxxxxxxx-star.edu.sg,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
154,154X-F3J5J6J3C6,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,Jinxxxx Zhxxx;Minxxxxx Waxx;Qxx Lxx and Jxx Zxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Monday,,Undecided (Machine Translation),"This paper proposes three distortion models to explicitly incorporate the word
reordering knowledge into attention-based Neural Machine Translation (NMT) for
further improving translation performance. Our proposed models enable attention
mechanism to attend to source words regarding both the semantic requirement and
the word reordering penalty. Experiments on Chinese-English translation show
that the approaches can improve word alignment quality and achieve significant
translation improvements over a basic attention-based NMT by large margins.
Compared with previous works on identical corpora, our system achieves the
state-of-the-art performance on translation quality.",23 Apr 2017 09:12:24 GMT,Empirical/Data-Driven,Machine translation,,Jinxxxx,Zhxxx,xxxxxxxxxxxo@ict.ac.cn,"Institute of Computing Technology, Chinese Academy of Sciences",No,Minxxxxx,Waxx,xxxxxxxxxxxn@ict.ac.cn,"Institute of Computing Technology, Chinese Academy of Sciences",No,Qxx,Lxx,xxxxxxx@dcu.ie,Dublin City University,No,Jxx,Zhxx,xxxxxxxxx@baidu.com,Baidu Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jinxxxx,Zhxxx,"Institute of Computing Technology, Chinese Academy of Sciences",,,1501xxxxxxx,,,xxxxxxxxxxxo@ict.ac.cn,,Beijing,Beijing,,China,,Jinxxxx Zhxxx;Minxxxxx Waxx;Qxx Lxx;Jxx Zhxx,xxxxxxxxxxxo@ict.ac.cn;xxxxxxxxxxxan@ict.ac.cn;xxxxxxxu@dcu.ie;xxxxxxxxxx@baidu.com,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,11,Jinchao Zhang,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,"Institute of Computing Technology, Chinese Academy of Sciences",,,No. Do not include my submission in this dataset.,No,None,None
155,155X-G3C6C3D2B6,Language Independent Named Entity Recognition with Convolutional Neural Networks,Chxxxx Waxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Most state-of-the-art models for named entity recognition (NER) rely on
recurrent neural networks (RNNs), in particular long short-term memory (LSTM).
Those models learn local and global features automatically by RNNs so that
hand-craft features can be droped, totally or partly. Recently, convolutional
neural networks (CNNs) have achieved great success on computer vision. However,
for NER problems, they are not well studied.
  In this work, we propose a novel architecture for NER problems based on CNN.
Compared with RNN based NER models, our proposed model has a remarkable
advantage on computational efficiency.
  We evaluate our model on three data sets in two significantly different
languages -- SIGHAN bakeoff 2006 MSRA portion for simplified Chinese NER and
CityU portion for traditional Chinese NER, CoNLL 2003 shared task English
portion for English NER. Our model obtains state-of-the-art performance on
these three data sets.",7 Feb 2017 08:01:55 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",named entity recognition,Chxxxx,Waxx,xxxxxxxxxxx015@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chxxxx,Waxx,Alibaba Group,,,+86-1xxxxxxxxxx,,,xxxxxxxxxxxxxlibaba-inc.com,,Beijing,Beijing,,China,,Chxxxx Waxx,xxxxxxxxxxx015@ia.ac.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
156,156X-P8C4F7H6A2,Short-Text Conceptualization Based on A Co-Ranking Framework,yaxxxx waxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"The problem of short-text conceptualization is important, and has attracted
increasing attention. Recent probabilistic algorithms have demonstrated
remarkable successes. However most of them are limited to the assumption that
all the observed terms in given short-text are conditionally independent,
ignoring the interaction among terms (and concepts), as well as the beneficial
reactions from concepts to terms. Therefore, previous works could not release
global concept representation. Faced with this problem, this paper proposes a
novel framework for co-ranking terms and their corresponding concepts
simultaneously, using several networks: the concept network, the term network
and the subordination network. As a result, improved rankings of terms and
their concepts depend on each other in a mutually reinforcing way, thus taking
advantage of the additional information implicit in such heterogeneous network
of terms and concepts. The experimental results show that our method achieves
higher accuracy and efficiency in short-text conceptualization than the
state-of-the-art algorithms.",7 Feb 2017 02:27:18 GMT,Empirical/Data-Driven,Semantics,graph-based algorithms;  semantic knowledge induction,yaxxxx,waxx,xxxxxxxxit.edu.cn,Beijing Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,yaxxxx,waxx,Beijing Institute of Technology,,,,,,xxxxxxxxit.edu.cn,,,,,China,,yaxxxx waxx,xxxxxxxxit.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
157,157X-A2C4D6B3C6,Bootstrapping a Verbal Shifter Lexicon using Linguistic Features,Maxx Schxxxxx;Micxxxx Wiexxxx;Joxxx Ruppxxxxxxx and Benxxxxx Rxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"We bootstrap the first high-coverage lexicon of verbal polarity shifters by
exploiting various linguistic features. Verbal polarity shifters, such as
""abandon"", are similar to negations (e.g. ""not"") in that they move the polarity
of a phrase towards its inverse, as in ""abandon all hope"". While there exist
lists of negation words, creating comprehensive lists of verbal polarity
shifters is a lot more challenging due to the sheer number of existing verbs.
On a sample of manually annotated verbs extracted from WordNet, we examine a
variety of linguistic features for this task. Then we build a supervised
classifier to classify the remaining WordNet verbs. We show that with this
approach we can drastically reduce the number of verbs to be categorized
manually. We also show that our acquired knowledge of verbal polarity shifters
improves phrase-level sentiment analysis.",7 Feb 2017 11:56:51 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  lexical semantics;  lexicon development;  learning with small datasets;  lexical paraphrasing;  opinion mining and extraction,Maxx,Schxxxxx,xxxxxxxxxxxxxxxxv.uni-saarland.de,Saarland University,No,Micxxxx,Wiexxxx,xxxxxxxxxxxxxxxxxsv.uni-saarland.de,Saarland University,No,Joxxx,Ruppxxxxxxx,xxxxxxxxxxxxxds-mannheim.de,Institute for German Language,No,Benxxxxx,Roxx,xxxxxxxxxxxxxi-muenchen.de,LMU Munich,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxx,Schxxxxx,Saarland University,,,,,,xxxxxxxxxxxxxxxxv.uni-saarland.de,,,,,Germany,,Maxx Schxxxxx;Micxxxx Wiexxxx;Joxxx Ruppxxxxxxx;Benxxxxx Roxx,xxxxxxxxxxxxxxxxv.uni-saarland.de;xxxxxxxxxxxxxxxxxxsv.uni-saarland.de;xxxxxxxxxxxxxxds-mannheim.de;xxxxxxxxxxxxxni-muenchen.de,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
158,158X-G4G3F2F7A8,Sentence-Level Attention Neural Network for Chinese Reading Comprehension,Yuaxxxxx Waxx and Rx Lx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Comprehension of unstructured text by machines is one of the primary challenges
in NLP research. In this work, we investigate Reading Comprehension on MCTest
and CNN/Daily Mail.  Prior work is mainly based on understanding of words. We
come up with a sentence- level attention neural network to understand
sentences. As we all know, for reading comprehension task, sentence
comprehension is more important than word comprehension. Specifically, the
Chinese reading comprehension dataset  is released firstly for multiple-choice
reading comprehension task. Then we present a neural network model for solving
the task, called sentence-level attention reader. Our model generates
a“sentence attention” between the optional sentence and the sentence of
document. And the answer is estimated by the softmax function for final
predictions. Experimental results show that our model significantly outperforms
various state-of-the-art baselines in three test sets.",5 Feb 2017 09:54:06 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",NLP applications;  context-aware question answering;  distributional similarity;  question interpretation;  question answering in restricted domains,Yuaxxxxx,Waxx,xxxxxxxxxu.edu.cn,ShanXi University,No,Rx,Lx,xxxxxxxu.edu.cn,ShanXi University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuaxxxxx,Waxx,ShanXi University,,,,,,xxxxxxxxxu.edu.cn,,,,,China,,Yuaxxxxx Waxx;Rx Lx,xxxxxxxxxu.edu.cn;xxxxxxxxu.edu.cn,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
159,159X-D2D7D2E8J3,DeepAlignment: Unsupervised Ontology Matching With Refined Word Vectors,Proxxxxxx Kolxxxxxx; Axxx and rxx Kalxxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Ontologies compartmentalize concepts and relations in a target domain and
provide the semantic backbone needed for a plethora of practical applications.
Very often different ontologies are developed independently for the same
domain. Such ""parallel"" ontologies raise the need for a process that will
establish alignments between their entities if one wants to exploit them in the
best manner.
In this work, we present a novel entity alignment method which we dub Deep
Alignment. DeepAlignment incorporates domain knowledge and information
extracted from semantic lexicons as well as from the ontologies over which the
matching is performed. 
We empirically evaluate our method on standard ontology matching benchmarks and
demonstrate significant performance improvements over the current
state-of-the-art.",4 Feb 2017 15:02:13 GMT,Applications/Tools,Machine learning,semantic relations;  alignment;  ontological semantics,Proxxxxxx,Kolxxxxxx,xxxxxxxxxxxxxyvakis@epfl.ch,"École polytechnique fédérale de Lausanne (EPFL), Lausanne, Switzerland",No,Alexxxxxxx,Kalxxxxx,xxxxxxxxxxxxxxousis@unige.ch,"Business Informatics Department, University of Applied Sciences, Western Switzerland, Carouge, Switzerland",No,Dimxxxxx,Kirxxxxx,xxxxxxxxxxxxitsis@epfl.ch,"École polytechnique fédérale de Lausanne (EPFL), Lausanne, Switzerland",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proxxxxxx,Kolxxxxxx,"École polytechnique fédérale de Lausanne (EPFL), Lausanne, Switzerland",,,,,,xxxxxxxxxxxxxyvakis@epfl.ch,,Lausanne,VD,,Switzerland,,Proxxxxxx Kolxxxxxx;Alexxxxxxx Kalxxxxx;Dimxxxxx Kirxxxxx and ,xxxxxxxxxxxxxyvakis@epfl.ch;xxxxxxxxxxxxxxlousis@unige.ch;xxxxxxxxxxxxxitsis@epfl.ch,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
160,160X-F3F4P5C2C6,HAN: Hierarchical Association Network for Computing Semantic Relatedness,Xiaxxxxx Goxx;Linxxxx Huxxx and Hxx X,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"Measuring semantic relatedness between two words is a deep problem in many
areas such as natural language processing. Existing approaches to semantic
relatedness problem mainly adopt the co-occurrence principle and regard two
words as highly related if they appear in the same sentence frequently.
However, such solutions suffer from low coverage and low precision because i)
two highly related words may not appear close to each other in the sentences,
e.g., the synonyms; and ii) the co-occurrence of words may happen by chance
rather than imply the closeness in their semantics. 

In this paper, we explore the latent semantics (i.e., concepts) of the words to
identify highly related word pairs. We propose a hierarchical association
network to specify the complex relationships among the words and the concepts,
and quantify each relationship with appropriate measurements. Extensive
experiments are conducted on real datasets and the results show that our
proposed method improves correlation precision compared with the
state-of-the-art approaches.",6 Feb 2017 13:23:34 GMT,Theoretical,Cognitive modeling and psycholinguistics,lexical semantics;  semantic relations;  word sense induction,Xiaxxxxx,Goxx,xxxxxxxxxxsjtu.edu.cn,Shanghai Jiao Tong University,No,Linxxxx,Huxxx,xxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong Uniersity,No,Hxx,Xx,xxxxxxxxxxsjtu.edu.cn,Shanghai Jiao Tong Uniersity,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxxx,Goxx,Shanghai Jiao Tong University,,,,,,xxxxxxxxxxsjtu.edu.cn,,,,,China,,Xiaxxxxx Goxx;Linxxxx Huxxx;Hxx Xx,xxxxxxxxxxsjtu.edu.cn;xxxxxxxxxxjtu.edu.cn;xxxxxxxxxxxsjtu.edu.cn,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
161,161X-J8A5G9C3G2,Deep Neural Machine Translation with Linear Associative Unit,Minxxxxx Waxx;Zhexxxxxx Lx;Jxx Zhxx and Qxx Lx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Oral Monday,,Undecided (Machine Translation),"Deep Neural Networks (DNNs) have provably enhanced the
state-of-the-art Neural  Machine Translation (NMT) with
 its capability in modeling complex functions and capturing
  complex linguistic structures.
  However NMT with deep architecture in its encoder or
  decoder RNNs often suffer from severe gradient diffusion
  due to the non-linear recurrent activations, which often
  makes the optimization much more difficult.
   To address this problem we propose a novel linear
   associative units (LAU)  to reduce the gradient
    propagation path inside the recurrent unit.
    Different from conventional
    approaches (LSTM unit and GRU),
   LAUs uses linear associative connections
   between input and
   output of the recurrent unit,
   which allows unimpeded information flow through both
    space and time  The model is quite simple,
     but it is surprisingly effective. Our empirical
     study on Chinese-English translation shows that our
     model with proper configuration can improve
      by 11.7 BLEU upon Groundhog and the best
      reported on results in the same setting.
      On WMT14 English-German task and a larger WMT14
       English-French task, our
 model achieves comparable results with the state-of-the-art.",22 Apr 2017 09:23:44 GMT,Empirical/Data-Driven,Machine translation,,Minxxxxx,Waxx,xxxxxxxxxxxn@ict.ac.cn,"Institute of Computing Technology, Chinese Academy of Sciences",No,Zhexxxxxx,Lx,xxxxxxxxxxxg@huawei.com,"Noah's Ark Lab, Huawei",No,Jxx,Zhxx,xxxxxxxxx@baidu.com,Baidu,No,Qxx,Lxx,xxxxxxx@dcu.ie,Dublin City University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minxxxxx,Waxx,Tencent Technology,,,1340xxxxxxx,,,xxxxxxxxxxtencent.com,,,,,China,,Minxxxxx Waxx;Zhexxxxxx Lx;Jxx Zhxx;Qxx Lxx,xxxxxxxxxxxn@ict.ac.cn;xxxxxxxxxxxxg@huawei.com;xxxxxxxxxx@baidu.com;xxxxxxxu@dcu.ie,Deep Neural Machine Translation with Linear Associative Unit,Deep Neural Machine Translation with Linear Associative Unit,10,,,,on,,No. Do not include my submission in this dataset.,No,None,None
164,164X-D8P6A3J3E7,Learning to Ask: Neural Question Generation for Reading Comprehension,Xixxx Dx;Juxxx Shxx and Clxxxx Caxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Poster Monday,,Undecided (Generation Summarization),"We study automatic question generation for sentences from text passages in
reading comprehension. We introduce an attention-based sequence learning model
for the task and investigate the effect of encoding sentence- vs.
paragraph-level information. In contrast to all previous work, our model does
not rely on hand-crafted rules or a sophisticated NLP pipeline;  it is instead
trainable end-to-end via sequence-to-sequence learning. Automatic evaluation
results show that our system significantly outperforms the state-of-the-art
rule-based system. In human evaluations, questions generated by our system are
also rated as being more natural (\ie, grammaticality, fluency) and as more
difficult to answer (in terms of syntactic and lexical divergence from the
original text and reasoning needed to answer).",23 Apr 2017 03:57:19 GMT,Empirical/Data-Driven,Generation,,Xixxx,Dx,xxxxxxxxnell.edu,Cornell University,No,Juxxx,Shxx,xxxxxxxxxtu.edu.cn,Shanghai Jiao Tong University,No,Clxxxx,Caxxxx,xxxxxxxxxxcornell.edu,Cornell University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixxx,Dx,Cornell University,,,,,,xxxxxxxxnell.edu,,,,,United States,,Xixxx Dx;Juxxx Shxx;Clxxxx Caxxxx,xxxxxxxxnell.edu;xxxxxxxxxjtu.edu.cn;xxxxxxxxxxxcornell.edu,Learning to Ask: Neural Question Generation for Reading Comprehension,Learning to Ask: Neural Question Generation for Reading Comprehension,11,Xinya Du,student,"Name: Department of Computer Science, Cornell University.

Address: Gates Hall, Cornell University, Ithaca, NY 14853",on,on,No. Do not include my submission in this dataset.,No,None,None
165,165X-A2D2E3D7E7,Multilingual Hierarchical Attention Networks for Document Classification,Nikxxxxx Paxxxx and Anxxxx Popexxxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Hierarchical attention networks have recently achieved remarkable performance
for document classification in a given language.  However, when multilingual
document collections are considered, training such models separately for each
language entails linear parameter growth and lack of cross-language transfer.
Learning a single multilingual model with fewer parameters is therefore a
challenging but potentially beneficial objective. To this end, we propose
multilingual hierarchical attention networks to learn document structures with
shared encoders and/or attention mechanisms across languages, using multi-task
learning and an aligned semantic space as input.  We evaluate the proposed
models on multilingual document classification with disjoint label sets, on a
large dataset which we provide, with 600k news documents in 8 languages, and 5k
labels.  The multilingual models outperform strong monolingual ones in
low-resource as well as full-resource settings using fewer parameters, thus
confirming their computational efficiency and the utility of cross-language
transfer.",6 Feb 2017 23:02:45 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",multiword semantics/compositionality;  scalability/efficiency of ML methods;  multilingual applications;  discriminative learning methods;  multilingual resources;  text mining;  text classification;  document mining,Nikxxxxx,Paxxxx,xxxxxxxxxxxxpas@idiap.ch,Idiap Research Institute,No,Anxxxx,Popexxxxxxxxx,xxxxxxxxxxxxxx-belis@idiap.ch,Idiap Research Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nikxxxxx,Paxxxx,Idiap Research Institute,,,4127xxxxxxx,,,xxxxxxxxxxxxpas@idiap.ch,,,,,Switzerland,,Nikxxxxx Paxxxx;Anxxxx Popexxxxxxxxx,xxxxxxxxxxxxpas@idiap.ch;xxxxxxxxxxxxxxx-belis@idiap.ch,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
166,166X-B7E9J4G6A6,Multimodal Classification Fusion in Real-World Scenarios,Ignxxxx Gaxxx; Axxxx and rx Calxxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"In this paper, we propose a multimodal setting in real-world scenario based on
weighting and meta-learning combination methods that combine the output
probabilities obtained from textual and visual classifiers.  
While the classifer built on the concatenation of textual and visual features
can lead to a worsening of the results, on the contrary, the model that we
propose, can increase by more than 6% classification accuracy.
Text and images usually contain information describing the same objects or
concepts, so to resolve ambiguous situations it is very useful to combine them.
In our approach textual classifier is trained on Bag of Words and visual
classifier is trained on feature extracted using a Deep Conventional Neural
Network.
We propose a new dataset of real-world text and images, called Ferramenta. 
Experimental results reported on Ferramenta and PASCAL VOC2007 datasets
indicate that the proposed methods combination performs better in a multimodal
setting.",10 Feb 2017 02:27:56 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",meta-learning and combination;  multimodal representations and processing;  text classification,Ignxxxx,Gaxxx,xxxxxxxxxxxxx@uninsubria.it,University of Insubria,No,Alexxxxxxx,Calxxxxx,xxxxxxxxxxxxxxxxnti.uninsubria.it,University of Insubria,No,Naxxx,Shxx,xxxxxxxxxxnsubria.it,University of Insubria,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ignxxxx,Gaxxx,University of Insubria,,,,,,xxxxxxxxxxxxx@uninsubria.it,,,,,Italy,,Ignxxxx Gaxxx;Alexxxxxxx Calxxxxx;Naxxx Shxx,xxxxxxxxxxxxx@uninsubria.it;xxxxxxxxxxxxxxxxxnti.uninsubria.it;xxxxxxxxxxinsubria.it,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
167,167X-C3G6G5E3F4,Towards a Seamless Integration of Word Senses into Downstream NLP Applications,Mohxxxxx Taxxx;Joxx Camacxxxxxxxxxxx;Robxxxx Navxxxx and Nixxx Coxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"Lexical ambiguity can impede NLP systems from accurate understanding of
semantics. Despite its potential benefits, the integration of sense-level
information into NLP systems has remained understudied. By incorporating a
novel disambiguation algorithm into a state-of-the-art classification model, we
create a pipeline to integrate sense-level information into downstream NLP
applications. We show that a simple disambiguation of the input text can lead
to consistent performance improvement on multiple topic categorization and
polarity detection datasets, particularly when the fine granularity of the
underlying sense inventory is reduced and the document is sufficiently large.
Our results also point to the need for sense representation research to focus
more on in vivo evaluations which target the performance in downstream NLP
applications rather than artificial benchmarks.",22 Apr 2017 19:07:37 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",,Mohamxxxxxxxxx,Pilxxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Joxx,Camacxxxxxxxxxxx,xxxxxxxxxxx.uniroma1.it,Sapienza University of Rome,No,Robxxxx,Navxxxx,xxxxxxxxxxxuniroma1.it,Sapienza University of Rome,No,Nixxx,Colxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mohamxxxxxxxxx,Pilxxxxx,University of Cambridge,,,,,,xxxxxxxam.ac.uk,,Cambridge,,,United Kingdom,,Mohxxxxx Taxxx;Joxx Camacxxxxxxxxxxx;Robxxxx Navxxxx;Nixxx Colxxxx,xxxxxxxam.ac.uk;xxxxxxxxxxxx.uniroma1.it;xxxxxxxxxxx.uniroma1.it;xxxxxxxxam.ac.uk,Towards a Seamless Integration of Word Senses into Downstream NLP Applications,Towards a Seamless Integration of Word Senses into Downstream NLP Applications,13,Mohammadtaher Pilehvar,Research Associate,University of Cambridge,on,on,No. Do not include my submission in this dataset.,No,None,None
168,168X-A2J5F9D6J9,Improving Cross-Lingual Document Alignment withWord Vectors,Pixxx Loxxx;Debxxxx Ganxxxx;Haixxxx Afxx;Gaxxxx Joxxx and Anxx Wx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"Crosslingual information retrieval (CLIR)
finds its application in aligning documents
across comparable corpora. However, traditional
CLIR, due to the term independence
assumption, cannot consider the semantic
similarity between the constituent
words of the candidate pairs of documents
in two different languages. Moreover, traditional
CLIR models score a document by
aggregating only the weights of the constituent
terms that match with those of the
query, while the other non-matching terms
of the document do not significantly contribute
to the similarity function. Word
vector embedding allows the provision
to model the semantic distances between
terms by the application of standard distance
metrics between their corresponding
real valued vectors. This paper develops
a word vector embedding based CLIR
model that uses the average distances between
the embedded word vectors of the
source and target language documents to
rank candidate document pairs. Our experiments
with the WMT bilingual document
alignment dataset reveal that the word vector
based similarity significantly improves
the recall of crosslingual document alignment
in comparison to the classical language
modeling based CLIR.",6 Feb 2017 22:24:49 GMT,Empirical/Data-Driven,Multilinguality,cross-lingual approaches;  cross-language information retrieval;  alignment,Pixxx,Loxxx,xxxxxxxxxxxx@mail.dcu.ie,Dublin City University,No,Debxxxx,Ganxxxx,xxxxxxxxxgmail.com,"IBM Research Lab, Dublin",No,Haixxxx,Afxx,xxxxxxxxxxm@gmail.com,ADAPT Centre,No,Gaxxxx,Joxxx,xxxxxxxxxxxxxxomputing.dcu.ie,Dublin City University,No,Anxx,Wxx,xxxxxxxxxxxaptcentre.ie,Dublin City University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Debxxxx,Ganxxxx,"IBM Research Lab, Dublin",,,860xxxxxx,,,xxxxxxxxxgmail.com,,,,,Ireland,,Pixxx Loxxx;Debxxxx Ganxxxx;Haixxxx Afxx;Gaxxxx Joxxx;Anxx Wxx,xxxxxxxxxxxx@mail.dcu.ie;xxxxxxxxx@gmail.com;xxxxxxxxxxxm@gmail.com;xxxxxxxxxxxxxxxomputing.dcu.ie;xxxxxxxxxxxxaptcentre.ie,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
169,169X-F8H2F3C4A4,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,Chrixxxxxxx Brxxxx;Marxxxx Fexxxx and Txx Brxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Accept - Oral Tuesday,,Undecided (Resources Evaluation),"Until now, error type performance for Grammatical Error Correction (GEC)
systems could only be measured in terms of recall because system output is not
annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor
ANnotation Toolkit designed to automatically extract edits from parallel
original and corrected sentences and classify them according to a new,
dataset-agnostic, rule-based framework. This not only facilitates error type
evaluation at different levels of granularity, but can also be used to reduce
annotator workload and standardise existing GEC datasets. Human experts rated
the automatic edits as ``Good'' or ``Acceptable'' in at least 95\% of cases, so
we applied ERRANT to the system output of the CoNLL-2014 shared task to carry
out a detailed error type analysis for the first time.",16 Apr 2017 16:02:48 GMT,Resources/Evaluation,Resources and evaluation,,Chrixxxxxxx,Brxxxx,xxxxxxxxx.cam.ac.uk,University of Cambridge,No,Marxxxx,Fexxxx,xxxxxxxxxxxxxe@cl.cam.ac.uk,University of Cambridge,No,Txx,Brixxxx,xxxxxxxxam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chrixxxxxxx,Brxxxx,University of Cambridge,,,,,,xxxxxxxxx.cam.ac.uk,,,,,United Kingdom,,Chrixxxxxxx Brxxxx;Marxxxx Fexxxx;Txx Brixxxx,xxxxxxxxx.cam.ac.uk;xxxxxxxxxxxxxxe@cl.cam.ac.uk;xxxxxxxxcam.ac.uk,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,13,Christopher Bryant,,"ALTA Institute, Computer Laboratory, University of Cambridge, Cambridge, UK",on,,"Yes, include my submission even if the paper is rejected.",No,None,None
170,170X-G6F9E7P3A8,A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data,Qiaxxxx Xxx;Lxx Sxx;Baxxxx Chxxx and Zhixxxx Sx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"Previous studies on Chinese semantic role labeling (SRL) have concentrated on a
single semantically annotated corpus. But the training data of single corpus is
often limited. Whereas the other existing semantically annotated corpora for
Chinese SRL are scattered across different annotation frameworks. But still,
Data sparsity remains a bottleneck. This situation calls for larger training
datasets, or effective approaches which can take advantage of highly
heterogeneous data. In this paper, we focus mainly on the latter, that is, to
improve Chinese SRL by using heterogeneous corpora together. We propose a novel
progressive learning model which augments the Progressive Neural Network with
Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and
effectively transfer knowledge between them. We also release a new corpus,
Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our model
outperforms state-of-the-art methods.",21 Apr 2017 02:58:32 GMT,Empirical/Data-Driven,Semantics,,Qiaxxxx,Xxx,xxxxxxx.edu.cn,Peking University,No,Lxx,Sxx,xxxxxxxxx@sina.com,Peking University,No,Baxxxx,Chxxx,xxxxxxxu.edu.cn,Peking University,No,Zhixxxx,Sxx,xxxxxxx.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qiaxxxx,Xxx,Peking University,,,,,,xxxxxxx.edu.cn,,,,,China,,Qiaxxxx Xxx;Lxx Sxx;Baxxxx Chxxx;Zhixxxx Sxx,xxxxxxx.edu.cn;xxxxxxxxx0@sina.com;xxxxxxxxu.edu.cn;xxxxxxxu.edu.cn,A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data,A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data,9,Qiaolin Xia,,"Key Laboratory of Computational Linguistics (Ministry of Education), EECS, Peking University, 100871, Beijing, China",,on,No. Do not include my submission in this dataset.,No,None,None
171,171X-G7J6A6P6A3,Text Simplification with Deep Reinforcement Learning,Xinxxxxx Zhxxx and Mirxxxx Laxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Text simplification aims to make texts easier to read and
  understand. Most recent approaches draw on insights from machine
  translation to learn simplification rewrites from monolingual
  corpora of complex and simple sentences. We address the
  simplification problem with an encoder-decoder model coupled with a
  deep reinforcement learning framework. Our model explores the space
  of possible simplifications while learning to optimize a reward
  function that encourages outputs which are simple, fluent, and
  preserve the meaning of the input. Experiments on three datasets
  demonstrate that our model brings significant improvements over the
  state of the art.\footnote{Our code and data are publicly available
    at \url{http://anonymized.url}.}",4 Feb 2017 23:52:52 GMT,Empirical/Data-Driven,Generation,language generation;  sentence simplification;  reinforcement learning,Xinxxxxx,Zhxxx,xxxxxxxxmail.com,"Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",No,Mirxxxx,Laxxxx,xxxxxxxx.ed.ac.uk,"School of Informatics, University of Edinburgh",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xinxxxxx,Zhxxx,Microsoft Research Asia,,,+86 1xxxxxxxxxx,,,xxxxxxxxmail.com,,,,,China,,Xinxxxxx Zhxxx;Mirxxxx Laxxxx,xxxxxxxxmail.com;xxxxxxxxx.ed.ac.uk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
172,172X-J4F9G3H5G2,FundingFinder: Extracting Funding Information from Text,Subxxxxxxx Kaxxx;Zuxxxx Afxxx;Gexxxx Tsatxxxxxxx;Soxxxx Katxxxxx;Saxxx Ahxxx;Paxxxx Coxxxx;Maxxxx Dooxxxxxxx;Micxxxxx Grexxxx and M'hxxxx Aixxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Many would argue that the ``currency"" of research is citations; however,
researchers and funders alike are lacking tools with which they can explore
analytically how this ``currency"" translates into funding opportunities, as
well as how it is embedded in decision making processes for assigning funds in
research areas. Motivated by this need in this paper we address one of the
fundamental problems facing the development of such tools, namely the problem
of automatically extracting funding information from scientific articles. For
this purpose we introduce FundingFinder, a novel natural language processing
and machine learning pipeline which addresses the problem by learning the way
to combine a number of base approaches for the task, in what is widely known as
an ensemble. For its experimental validation we present a comparative analysis
of several methods in a benchmark dataset that has been instituted for the
task, and that is released for the first time to the public. Results indicate
that FundingFunder can extract funding bodies and associated grants from
scientific articles with an F1 score of 68% for the former entity type and 92
for the latter.",6 Feb 2017 23:40:48 GMT,Applications/Tools,"Information extraction, text mining, and question answering",information extraction;  named entity recognition;  text mining;  NLP applications;  NLP in business application,Subxxxxxxx,Kaxxx,xxxxxxxxxxsevier.com,Elsevier B.V.,No,Zuxxxx,Afxxx,xxxxxxxxxxxlsevier.com,Elsevier B.V.,No,Gexxxx,Tsatxxxxxxx,xxxxxxxxxxxxx@elsevier.com,Elsevier B.V.,No,Soxxxx,Katxxxxx,xxxxxxxxxxxelsevier.com,Elsevier B.V.,No,Saxxx,Ahmaxxxxxxxxx,xxxxxxxxxxxlsevier.com,Elsevier B.V.,No,Paxxxx,Coxxxx,xxxxxxxxxxlsevier.com,Elsevier B.V.,No,Maxxxx,Dooxxxxxxx,xxxxxxxxxxxx@elsevier.com,Elsevier B.V.,No,Micxxxxx,Grexxxx,xxxxxxxxxxxlsevier.com,Elsevier B.V.,No,M'hxxxx,Aixxxx,xxxxxxxxxxlsevier.com,Elsevier B.V.,No,,,,,,,Gexxxx,Tsatxxxxxxx,Elsevier B.V.,,,,,,xxxxxxxxxxxxx@elsevier.com,,,,,Netherlands,,Subxxxxxxx Kaxxx;Zuxxxx Afxxx;Gexxxx Tsatxxxxxxx;Soxxxx Katxxxxx;Saxxx Ahxxx;Paxxxx Coxxxx;Maxxxx Dooxxxxxxx;Micxxxxx Grexxxx;M'hxxxx Aixxxx,xxxxxxxxxxsevier.com;xxxxxxxxxxxelsevier.com;xxxxxxxxxxxxxs@elsevier.com;xxxxxxxxxxxxelsevier.com;xxxxxxxxxxxelsevier.com;xxxxxxxxxxxlsevier.com;xxxxxxxxxxxxx@elsevier.com;xxxxxxxxxxxelsevier.com;xxxxxxxxxxxlsevier.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
173,173X-P2A9C5P5C6,Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering,Jixxxx Yx;Yaxxxx Lx;Zhaxxxx Wx;Jaxxx Zx;Wexxxx Lx and Jxx L,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"Word embeddings have become widely-used in document analysis. While a large
number of models for mapping words to vector spaces have been developed, it
remains undetermined how much net gain can be achieved over traditional
approaches based on bag-of-words. In this paper, we propose a new document
clustering approach by combining any word embedding with a state-of-the-art
algorithm for clustering empirical distributions. By using the Wasserstein
distance between distributions, the word-to-word semantic relationship is taken
into account in a principled way. The new clustering method is easy to use and
consistently outperforms other methods on a variety of data sets. More
importantly, the method provides an effective framework for determining when
and how much word embeddings contribute to document analysis. Experimental
results with multiple embedding models are reported.",10 Apr 2017 20:59:16 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Jixxxx,Yx,xxxxxxxxxt.psu.edu,Penn State University,No,Yaxxxx,Lx,xxxxxxxxxxxxmer@gmail.com,The Hong Kong Polytechnic University,No,Zhaxxxx,Wx,xxxxxxxxmail.com,Microsoft,No,Jamxxxxx,Waxx,xxxxxxxxt.psu.edu,Penn State University,No,Wexxxx,Lx,xxxxxxxxxxxxpolyu.edu.hk,The Hong Kong Polytechnic University,No,Jxx,Lx,xxxxxxxxxt.psu.edu,Penn State University,No,,,,,,,,,,,,,,,,,,,,,,Jixxxx,Yx,Penn State University,,,,,,xxxxxxxxxt.psu.edu,,,,,United States,,Jixxxx Yx;Yaxxxx Lx;Zhaxxxx Wx;Jaxxx Zx;Wexxxx Lx;Jxx Lx,xxxxxxxxxt.psu.edu;xxxxxxxxxxxxxmer@gmail.com;xxxxxxxxgmail.com;xxxxxxxxxt.psu.edu;xxxxxxxxxxxx.polyu.edu.hk;xxxxxxxxxat.psu.edu,Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering,Determining Gains Acquired from Word Embedding Quantitatively Using D2-Clustering,10,Jianbo Ye,,"Penn State University, University Park, PA 16802",on,on,Only include my submission if it is accepted.,No,None,None
174,174X-C3C3A6H4E7,Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks,Haxx Amxxx;Timxxxx Mixxxx and Guexxxxx Saxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"We present a novel approach for training artificial neural networks. 
Our approach is inspired by broad evidence in psychology that show human 
learners can learn efficiently and effectively by increasing intervals of 
time between subsequent reviews of previously learned materials (spaced 
repetition). We investigate the analogy between training neural 
models and findings in psychology about human memory model and 
develop an efficient and effective algorithm to train neural models. 
The core part of our algorithm is a scheduler according to which training 
instances and their ""reviews"" are spaced over time. Our algorithm uses 
less than 34\% of data while producing similar performance to 
the state-of-the-art systems.",7 Feb 2017 04:20:06 GMT,Empirical/Data-Driven,Semantics,scalability/efficiency of ML methods;  semantic relations,Haxx,Amxxx,xxxxxxxxgmail.com,University of Maryland,No,Timxxxx,Mixxxx,xxxxxxxxxxxxxxxxxxldrens.harvard.edu,Boston Children's Hospital and Harvard Medical School,No,Guexxxxx,Saxxxx,xxxxxxxxxxxxxxxxxxildrens.harvard.edu,Harvard,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Haxx,Amxxx,Harvard University,,,301-xxxxxxxx,,,xxxxxxxxgmail.com,,College Park,MD,,United States,,Haxx Amxxx;Timxxxx Mixxxx;Guexxxxx Saxxxx,xxxxxxxxgmail.com;xxxxxxxxxxxxxxxxxxildrens.harvard.edu;xxxxxxxxxxxxxxxxxxxildrens.harvard.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
175,175X-B2F9F5G8H2,Constructing Topic-Adaptive Sentiment Lexicon for Sentiment Classification,Doxx Dexx;Lixxxx Jixx and Jixx Y,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In this paper, we present a new method to construct topic-adaptive sentiment
lexicon (TaSL). TaSL simultaneously models the topic and sentiment from the
training data. Each word is generated by a multinomial distribution over topics
and sentiments under the supervision of document's sentiment label, so that the
sentiment polarity under different topics can be sufficiently captured. This
model is beneficial to construct the domain-specific sentiment lexicon and then
effectively improve the performance of sentiment classification. The
experiments on four publicly available dataset (MR, OMD, semEval13 and
semEval16) were conducted and the results have shown that TaSL performs better
than the existing topic model (ssLDA), the expanded lexicon based on corpus
(Weka-ED and Weka-STS), and deep learning-based lexicon (NNLexicon).",6 Feb 2017 03:56:10 GMT,Theoretical,Sentiment analysis and opinion mining,sentiment analysis;  generative models,Doxx,Dexx,xxxxxxxxxxjtu.edu.cn,Beijing Jiaotong University,No,Lixxxx,Jixx,xxxxxxxxxtu.edu.cn,Beijing Jiaotong Univesity,No,Jixx,Yx,xxxxxxxxxtu.edu.cn,Beijing Jiaotong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lixxxx,Jixx,Beijing Jiaotong Univesity,,,,,,xxxxxxxxxtu.edu.cn,,,,,China,,Doxx Dexx;Lixxxx Jixx;Jixx Yx,xxxxxxxxxxjtu.edu.cn;xxxxxxxxxjtu.edu.cn;xxxxxxxxxjtu.edu.cn,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
176,176X-C3H7H6D2J5,On Relation between Semantic Density and Frequency of Words,Lx Faxx and Xiaxxxx WAxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"This paper investigates relations between word semantic density and word
frequency. A distributed representations based word average similarity is
defined as the measure of word semantic density. We find that the average
similarities of low-frequency words are always bigger than that of
high-frequency words, when the frequency approaches to 400 around, the average
similarity tends to stable. The finding keeps correct with changes of the size
of training corpus, dimension of distributed representations and number of
negative samples in skip-gram model. It also keeps on 17 different languages.
Clustered results on average similarities of different languages are
interesting as well. Basing on the finding, we define a rank-based measure for
word similarity evaluation. Experimental results on three publicly available
datasets indicate the measure is better than cosine measure.",5 Feb 2017 12:56:58 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  mixed-language;  distributional similarity;  semantic relations;  evaluation metrics,Lx,Faxx,xxxxxxxxxxupt.edu.cn,Beijing University of Posts and Telecommunication,No,Xiaxxxx,WAxx,xxxxxxxxxpt.edu.cn,Beijing University of Posts and Telecommunications,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lx,Faxx,Beijing University of Posts and Telecommunication,,,,,,xxxxxxxxxxupt.edu.cn,,,,,China,,Lx Faxx;Xiaxxxx WAxx and  Telecxxxxxxxxxxxxxx,xxxxxxxxxxupt.edu.cn;xxxxxxxxxupt.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
178,178X-B2F3B5F8P3,"A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words",Dexxx Newmaxxxxxxxxx;Alxxxx Lxx and Erxx Foslxxxxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Recent work on embedding ontology concepts has relied on either expensive
manual annotation or automated concept tagging methods that ignore the textual
contexts around concepts.  We propose a novel method for jointly learning
concept, phrase, and word embeddings from an unlabeled text corpus, by using
the representative phrases for ontology concepts as distant supervision.  We
learn embeddings for medical concepts in the Unified Medical Language System
and general-domain concepts in YAGO, using various corpora.  Our embeddings
show performance competitive with existing methods on concept similarity and
relatedness tasks, while requiring no human corpus annotation and demonstrating
more than 3x coverage in the vocabulary size.",7 Feb 2017 06:49:51 GMT,Theoretical,Semantics,multiword semantics/compositionality;  lexical semantics;  distributional similarity;  biomedical text mining;  ontological semantics,Dexxx,Newmaxxxxxxxxx,xxxxxxxxx0@osu.edu,The Ohio State University,No,Alxxxx,Lxx,xxxxxxxustl.edu,Washington University in St. Louis,No,Erxx,Foslexxxxxxxxx,xxxxxxxxxxxxhio-state.edu,The Ohio State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dexxx,Newmaxxxxxxxxx,The Ohio State University,,,,,,xxxxxxxxxxxxis.1@osu.edu,,Columbus,OH,,United States,,Dexxx Newmaxxxxxxxxx;Alxxxx Lxx;Erxx Foslexxxxxxxxx,xxxxxxxxx0@osu.edu;xxxxxxxxustl.edu;xxxxxxxxxxxxxhio-state.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
179,179X-G9A4D3B5C3,Identifying Humor in Reviews using External Knowledge,Alxx Morxxxx and Chexxxxxxx Zhxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We study the problem of automatically
identifying humorous text from a new kind
of text data, i.e., online reviews and propose
a generative language model based
on the theory of incongruity to model humorous
text, which allows us to leverage
external knowledge for modeling the incongruity
in reviews probabilistically and
enables construction of multiple features
for identifying humorous reviews. Evaluation
of these features using supervised
learning for classifying reviews into humorous
and non-humorous reviews shows
that the features constructed based on the
proposed generative model are much more
effective than the major features proposed
in the existing literature, allowing us to
achieve almost 86% accuracy and that humorous
review prediction can supply good
indicators for identifying helpful reviews
for consumers.",7 Feb 2017 09:03:34 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",generative models;  verbally expressed humor;  NLP applications;  NLP on noisy unstructured text;  opinion representation;  text mining;  text classification,Alxx,Morxxxx,xxxxxxxxxxllinois.edu,UIUC,No,Chexxxxxxx,Zhxx,xxxxxxxxxinois.edu,UIUC,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alxx,Morxxxx,UIUC,,,,,,xxxxxxxxxxllinois.edu,,,,,United States,,Alxx Morxxxx;Chexxxxxxx Zhxx,xxxxxxxxxxllinois.edu;xxxxxxxxxlinois.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
180,180X-A4G9P6G6J9,Identifying Products in Online Cybercrime Marketplaces: A Dataset and Fine-grained Domain Adaptation Task,Grxx Durxxxx;Jonxxxxx Kx;Taxxxx Berg-xxxxxxxxxxx;Rebxxxx Sx;Saxxx Afxxx;Daxxx Mcxxx;Kixxxx Levxxxxxx and Vexx Paxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"One weakness of machine-learned NLP models is that they typically perform
poorly on out-of-domain data. In this work, we study the task of identifying
products being bought and sold in online cybercrime forums, which exhibits
particularly challenging cross-domain effects. We formulate a task that
represents a hybrid of slot-filling information extraction and named entity
recognition and annotate datasets consisting of data from four different
forums. Each of these forums constitutes its own ""fine-grained domain"" in that
the forums cover different market sectors with different properties, even
though all forums are in the broad domain of cybercrime. We characterize these
domain differences in the context of a learning-based system: supervised models
see decreased accuracy when applied to new forums, and standard techniques for
semi-supervised learning and domain adaptation have limited effectiveness on
this data, which suggests the need to improve these techniques.

We release a dataset of 93,924 posts from across 4 forums, with annotations for
1,938 posts.",5 Feb 2017 04:27:13 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",information extraction;  named entity recognition;  NLP applications;  domain adaptation,Grxx,Durxxxx,xxxxxxxxxxx.utexas.edu,UT Austin,No,Jonaxxxxxxx,Kumxxxxxxx,xxxxxxxxxumich.edu,University of Michigan,No,Taxxxx,Berg-xxxxxxxxxxx,xxxxxxxxxxxberkeley.edu,UC Berkeley,No,Rebxxxxxxx,Porxxxxx,xxxxxxxxxxxxx.berkeley.edu,UC Berkeley,No,Saxxx,Afxxx,xxxxxxxxxxxberkeley.edu,ICSI Berkeley,No,Daxxx,Mcxxx,xxxxxxnyu.edu,New York University,No,Kixxxx,Levxxxxxx,xxxxxxxxxxs.ucsd.edu,UC San Diego,No,Vexx,Paxxxx,xxxxxxxxkeley.edu,UC Berkeley,No,,,,,,,,,,,,Grxx,Durxxxx,UT Austin,,,607-xxxxxxxx,,,xxxxxxxxxxx.utexas.edu,,Austin,TX,,United States,,Grxx Durxxxx;Jonxxxxx Kx;Taxxxx Berg-xxxxxxxxxxx;Rebxxxx Sx;Saxxx Afxxx;Daxxx Mcxxx;Kixxxx Levxxxxxx;Vexx Paxxxx,xxxxxxxxxxx.utexas.edu;xxxxxxxxx@umich.edu;xxxxxxxxxxxxberkeley.edu;xxxxxxxxxxxxxs.berkeley.edu;xxxxxxxxxxxxberkeley.edu;xxxxxxxnyu.edu;xxxxxxxxxxcs.ucsd.edu;xxxxxxxxxkeley.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
181,181X-A6E6P8F3C6,Dependency-based Pre-ordering Preposition Phrase in Chinese-Vietnamese Machine Translation,Phxxx Trxx;Dixx Dixx;T�xxx Lxx and Axx Txx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Word order is one of the biggest differences between Chinese and Vietnamese
languages. In particular, the order of the preposition phrase is a grammar type
which has a big difference compared to other grammar types. The
state-of-the-art phrase-based statistical machine translation cannot overcome
the mistakes of the word order of Chinese-Vietnamese translation. Moreover,
Chinese-Vietnamese is considered as a low-resource language pair, so the errors
of word order in the translation system are more serious than the other
rich-resource language pairs. In this paper, we propose an approach by using
Chinese dependency relation in order to pre-order Chinese word order to be
suitable to Vietnamese word order. The experimental results show that our
approach has improved the performance of machine translation system compared to
the machine translation system using only the reordering model of the
phrase-based statistical machine translation.",5 Feb 2017 04:27:25 GMT,Empirical/Data-Driven,Machine translation,hybrid MT;  word segmentation;  syntax;  parsing;  alignment,Phxxx,Trxx,xxxxxxxxxxxxoc@tdt.edu.vn,"Faculty of information technology, Ton Duc Thang University",No,Dixx,Dixx,xxxxxxxxxxxhcmus.edu.vn,"VNU-HCM University of Science, Ho Chi Minh City, Vietnam",No,T�xxx,Lxx,xxxxxxxxxxxxxxurrier.uqam.ca,"Université Du Québec À Montréal, Montréal, Canada",No,Axx,Trxx,xxxxxxxxxx@gmail.com,"School of Computer Science, Beijing Institute of Technology, China",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Phxxx,Trxx,"Faculty of information technology, Ton Duc Thang University",,,,,,xxxxxxxxxxxxoc@tdt.edu.vn,,HCM,,,Viet Nam,,Phxxx Trxx;Dixx Dixx;T�xxx Lxx;Axx Trxx,xxxxxxxxxxxxoc@tdt.edu.vn;xxxxxxxxxxxxhcmus.edu.vn;xxxxxxxxxxxxxxourrier.uqam.ca;xxxxxxxxxx6@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
182,182X-G4P8P6C9B5,Context-Dependent Sentiment Analysis in User-Generated Videos,Souxxxxx Poxxx;Erxx Camxxxx;Devxxxxxx Hazxxxxx;Navxxxx Majxxxxx;Amxx Zaxxx and Louisxxxxxxxxx Moxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Oral Tuesday,,Undecided (Sentiment Analysis Opinion Mining),"Multimodal sentiment analysis is a developing
area of research, which involves
the identification of sentiments in videos.
Current research considers utterances as
independent entities, i.e., ignores the interdependencies
and relations among the utterances
of a video. In this paper, we propose
a LSTM-based model that enables
utterances to capture contextual information
from their surroundings in the same
video, thus aiding the classification process.
Our method shows 5-10% performance
improvement over the state of the
art and high robustness to generalizability.",23 Apr 2017 10:35:14 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Souxxxxx,Poxxx,xxxxxxxxxxxxia@gmail.com,Researcher,No,Erxx,Camxxxx,xxxxxxxxxxdia.mit.edu,Nanyang Technological University,No,Devxxxxxx,Hazxxxxx,xxxxxxxxxxsentic.net,"National Institute of Technology, Warangal, India",No,Navxxxx,Majxxxxx,xxxxxxxxxxxx009@gmail.com,CIC,No,Amxx,Zaxxx,xxxxxxxxxxxdrew.cmu.edu,CMU,No,Louisxxxxxxxxx,Morxxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,Souxxxxx,Poxxx,Nanyang Technological University,,,,,,xxxxxxxxxxxxia@gmail.com,,Singapore,Singapore,,Singapore,,Souxxxxx Poxxx;Erxx Camxxxx;Devxxxxxx Hazxxxxx;Navxxxx Majxxxxx;Amxx Zaxxx;Louisxxxxxxxxx Morxxxx,xxxxxxxxxxxxia@gmail.com;xxxxxxxxxxxdia.mit.edu;xxxxxxxxxx@sentic.net;xxxxxxxxxxxxx009@gmail.com;xxxxxxxxxxxxdrew.cmu.edu;xxxxxxxxxcs.cmu.edu,Context-Dependent Sentiment Analysis in User-Generated Videos,Context-Dependent Sentiment Analysis in User-Generated Videos,11,Soujanya Poria,,"Temasek Laboratories, 
Nanyang Technological University, Nanyang Avenue 5, Singapore",on,on,Only include my submission if it is accepted.,No,None,None
183,183X-H9A8J2E3C2,Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems,Satxxxx Akaxxxx and Nobxxxxx Kaxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Poster Monday,,Undecided (Dialog Interactive Systems),"Recently emerged intelligent assistants on smartphones and home electronics
(e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific
task-oriented spoken dialogue systems and open-domain non-task-oriented ones.
To realize such hybrid dialogue systems, this paper investigates determining
whether or not a user is going to have a chat with the system. To address the
lack of benchmark datasets for this task, we construct a new dataset consisting
of 15,160 utterances collected from the real log data of a commercial
intelligent assistant (and will release the dataset to facilitate future
research activity). In addition, we investigate using tweets and Web search
queries for handling open-domain user utterances, which characterize the task
of chat detection. Experimental experiments demonstrated that, while simple
supervised methods are effective, the use of the tweets and search queries
further improves the F$_1$-score from 86.21 to 87.53.",17 Apr 2017 07:56:12 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Satxxxx,Akaxxxx,xxxxxxxxxxxxxxs.u-tokyo.ac.jp,University of Tokyo,No,Nobxxxxx,Kaxx,xxxxxxxxxoo-corp.jp,Yahoo Japan Corporation,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nobxxxxx,Kaxx,Yahoo Japan Corporation,,,,,,xxxxxxxxxoo-corp.jp,,,,,Japan,,Satxxxx Akaxxxx;Nobxxxxx Kaxx,xxxxxxxxxxxxxxs.u-tokyo.ac.jp;xxxxxxxxxxoo-corp.jp,Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems,Chat Detection in an Intelligent Assistant,12,Nobuhiro Kaji,senior chief researcher,"Yahoo Japan Corporation
1-3, Kioi-cho, Chiyoda-ku, Tokyo, Japan",,on,No. Do not include my submission in this dataset.,No,None,None
184,184X-F2C9G9A6J9,Employing Pivot Language Technique through Statistical and Neural Frameworks: The Case of Under-Resourced Persian-Spanish Machine Translation,Benxxxxx Ahmxxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"The quality of Neural Machine Translation (NMT) systems like Statistical
Machine Translation (SMT) systems, heavily depends on the size of training data
set, while for some pairs of languages, high-quality parallel data are poor
resources. In order to respond to this low-resourced training data bottleneck
reality, we employ
the pivoting approach in both neural MT and statistical MT frameworks. During
our experiments on the Persian-Spanish, taken as an under-resourced translation
task, we discovered that, the aforementioned method, in both frameworks,
significantly improves the translation quality in comparison to the standard
direct translation approach.",5 Feb 2017 07:09:08 GMT,Resources/Evaluation,Machine translation,learning with small datasets;  phrase-based SMT,Benxxxxx,Ahmxxxxx,xxxxxxxxxxxxari@gmail.com,Autonomous University of Barcelona,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benxxxxx,Ahmxxxxx,Autonomous University of Barcelona,,,+34 9xxxxxxxxxxx,,,xxxxxxxxxxxxari@gmail.com,,Cerdanyola del Valles,Barcelona,,Spain,,Benxxxxx Ahmxxxxx,xxxxxxxxxxxxari@gmail.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
185,185X-E3C7P2P6H7,A Context-Aware Recurrent Encoder for Neural Machine Translation,Bixx Zhxxx;Dexx Xixxx;jinxxxx sx and Hoxx Dxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation (NMT) heavily
relies on its encoder to capture the underlying
meaning of a source sentence so
as to generate a faithful translation. However,
most NMT encoders are built upon
either unidirectional or bidirectional recurrent
neural networks, which either do not
deal with future context or simply concatenate
the history and future context to
form context-dependent word representations,
implicitly assuming the independence
of the two types of contextual information.
In this paper, we propose a novel
context-aware recurrent encoder (CAEncoder),
as an alternative to the widely-used
bidirectional encoder, such that the future
and history contexts can be fully incorporated
into the learned source representations.
Our CAEncoder involves a twolevel
hierarchy: the bottom level summarizes
the history information, while the upper
level assembles the summarized history
and future context into source representations.
Additionally, CAEncoder is as
efficient as the bidirectional RNN encoder
in terms of both training and decoding.
Experiments on Chinese-English translation
tasks show that CAEncoder achieves
significant improvements over the bidirectional
RNN encoder on a state-of-the-art
NMT system.",7 Feb 2017 07:09:24 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation,Bixx,Zhxxx,xxxxxxxxmu.edu.cn,Xiamen University,No,Dexx,Xixxx,xxxxxxxxxx@gmail.com,Soochow University,No,jinxxxx,sx,xxxxxxxu.edu.cn,Xiamen university,No,Hoxx,Duxx,xxxxxxxxu.edu.cn,Xiamen University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bixx,Zhxxx,Xiamen University,,,+86 1xxxxxxxxxx,,,xxxxxxxxmu.edu.cn,,,,,China,,Bixx Zhxxx;Dexx Xixxx;jinxxxx sx;Hoxx Duxx,xxxxxxxxmu.edu.cn;xxxxxxxxxxg@gmail.com;xxxxxxxxu.edu.cn;xxxxxxxxmu.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
186,186X-D6H9A6E3P7,Cseq2seq: Cyclic Sequence-to-Sequence Learning,Bixx Zhxxx;Dexx Xixxx;jinxxxx sx and Hoxx Dxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"The vanilla sequence-to-sequence learning
(seq2seq) reads a source sequence and encodes
it into a fixed-length vector only
once. We hypothesize that the partial target
sequence generated according to the
source vector can act as a feedback to
improve the understanding of the source
sequence and therefore to update the encoding
of the source sequence. To test
this hypothesis, in this paper, we propose
cyclic sequence-to-sequence learning
(Cseq2seq) which, rather than encoding
the source sequence only once, rereads
and reencodes the source sequence according
to the partial target sequence during
decoding. Different from the seq2seq that
uses the fixed-size vector across all decoding
steps, Cseq2seq reencodes the source
sequence at each decoding step to dynamically
produce a target-sequence-guided
context vector for the next word prediction.
We further perform parameter sharing
on Cseq2seq that shares the weights of
two RNNs (the encoder and decoder) and
two target-side word embeddings, making
Cseq2seq equivalent to a single conditional
RNN model, with 31% parameters
pruned but very little performance loss.
Cseq2seq not only preserves the simplicity
of seq2seq but also yields comparable and
promising results on machine translation
tasks. Experiments on Chinese-English
and English-German translation show that
Cseq2seq achieves significant and consistent
improvements over seq2seq and is as
competitive as the attention-based seq2seq
model.",7 Feb 2017 07:10:07 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation,Bixx,Zhxxx,xxxxxxxxmu.edu.cn,Xiamen University,No,Dexx,Xixxx,xxxxxxxxxx@gmail.com,Soochow University,No,jinxxxx,sx,xxxxxxxu.edu.cn,Xiamen university,No,Hoxx,Duxx,xxxxxxxxu.edu.cn,Xiamen University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bixx,Zhxxx,Xiamen University,,,+86 1xxxxxxxxxx,,,xxxxxxxxmu.edu.cn,,,,,China,,Bixx Zhxxx;Dexx Xixxx;jinxxxx sx;Hoxx Duxx,xxxxxxxxmu.edu.cn;xxxxxxxxxxg@gmail.com;xxxxxxxxu.edu.cn;xxxxxxxxmu.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
187,187X-F6C2A2A5P6,KGs Fusion Embedding with Multiple Contextual Understanding,Zhixxxx Dx;Zexxx Hxx and Xiaxxxxx Mxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"To date, embedding methods for KGs have mushroomed, however, KG is simply seen
as symbol triples in most methods, only two methods consider semantic
knowledge. However, they are limited to descriptions for entities, and
structural knowledge and semantic one are simply stitched together, could
hardly play a reasonable role. To this end, we propose a new model FMCUE that
is learning embedding for KG via semantic-aided with multiple contextual
knowledge (MCK). There are two challenges: (1) how to obtain and pick out
valuable MCK? (2) how to use fully structural knowledge and MCK. More
specifically, we employ a Dba_LSTM model to understand implicit semantic from
MCK and a harmonic me-chanism to fusion embedding for structural knowledge and
MCK. Also we evaluate FMCUE on KG completion task, entity classification task
and semantic analysis task. The results show FMCUE achieves the substantial
improvements against the state-of-the-art baselines.",5 Feb 2017 07:49:48 GMT,Theoretical,Machine learning,NLP applications;  relational Learning;  semantic relations;  semantic knowledge induction,Zhixxxx,Dx,xxxxxxxxxxx@ruc.edu.cn,Renmin University of China,No,Zexxx,Hxx,xxxxxxx163.com,Renmin University of China,No,Xiaxxxxx,Mexx,xxxxxxxxx@sina.com,Renmin University of China,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhixxxx,Dx,Renmin University of China,,,,,,xxxxxxxxxxx@ruc.edu.cn,,,,,China,,Zhixxxx Dx;Zexxx Hxx;Xiaxxxxx Mexx,xxxxxxxxxxx@ruc.edu.cn;xxxxxxx@163.com;xxxxxxxxxC@sina.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
188,188X-F3J5E6G4D8,Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization,Pixx Lx;Wxx Lxx;Lixxxx Bixx;Wexxxx Gxx and Haxx L,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"When people recall and digest what they have read for writing summaries, the
important content is more likely to attract their attention.
Inspired by this observation, we propose a cascaded attention based
unsupervised
model to estimate the salience information from the text for compressive
multi-document summarization.
The attention weights are learned automatically by an unsupervised data
reconstruction framework which can capture the sentence salience.
By adding sparsity constraints on the number of output vectors, we can generate
condensed information which can be treated as word salience.
A diversified attention mechanism is also incorporated to guarantee the
semantic diversity in the condensed output vectors.
Fine-grained and coarse-grained sentence compression strategies are
incorporated to produce compressive summaries. 
We thoroughly investigate the performance of combining different attention
architectures and cascaded structures.
Experiments on the benchmark data sets DUC 2007 and TAC 2011 show that our
framework achieves better results than state-of-the-art methods.",7 Feb 2017 11:37:29 GMT,Applications/Tools,Summarization,text mining;  document summarization;  multi-document summarization,Pixx,Lx,xxxxxxxxx@gmail.com,The Chinese University of Hong Kong,No,Wxx,Lxx,xxxxxxxxxuhk.edu.hk,The Chinese University of Hong Kong,No,Lixxxx,Bixx,xxxxxxxxxx@gmail.com,"AI Platform Department, Tencent Inc.",No,Wexxxx,Gxx,xxxxxxxxgmail.com,Linkedin,No,Haxx,Lx,xxxxxxxxxxhuawei.com,"Noah's Ark Lab, Huawei Technologies",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Pixx,Lx,The Chinese University of Hong Kong,,,,,,xxxxxxxxx@gmail.com,,,,,China,,Pixx Lx;Wxx Lxx;Lixxxx Bixx;Wexxxx Gxx;Haxx Lx,xxxxxxxxx@gmail.com;xxxxxxxxxxuhk.edu.hk;xxxxxxxxxxg@gmail.com;xxxxxxxxxgmail.com;xxxxxxxxxx@huawei.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
189,189X-H7P9B8H2P6,Cascaded Convolutional Neural Networks with Dual Attention and Multiple Word Embeddings for Why-Question Answering,Jonxxxxxx Ox;Kenxxxx Torxxxxx;Canxxxx Kruxxxxxxx;Rxx Iixx and Juxxxx Kloxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Why-question answering (why-QA) methods, which retrieve text passages as
answers to a given why-question, have to consider three aspects of the
relevance of answers. The answers must contain (1) the causes or reasons that a
question asks, (2) the topic of the question, and (3) the
expressions/structures indicating the causality between the causes and the
question topic. Although recent why-QA methods have achieved significant
performance improvement by introducing neural networks (NNs), they paid
attention only to some of the aspects. In this paper, we present cascaded
convolutional neural networks with dual attention (DA-cCNNs) which are designed
to model all the three aspects of an answer's relevance. Through experiments on
Japanese why-QA, we confirmed that our proposed method outperformed the
state-of-the-art NN-based system.",5 Feb 2017 09:26:22 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",open-domain question answering,Jonxxxxxx,Ox,xxxxxxxxxnict.go.jp,NICT,No,Kenxxxx,Torxxxxx,xxxxxxxxxnict.go.jp,NICT,No,Canxxxx,Kruxxxxxxx,xxxxxxxxxict.go.jp,NICT,No,Rxx,Iixx,xxxxxxxxxnict.go.jp,National Institute of Information and Communications Technology,No,Juxxxx,Kloxxxxx,xxxxxxxxict.go.jp,National Institute of Information and Communications Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Jonxxxxxx,Ox,NICT,,,,,,xxxxxxxxxnict.go.jp,,Kyoto,,,Japan,,Jonxxxxxx Ox;Kenxxxx Torxxxxx;Canxxxx Kruxxxxxxx;Rxx Iixx;Juxxxx Kloxxxxx and Commuxxxxxxxxx Techxxxxxxx,xxxxxxxxxnict.go.jp;xxxxxxxxxxnict.go.jp;xxxxxxxxxnict.go.jp;xxxxxxxxxxnict.go.jp;xxxxxxxxxict.go.jp,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
191,191X-P2D7C7C6F5,Unsupervised Dialogue Act Modeling for Conversation Summarization,Jixx Lx;Xinxxxxx Zexx;Mixx Lixx and Kamxxxx Wxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Dialogue acts have been proven useful to indicate key contents for conversation
summarization. Most existing summarization models rely on detected dialogue act
tags obtained following an annotate-train-test paradigm, which requires
domain-specific dialogue act definition and data annotation. This work proposes
a fully unsupervised LDA-based probabilistic model that detects dialogue acts
and key contents for summarization simultaneously. Empirical evaluations on
email and forum conversations demonstrate that our model can both effectively
detect dialogue acts and produce informative summaries.",7 Feb 2017 07:24:22 GMT,Empirical/Data-Driven,Summarization,discourse;  NLP applications;  NLP on noisy unstructured text;  dialogue;  text mining;  document summarization;  graphical models;  NLP for Web 2.0,Jixx,Lx,xxxxxxxxxxcuhk.edu.hk,The Chinese University of Hong Kong,No,Xinxxxxx,Zexx,xxxxxxxxxxcuhk.edu.hk,The Chinese University of Hong Kong,No,Mixx,Lixx,xxxxxxxxxxuhk.edu.hk,The Chinese University of Hong Kong,No,Kamxxxx,Woxx,xxxxxxxxxxcuhk.edu.hk,"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixx,Lx,Tencent AI Lab,,,86-13xxxxxxxxx,,,xxxxxxxxxxtencent.com,,,,,China,,Jixx Lx;Xinxxxxx Zexx;Mixx Lixx;Kamxxxx Woxx and Engixxxxxxx Manxxxxxxx,xxxxxxxxxxcuhk.edu.hk;xxxxxxxxxxxcuhk.edu.hk;xxxxxxxxxxcuhk.edu.hk;xxxxxxxxxxxcuhk.edu.hk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
192,192X-F4F3D3P3F2,Imbalanced Sentiment Classification with Multi-Task Learning and Prior Sentiment Knowledge,Fanxxxxx Wx;Qiaxxxx Zhxx and Yonxxxxx Huxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Machine learning methods are widely used in sentiment classification.
However, when sentiment distribution is imbalanced, the performance of these
methods declines significantly. In this paper, we propose an effective approach
for imbalanced sentiment classification. In our approach, multiple balanced
subsets are sampled from the imbalanced training data and a multi-task learning
based framework is proposed to learn robust sentiment classifier from these
subsets collaboratively. In addition, we incorporate prior knowledge of
sentiment expressions extracted from both existing sentiment lexicons and
massive unlabeled data into our approach to enhance the learning of sentiment
classifier in imbalanced scenario. Experimental results on benchmark datasets
validate the effectiveness of our approach in improving imbalanced sentiment
classification.",7 Feb 2017 07:03:00 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction,Fanxxxxx,Wx,xxxxxxxxxx@gmail.com,Tsinghua Univeristy,No,Qiaxxxx,Zhxx,xxxxxxxxxxotmail.com,Tsinghua University,No,Yonxxxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fanxxxxx,Wx,Microsoft Research Asia,,,,,,xxxxxxxxxx@gmail.com,,,,,China,,Fanxxxxx Wx;Qiaxxxx Zhxx;Yonxxxxx Huxxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxhotmail.com;xxxxxxxxxxxxnghua.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
193,193X-F9P6E3J2D3,A Transition-Based Directed Acyclic Graph Parser for UCCA,Daxxxx Hersxxxxxxx;Omxx Abxxx and Axx Rapxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Wednesday,,Undecided (Semantics),"We present the first parser for UCCA, a cross-linguistically applicable
framework for semantic representation, which builds on extensive typological
work and supports rapid annotation. UCCA poses a challenge for existing parsing
techniques, as it exhibits reentrancy (resulting in DAG structures),
discontinuous structures and non-terminal nodes corresponding to complex
semantic units. To our knowledge, the conjunction of these formal properties is
not supported by any existing parser. Our transition-based parser, which uses a
novel transition set and features based on bidirectional LSTMs, has value not
just for UCCA parsing: its ability to handle more general graph structures can
inform the development of parsers for other semantic DAG structures, and in
languages that frequently use discontinuous structures.",3 Apr 2017 09:33:02 GMT,Empirical/Data-Driven,Semantics,,Daxxxx,Hersxxxxxxx,xxxxxxxxxxxxxxvich@gmail.com,Hebrew University,No,Omxx,Abxxx,xxxxxxxxxxxxxil.huji.ac.il,The Hebrew University of Jerusalem,No,Axx,Rapxxxxxx,xxxxxxxxxuji.ac.il,The Hebrew University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxxx,Hersxxxxxxx,Hebrew University,,,9725xxxxxxxx,,,xxxxxxxxxxxxxxvich@gmail.com,,,,,Israel,"I am a Ph.D. student at The Hebrew University of Jerusalem. My advisors are Prof. Ari Rappoport and Dr. Omri Abend. I did my B.Sc. in mathematics and computer science at the Open University of Israel. Since 2008, I have been a software engineer at IBM Research, where I am a member of the Debating Technologies group.",Daxxxx Hersxxxxxxx;Omxx Abxxx;Axx Rapxxxxxx,xxxxxxxxxxxxxxvich@gmail.com;xxxxxxxxxxxxxail.huji.ac.il;xxxxxxxxxhuji.ac.il,A Transition-Based Directed Acyclic Graph Parser for UCCA,A Transition-Based DAG Parser for UCCA,12,Daniel,,"The Hebrew University, The Edmond J. Safra Campus - Givat Ram, Jerusalem 9190401, Israel",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
194,194X-A3G3G2F9J3,SynSem: Learning Embedding with a Two-Stream Neural Language Model,Qixxxx Gxx;Shuxxxxxx Chxxx;Quxx Gxx;Xixx Tixx;Xiaxxxxxx Xxx and Zhxxx Zhxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"This paper borrows two key neuroscience results to re-examine proper constructs
of embedding in neural language models. First, splitting embedding into
syntactic and semantics portion mirrors the partitioned processing stages in
human speech production. Second, contextually induced senses from one shared
vector approximates the integration of selected meaning of a word. We show that
the resulting two-stream recurrent model, SynSem, delivers superior
performance, and at the same time demonstrates interesting properties.",7 Feb 2017 10:41:48 GMT,Empirical/Data-Driven,Semantics,unsupervised and semi-supervised learning;  part-of-speech tagging;  word sense induction,Qixxxx,Gxx,xxxxxxxxxxx1@gmail.com,Fudan University,No,Shuxxxxxx,Chxxx,xxxxxxxxxxxudan.edu.cn,Fudan University,No,Quxx,Gxx,xxxxxxnyu.edu,New York University,No,Xixx,Tixx,xxxxxxxxn@nyu.edu,New York University Shanghai,No,Xiaxxxxxx,Xxx,xxxxxxxxxan.edu.cn,Fudan University,No,Zhxxx,Zhxxx,xxxxxu.edu,NYU Shanghai,No,,,,,,,,,,,,,,,,,,,,,,Qixxxx,Gxx,Fudan University,,,,,,xxxxxxxxxxdan.edu.cn,,,,,China,,Qixxxx Gxx;Shuxxxxxx Chxxx;Quxx Gxx;Xixx Tixx;Xiaxxxxxx Xxx;Zhxxx Zhxxx,xxxxxxxxxxx1@gmail.com;xxxxxxxxxxxfudan.edu.cn;xxxxxxxnyu.edu;xxxxxxxxxn@nyu.edu;xxxxxxxxxdan.edu.cn;xxxxxyu.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
195,195X-H6A3P6P2F3,Automatic Acquisition for Chinese Noun-Verb Qualia Pairs via Convolutional Deep Neural Network,Lvexxxx Zhxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Qualia noun-verb(N-V) pairs are important lexical resources for Natural
Language Processing. Previous methods of extracting qualia N-V pairs primarily
utilize lexico-syntactic patterns, and the performance strongly depends on the
domain expertise. In this paper, we propose a general and effective
convolutional deep neural network to acquiring qualia N-V pairs from Chinese
corpora. Our method takes several sentences containing a given N-V pair as an
input. The lexical level features are extracted according to the given N-V
pair. Meanwhile, sentence level features are learned from the modifiers of the
noun and verb using a convolutional neural network. These two level features of
this input are fed into a softmax classifier to predict qualia relation between
the noun and verb. Finally, we rank the different qualia elements (verbs) of a
noun to get the frequent and credible qualia N-V pairs. The experiment results
show that the N-V qualia pairs produced are indeed acceptable.",10 Feb 2017 05:29:22 GMT,Resources/Evaluation,"Tagging, chunking, syntax, and parsing",multilingual resources;  relation/event extraction,Lvexxxx,Zhxxx,xxxxxxxxxn@163.com,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lvexxxx,Zhxxx,Peking University,,,1520xxxxxxx,,,xxxxxxxxxn@163.com,,beijin,,,China,,Lvexxxx Zhxxx,xxxxxxxxxn@163.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
196,196X-P9E8F6D5J9,Context-Sensitive Word Composition,Bx Ax;Xiaxxxx Hxx;Lx Sxx and Bx Cxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Word composition is a promising technique for representation learning of large
linguistic units (e.g., phrases, sentences and documents). Currently, most of
the composition models represent a word in different contexts using the same
word vector (e.g., the vector learned by word2vec). However, a word may have
different meanings in different contexts, therefore such a uniform
representation cannot accurately capture the meanings of a word in different
contexts. To resolve this problem, we propose a context-sensitive word
composition model, which first represents a word in different contexts by
learning context-sensitive word vectors, then a context-sensitive composition
framework is proposed to learn the representation of linguistic units by
composing context-sensitive word vectors. Experimental results verified the
effectiveness of our model on representing linguistic units at different
granularity levels.",7 Feb 2017 04:58:22 GMT,Empirical/Data-Driven,Semantics,multiword semantics/compositionality;  lexical semantics;  lexical paraphrasing;  distributional similarity,Bx,Ax,xxxxxxxxxxscas.ac.cn,"Institute of Software, Chinese Academy of Sciences",No,Xiaxxxx,Hxx,xxxxxxxxxx@gmail.com,"Institute of Software, Chinese Academy of Sciences",No,Lx,Sxx,xxxxxxx@163.com,ISCAS,No,Bx,Chxx,xxxxxxxxxx4@gmail.com,ISCAS,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bx,Ax,"Institute of Software, Chinese Academy of Sciences",,,,,,xxxxxxxxas.ac.cn,,Beijing,Beijing,,China,,Bx Ax;Xiaxxxx Hxx;Lx Sxx;Bx Chxx,xxxxxxxxxxscas.ac.cn;xxxxxxxxxxi@gmail.com;xxxxxxxx@163.com;xxxxxxxxxxx4@gmail.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
197,197X-C3A6C6C8J7,CNN-LON: An ensemble domain adaptation for sentiment classification,Honxxxx Waxx and Yaxxx Xxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"The language model of neural network avoids the artificial design of word
representation, which brings the task of sentiment classification into neural
era. However, when it comes to cross domain sentiment classification, the
accuracy using existing domain adaptation methods are not that satisfactory for
neural classifiers. For example, as shown in the previous experiments, the
sentence model proposed by Yoon Kim et al. achieves excellent results in large
dataset of a single domain. But if training the Kim model on small dataset from
different domains, the accuracy may decrease. Based on Kim model, we introduce
the idea of Linear Output Network(LON). We propose a ensemble domain adaptation
approach, CNN-LON(Convolutional Neural Network with Linear Output Network). In
the Amazon Product Review set, our experiments show that CNN-LON can achieve
more performance gains in comparision with other domain adaptation methods,
which verifies its effectiveness for cross domain sentiment classification.",7 Feb 2017 02:45:08 GMT,Survey Papers,Sentiment analysis and opinion mining,sentiment analysis;  domain adaptation,Honxxxx,Waxx,xxxxxxxxxxxustb.edu.cn,"School of Computer and Communication Engineering, University of Science and Technology Beijing",No,Yaxxx,Xxx,xxxxxxxxxxxllo@163.com,"School of Computer and Communication Engineering, University of Science and Technology Beijing",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxxx,Xxx,"School of Computer and Communication Engineering, University of Science and Technology Beijing",,,1369xxxxxxx,,,xxxxxxxxxxxllo@163.com,,,,,China,,Honxxxx Waxx;Yaxxx Xxx and Commxxxxxxxxx Engixxxxxxx,xxxxxxxxxxxustb.edu.cn;xxxxxxxxxxxello@163.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
198,198X-C8J9C4A4P6,Deep Pyramid Convolutional Neural Networks for Text Categorization,Rxx Johxxxx and Toxx Zhxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"This paper proposes a low-complexity word-level deep convolutional neural
network (CNN) architecture for text categorization that can efficiently
represent long-range associations in text.  In the literature, several deep and
complex neural networks have been proposed for this task, assuming availability
of relatively large amounts of training data.  However, the associated
computational complexity increases as the networks go deeper, which poses
serious challenges in practical applications.  Moreover, it was shown recently
that shallow word-level CNNs are more accurate and much faster than the
state-of-the-art very deep nets such as character-level CNNs even in the
setting of large training data.  Motivated by these findings, we carefully
studied deepening of word-level CNNs to capture global representations of text,
and found a simple network architecture with which the best accuracy can be
obtained by increasing the network depth without increasing computational cost
by much.  We call it deep pyramid CNN.                                               
  The
proposed
model
with 15
weight
layers outperforms the previous best models on six benchmark datasets for
sentiment classification and topic categorization.",17 Apr 2017 18:43:18 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Rxx,Johxxxx,xxxxxxxxxx@gmail.com,RJ Research Consulting,No,Toxx,Zhxxx,xxxxxxxxxx@gmail.com,Rutgers University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rxx,Johxxxx,RJ Research Consulting,,,,,,xxxxxxxxxx@gmail.com,,,,,United States,,Rxx Johxxxx;Toxx Zhxxx,xxxxxxxxxx@gmail.com;xxxxxxxxxx0@gmail.com,Deep Pyramid Convolutional Neural Networks for Text Categorization,Deep Pyramid Convolutional Neural Networks for Text Categorization,9,Rie Johnson,,,on,on,No. Do not include my submission in this dataset.,No,None,None
199,199X-G4F6H7B2B6,MANet: A Dynamic Modal Attention Network for Describing Videos,Saxx Phxx;Yuxxxx Mixxx and Shixxxxxx Saxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"Exploiting multimodal features has become a standard approach towards many
video applications, including the video captioning task. 
One problem with the existing work is that it models the relevance of each type
of features evenly, which neutralizes the impact of each individual modality to
the word to be generated. 
This paper proposes a novel Modal Attention Network (MANet) to account for this
issue. Our MANet extends the standard encoder-decoder network such that it can
dynamically locating relevant modalities to generate a target word during the
captioning process. Experimental results demonstrate that MANet effectively
utilizes multimodal features to generate better video descriptions, with
promising application towards understanding a word sense through analyzing its
attended video modalities.",7 Feb 2017 11:52:49 GMT,Applications/Tools,"Vision, robots, and other grounding",language generation;  multimodal representations and processing,Saxx,Phxx,xxxxxxxxii.ac.jp,National Institute of Informatics,No,Yuxxxx,Mixxx,xxxxxxxxii.ac.jp,National Instutite of Informatics,No,Shixxxxxx,Saxxx,xxxxxxxii.ac.jp,National Institute of Informatics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Saxx,Phxx,National Institute of Informatics,,,,,,xxxxxxxxii.ac.jp,,,,,Japan,,Saxx Phxx;Yuxxxx Mixxx;Shixxxxxx Saxxx,xxxxxxxxii.ac.jp;xxxxxxxxnii.ac.jp;xxxxxxxxii.ac.jp,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
200,200X-H7D4D6A8P3,Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning,Shxxxx Hx;Cxx Lxx;Kaxx Lxx and Jxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"Generating answer with natural language sentence is very important in
real-world question answering systems, which needs to obtain a right answer as
well as a coherent natural response. In this paper, we propose an end-to-end
question answering system called COREQA in sequence-to-sequence learning, which
incorporates copying and retrieving mechanisms to generate natural answers
within an encoder-decoder framework. Specifically, in COREQA, the semantic
units (words, phrases and entities) in a natural answer are dynamically
predicted from the vocabulary, copied from the given question and/or retrieved
from the corresponding knowledge base jointly. Our empirical study on both
synthetic and real-world datasets demonstrates the efficiency of COREQA, which
is able to generate correct, coherent and natural answers for knowledge
inquired questions.",22 Apr 2017 23:58:52 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Shxxxx,Hx,xxxxxxxxxxxlpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Cxx,Lxx,xxxxxxxxxxpr.ia.ac.cn,Chinese Academy of Sciences,No,Kaxx,Lxx,xxxxxxxxx.ia.ac.cn,Chinese Academy of Sciences,No,Jxx,Zhxx,xxxxxxxxxr.ia.ac.cn,Chinese Academy of Sciences,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Hx,"Institute of Automation, Chinese Academy of Sciences",,,,,,xxxxxxxxxxxlpr.ia.ac.cn,,,,,China,,Shxxxx Hx;Cxx Lxx;Kaxx Lxx;Jxx Zhxx,xxxxxxxxxxxlpr.ia.ac.cn;xxxxxxxxxxxpr.ia.ac.cn;xxxxxxxxxr.ia.ac.cn;xxxxxxxxxxr.ia.ac.cn,Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning,Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning,10,,,,on,on,Only include my submission if it is accepted.,No,None,None
201,201X-F8C3H8C2D5,Investigating Different Context Types and Representations for Learning Word Embeddings,lx boxxxx;Txx Lxx;Zxx Zhxx and Xiaxxxxx D,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"The number of word embedding models is growing every year. Most of them learn
word embeddings based on the co-occurrence information of words and their
contexts. However, it's still an open question what is the best definition of
context. We provide the first systematical investigation of different context
types and context representations for learning word embeddings. Comprehensive
experiments are conducted to evaluate their effectiveness under 6 tasks, which
give us some insights about context selection. We hope that this paper, along
with the published code, can serve as a guideline of choosing context for our
community.",7 Feb 2017 12:49:25 GMT,Empirical/Data-Driven,Machine learning,sentiment analysis;  unsupervised and semi-supervised learning;  part-of-speech tagging;  named entity recognition;  distributional similarity;  chunking;  parsing;  text classification;  document summarization;  NLP on Wikipedia and other collaboratively constructed resources,lx,boxxxx,xxxxxxxxxruc.edu.cn,Renmin University of China,No,Txx,Lxx,xxxxxxxc.edu.cn,Renmin University of China,No,Zxx,Zhxx,xxxxxxxxxx@ruc.edu.cn,Renmin University of China,No,Xiaxxxxx,Dx,xxxxxxxxuc.edu.cn,Renmin University of China,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Boxxxx,Lx,Renmin University of China,,,,,,xxxxxxxxxruc.edu.cn,,,,,China,,lx boxxxx;Txx Lxx;Zxx Zhxx;Xiaxxxxx Dx,xxxxxxxxxruc.edu.cn;xxxxxxxxc.edu.cn;xxxxxxxxxxx@ruc.edu.cn;xxxxxxxxxuc.edu.cn,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
202,202X-F5F7C6B2B6,Spherical Paragraph Model,Ruxxxx Zhxxx;Jiaxxxx Gxx;Yaxxxx Lxx;Jxx Xx and Xuxxx Chxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Representing texts as fixed-length vectors is central to many language
processing tasks. Most traditional methods build text representations based on
the simple Bag-of-Words (BoW) representation, which loses the rich semantic
relations between words. Recent advances in natural language processing have
shown that semantically meaningful representations of words can be efficiently
acquired by distributed models, making it possible to build text
representations based on a better foundation called the Bag-of-Word-Embedding
(BoWE) representation. However, existing text representation methods using BoWE
often lack sound probabilistic foundations or cannot well capture the semantic
relatedness encoded in word vectors. To address these problems, we introduce
the Spherical Paragraph Model (SPM), a probabilistic generative model based on
BoWE, for text representation. SPM has good probabilistic interpretability and
can fully leverage the rich semantics of words, the word co-occurrence
information as well as the corpus-wide information to help the representation
learning of texts. Experimental results on topical classification and sentiment
analysis demonstrate that SPM can achieve new state-of-the-art performances on
several benchmark datasets.",7 Feb 2017 03:56:37 GMT,Theoretical,"Document analysis including text categorization, topic models, and retrieval",generative models;  NLP applications;  experimental evaluation/comparison of ML methods;  distributional similarity;  semantic relations;  text classification;  Bayesian learning,Ruxxxx,Zhxxx,xxxxxxxxxxxxxxxtware.ict.ac.cn,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",No,Jiaxxxx,Gxx,xxxxxxxxxx@ict.ac.cn,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",No,Yaxxxx,Lxx,xxxxxxxxx@ict.ac.cn,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",No,Jxx,Xx,xxxxxxxct.ac.cn,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",No,Xuxxx,Chxxx,xxxxxxt.ac.cn,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Ruxxxx,Zhxxx,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",,,1371xxxxxxx,,,xxxxxxxxxxxxxxxtware.ict.ac.cn,,Beijing,Beijing,,China,,Ruxxxx Zhxxx;Jiaxxxx Gxx;Yaxxxx Lxx;Jxx Xx;Xuxxx Chxxx and Tecxxxxxxx Insxxxxxx,xxxxxxxxxxxxxxxtware.ict.ac.cn;xxxxxxxxxxg@ict.ac.cn;xxxxxxxxxx@ict.ac.cn;xxxxxxxxct.ac.cn;xxxxxxxt.ac.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
203,203X-G3P7G5J5D5,GRASP: Rich Patterns for Argumentation Mining,Eyxx Shnxxxx;Rxx Lexx;Vixxx Raxxxx and Noxx Slxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present GRASP (GReedy Augmented Sequential Patterns) algorithm for
automatically extracting patterns that characterize subtle linguistic
phenomena.
To that end, GRASP augments each term of input text with multiple layers from a
variety of linguistic information. These different facets of the text terms are
systematically combined to reveal rich patterns. 
We report highly promising experimental results in several challenging text
analysis tasks within the field of Argumentation Mining, especially when
integrating GRASP patterns as features in existing argumentation mining
systems.",6 Feb 2017 08:13:58 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  rule-based/symbolic learning methods;  opinion mining and extraction;  opinion representation;  semantic relations;  relation/event extraction,Eyxx,Shnxxxx,xxxxxxxx.ibm.com,IBM Haifa Research Labs,No,Rxx,Lexx,xxxxxxx.ibm.com,IBM Haifa Research Lab,No,Vixxx,Raxxxx,xxxxxxxxxin.ibm.com,IBM Research India,No,Noxx,Slxxxx,xxxxxxxx.ibm.com,"IBM Haifa Research Lab, Haifa, Israel",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Eyxx,Shnxxxx,IBM Haifa Research Labs,,,,,,xxxxxxxx.ibm.com,,,,,Israel,,Eyxx Shnxxxx;Rxx Lexx;Vixxx Raxxxx;Noxx Slxxxx,xxxxxxxx.ibm.com;xxxxxxxx.ibm.com;xxxxxxxxxxin.ibm.com;xxxxxxxxl.ibm.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
204,204X-D8D3P4C2F9,Memory-augmented Dialogue Management for Task-oriented Dialogue Systems,Zhxxx Zhxxx;Mixxxx Huxxx and Xiaxxxx Zx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Dialogue management (DM) plays a central role in task-oriented dialogue
systems. Since dialogue management requires to access not only local
utterances, but also the global information of the entire dialogue session,
modeling the long-range history information is a critical issue. To this end,
we propose a novel memory-augmented dialogue management model (MAD) which
employs two additional memory structures, a slot-value memory and an external
memory. The slot-value memory tracks the state of dialogue by memorizing and
updating the values of semantic slots, and the external memory augments the
state representation of traditional recurrent neural networks by storing more
context information. Experiments on a benchmark dataset show that our model can
achieve competitive results for the restaurant reservation task.",7 Feb 2017 12:03:09 GMT,Empirical/Data-Driven,Dialog and interactive systems,NLP applications;  dialogue control;  dialogue;  contex modeling for dialogues,Zhxxx,Zhxxx,xxxxxxxxxxl@gmail.com,Tsinghua University,No,Mixxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,Xiaxxxx,Zxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhxxx,Zhxxx,Tsinghua University,,,,,,xxxxxxxxxxl@gmail.com,,Beijing,,,China,,Zhxxx Zhxxx;Mixxxx Huxxx;Xiaxxxx Zxx,xxxxxxxxxxl@gmail.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxxxxnghua.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
205,205X-G4B4C8J7A5,Semi-supervised Lexicon Learning from Unlabeled Corpus for Wide-Coverage Semantic Parsing,Bx Chxx;Lx Sxx and Xiaxxxx Hx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"A semantic parser critically relys on the quality of lexicon, including the
covarege and accuracy. In this paper, we propose a novel approach to learn
lexicon with high-covarege for open-domain semantic parsing.  Our approach
makes use of large amounts of text corpus and easilly getting lexical resources
in a graph-based semi-supervised learning framework. This framework first
constructs graph with phrase similarity learned by utilizing many text corpus
and lexical resources. Next, graph propagation identifies the label
distribution for the
  unlabeled phrases from the labeled ones. We evaluate our approach on two
benchmark: WEBQUESTIONS and FREE917. The results show that, in both datasets,
  our method achived absolute improvement when comparing to the base system
which does not utilize the learned lexicon and gained compitive result when
comparing to the state-of-the-art systems.",7 Feb 2017 07:49:05 GMT,Empirical/Data-Driven,Semantics,parsing;  open-domain question answering,Bx,Chxx,xxxxxxxxxxxiscas.ac.cn,"Institute of Software, Chinese Academy of Sciences",No,Lx,Sxx,xxxxxxx@163.com,ISCAS,No,Xiaxxxx,Hxx,xxxxxxxxxx@gmail.com,"Institute of Software, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bx,Chxx,"Institute of Software, Chinese Academy of Sciences",,,,,,xxxxxxxxxxxiscas.ac.cn,,Beijing,Beijing,,China,,Bx Chxx;Lx Sxx;Xiaxxxx Hxx,xxxxxxxxxxxiscas.ac.cn;xxxxxxxx@163.com;xxxxxxxxxxi@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
206,206X-H4P8E6C3P2,Byte-level Sequence-to-sequence Models for Large-scale Machine Reading in Morphologically Rich Languages,Txx Kexxxx;Llxxx Joxxx and Daxxxx Hexxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Machine reading is an important task in artificial intelligence research.
Word-level models, in which words are the units of input and output, have
proven to yield good results when evaluated on English datasets.
However, in morphologically richer languages, many more unique words exist than
in English due to highly productive prefix and suffix mechanisms.
This may set back word-level models, since vocabulary sizes too big to allow
for efficient computing may have to be employed.
In this paper we focus on byte-level models that do not suffer from drawbacks
caused by large word vocabularies.
We augment 4 machine reading models from literature to byte-level and propose a
novel contemplative model.
We introduce two large-scale machine reading datasets in morphologically
rich languages, Russian and Turkish, which we publicly release.
We show that, for both languages, the byte-level contemplative model
outperforms the current state-of-the-art word-level baseline.",7 Feb 2017 10:21:58 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  open-domain question answering,Txx,Kexxxx,xxxxxxxxer@uva.nl,University of Amsterdam,No,Llxxx,Joxxx,xxxxxxxxogle.com,Google,No,Daxxxx,Hewxxxx,xxxxxxxxxgoogle.com,Google,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Txx,Kexxxx,Bla,,,,,,xxxxxxxxxx@gmail.com,,,,,Netherlands,,Txx Kexxxx;Llxxx Joxxx;Daxxxx Hewxxxx,xxxxxxxxer@uva.nl;xxxxxxxxoogle.com;xxxxxxxxxxgoogle.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
208,208X-G3G6A8P3H7,Multi-gram CNN based Attention Mechanism for Relation Classification,Chuxxxx Zhxxx;Wexxxx Xx;Shxxx Gxx;Xiuxxxx Nxx;Xiaxxxxx Xx and Yixxxx Yx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Relation classification is a crucial ingredient in numerous information
extraction systems, which has achieved great attention in recent years. To
overcome the feature engineering difficulty of conventional methods, deep
neural network based methods have been proposed. However, most relation trigger
features still can't be located and utilized effectively. To this end, we
propose a novel multi-gram CNN based attention mechanism to automatically learn
the ''importance'' distribution over inputs in a recurrent neural network
framework. Experimental results on SemEval-2010 Task 8 dataset shows that our
method outperforms most of the state-of-the-art models without external
linguistic features.",6 Feb 2017 13:25:18 GMT,Applications/Tools,"Information extraction, text mining, and question answering",information extraction;  relation discovery;  relation/event extraction,Chuxxxx,Zhxxx,xxxxxxxxxxxx1009@126.com,Shandong University of Finance and Economics,No,Wexxxx,Xx,xxxxxxxxxxupt.edu.cn,Beijing University of Posts and Telecommunications,No,Shxxx,Gxx,xxxxxxxxxxupt.edu.cn,Beijing University of Posts and Telecommunications,No,Xiuxxxx,Nxx,xxxxxxxxxn@163.com,Shandong University of Finance and Economics,No,Xiaxxxxx,Xx,xxxxxxx126.com,Shandong University of Finance and Economics,No,Yixxxx,Yxx,xxxxxxxxu.edu.cn,Shandong University,No,,,,,,,,,,,,,,,,,,,,,,Chuxxxx,Zhxxx,Shandong University of Finance and Economics,,,,,,xxxxxxxxxxxx1009@126.com,,,,,China,,Chuxxxx Zhxxx;Wexxxx Xx;Shxxx Gxx;Xiuxxxx Nxx;Xiaxxxxx Xx;Yixxxx Yxx and oxx Univxxxxxxx,xxxxxxxxxxxx1009@126.com;xxxxxxxxxxbupt.edu.cn;xxxxxxxxxxbupt.edu.cn;xxxxxxxxxan@163.com;xxxxxxx@126.com;xxxxxxxxdu.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
211,211X-P8A6J9E7P7,A Hybrid Convolutional Variational Autoencoder for Text Generation,Staxxxxxx Semxxxxxx;Alixxxxx Sevxxxx and Erhxxxx Baxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"In this paper we explore the effect of architectural choices on learning a
Variational Autoencoder (VAE) for text generation. In contrast to the
previously introduced VAE model for text where both the encoder and decoder are
RNNs, we propose a novel hybrid architecture that blends fully feed-forward
convolutional and deconvolutional components with a recurrent language model.
Our architecture exhibits several attractive properties such as faster run time
and convergence, ability to better handle long sequences and, more importantly,
it helps to avoid some of the major difficulties posed by training VAE models
on textual data.",6 Feb 2017 21:54:07 GMT,Empirical/Data-Driven,Machine learning,generative models;  language generation,Staxxxxxx,Semxxxxxx,xxxxxxxxxxxi-luebeck.de,"Universität zu Lübeck, Institut für Neuro- und Bioinformatik",No,Alixxxxx,Sevxxxx,xxxxxxxxxoogle.com,Google Research,No,Erhxxxx,Baxxx,xxxxxxxxxxxxi-luebeck.de,"Universität zu Lübeck, Institut für Neuro- und Bioinformatik",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Staxxxxxx,Semxxxxxx,"Universität zu Lübeck, Institut für Neuro- und Bioinformatik",,,,,,xxxxxxxxxxxi-luebeck.de,,,,,Germany,,Staxxxxxx Semxxxxxx;Alixxxxx Sevxxxx;Erhxxxx Baxxx,xxxxxxxxxxxi-luebeck.de;xxxxxxxxxgoogle.com;xxxxxxxxxxxxni-luebeck.de,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
212,212X-J3H6G6G2D8,Romanian Word Production: an Orthographic Approach Based on Sequence Labeling,Lixxx Px,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"Languages borrow words from one another for various reasons. How the borrowing
process takes place, how new words enter a target language is one of
the key questions of historical linguistics.
In this paper, we propose a multilingual method for word form production based
on the orthography of the words. For borrowed words, we investigate the
derivation from a source language into a target language.
We also address the problem of genetic cognates derivation. We experiment
with Romanian as a target language and we investigate borrowings from multiple
source languages. The advantages of the proposed method are that it does not
use any external knowledge, except for the training data, and it does not
require the phonetic transcriptions of the input words.",5 Feb 2017 14:11:26 GMT,Empirical/Data-Driven,Multilinguality,cross-lingual approaches;  multilingual applications;  multilingual resources;  lexical borrowing;  alignment,Livxxxxx,Dixx,xxxxxxxxxxxu@gmail.com,University of Bucharest,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Livxxxxx,Dixx,University of Bucharest,,,,,,xxxxxxxxxxxu@gmail.com,,,,,Romania,,Lixxx Px,xxxxxxxxxxxu@gmail.com,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
214,214X-H8G2J5G6F9,Exploring Macro Discourse Structure with Macro-micro Unified Primary-secondary Relationship,Xiaxxxx Cxx;Qiaxxxxx Zxx and Guoxxxx Zxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Discourse structure analysis is helpful for the machine to identify different
types of discourse writing styles, and lays the foundation for the study of
discourse automatic generation. In this paper, after studying the difference
and the relationship between micro and macro discourse structures, we explore
the macro discourse structure, and put forward a Macro Chinese Discourse
Treebank (MCDTB) on the top of existing Chinese Discourse Treebank (CDTB), by
unifying micro and macro discourse structures. Especially, at the micro level,
we put forward the primary-secondary relationship from the logical semantic
perspective, while at the macro level, we put forward the primary-secondary
relationship from the pragmatic function perspective. Preliminary experiments
show that our macro-micro unified schema is appropriate for the discourse
structure analysis.",7 Feb 2017 11:22:05 GMT,Resources/Evaluation,Discourse and pragmatics,corpus development;  discourse,Xiaxxxx,Cxx,xxxxxxxxxxu@gmail.com,Soochow University,No,Qiaxxxxx,Zxx,xxxxxxxxda.edu.cn,Soochow University,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Cxx,Soochow University,,,1386xxxxxxx,,,xxxxxxxxxxu@gmail.com,,,,,China,,Xiaxxxx Cxx;Qiaxxxxx Zxx;Guoxxxx Zhxx,xxxxxxxxxxu@gmail.com;xxxxxxxxxda.edu.cn;xxxxxxxxxuda.edu.cn,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
215,215X-F6C4D8B3P3,A General Framework for Chinese Reading Comprehension of National College Entrance Examination based on Deep Learning,Shxxxx Gxx;Rx Lx;Yoxx Guxx;Qx Zhxxx;Honxxxx Zhxx and Qinxxxx Cxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Reading comprehension is an elusive challenge which attracts more and more
attention. National College Entrance Examination (NCEE) is an academic
examination held annually in the People's Republic of China that students have
to take when they want to be admitted to a college. In this work, we firstly
present a general framework based on deep learning to solve different kinds of
multiple-choice questions of scientific articles reading comprehension in the
National College Entrance Ex-amination (NCEE) on Chinese. Also, we collect 1011
Chinese reading comprehen-sion dataset, which come from NCEE ex-amination and
practice test questions. Experimental results on our collect corpus demonstrate
the effective of our approach.",6 Feb 2017 07:45:45 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  answer extraction;  collaborative methods for question answering,Shxxxx,Gxx,xxxxxxxxxx928@163.com,Shanxi University,No,Rx,Lx,xxxxxxxu.edu.cn,Shanxi University,No,Yoxx,Guxx,xxxxxxxxxx30@163.com,Shanxi University,No,Qx,Zhxxx,xxxxxxxxxx23@126.com,Fuzhou University,No,Honxxxx,Zhxx,xxxxxxxx2@qq.com,Shanxi University,No,Qinxxxx,Chxx,xxxxxxxxxxu.edu.cn,Shanxi University,No,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Gxx,Shanxi University,,,1573xxxxxxx,,,xxxxxxxxxx928@163.com,,Taiyuan,,,China,,Shxxxx Gxx;Rx Lx;Yoxx Guxx;Qx Zhxxx;Honxxxx Zhxx;Qinxxxx Chxx,xxxxxxxxxx928@163.com;xxxxxxxxu.edu.cn;xxxxxxxxxx130@163.com;xxxxxxxxxx023@126.com;xxxxxxxx82@qq.com;xxxxxxxxxsxu.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
216,216X-J4E4P3G3G4,Topical Coherence in LDA-based Models through Induced Segmentation,Hexxx Amoxxxxxx;Wxx Lx;Erxx Gauxxxxx;Geoxxxxx Balxxxx;Maxxxx R and Marxxxxx Clxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"This paper presents an LDA-based model that generates topically coherent
segments within documents by jointly segmenting documents and assigning topics
to their words. The coherence between topics is ensured through a copula,
binding the topics associated to the words of a segment. In addition, this
model relies on both document and segment specific topic distributions so as to
capture fine grained differences in topic assignments. We show that the
proposed model naturally encompasses other state-of-the-art LDA-based models
designed for similar tasks. Furthermore, our experiments, conducted on six
different publicly available datasets, show the effectiveness of our model in
terms of perplexity, Normalized Pointwise Mutual Information, which captures
the coherence between the generated topics, and the Micro F1 measure for text
classification.",22 Apr 2017 12:08:36 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Hexxx,Amoxxxxxx,xxxxxxxxxxxlian@imag.fr,University of Grenoble Alpes,No,Wxx,Lx,xxxxxxxxxtd.edu.sg,Singapore University of Technology and Design,No,Erxx,Gauxxxxx,xxxxxxxxxxier@imag.fr,Univ. Grenoble 1,No,Geoxxxxx,Balxxxx,xxxxxxxxxxxxikas@imag.fr,University Grenoble-Alpes,No,Masxxxxx,Amxxx,xxxxxxxxxxxxamini@imag.fr,University Grenoble Alps,No,Marxxxxx,Claxxxx,xxxxxxxxxxxxusel@imag.fr,University of Grenoble Alpes,No,,,,,,,,,,,,,,,,,,,,,,Hexxx,Amoxxxxxx,University of Grenoble Alpes,,,3375xxxxxxx,,,xxxxxxxxxxxlian@imag.fr,,Grenoble,Isére,,France,,Hexxx Amoxxxxxx;Wxx Lx;Erxx Gauxxxxx;Geoxxxxx Balxxxx;Maxxxx R;Marxxxxx Claxxxx,xxxxxxxxxxxlian@imag.fr;xxxxxxxxxutd.edu.sg;xxxxxxxxxxxier@imag.fr;xxxxxxxxxxxxlikas@imag.fr;xxxxxxxxxxxxxamini@imag.fr;xxxxxxxxxxxxausel@imag.fr,Topical Coherence in LDA-based Models through Induced Segmentation,Topical Coherence in LDA-based Models through Induced Segmentation,11,Hesam Amoualian,,"University of Grenoble Alps
Address: Bâtiment IMAG - 700 Avenue des Résidences
Domaine universitaire de Saint Martin d’Hères - BP53 - 38041 Grenoble Cedex 9, France",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
217,217X-H4E3C3P7H9,Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension,Hoxxxx Lxx;Lx Sxx and Xiaxxxx Hx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Reasoning with commonsense knowledge is critical for natural language
understanding. Traditional methods for commonsense machine comprehension mostly
only focus on one specific kind of knowledge, neglecting the fact that
commonsense reasoning requires simultaneously considering different kinds of
commonsense knowledge. In this paper, we propose a multi-knowledge reasoning
method, which can exploit heterogeneous knowledge for commonsense machine
comprehension. Specifically, we first mine different kinds of knowledge
(including event narrative knowledge, entity semantic knowledge and sentiment
coherent knowledge) and encode them as inference rules with costs. Then we
propose a multi-knowledge reasoning model, which selects inference rules for a
specific reasoning context using attention mechanism, and makes reasoning by
summarizing all valid inference rules. Experiments on RocStories show that our
method outperforms traditional models significantly.",7 Feb 2017 09:03:15 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  textual entailment and paraphrasing;  open-domain question answering,Hoxxxx,Lxx,xxxxxxxxxxxxe@foxmail.com,"Institute of Software, Chinese Academy of Sciences",No,Lx,Sxx,xxxxxxx@163.com,ISCAS,No,Xiaxxxx,Hxx,xxxxxxxxxx@gmail.com,"Institute of Software, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hoxxxx,Lxx,"Institute of Software, Chinese Academy of Sciences",,,,,,xxxxxxxxxxxxe@foxmail.com,,Beijing,Beijing,,China,,Hoxxxx Lxx;Lx Sxx;Xiaxxxx Hxx,xxxxxxxxxxxxe@foxmail.com;xxxxxxxx@163.com;xxxxxxxxxxi@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
218,218X-D6C4D9G9A7,Prerequisite Relation Learning for Concepts in MOOCs,Liaxxxxxx Pxx;Chexxxxxxx Lx;Juxxxx Lx and Jxx Txx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"What prerequisite knowledge should students achieve a level of mastery before
moving forward to learn subsequent coursewares? We study the extent to which
the prerequisite relation between knowledge concepts in Massive Open Online
Courses (MOOCs) can be inferred automatically. In particular, what kinds of
information can be leverage to uncover the potential prerequisite relation
between knowledge concepts. We first propose a representation learning-based
method for learning latent representations of course concepts, and then
investigate how different features capture the prerequisite relations between
concepts. Our experiments on three datasets form Coursera show that the
proposed method achieves significant improvements (+5.9-48.0% by F1-score)
comparing with existing methods.",21 Apr 2017 11:03:08 GMT,Theoretical,"Information extraction, text mining, and question answering",,Liaxxxxxx,Pxx,xxxxxxxxxxxx1020@163.com,Tsinghua University,No,Chexxxxxxx,Lx,xxxxxxxxxxxxxxtsinghua.edu.cn,Tsinghua University,No,Juxxxx,Lx,xxxxxxxxxxx8@gmail.com,Tsinghua University,No,Jxx,Taxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Liaxxxxxx,Pxx,Tsinghua University,,,1501xxxxxxx,,,xxxxxxxxxxxx1020@163.com,,Beijing,,,China,,Liaxxxxxx Pxx;Chexxxxxxx Lx;Juxxxx Lx;Jxx Taxx,xxxxxxxxxxxx1020@163.com;xxxxxxxxxxxxxxxtsinghua.edu.cn;xxxxxxxxxxx08@gmail.com;xxxxxxxxxxxxnghua.edu.cn,Prerequisite Relation Learning for Concepts in MOOCs,Prerequisite Relation Learning for Concepts in MOOCs,10,Liangming Pan,,"Knowledge Engineering Laboratory, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China",on,on,No. Do not include my submission in this dataset.,No,None,None
219,219X-C3C5F9A3H9,Learning Structured Natural Language Representations for Semantic Parsing,Jiaxxxxx Chxxx;Sixx Rexxx;Vixxx Sarxxxxx and Mirxxxx Laxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Monday,,Undecided (Semantics),"We introduce a neural semantic parser which is interpretable and scalable. Our
model converts natural language utterances to intermediate, domain-general
natural language representations in the form of predicate-argument structures,
which are induced with a transition system and subsequently mapped to target
domains. The semantic parser is trained end-to-end using annotated logical
forms or their denotations. We achieve the state of the art on SPADES and
GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The
induced predicate-argument structures shed light on the types of
representations useful for semantic parsing and how these are dif- ferent from
linguistically motivated ones.",22 Apr 2017 10:35:18 GMT,Empirical/Data-Driven,Semantics,,Jiaxxxxx,Chxxx,xxxxxxxxxxxeng@ed.ac.uk,University of Edinburgh,No,Sixx,Rexxx,xxxxxxxxxnford.edu,Stanford University,No,Vixxx,Sarxxxxx,xxxxxxxxxus.ibm.com,IBM,No,Mirxxxx,Laxxxx,xxxxxxxx.ed.ac.uk,"School of Informatics, University of Edinburgh",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jiaxxxxx,Chxxx,University of Edinburgh,,,,,,xxxxxxxxxxxeng@ed.ac.uk,,,,,United Kingdom,,Jiaxxxxx Chxxx;Sixx Rexxx;Vixxx Sarxxxxx;Mirxxxx Laxxxx,xxxxxxxxxxxeng@ed.ac.uk;xxxxxxxxxanford.edu;xxxxxxxxxxus.ibm.com;xxxxxxxxx.ed.ac.uk,Learning Structured Natural Language Representations for Semantic Parsing,Learning Structured Natural Language Representations for Semantic Parsing,12,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
220,220X-E5G2F8C8J3,Vancouver Welcomes You! Minimalist Location Metonymy Resolution,Mixxx Grxxxx;Mohxxxxx Taxxx;Nxx Limsxxxxxxx and Nixxx Coxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Wednesday,,Undecided (IE QA Text Mining Applications),"Named entities are frequently used in a metonymic manner. They serve as
references to related entities such as people and organisations. Accurate
identification and interpretation of metonymy can be directly beneficial to
various NLP applications, such as Named Entity Recognition and Geographical
Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers,
taggers, dictionaries, external word lists and other handcrafted lexical
resources. We show how a minimalist neural approach combined with a novel
predicate window method can achieve competitive results on the SemEval 2007
task on Metonymy Resolution. Additionally, we contribute with a new
Wikipedia-based MR dataset called RelocaR, which is tailored towards locations
as well as improving previous deficiencies in annotation guidelines.",23 Apr 2017 09:48:01 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Mixxx,Grxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Mohamxxxxxxxxx,Pilxxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Nxx,Limsxxxxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Nixxx,Colxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mixxx,Grxxxx,University of Cambridge,,,,,,xxxxxxxam.ac.uk,,,,,United Kingdom,,Mixxx Grxxxx;Mohxxxxx Taxxx;Nxx Limsxxxxxxx;Nixxx Colxxxx,xxxxxxxam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxam.ac.uk,Vancouver Welcomes You! Minimalist Location Metonymy Resolution,Vancouver Welcomes You! Minimalist Location Metonymy Resolution,12,Milan Gritta,,"Language Technology Lab, Department of Theoretical and Applied Linguistics, University of Cambridge, English Faculty Building, 9 West Road, Cambridge CB3 9DA, United Kingdom",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
221,221X-P8J4J6J8E7,Translating Phrases in Neural Machine Translation,Xixx Waxx;Dexx Xixxx;Zhaxxxxx Tx and Mxx Zhxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Phrases play an important role in natural language understanding and machine
translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is
difficult to integrate them into current neural machine translation (NMT) which
reads and generates sentences word by word. In this work, we propose a method
to translate phrases in NMT by integrating a phrase cache storing phrasal
recommendations from a phrase-based statistical machine translation
(SMT) system into the encoder-decoder architecture of NMT. At each decoding
step, the SMT model dynamically generates relevant phrasal recommendations in
the cache with contextual information provided by the NMT system. If the NMT
decoder decides to generate a phrase rather than a word, it enquires the cache
and selects an appropriate phrase to perform phrase translation. In addition,
we enrich the NMT encoder with source-side chunk information to help the NMT
decoder make better decisions on phrase generation. Experiment results on
Chinese→English translation show that the proposed model achieves significant
improvements over the baseline on various test sets.",7 Feb 2017 08:45:01 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation,Xixx,Waxx,xxxxxxxxx@gmail.com,Soochow University,No,Dexx,Xixxx,xxxxxxxxxx@gmail.com,Soochow University,No,Zhaxxxxx,Tx,xxxxxxxxxx@gmail.com,Huawei Noah's Ark Lab,No,Mxx,Zhxxx,xxxxxxxxxxuda.edu.cn,Suda,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixx,Waxx,Soochow University,,,,,,xxxxxxxxx@gmail.com,,,,,China,,Xixx Waxx;Dexx Xixxx;Zhaxxxxx Tx;Mxx Zhxxx,xxxxxxxxx@gmail.com;xxxxxxxxxxg@gmail.com;xxxxxxxxxxg@gmail.com;xxxxxxxxxxsuda.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
222,222X-J8A5B6G3A4,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,Sunxxxx Zhxxx;Fexx Waxx;Honxxxx Bxx;Yuexxxx Hxx;Pexx Zhxx and Bx X,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Wednesday,,Undecided (IE QA Text Mining Applications),"Joint extraction of entities and relations is an important task in information
extraction. To tackle this problem, we firstly propose a novel tagging scheme
that can convert the joint extraction task to a tagging problem.. Then, based
on our tagging scheme, we study different end-to-end models to extract entities
and their relations directly, without identifying entities and relations
separately. We conduct experiments on a public dataset produced by distant
supervision method and the experimental results show that the tagging based
methods are better than most of the existing pipelined and joint learning
methods. What’s more, the end-to-end model proposed in this paper, achieves
the best results on the public dataset.",21 Apr 2017 02:01:20 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Sunxxxx,Zhxxx,xxxxxxxxxxxng@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Fexx,Waxx,xxxxxxxxx@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Honxxxx,Bxx,xxxxxxxxxxo@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Yuexxxx,Hxx,xxxxxxxxxxx014@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Pexx,Zhxx,xxxxxxxxx@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Bx,Xx,xxxxxxa.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,Sunxxxx,Zhxxx,"Institute of Automation, Chinese Academy of Sciences",,,8613xxxxxxxxx,,,xxxxxxxxxxxng@ia.ac.cn,,,,,China,,Sunxxxx Zhxxx;Fexx Waxx;Honxxxx Bxx;Yuexxxx Hxx;Pexx Zhxx;Bx Xx,xxxxxxxxxxxng@ia.ac.cn;xxxxxxxxxg@ia.ac.cn;xxxxxxxxxxao@ia.ac.cn;xxxxxxxxxxxx014@ia.ac.cn;xxxxxxxxxu@ia.ac.cn;xxxxxxxa.ac.cn,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,10,Suncong Zheng,,"Institute of Automation, Chinese Academy of Sciences, 100190, Beijing, P.R. China",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
223,223X-D8A6F4E6D8,Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG),Toxxx Caxxx;Stxxxx Lx and Rexx Tsaxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Poster Monday,,Undecided (Generation Summarization),"Opinionated Natural Language Generation (ONLG) is a new, challenging, task that
aims to automatically generate human-like, subjective, responses to opinionated
articles online. 

We present a data-driven architecture for ONLG that generates subjective
responses triggered by users’ agendas, consisting of topics and sentiments,
and based on wide-coverage automatically-acquired generative grammars.
We compare three types of grammatical representations that we design for ONLG,
which interleave different layers of linguistic information and are induced
from a new, enriched dataset we developed.

Our evaluation shows that generation with Relational-Realizational (Tsarfaty
and Sima’an, 2008) inspired grammar gets better language model scores than
lexicalized grammars `a la Collins (2003), and that the latter gets better
human-evaluation scores. 

We also show that conditioning the generation on topic models makes generated
responses more relevant to the document content.",23 Apr 2017 10:53:59 GMT,Empirical/Data-Driven,Generation,,Toxxx,Caxxx,xxxxxxxxxxxxxost.idc.ac.il,"The Interdisciplinary Center, Herzlia",No,Stexxxxxx,Frxxx,xxxxxxxxlet.ru.nl,Radboud University Nijmegen,No,Rexx,Tsaxxxxx,xxxxxxxxxenu.ac.il,Open University of Israel,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Toxxx,Caxxx,"The Interdisciplinary Center, Herzlia",,,+972(xxxxxxxxxx,,,xxxxxxxxxxxxxost.idc.ac.il,,,,,Israel,"M.Sc. student at Efi Arazi School of Computer Science, The Interdisciplinary Center, Herzlia",Toxxx Caxxx;Stxxxx Lx;Rexx Tsaxxxxx,xxxxxxxxxxxxxost.idc.ac.il;xxxxxxxxxlet.ru.nl;xxxxxxxxxpenu.ac.il,Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG),Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG),11,Tomer Cagan,,"Interdisciplinary Center (IDC)
PO Box 167, 1 Kanfei Nesharim Street
Herzliya, Israel 4610101",on,on,No. Do not include my submission in this dataset.,No,None,None
225,225X-F8P7H3P7B6,A target-oriented text scanning mechanism,Bxx Xx and Hxx Zhxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"One important human reading style is tar-get-oriented, which only focuses on
target-related contents. This paper reports the realization of a text scanning
mechanism that simulates human target reading process. The reading process is
simulated by identifying target reading scope and modeling memory operations
such as recall, association and forget. The impressions of words change during
reading. Experiments show that: (1) the mechanism is in line with human target
reading process, (2) the mechanism can extract the target-related content from
text, and (3) the mechanism can measure target-oriented text similarity.",5 Feb 2017 17:50:18 GMT,Theoretical,Cognitive modeling and psycholinguistics,generative models,Bxx,Xx,xxxxxxxxxpt.edu.cn,"Nanjing University of Posts and Telecommunications, China",No,Hxx,Zhxxx,xxxxxxxct.ac.cn,Aston University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bxx,Xx,"Nanjing University of Posts and Telecommunications, China",,,+86-1xxxxxxxxxx,,,xxxxxxxxxpt.edu.cn,,,,,China,,Bxx Xx;Hxx Zhxxx,xxxxxxxxxpt.edu.cn;xxxxxxxxct.ac.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
226,226X-C3J8C3F2D6,Polish evaluation dataset for compositional distributional semantics models,Alxxx Wróxxxxxxx and Katxxxxxx Krasnoxxxxxxxxxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Accept - Oral Tuesday,,Undecided (Resources Evaluation),"The paper presents a procedure of building an evaluation dataset. for the
validation of compositional distributional semantics models estimated for
languages other than English. The procedure generally builds on steps designed
to assemble the SICK corpus, which contains pairs of English sentences
annotated for semantic relatedness and entailment, because we aim at building a
comparable dataset. However, the implementation of particular building steps
significantly differs from the original SICK design assumptions, which is
caused by both lack of necessary extraneous resources for an investigated
language and the need for language-specific transformation rules. The designed
procedure is verified on Polish, a fusional language with a relatively free
word order, and contributes to building a Polish evaluation dataset. The
resource consists of 10K sentence pairs which are human-annotated for semantic
relatedness and entailment. The dataset may be used for the evaluation of
compositional distributional semantics models of Polish.",21 Apr 2017 14:01:14 GMT,Resources/Evaluation,Resources and evaluation,,Alxxx,Wróxxxxxxx,xxxxxxxxxpan.waw.pl,"Institute of Computer Science, Polish Academy of Sciences",No,Katxxxxxx,Krasnoxxxxxxxxxxxx,xxxxxxxxxxxxxska@gmail.com,"Institute of Computer Science, Polish Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alxxx,Wróxxxxxxx,"Institute of Computer Science, Polish Academy of Sciences",,,,,,xxxxxxxxxpan.waw.pl,,Warsaw,,,Poland,,Alxxx Wróxxxxxxx;Katxxxxxx Krasnoxxxxxxxxxxxx,xxxxxxxxxpan.waw.pl;xxxxxxxxxxxxxwska@gmail.com,Polish evaluation dataset for compositional distributional semantics models,Polish evaluation dataset for compositional distributional semantics models,9,Alina Wróblewska,,"Institute of Computer Science, Polish Academy of Sciences
Jana Kazimierza 5
01-248 Warsaw
POLAND",,on,Only include my submission if it is accepted.,No,None,None
227,227X-B9B6F3D9J5,Using Context Information for Dialog Act Classification in DNN Framework,Yaxx Lxx;Kxx Hxx;Zhxx Txx and Yxx Lx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Accurate classification of dialog acts (DAs) is an important part in
conversational systems. Previous work has investigated different methods, such
as hidden Markov models, maximum entropy, conditional random fields, graphical
models, and support vector machines. There is some recent work using deep
learning neural networks for DA classification, but dialog context or DA
sequential information has not been well utilized. This paper proposes and
compares various ways of using context information for DA classification in the
deep learning framework. The baseline system classifies each utterance using
the convolutional neural networks (CNN). Our proposed methods include using
hierarchical models (recurrent neural networks (RNN) or CNN) for DA sequence
tagging where the bottom layer takes the sentence CNN representation as input,
concatenating predictions from the previous utterances with the CNN vector for
classification, and performing sequence decoding based on the predictions from
the sentence CNN model. Our experiments on the Switchboard corpus demonstrate
that incorporating context information significantly improves DA
classification, and we achieve the state-of-the-art performance for this task.
This is the first study with thorough experiments and comparisons for the use
of context information in the deep learning framework for DA classification.",10 Feb 2017 02:27:01 GMT,Empirical/Data-Driven,Dialog and interactive systems,spoken language understanding;  dialogue;  text classification,Yaxx,Lxx,xxxxxxxxxxxtdallas.edu,University of Texas at Dallas,No,Kxx,Hxx,xxxxxx@fb.com,Facebook,No,Zhxx,Txx,xxxxxfb.com,Facebook,No,Yxx,Lxx,xxxxxb.com,Facebook,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Lxx,LingoChamp,,,214xxxxxxx,,,xxxxxxxxxx@gmail.com,,,,,United States,,Yaxx Lxx;Kxx Hxx;Zhxx Txx;Yxx Lxx,xxxxxxxxxxxtdallas.edu;xxxxxxx@fb.com;xxxxxxfb.com;xxxxxfb.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
228,228X-P3E6C9J3G5,Modeling Relevance via End-to-End Learning to Rank,Yaxx Lxx;Wexxx Roxx and Zhxxx Xixx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper aims to model sequences relevance from the learning-to-rank(LtR)
perspective. We approach the tasks of answer selection(AS) and textual
entailment(TE) using an end-to-end listwise ranking method, much different from
the conventional classification methods. The system should be capable of
recognizing the key points within an answer as well as capturing nuances among
different relevant answers. As for the former, we propose the local matching
model(LMM) to decompose long answer and match its components with question
respectively for a more precise comparison; As for the latter, we adopt
listwise ranker to distinguish diverse relevance answers globally. Our method
has achieved state-of-the-art performance on two popular AS benchmarks and also
shows effectiveness in TE task. In addition, we further release the ZhihuRank
dataset where answers are labeled from 0(irrelevant) to 4(perfectly relevant)
compared to the existing binary-graded AS datasets. ZhihuRank is a multi-graded
textual QA dataset extracted from real QA forum, which can be fed to a
complicated neural architecture to train an end-to-end LtR model. On this newly
released data with multi-graded long answers, LMM shows advantages over
document-level encoders and the ranking methods also outperform classification
methods by a large margin.",6 Feb 2017 09:22:04 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",corpus development;  NLP applications;  information retrieval;  textual entailment and paraphrasing;  context-aware question answering;  experimental evaluation/comparison of ML methods;  semantic relations;  text classification;  collaborative methods for question answering;  open-domain question answering;  NLP on Wikipedia and other collaboratively constructed resources,Yaxx,Lxx,xxxxxxxxxxx@buaa.edu.cn,Beihang University,No,Wexxx,Roxx,xxxxxxxxxaa.edu.cn,Beihang University,No,Zhxxx,Xixxx,xxxxxxxxxaa.edu.cn,Beihang University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Lxx,Beihang University,,,,,,xxxxxxxxxxx@buaa.edu.cn,,Beijing,Beijing,,China,,Yaxx Lxx;Wexxx Roxx;Zhxxx Xixxx,xxxxxxxxxxx@buaa.edu.cn;xxxxxxxxxuaa.edu.cn;xxxxxxxxxuaa.edu.cn,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
229,229X-E3C6G2G7B6,Longitudinal Modeling of Social Media with Hawkes Process based on Users and Networks,Px Kx;Mixxxx Lukxxxx;Kaxxxx Bonxxxxxx and Trxxxx Cxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Online social networks provide a platform for sharing information at an
unprecedented scale. Users generate information which propagates across the
network resulting in
information cascades. In this paper, we study the evolution of information
cascades in
Twitter using a point  process model of user activity. We develop several
Hawkes process models considering various  properties including conversational
structure, users' connections and general features of users including the
textual information, and show how they are helpful in modeling the social
network activity. 
We consider low-rank embeddings of users and user features, and learn the
features helpful in identifying the influence and susceptibility of users. 
Evaluating on Twitter data sets associated with civil unrest, we show that
incorporating richer properties improves the  performance in predicting future
activity of users and memes.",7 Feb 2017 12:03:10 GMT,Empirical/Data-Driven,Social media,social network,P.xxx,Srixxxx,xxxxxxxxxith.ac.in,Indian Institute of Technology Hyderabad,No,Mixxxx,Lukxxxx,xxxxxxxxxxshef.ac.uk,University of Sheffield,No,Kaxxxx,Bonxxxxxx,xxxxxxxxxxxxxcs.shef.ac.uk,University of Sheffield,No,Trxxxx,Coxx,xxxxxxxxxxelb.edu.au,University of Melbourne,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,P.xxx,Srixxxx,Indian Institute of Technology Hyderabad,,,,,,xxxxxxxxxith.ac.in,,,,,India,,Px Kx;Mixxxx Lukxxxx;Kaxxxx Bonxxxxxx;Trxxxx Coxx,xxxxxxxxxith.ac.in;xxxxxxxxxx@shef.ac.uk;xxxxxxxxxxxxxdcs.shef.ac.uk;xxxxxxxxxxmelb.edu.au,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
230,230X-G3G8J5H7B6,Two Problems in Knowledge Graph Embedding: Non-Exclusive Relation Types and Vanishing Gradients,Hyuxxxxxx Kaxx;Nosxxxx Paxx;Kooxxxx Lxx;Yoxxxx Lxx and Sooxxxxx Kxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Knowledge Graph Embedding (KGE) learns latent vector representations of
vertices and relations. We address two problems in the KGE.

First, relations may belong to one or multiple of functional, symmetric,
transitive and reflexive types --- i.e., relations types are not exclusive.

Second, all existing KGE algorithms use a hinge loss function based on energy
gap. We found that at most half of the training triples cannot contribute to a
loss minimization process via stochastic gradient descent due to vanishing
gradients.

We propose to i) convert a knowledge graph to a bipartite graph --- we do not
physically convert but use an equivalent implementation trick --- ii) use
multiple vector representations for a relation, and iii) use a new hinge loss
based on energy ratio instead of energy gap that does not have vanishing
gradients. We confirmed that our method improve the answer accuracy
significantly after very extensive experiments with many other baseline KGE
algorithms.",6 Feb 2017 01:44:26 GMT,Theoretical,"Information extraction, text mining, and question answering",context-aware question answering;  answer extraction;  semantic relations;  semantic knowledge induction;  question answering in restricted domains,Hyuxxxxxx,Kaxx,xxxxxxxxtri.re.kr,Electronics and Telecommunications Research Institute,No,Nosxxxx,Paxx,xxxxxxxuncc.edu,"University of North Carolina, Charlotte",No,Kooxxxx,Lxx,xxxxxxx.umd.edu,"University of Maryland, College Park",No,Yoxxxx,Lxx,xxxxxxxx.umd.edu,"University of Maryland, College Park",No,Sooxxxxx,Kwxx,xxxxxxxxxxx@etri.re.kr,Electronics and Telecommunications Research Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Nosxxxx,Paxx,"University of North Carolina, Charlotte",,,,,,xxxxxxxuncc.edu,,,,,United States,,Hyuxxxxxx Kaxx;Nosxxxx Paxx;Kooxxxx Lxx;Yoxxxx Lxx;Sooxxxxx Kwxx and Telecoxxxxxxxxxxxx Resxxxxx,xxxxxxxxtri.re.kr;xxxxxxxxuncc.edu;xxxxxxxx.umd.edu;xxxxxxxxs.umd.edu;xxxxxxxxxxxg@etri.re.kr,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
231,231X-G3H9J9D7E9,Neural Probabilistic Model for Non-projective MST Parsing,Xuxxxx Mx and Edxxxx Hoxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"In this paper, we propose a probabilistic parsing model that defines a proper
conditional probability distribution over nonprojective dependency trees for a
given sentence, using neural representations as inputs. 
The neural network architecture is based on bi-directional LSTMCNNs,
which automatically benefits from both word- and character-level
representations, by using a combination of bidirectional LSTMs and CNNs. 
On top of the neural network, we introduce a probabilistic structured layer,
defining a conditional log-linear model over nonprojective trees. 
By exploiting Kirchhoff’s Matrix-Tree Theorem (Tutte, 1984), the partition
functions and marginals can be computed efficiently, leading to a
straightforward end-to-end model training procedure via back-propagation. 
We evaluate our model on 17 different datasets, across 14 different languages.
Our parser achieves state-of-the-art parsing performance on nine datasets.",5 Feb 2017 19:14:46 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",syntax;  parsing,Xuxxxx,Mx,xxxxxxxxxs.cmu.edu,"Language Technologies Institute, Carnegie Mellon University",No,Edxxxx,Hoxx,xxxxxxmu.edu,CMU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xuxxxx,Mx,"Language Technologies Institute, Carnegie Mellon University",,,206xxxxxxx,,,xxxxxxxxxs.cmu.edu,,,,,United States,,Xuxxxx Mx;Edxxxx Hoxx,xxxxxxxxxs.cmu.edu;xxxxxxcmu.edu,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
232,232X-A4D5B6E5D6,Improving Probabilistic Topic Models using Word Embeddings,Stxxxx Buxx and Raxx Krexxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"The distributional hypothesis poses that similar words tend to have similar
contexts in which they occur. Probabilistic topic models utilize word
co-occurrences across documents, while word embedding models learn word vectors
by predicting the local context of a word. Due to their complementary nature,
the models define different notions of word similarity, which can be exploited
by combining them. In this paper, we propose WELDA, a new type of topic model,
which combines both models to improve topic quality. We achieve this by
estimating topic distributions in the word embedding space and sampling new
words from them. We compare WELDA with standard LDA and competing approaches
and present an extensive evaluation, showing that WELDA outperforms all other
approaches with respect to topic coherence.",6 Feb 2017 22:46:10 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",generative models;  unsupervised and semi-supervised learning;  distributional similarity;  graphical models,Stxxxx,Buxx,xxxxxxxxxxxxxtudent.hpi.de,Hasso Plattner Institute,No,Raxx,Krexxxx,xxxxxxxxxtel@hpi.de,Hasso Plattner Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Raxx,Krexxxx,"Hasso Plattner Institute, University of Potsdam",,,,,,xxxxxxxxxtel@hpi.de,,,,,Germany,,Stxxxx Buxx;Raxx Krexxxx,xxxxxxxxxxxxxtudent.hpi.de;xxxxxxxxxxtel@hpi.de,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
233,233X-A3E4P3A6D2,Improving a tf-idf weighted document vector embedding,Crxxx Schxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We examine a number of methods to compute a dense vector embedding for a
document in a corpus, given a set of word vectors such as those from word2vec
or GloVe.  We describe two methods that can improve upon a simple weighted sum,
that are optimal in the sense that they maximizes a particular weighted cosine
similarity measure.

  We consider several weighting functions, including inverse document frequency
(idf), smooth inverse frequency (SIF), and the sub-sampling function used in
word2vec.  We find that idf works best for our applications. We also use common
component removal proposed by Arora et al. as a post-process and find it is
helpful in most cases. 

  We compare these embeddings variations to the doc2vec embedding on a new
evaluation task using TripAdvisor reviews, and also on the CQADupStack
benchmark from the literature.",5 Feb 2017 19:48:56 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",distributional similarity;  document clustering,Crxxx,Schxxxx,xxxxxxxxxxxxpadvisor.com,TripAdvisor,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Crxxx,Schxxxx,TripAdvisor,,,,,,xxxxxxxxxxxxpadvisor.com,,,,,United States,,Crxxx Schxxxx,xxxxxxxxxxxxpadvisor.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
234,234X-A2F5G6J6C2,Understanding Executable Programs with Attention Tree-To-Sequence Neural Model,Honxxxxx Mxx and Chxxx Quxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"We develop models to generate natural language descriptions of executable
programs, a crucial step in multi-turn goal-oriented dialogs. Programs can be
naturally represented as K-ary tree structures of functions and literals. Our
model first encodes these programs using a recursive neural network (TreeRNN)
over their tree structures. These learned representations inform a decoder that
generates human language descriptions using a recurrent neural network (RNN). A
neural attention mechanism associates each generated token with its
corresponding sub-tree in the program, and a word-character hybrid decoder
vastly increases the vocabulary size while minimizing computational complexity.
On the benchmark IFTTT dataset, our model outperforms a strong baseline of
sequence-to-sequence neural model by a relative improvement of 180% on two
evaluation metrics. Synthetic data experiments further demonstrate the
importance of modeling tree structure when encoding complex programs.",6 Feb 2017 21:17:58 GMT,Empirical/Data-Driven,Generation,NLP applications;  language generation;  structured input/output;  experimental evaluation/comparison of ML methods;  multimodal communication;  multimodal representations and processing;  alignment,Honxxxxx,Mxx,xxxxxxhu.edu,Johns Hopkins University,No,Chxxx,Quxxx,xxxxxxxxxxrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Honxxxxx,Mxx,Johns Hopkins University,,,,,,xxxxxxhu.edu,,Baltimore,MD,,United States,,Honxxxxx Mxx;Chxxx Quxxx,xxxxxxhu.edu;xxxxxxxxxxcrosoft.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
235,235X-D3G3F2B7J9,Prepositions in Context,Hoxxxx Goxx;Jixxx Mx;Suxx Bhxx and Prxxxx Visxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Prepositions are  highly polysemous, and their variegated senses encode
significant semantic information. In this paper we match each preposition's
complement and attachment  and their interplay crucially to the geometry of the
word vectors to the left and right of the preposition. Extracting such features
from the vast number of instances of each preposition and clustering them makes
for an efficient preposition sense disambigution (PSD) algorithm, which is
comparable to and better than state-of-the-art on two benchmark datasets. Our
reliance on no external linguistic resource allows us to scale the PSD 
algorithm  to a large WikiCorpus and learn sense-specific preposition
representations -- which we show  to encode   semantic relations and 
paraphrasing of verb particle compounds, via simple vector  operations.",7 Feb 2017 03:35:38 GMT,Empirical/Data-Driven,Semantics,word sense disambiguation,Hoxxxx,Goxx,xxxxxxxxxlinois.edu,University of Illinois at Urbana Champaign,No,Jixxx,Mx,xxxxxxxxxxllinois.edu,University of Illinois at Urbana Champaign,No,Suxx,Bhxx,xxxxxxxxxxlinois.edu,Beckman Institute,No,Prxxxx,Visxxxxxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana Champaign,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hoxxxx,Goxx,University of Illinois at Urbana Champaign,,,,,,xxxxxxxxxlinois.edu,,,,,United States,,Hoxxxx Goxx;Jixxx Mx;Suxx Bhxx;Prxxxx Visxxxxxx,xxxxxxxxxlinois.edu;xxxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
236,236X-J7B8A8A3H3,Content-Based Conflict of Interest Detection on Wikipedia,Udoxxxxxx Orxxx and Yuxxx Hx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Wikipedia is one of the most visited websites in the world. On Wikipedia,
Conflict-of-Interest (CoI) editing happens when an editor uses Wikipedia to
advance their interests or relationships. This includes paid editing done by
organisations for public relations purposes, etc. CoI detection is a highly
subjective problem and though closely related to vandalism and bias detection,
it is a more difficult problem. In this paper, we frame CoI detection as a
binary classification problem and explore various features which can be used to
train supervised classifiers for CoI detection on Wikipedia articles. Our
experimental results show that the best F-measure achieved is 0.67 by training
SVM from a combination of features including stylometric, bias and emotion
features. As we are not certain that our non-CoI set does not contain any CoI
articles, we have also explored the use of one-class classification for CoI
detection. The results show that using stylometric features outperforms other
types of features or a combination of them and gives an F-measure of 0.63.
Also, while binary classifiers give higher recall values (0.81~0.94), one-class
classifier attains higher precision values (0.69~0.74).",6 Feb 2017 12:21:48 GMT,Empirical/Data-Driven,Social media,NLP applications;  NLP on Wikipedia and other collaboratively constructed resources;  NLP for Web 2.0,Udoxxxxxx,Orxxx,xxxxxxxxxston.ac.uk,Aston University,No,Yuxxx,Hx,xxxxxxxxton.ac.uk,Aston University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxx,Hx,Aston University,,,,,,xxxxxxxntab.net,,,,,United Kingdom,,Udoxxxxxx Orxxx;Yuxxx Hx,xxxxxxxxxston.ac.uk;xxxxxxxxxton.ac.uk,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
237,237X-H3B4A6P8C8,A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space,Youxxxxx Kxx and Hyxxxx Shxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"This study implements a vector space model approach to measure the sentiment
orientations of words. Two representative vectors for positive/negative
polarity are constructed using high-dimensional vec-tor space in both an
unsupervised and a semi-supervised manner. A sentiment ori-entation value per
word is determined by taking the difference between the cosine distances
against the two reference vec-tors. These two conditions (unsupervised and
semi-supervised) are compared against an existing unsupervised method (Turney,
2002). As a result of our experi-ment, we demonstrate that this novel ap-proach
significantly outperforms the pre-vious unsupervised approach and is more
practical and data efficient as well.",6 Feb 2017 14:23:58 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  unsupervised and semi-supervised learning;  opinion mining and extraction;  experimental evaluation/comparison of ML methods,Youxxxxx,Kxx,xxxxxxxxx@gmail.com,Seoul National University,No,Hyxxxx,Shxx,xxxxxxxxnu.ac.kr,Seoul National University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Youxxxxx,Kxx,Seoul National University,,,8210xxxxxxxx,,,xxxxxxxxx@gmail.com,,,,,Republic of Korea,,Youxxxxx Kxx;Hyxxxx Shxx,xxxxxxxxx@gmail.com;xxxxxxxxsnu.ac.kr,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
238,238X-J4D3C4E5F7,Building Dense Representations from Distributional Thesauri for Synonym Extraction and Expansion,Olixxxx Fexxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"In this article, we propose considering a new problem consisting in turning a
distributional thesaurus into dense word vectors. We propose more precisely a
method for performing such task based on the association of graph embedding and
knowledge injection into distributed representations. We have applied and
evaluated it for English nouns at a large scale about its ability to retrieve
synonyms. In this context, we have also illustrated the interest of the
developed method for three different tasks: the endogenous improvement of
already existing word embeddings, the fusion of heterogeneous representations
and the expansion of synsets.",7 Feb 2017 11:29:50 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  distributional similarity;  semantic relations;  semantic knowledge induction,Olixxxx,Fexxxx,xxxxxxxxxxrret@cea.fr,CEA LIST,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Olixxxx,Fexxxx,CEA LIST,,,,,,xxxxxxxxxxrret@cea.fr,,Gif-sur-Yvette,,,France,,Olixxxx Fexxxx,xxxxxxxxxxrret@cea.fr,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
239,239X-D3C9D9P2J3,How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks,Wojxxxxx Czaxxxxxx and Daxxxx Lesxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Recent practice in word embeddings points towards importance of learning
specialized representations.  Additionally, in many applications purely
unsupervised training is useful only under limited dataset size. We argue that
focus of word representation evaluation should reflect those trends and shift
towards evaluating what \emph{useful} information is \emph{easily} accessible.
Specifically, we argue that evaluation should focus on data efficiency and
simple supervised tasks, where the amount of available data should be varied
and scores of a \emph{supervised} model for each subset should be reported (as
commonly done in transfer learning). 

In order to illustrate significance of such analysis, a comprehensive
evaluation of selected word embeddings is presented. Proposed approach yields a
more complete picture and  brings new insight into performance characteristics,
for instance information about word similarity or analogy tends to be
non--linearly encoded in the embedding space, which questions the
cosine--based, unsupervised, evaluation methods.  All results and analysis
scripts are available online.",7 Feb 2017 10:16:48 GMT,Resources/Evaluation,Machine learning,unsupervised and semi-supervised learning;  scalability/efficiency of ML methods;  domain adaptation;  learning with small datasets;  discriminative learning methods;  experimental evaluation/comparison of ML methods;  distributional similarity;  text classification;  evaluation metrics,Wojxxxxx,Czaxxxxxx,xxxxxxxxxxxxxxecki@uj.edu.pl,"Jagiellonian University, DeepMind",No,Daxxxx,Lesxxxx,xxxxxxxxx@gmail.com,Jagiellonian University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Staxxxxxx,Jastxxxxxxx,Jagiellonian University,,,,,,xxxxxxxxxxxxxxebski@gmail.com,,,,,Poland,,Wojxxxxx Czaxxxxxx;Daxxxx Lesxxxx,xxxxxxxxxxxxxxecki@uj.edu.pl;xxxxxxxxxx@gmail.com,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
240,240X-B3F7E4H7G9,Unsupervised Text Segmentation Based on Native Language Characteristics,Shexxxx Malxxxx;Maxx Drxx;Maxx Johxxxx;Lxx Dx and Magxxxxxx Woxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"Most work on segmenting text does so on the basis of topic changes,
but it can be of interest to segment by other, stylistically expressed
characteristics such as change of authorship or native language.  We
propose a Bayesian unsupervised text segmentation approach to the
latter.  While baseline models achieve essentially random segmentation
on our task, indicating its difficulty, a Bayesian model that
incorporates appropriately compact language models and alternating
asymmetric priors can achieve scores on the standard metrics around
halfway to perfect segmentation.",21 Apr 2017 11:33:14 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Shexxxx,Malxxxx,xxxxxxxxxxxxxxxxxtudents.mq.edu.au,Harvard Medical School,No,Maxx,Drxx,xxxxxxxxx@mq.edu.au,Macquarie University,No,Maxx,Johxxxx,xxxxxxxxxxxn@MQ.edu.au,Macquarie University,No,Lxx,Dx,xxxxxxxxonash.edu,Monash University,No,Magxxxxxx,Woxxxx,xxxxxxxxxxxxxxxx@uni-tuebingen.de,Eberhard Karls Universität Tübingen,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxx,Drxx,Macquarie University,,,,,,xxxxxxxxx@mq.edu.au,,,,,Australia,,Shexxxx Malxxxx;Maxx Drxx;Maxx Johxxxx;Lxx Dx;Magxxxxxx Woxxxx,xxxxxxxxxxxxxxxxxtudents.mq.edu.au;xxxxxxxxxx@mq.edu.au;xxxxxxxxxxxon@MQ.edu.au;xxxxxxxxxonash.edu;xxxxxxxxxxxxxxxxx@uni-tuebingen.de,Unsupervised Text Segmentation Based on Native Language Characteristics,Unsupervised Text Segmentation Based on Native Language Characteristics,13,Mark Dras,,"Macquarie University, Herring Rd, North Ryde, NSW 2109, Australia",on,,No. Do not include my submission in this dataset.,No,None,None
241,241X-P7J3J9E7F4,Dual Attention Network for Inter-Product Compatibility Analysis,Hx Xx;Sixxxx Xxx;Lxx Sxx and Phxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Compatibility is one of the customers' top concerns when they seek for a
product. Although Product Community Question and Answering (PCQA) has many QA
pairs containing compatibility information, it is challenging to accurately
extract such information given the varieties of linguistic patterns in short
questions and the implicitly expressed compatibility information in the
answers. In this paper, we study the problem of \emph{product compatibility
analysis}, aiming to identify \emph{compatible} or \emph{incompatible entities}
for a given target product in PCQA.
We observe that while the questions mostly contain complementary product names,
the answers contain the compatibility information, and thus the questions and
the answers contain two sources of information that needs to be mined jointly.
We propose an end-to-end Dual Attention Network (DAN) %based on LSTM to handle
all the above challenges, that reads the QA pairs as context stories to extract
compatible/incompatible products. We conduct experiments quantitatively and
qualitatively to show that the extraction performance of the proposed network
outperforms that of a wide spectrum of baselines.",7 Feb 2017 06:04:57 GMT,Applications/Tools,"Information extraction, text mining, and question answering",sentiment analysis;  NLP applications;  information extraction;  named entity recognition;  opinion mining and extraction;  opinion representation;  question interpretation;  text classification;  question answering in restricted domains,Hx,Xx,xxxxxxuic.edu,University of Illinois at Chicago,No,Sixxxx,Xxx,xxxxxxxxxlehigh.edu,Lehigh University,No,Lxx,Sxx,xxxxxxuic.edu,"Department of Computer Science, University of Illinois at Chicago",No,Phixxxxxx,Yx,xxxxxxic.edu,University of Illinois at Chicago,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hx,Xx,University of Illinois at Chicago,,,,,,xxxxxxuic.edu,,,IL,,United States,,Hx Xx;Sixxxx Xxx;Lxx Sxx;Phxxxx Sx,xxxxxxuic.edu;xxxxxxxxxxlehigh.edu;xxxxxxxuic.edu;xxxxxxuic.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
242,242X-P5G5H8E7H5,Sentence-level Combination of Statistical and Neural Machine Translation Systems,Zhaxxxxx Tx;Yaxx Lxx;Xiaxxxx Lxx and Haxx L,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Although Neural Machine Translation (NMT) has advanced the state of the art in
the past several years, it has one commonly-cited weakness: translations
generated by NMT systems often lack of adequacy. In response to this problem,
we use relatively more adequate translations generated by Statistical Machine
Translation (SMT) as substitutes when NMT suffers from serious inadequate
translation problems. We accomplish this with sentence-level system
combination, and design multiple features to measure the adequacy of NMT and
SMT translations. Experimental results show that the proposed approach
significantly improves translation performance by 3.0 BLEU points over strong
NMT and SMT baselines, and produces translations with reasonable adequacy and
fluency.",7 Feb 2017 04:48:31 GMT,Empirical/Data-Driven,Machine translation,hybrid MT,Zhaxxxxx,Tx,xxxxxxxxxx@gmail.com,Huawei Noah's Ark Lab,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Xiaxxxx,Lxx,xxxxxxxxxxx@huawei.com,Huawei Noah's Ark Lab,No,Haxx,Lx,xxxxxxxxxxhuawei.com,Huawei Technologies,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhaxxxxx,Tx,Tencent AI Lab,,,,,,xxxxxxxxxx@gmail.com,,,,,China,"My research focuses on the natural language processing (NLP). Some topics of interest to me are:
1. Machine translation
2. Sentiment analysis
3. Language modeling (for programming languages)",Zhaxxxxx Tx;Yaxx Lxx;Xiaxxxx Lxx;Haxx Lx,xxxxxxxxxx@gmail.com;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxx3@huawei.com;xxxxxxxxxx@huawei.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
243,243X-P6D3F3J7D8,Sentiment Analysis Using Cognition Based Attention Models,Yuxxxx Loxx;Roxx Xixxx;Lx Qxx and Minxxxx L,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Attention models are proposed in sentiment analysis because not all words and
sentences are created equal. However, most existing attention methods in
sentiment analysis only focus on local context or preference information. In
this work, we propose to add a cognitive based attention mechanism into a
neural network model to highlight important words and sentences in sentiment
analysis. More specifically, we first learn the reading time of words and
sentences through eye tracking modeling using an eye-tracking data set. Then we
use the predicted reading time to build a cognition based attention layer. This
cognition layer works for both the word-to-sentence level and the
sentence-to-document level. Our model has the ability to work together with
other attention models to improve the performance of sentiment analysis.
Performance evaluation shows that our model achieves significant and
consistently improvements compared to the use of state-of-the-art attention
methods. This proves that the use of cognitive data works better than using
attention models built through local context.",7 Feb 2017 03:41:01 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction;  text classification,Yuxxxx,Loxx,xxxxxxxxxxxx.polyu.edu.hk,"Department of Computing,The Hong Kong Polytechnic University",No,Roxx,Xixxx,xxxxxxxxxg@amd.com,"Advanced Micro Devices,Shanghai co.ltd",No,Lx,Qxx,xxxxxxxxxxxx.polyu.edu.hk,Hong Kong Polytechnic University,No,Minxxxx,Lx,xxxxxxxxxxxpolyu.edu.hk,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxxx,Loxx,"Department of Computing,The Hong Kong Polytechnic University",,,(852xxxxxxxxx,,,xxxxxxxxxxxx.polyu.edu.hk,,Kowloon,HONG KONG,,China,,Yuxxxx Loxx;Roxx Xixxx;Lx Qxx;Minxxxx Lx,xxxxxxxxxxxx.polyu.edu.hk;xxxxxxxxxng@amd.com;xxxxxxxxxxxxx.polyu.edu.hk;xxxxxxxxxxxxpolyu.edu.hk,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
244,244X-A9J2B9J3J9,Script Event Prediction Using Deep Memory Network,Zhoxxxxxx Waxx;Yxx Zhxxx and Chixxxxxx Chxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Script knowledge encodes partially ordered event chains in stereotypical
scenarios, such as restaurant visiting and gardening. There has been a recent
line of work automatically learning scripts from unstructured texts, by
modeling narrative event chains. While the dominant approach group events using
event pair relations, LSTMs have been used to encode full chains of narrative
events. The latter has the advantage of learning long-range temporal orders,
yet the former is more adaptive to partial orders. We propose a neural model
that leverages the advantages of both methods, by using LSTM hidden states as
features for event pair modelling. A dynamic memory network is utilized to
automatically induce weights on existing events for inferring a subsequent
event. Standard evaluation shows that our method significantly outperforms both
methods above, giving the best results reported so far.",7 Feb 2017 07:53:36 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction,Zhoxxxxxx,Waxx,xxxxxxxxxxxny@gmail.com,Singapore University of Technology and Design,No,Yxx,Zhxxx,xxxxxxxxxxsutd.edu.sg,Singapore University of Technology and Design,No,Chixxxxxx,Chxxx,xxxxxxxxxxxie@gmail.com,Singapore University of Technology and Design,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhoxxxxxx,Waxx,Soochow University,,,,,,xxxxxxxxxxxny@gmail.com,,,,,China,,Zhoxxxxxx Waxx;Yxx Zhxxx;Chixxxxxx Chxxx and  Dexxxxx,xxxxxxxxxxxny@gmail.com;xxxxxxxxxxxsutd.edu.sg;xxxxxxxxxxxxie@gmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
245,245X-H4G3H9C8J8,Opinion Recommendation using Neural Memory Model,Zhoxxxxxx Waxx and Yxx Zhxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"We present opinion recommendation, a novel task of jointly predicting a custom
review with a rating score that a certain user would give to a certain product
or service, given existing reviews and rating scores to the product or service
by other users, and the reviews that the user has given to other products and
services. A characteristic of opinion recommendation is the reliance of
multiple data sources for multi-task joint learning, which is the strength of
neural models. We use a single neural network to model users and products,
capturing their correlation and generating customised product representations
using a deep memory network, from which customised ratings and reviews are
constructed jointly. Results show that our opinion recommendation system gives
ratings that are closer to real user ratings on Yelp.com data compared with
Yelp's own ratings, and our methods give better results compared to several
pipelines baselines using state-of-the-art sentiment rating and summarization
systems.",7 Feb 2017 05:29:23 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis,Zhoxxxxxx,Waxx,xxxxxxxxxxxny@gmail.com,Singapore University of Technology and Design,No,Yxx,Zhxxx,xxxxxxxxxxsutd.edu.sg,Singapore University of Technology and Design,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhoxxxxxx,Waxx,Soochow University,,,,,,xxxxxxxxxxxny@gmail.com,,,,,China,,Zhoxxxxxx Waxx;Yxx Zhxxx and  Dexxxxx,xxxxxxxxxxxny@gmail.com;xxxxxxxxxxxsutd.edu.sg,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
246,246X-E2C9B6E2A3,Reinforcement Learning with External Knowledge and Double Q-functions for Predicting Popular Reddit Threads,Jx Hx;Maxx Ostxxxxxx and Xiaxxxxx H,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"This paper addresses the problem of predicting popularity of comments in an
online discussion forum using reinforcement learning, particularly addressing
two challenges that arise from having natural language state and action spaces.
First, the state representation, which characterizes the history of comments
tracked in a discussion at a particular point, is augmented to incorporate the
global context represented by discussions on world events available in an
external knowledge source. Second, a double Q-learning framework is introduced,
making it feasible to search the combinatorial action space while also
accounting for redundancy among sub-actions. We experiment with five Reddit
communities, showing that the two methods improve over previous reported
results on this task.",7 Feb 2017 02:01:18 GMT,Empirical/Data-Driven,Machine learning,NLP applications;  experimental evaluation/comparison of ML methods;  reinforcement learning,Jx,Hx,xxxxxx@uw.edu,University of Washington,No,Maxx,Ostxxxxxx,xxxxxxxr@uw.edu,University of Washington,No,Xiaxxxxx,Hx,xxxxxxxxxxrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jx,Hx,University of Washington,,,,,,xxxxxx@uw.edu,,,,,United States,,Jx Hx;Maxx Ostxxxxxx;Xiaxxxxx Hx,xxxxxx@uw.edu;xxxxxxxxr@uw.edu;xxxxxxxxxxcrosoft.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
247,247X-G7G5C7H6J5,Nonparametric Bayesian Induction of Binary Parameters from Typological Features,Yuxx Murxxxxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),"Principles and parameters is one of the frameworks of generative grammar that
explain structural diversity of the world's languages.
The latter part of the framework, a set of latent binary parameters, arguably
controls variability among surface typological features.
In this paper, we propose a computational model with which binary parameters
are induced from surface features.
The proposed method is a nonparametric Bayesian model with an Indian buffet
process straightforwardly generating parameters.
A key challenge lies in inference:
Parameters are tied with a large number of weights that are hard to optimize.
To address this problem, we employ a block sampling procedure based on
Hamiltonian Monte Carlo.
Experiments show that the proposed model successfully captured regularity in
surface features.",7 Feb 2017 05:07:35 GMT,Empirical/Data-Driven,Multidisciplinary,generative models;  unsupervised and semi-supervised learning;  Bayesian learning,Yuxx,Murxxxxx,xxxxxxxxxxxxyoto-u.ac.jp,Kyoto University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxx,Murxxxxx,Kyoto University,,,,,,xxxxxxxxxxxxyoto-u.ac.jp,,,,,Japan,,Yuxx Murxxxxx,xxxxxxxxxxxxyoto-u.ac.jp,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
248,248X-B3B6J5D2P9,Semantic Refinement GRU-based Neural Language Generation for Spoken Dialogue Systems,Vanxxxxxx Trxx and Le-xxxx Ngxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Natural language generation (NLG) plays a critical role in spoken dialogue
systems. This paper presents a new approach to NLG by using recurrent neural
networks(RNN), in which a gating mechanism is applied before RNN computation.
This allows the proposed model to generate natural sentences with more
correctly ordered than the previous methods. The RNN-based generator can be
learned from unaligned data by jointly training sentence planning and surface
realization to produce natural language responses. The model was extensively
evaluated on four different NLG domains. The results show that the proposed
generator achieved better performance on all the NLG domains compared to
previous generators.",7 Feb 2017 05:24:46 GMT,Empirical/Data-Driven,Generation,language generation;  dialogue;  spoken language  generation,Vanxxxxxx,Trxx,xxxxxxxxxaist.ac.jp,Japan Advanced Institute of Science Information,No,Le-xxxx,Ngxxxx,xxxxxxxxxxaist.ac.jp,Japan Advanced Institute of Science Information,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vanxxxxxx,Trxx,Japan Advanced Institute of Science and Technology,,,080.xxxxxxxxx,,,xxxxxxxxxaist.ac.jp,,Nomi,Ishikawa,,Viet Nam,,Vanxxxxxx Trxx;Le-xxxx Ngxxxx,xxxxxxxxxaist.ac.jp;xxxxxxxxxxjaist.ac.jp,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
249,249X-A8C3E6G5J5,What the Bible Tells Us about Modern Topics: Evaluating Multilingual Representations,Shuxxxx Hxx;Joxxxx Boydxxxxxxx and Micxxxx Jx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"Multilingual topic models enable document analysis
across languages through multilingual topics.  However, there
is no standard, effective evaluation metric to separate good
multilingual topic models from bad ones, particularly for
low-resources languages.  To address this lacuna, we introduce
the first intrinsic evaluation of multilingual topic models: cross normalized
pointwise mutual information (CNPMI).  We show that this
measure correlates well with human judgments of multilingual
topic coherence.  Additionally, we extend \cnpmi{} to
low-resource languages where the Bible is the only
multilingual resource available.  We use machine learning to
calibrate CNPMI to account for the archaic language and
skewed topic distribution of the Bible.",7 Feb 2017 04:37:13 GMT,Resources/Evaluation,Multilinguality,cross-lingual approaches;  multilingual applications;  evaluation metrics,Shuxxxx,Hxx,xxxxxxxxxxolorado.edu,University of Colorado Boulder,No,Joxxxx,Boydxxxxxxx,xxxxxxxxxxxxxxxber@colorado.edu,University of Colorado,No,Micxxxxxxx,Paxx,xxxxxxxxgmail.com,Johns Hopkins University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shuxxxx,Hxx,University of Colorado Boulder,,,,,,xxxxxxxxxxolorado.edu,,,,,United States,,Shuxxxx Hxx;Joxxxx Boydxxxxxxx;Micxxxx Jx,xxxxxxxxxxolorado.edu;xxxxxxxxxxxxxxxxber@colorado.edu;xxxxxxxxxgmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
250,250X-B9A5G4J7B5,ASBN: A Novel Bayesian Network Approach for Fine-Grained Opinion Mining,Thxxx Hxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"The goal of this paper is to identify the sentiment category for an aspect in a
sentence. We propose a novel Bayesian network for aspect-based sentiment
analysis.  The sentiment of the aspect is represented by propagating the
semantic through the dependency tree of the sentence. Results of an experiment
show that our method significantly outperforms previous methods. Our ASBN is
better than ASA w/o RE, ASA with RE, RNN and AdaRNN more than 3\% accuracy and
5\% F-measure.",6 Feb 2017 02:25:56 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction,Thixxxxxx,Ngxxxx,xxxxxxxxx@gmail.com,Japan Advanced Institute of Science and Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thixxxxxx,Ngxxxx,Japan Advanced Institute of Science and Technology,,,,,,xxxxxxxxx@gmail.com,,,,,Japan,,Thxxx Hxx and  Tecxxxxxxxx,xxxxxxxxx@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
251,251X-F7A2P9P4A5,Skip-Gram - Zipf + Uniform = Vector Additivity,Alxx Gitxxxx;Dimxxxxx Achxxxxxxx and Micxxxx Wx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Monday,,Undecided (Semantics),"In recent years word-embedding models have gained great popularity due to their
remarkable performance on several tasks, including word analogy questions and
caption generation. An unexpected ""side-effect"" of such models is that their
vectors often exhibit compositionality, i.e., \emph{adding} two word-vectors
results in a vector that is only a small angle away from the vector of a word
representing the semantic composite of the original words, e.g., ""man"" +
""royal"" = ""king"".

This work provides a theoretical justification for the presence of additive
compositionality in word vectors learned using the Skip-Gram model. In
particular, it shows that additive compositionality holds in an even stricter
sense (small distance rather than small angle) under certain assumptions on the
process generating the corpus. As a corollary, it explains the success of
vector calculus in solving word analogies. When these assumptions do not hold,
this work describes the correct non-linear composition operator. 

Finally, this work establishes a connection between the Skip-Gram model and the
Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby:
the parameters of SDR models can be obtained from those of Skip-Gram models
simply by adding information on symbol frequencies. This shows that Skip-Gram
embeddings are optimal in the sense of Globerson and Tishby and, further,
implies that the heuristics commonly used to approximately fit Skip-Gram models
can be used to fit SDR models.",23 Apr 2017 00:00:10 GMT,Theoretical,Semantics,,Alxx,Gitxxxx,xxxxxxxrpi.edu,Rensselaer Polytechnic Institute,No,Dimxxxxx,Achxxxxxxx,xxxxxxxxx.ucsc.edu,"Department of Computer Science, UC Santa Cruz",No,Micxxxxxxx,Mahxxxx,xxxxxxxxxxxxx.berkeley.edu,"International Computer Science Institute and University of California at Berkeley, Department of Statistics",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alxx,Gitxxxx,Rensselaer Polytechnic Institute,,,,,,xxxxxxxrpi.edu,,Troy,NY,,United States,,Alxx Gitxxxx;Dimxxxxx Achxxxxxxx;Micxxxx Wx and Unixxxxxxx ox,xxxxxxxrpi.edu;xxxxxxxxxe.ucsc.edu;xxxxxxxxxxxxxt.berkeley.edu,Skip-Gram - Zipf + Uniform = Vector Additivity,Skip-Gram - Zipf + Uniform = Vector Additivity,8,Alex Gittens,,"Rensselaer Polytechnic Institute
CS Department
110 8th St, Troy, NY 12180",,,Only include my submission if it is accepted.,No,None,None
252,252X-E4D7P6D2E7,A Nested Attention Neural Hybrid Model for Grammatical Error Correction,Jiaxxxx Jx;Qinxxxx Waxx;Krixxxxx Touxxxxxx;Yoxxxx Goxx;Stxxxx Trxxxx and Jiaxxxxx Gx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Accept - Oral Tuesday,,Undecided (Multidisciplinary),"Grammatical error correction (GEC) systems strive to correct both global errors
inword order and usage, and local errors inspelling and inflection. Further
developing upon recent work on neural machine translation, we propose a new
hybrid neural model with nested attention layers for GEC.Experiments show that
the new model can effectively correct errors of both types by incorporating
word and character-level information, and that the model significantly
outperforms previous  neural models for GEC as measured on the standard
CoNLL-14 benchmark dataset.Further analysis also shows that the superiority of
the proposed model can be largely attributed to the use of the nested attention
mechanism, which has proven particularly effective incorrecting local errors
that involve small edits in orthography.",21 Apr 2017 06:39:36 GMT,Empirical/Data-Driven,Other,,Jiaxxxx,Jx,xxxxxxxxxxxcrosoft.com,Microsoft AI & Research,No,Qinxxxx,Waxx,xxxxxxxxxxxcrosoft.com,Microsoft AI & Research,No,Krixxxxx,Touxxxxxx,xxxxxxxxxgoogle.com,Google Research,No,Yoxxxx,Goxx,xxxxxxxxxxrosoft.com,Microsoft AI & Research,No,Stxxxx,Trxxxx,xxxxxxxxxxcrosoft.com,Microsoft AI & Research,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,"Microsoft Research, Redmond",No,,,,,,,,,,,,,,,,,,,,,,Jiaxxxx,Jx,Microsoft AI & Research,,,,,,xxxxxxxxxxxcrosoft.com,,,,,United States,,Jiaxxxx Jx;Qinxxxx Waxx;Krixxxxx Touxxxxxx;Yoxxxx Goxx;Stxxxx Trxxxx;Jiaxxxxx Gxx,xxxxxxxxxxxcrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxgoogle.com;xxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxrosoft.com,A Nested Attention Neural Hybrid Model for Grammatical Error Correction,A Nested Attention Neural Hybrid Model for Grammatical Error Correction,10,Jianshu Ji,,"Name : Microsoft AI & Research
Address : B25 Microsoft, No. 328 Xinghu Street, SIP, Suzhou, China",,,No. Do not include my submission in this dataset.,No,None,None
253,253X-P3D3F4G7G9,Natural Language Generation to Describe Semantic Representations of Brain Activity Evoked by Visual Stimuli,Exx Maxxxx;Icxxxx Kobxxxxxx;Shxxxx Nisxxxxxx;Satxxxx Nisxxxx and Hixxxx Axx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Quantitative analysis of human brain activity based on language
representations, such as semantic categories of words, has been actively
studied in brain and neuroscience. This study attempts to generate natural
language descriptions for human brain activation phenomena evoked by visual
stimuli by employing deep learning. Deep Learning has attracted attention as an
effective method to capture the features of various types of information and
generate natural language expressions automatically. Due to the lack of brain
training data, the proposed method employs a pre-trained image-captioning
system using a deep learning framework. To apply brain activity data to the
image-captioning model, we train a model to learn the corresponding
relationships between brain activity data and image features. The results
demonstrate that the proposed model can recognize semantic information of human
brain activity and generate description using natural language sentences. We
conducted several experiments with data from brain regions known to process
visual stimuli. The results suggest that semantic processing of visual stimuli
is performed using the entire cerebral cortex.",6 Feb 2017 02:52:10 GMT,Empirical/Data-Driven,Generation,language generation;  information extraction;  multimodal representations and processing,Exx,Maxxxx,xxxxxxxxxxx.ocha.ac.jp,Ochanomizu University,No,Icxxxx,Kobxxxxxx,xxxxxxxxxcha.ac.jp,Ochanomizu University,No,Shxxxx,Nisxxxxxx,xxxxxxxxxxnict.go.jp,National Institute of Information and Communications Technology,No,Satxxxx,Nisxxxx,xxxxxxxxxxnict.go.jp,National Institute of Information and Communications Technology,No,Hixxxx,Asxx,xxxxxxxxist.go.jp,National Institute of Advanced Industrial Science and Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Exx,Maxxxx,Ochanomizu University,,,,,,xxxxxxxxxxx.ocha.ac.jp,,,,,Japan,,Exx Maxxxx;Icxxxx Kobxxxxxx;Shxxxx Nisxxxxxx;Satxxxx Nisxxxx;Hixxxx Asxx and  Tecxxxxxxxx,xxxxxxxxxxx.ocha.ac.jp;xxxxxxxxxocha.ac.jp;xxxxxxxxxx@nict.go.jp;xxxxxxxxxx@nict.go.jp;xxxxxxxxxist.go.jp,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
254,254X-P4B2B8H6B8,"A Joint Model for Semantic Sequences: Frames, Entities, Sentiments",Haxxxx Pexx;Snixxxx Chaxxxxxxx and Dxx Rxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Understanding stories -- sequences of events -- is a crucial yet challenging
natural language understanding task. These events typically carry multiple
aspects of semantics including actions, entities and emotions. Not only does
each individual aspect contribute to modeling the story's semantics, but also
the interactions among these aspects.             
Building on this intuition, we propose to jointly model important aspects of
semantic knowledge -- frames, entities and sentiments -- via a semantic
language model. We achieve this by first representing these aspects' semantic
units at an appropriate level of abstraction and then using the resulting
vector representations for each semantic aspect to learn a joint representation
using a neural language model.

We study the joint semantic language model in two scenarios: story cloze test
for short commonsense stories and shallow discourse parsing for long news
documents. We demonstrate that our joint model supports improved performance on
both tasks, and that each semantic aspect contributes to the joint model.",6 Feb 2017 23:48:35 GMT,Empirical/Data-Driven,Semantics,discourse;  NLP applications;  language generation;  semantic knowledge induction,Haxxxx,Pexx,xxxxxxxxxlinois.edu,UIUC,No,Snixxxx,Chaxxxxxxx,xxxxxxxxxgmail.com,University of Illinois at urbana-champaign,No,Dxx,Roxx,xxxxxxxxinois.edu,University of Illinois,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Haxxxx,Pexx,UIUC,,,,,,xxxxxxxxxlinois.edu,,,,,United States,,Haxxxx Pexx;Snixxxx Chaxxxxxxx;Dxx Roxx,xxxxxxxxxlinois.edu;xxxxxxxxx@gmail.com;xxxxxxxxxinois.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
255,255X-C2C3J6F8F9,Towards A Noise-Tolerant Neural Network Model for Distantly Supervised Relation Extraction,Tinxxxxx Jixxx;Baxxxx Chxxx and Zhixxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Distantly supervised relation extraction
has been widely used to extract semantic
relations from text. However, it suffers
from wrong labeling problems and hinder-
s the performance of a model trained on
suchnoisydata. Todealwiththisproblem,
previous neural network model assumed at
least one instance is true and only select-
ed the most likely one instance in a bag
for training, which missed rich informa-
tion by discarding all other true positive
instances. Instead of using the at-least-one
assumption, we assume that most of the
distantly labeled instances are true posi-
tive instances and true positive instances
often share the same feature patterns but
false positive instances do not. We argue
that all instances should be given to the
model and leave the model to decide the
contribution of different instances. There-
fore we propose two kinds of methods to
dynamically determine the weights of all
the instances and make the neural network
modelmoretoleranttonoise. Experiments
show that our approach is effective, and it
outperforms several competitive baseline
methods.",6 Feb 2017 09:42:08 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",relation/event extraction,Tinxxxxx,Jixxx,xxxxxxxxxpku.edu.cn,"Institute of Computational Linguistics,Peking University",No,Baxxxx,Chxxx,xxxxxxxu.edu.cn,Peking University,No,Zhixxxx,Sxx,xxxxxxx.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tinxxxxx,Jixxx,"Institute of Computational Linguistics,Peking University",,,,,,xxxxxxxxxpku.edu.cn,,,,,China,,Tinxxxxx Jixxx;Baxxxx Chxxx;Zhixxxx Sxx,xxxxxxxxxpku.edu.cn;xxxxxxxxu.edu.cn;xxxxxxxu.edu.cn,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
256,256X-A8D3A5F7G4,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,Tiaxxxxxx Zhxx;Rxx Zhxx and Maxxxx Eskxxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Tuesday,,Undecided (Dialog Interactive Systems),"While recent neural encoder-decoder models have shown great promise in modeling
open-domain conversations, they often generate dull and generic responses.
Unlike past work that has focused on diversifying the output of the decoder
from word-level to alleviate this problem, we present a novel framework based
on conditional variational autoencoders that capture the discourse-level
diversity in the encoder. Our model uses latent variables to learn a
distribution over potential conversational intents and generates diverse
responses using only greedy decoders. We have further developed a novel variant
that is integrated with linguistic prior knowledge for better performance.
Finally, the training procedure is improved through introducing a bag-of-word
loss. Our proposed models have been validated to generate significantly more
diverse responses than baseline approaches and exhibit competence of
discourse-level decision-making.",22 Apr 2017 02:14:45 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Tiaxxxxxx,Zhxx,xxxxxxxxxcs.cmu.edu,"Language Technologies Institute, Carnegie Mellon University",No,Rxx,Zhxx,xxxxxxxxxxu@gmail.com,Carnegie Mellon University,No,Maxxxx,Eskxxxxx,xxxxxxxcmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tiaxxxxxx,Zhxx,"Language Technologies Institute, Carnegie Mellon University",,,424xxxxxxx,,,xxxxxxxxxcs.cmu.edu,,Pittsburgh,PA,,United States,,Tiaxxxxxx Zhxx;Rxx Zhxx;Maxxxx Eskxxxxx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxxxu@gmail.com;xxxxxxx.cmu.edu,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,11,Tiancheng Zhao,,"Carnegie Mellon University, 5000 Forbes Ave Pittsburgh PA USA",,,Only include my submission if it is accepted.,No,None,None
257,257X-C8P8E4E4A4,Cross-media Gender Classification with Joint Textual and Social User Embedding,Jinxxxxx Waxx;Shoxxxxx Lx and Guoxxxx Zxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"In realistic scenarios, a user gender classification model learned from one
social media might perform rather poorly when tested on another social media
due to the different data distributions in the two media. In this paper, we
address cross-media gender classification by bridging the knowledge between the
source
and target media with a uniform user embedding learning approach. In our
approach, we first construct a cross-media user-word network to capture the
relationship among users through the textual information and a modified
cross-media user-user network to capture the relationship among users through
the social
information. Then, we learn user embedding by joint learning the heterogeneous
network composed of above two networks. Finally, we train a LSTM classification
model with the obtained user embeddings as input to perform gender
classification. Empirical studies demonstrate the effectiveness of the proposed
approach to cross-media gender classification.",7 Feb 2017 09:55:41 GMT,Empirical/Data-Driven,Social media,NLP in social networking media;  social network,Jinxxxxx,Waxx,xxxxxxxxx@gmail.com,Soochow University,No,Shoxxxxx,Lx,xxxxxxxxxxxsuda.edu.cn,Soochow University,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jinxxxxx,Waxx,Soochow University,,,1537xxxxxxx,,,xxxxxxxxx@gmail.com,,,,,China,,Jinxxxxx Waxx;Shoxxxxx Lx;Guoxxxx Zhxx,xxxxxxxxx@gmail.com;xxxxxxxxxxx@suda.edu.cn;xxxxxxxxxuda.edu.cn,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
258,258X-C2C3P5E9J4,Entity Linking for Queries by Searching Wikipedia Sentences,Chuxxxx Txx;Fuxx Wxx;Penxxxx Rxx;Weixxxx Lx and Mixx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present a simple yet effective approach for linking entities in queries. The
key idea is to search sentences similar to a query from Wikipedia articles and
directly use the human-annotated entities in the similar sentences as candidate
entities for the query. Then, we employ a rich set of features, such as
link-probability, context-matching, word embeddings, and relatedness among
candidate entities as well as their related entities, to rank the candidates
under a regression based framework. The advantages of our approach lie in two
aspects, which contribute to the ranking process and final linking result.
First, it can greatly reduce the number of candidate entities by filtering out
irrelevant entities with the words in the query. Second, we can obtain the
query sensitive prior probability in addition to the static link-probability
derived from all Wikipedia articles. We conduct experiments on two benchmark
datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ
dataset. Experimental results show that our method outperforms state-of-the-art
systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ
dataset.",6 Feb 2017 08:44:47 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",named entity disambiguation;  entity disambiguation;  NLP on Wikipedia and other collaboratively constructed resources,Chuxxxx,Txx,xxxxxxxxxxxxxxde.buaa.edu.cn,Beihang University,No,Fuxx,Wxx,xxxxxxxxxrosoft.com,Microsoft Research Asia,No,Penxxxx,Rxx,xxxxxxxxxutlook.com,Shandong University,No,Weixxxx,Lx,xxxxxxxa.edu.cn,Beihang University,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Chuxxxx,Txx,Beihang University,,,,,,xxxxxxxxxxxxxxde.buaa.edu.cn,,,,,China,,Chuxxxx Txx;Fuxx Wxx;Penxxxx Rxx;Weixxxx Lx;Mixx Zhxx,xxxxxxxxxxxxxxde.buaa.edu.cn;xxxxxxxxxxrosoft.com;xxxxxxxxxxutlook.com;xxxxxxxxa.edu.cn;xxxxxxxxxxxicrosoft.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
262,262X-A3F6E4F5C2,Generating Causal Explanations with Symbolic and Neural Reasoning,Donxxxxx Kaxx;Vaxxx Gaxxxx and Edxxxx Hxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Explaining underlying causes or effects about an event is a very challenging
but valuable task. We define a novel problem of generating explanations between
two ends of an event by linking their cause-effect relations. The generation of
the sequence of causal entities needs coherent commonsense causative knowledge
base and its efficient reasoning. First, we construct a large and coherent
causal graph called CGraph by automatically extracting causal Frame semantics
from text. Then, our symbolic reasoner traverses the graph by expanding the
event with external knowledge graph such as Freebase to broaden the semantic
choices for searching. To take the advantages of symbolic (good
interpretability) and neural (lexical variation) representation, we also
proposed a neural reasoning algorithm by training the causal tuples and predict
the next cause (or effect) in forward (or backward) inference. We evaluate our
models in different tasks: cause/effect prediction and explanation generation
with either quantitative or qualitative methods.",7 Feb 2017 01:47:47 GMT,Empirical/Data-Driven,Generation,language generation;  rule-based/symbolic learning methods;  relation/event extraction,Donxxxxx,Kaxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Vaxxx,Gaxxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Edxxxx,Hoxx,xxxxxxx.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Donxxxxx,Kaxx,Carnegie Mellon University,,,412xxxxxxx,,,xxxxxxxxxcs.cmu.edu,,,,,United States,I am a Ph.D student at CMU.,Donxxxxx Kaxx;Vaxxx Gaxxxx;Edxxxx Hoxx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxcs.cmu.edu;xxxxxxxx.cmu.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
264,264X-E7E4G5B8D5,Latent Topic-Aware Neural Machine Translation,Honxxxxx Chxx;Zhaxxxxx Rxx;Daxxx Yxx and Qxx Lx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"End-to-end Neural Machine Translation (NMT) learns a conditional distribution
of a target sentence given a source sentence. Most of existing approaches aim
at enhancing the model's ability of discovering the intra-correlation within a
bilingual sentence pair. However, the inter-correlations among sentences are
rarely concerned. In this paper, we propose a latent topic-aware neural machine
translation model (LTNMT) to automatically discover the hidden semantics among
sentences. Given a source sentence, we generate a latent topic vector according
to the learned source representation and guide the translation process. Our
model is trained with all the parameters randomly initialized without any
pre-training. Experiments conducted on large benchmark datasets experimentally
show the effectiveness of the model on both Chinese-English and English-German
translation tasks, and demonstrate that latent topic-aware neural machine
translation significantly outperforms the state-of-the-art attention-based
baseline.",7 Feb 2017 06:04:26 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation,Honxxxxx,Chxx,xxxxxxxxxxxn@ict.ac.cn,"Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences",No,Zhaxxxxx,Rxx,xxxxxxxxxxxn@ucl.ac.uk,University College London,No,Daxxx,Yxx,xxxxxxxx1@jd.com,"Data Science Lab, JD.com",No,Qxx,Lxx,xxxxxxx@dcu.ie,Dublin City University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Honxxxxx,Chxx,JD.com,,,,,,xxxxxxxxxhen@jd.com,,,,,China,,Honxxxxx Chxx;Zhaxxxxx Rxx;Daxxx Yxx;Qxx Lxx,xxxxxxxxxxxn@ict.ac.cn;xxxxxxxxxxxen@ucl.ac.uk;xxxxxxxxi1@jd.com;xxxxxxxu@dcu.ie,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
265,265X-J6G9J3H5P7,Topically Driven Neural Language Model,Jxx Hxx;Timxxxx Balxxxx and Trxxxx Cxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Monday,,Undecided (Machine Learning),"Language models are typically applied at the sentence level, without
  access to the broader document context.  We present a neural language
  model that incorporates document context in the form of a topic
  model-like architecture, thus providing a succinct representation of the
  broader document context outside of the current sentence.  Experiments
  over a range of datasets demonstrate that our model outperforms a pure
  sentence-based model in terms of language model perplexity, and leads
  to topics that are potentially more coherent than those produced by a
  standard LDA topic model.  Our model also has the ability to generate
  related sentences for a topic, providing another way to interpret topics.",22 Apr 2017 07:38:53 GMT,Empirical/Data-Driven,Machine learning,,Jeyxxxx,Lxx,xxxxxxxxxx@gmail.com,IBM Research,No,Timxxxx,Balxxxx,xxxxxxin.net,The University of Melbourne,No,Trxxxx,Coxx,xxxxxxxxxxelb.edu.au,University of Melbourne,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jeyxxxx,Lxx,IBM Research,,,,,,xxxxxxxxxx@gmail.com,,Melbourne,,,Australia,,Jxx Hxx;Timxxxx Balxxxx;Trxxxx Coxx,xxxxxxxxxx@gmail.com;xxxxxxwin.net;xxxxxxxxxxmelb.edu.au,Topically Driven Neural Language Model,Topically Driven Neural Language Model,11,Jey Han Lau,,IBM Research,on,,No. Do not include my submission in this dataset.,No,None,None
266,266X-J5H4G6P2D5,Improving sentiment classification with task-specific data,Jexxxx Baxxxx;Paxxxx Lamxxxx and Toxx Baxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Current state-of-the-art sentiment analysis techniques rely heavily on
pre-trained word embeddings. However, the data used to train these embeddings
normally comes from large, generic datasets, such as Wikipedia or GoogleNews,
which may not include enough task-specific information to create reliable
representations. This paper proposes a method to determine the subjectivity of
a corpus using available tools and shows that word embeddings trained on
task-specific corpora tend to outperform those trained on generic data. We then
examine ways to combine information from generic and task-specific datasets and
finally demonstrate that our method can work well for under-resourced
languages.",6 Feb 2017 20:04:36 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  learning with small datasets;  opinion mining and extraction;  experimental evaluation/comparison of ML methods;  subjectivity analysis;  text mining,Jexxxx,Baxxxx,xxxxxxxxxxnes@upf.edu,Universitat Pomeu Fabra,No,Paxxxx,Lamxxxx,xxxxxxxxxxxxrt@gmail.com,Webinterpret,No,Toxx,Baxxx,xxxxxxxxxa@upf.edu,Universitat Pompeu Fabra,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jexxxx,Baxxxx,Universitat Pomeu Fabra,,,,,,xxxxxxxxxxnes@upf.edu,,,,,Spain,,Jexxxx Baxxxx;Paxxxx Lamxxxx;Toxx Baxxx,xxxxxxxxxxnes@upf.edu;xxxxxxxxxxxxert@gmail.com;xxxxxxxxxia@upf.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
267,267X-F7G2D8D7F4,DeepReply: Email Response Generation with Diverse and Variational Prior,Donxxxxx Kaxx;Micxxxx Gaxxx;Patxxxx Paxxxx;Ahxxx Haxxxx;Maxxxx Khxxxx;Maxx Encaxxxxxxx;Chxxxx Baxxxx and Edxxxx Hxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Automatically generating email responses can improve productivity in the
workplace. This paper presents an automatic email generation system called
DeepReply with the following novel features: (1) using a latent
sequence-to-sequence model to capture intents of email responses, (2)
incorporating additional prior knowledge such as personal writing style, and
(3) providing a diverse set of candidate responses by capturing local and
global context of the original email so users may choose one of them
appropriately. We tested our algorithms in terms of clustering, generation, and
ranking on a public email data set, and show quantitative results.",7 Feb 2017 04:47:42 GMT,Applications/Tools,Generation,unsupervised and semi-supervised learning;  language generation;  document clustering,Donxxxxx,Kaxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Micxxxx,Gaxxx,xxxxxxxxxxrosoft.com,Microsoft Research,No,Patxxxx,Paxxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Ahxxx,Hassaxxxxxxxxxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Maxxxx,Khxxxx,xxxxxxxxxxxxx@microsoft.com,Microsoft Research,No,Maxx,Encaxxxxxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Chxxxx,Baxxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Edxxxx,Hoxx,xxxxxxx.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,Donxxxxx,Kaxx,Carnegie Mellon University,,,412xxxxxxx,,,xxxxxxxxxcs.cmu.edu,,,,,United States,I am a Ph.D student at CMU.,Donxxxxx Kaxx;Micxxxx Gaxxx;Patxxxx Paxxxx;Ahxxx Haxxxx;Maxxxx Khxxxx;Maxx Encaxxxxxxx;Chxxxx Baxxxx;Edxxxx Hoxx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxxxx@microsoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxx.cmu.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
268,268X-F5H8D9J7A8,A Local Detection Approach for Named Entity Recognition and Mention Detection,Minxxxx Xx;Hxx Jixxx and Sedxxxxx Watchaxxxxxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Wednesday,,Undecided (IE QA Text Mining Applications),"In this paper, we study a novel approach for named entity recognition (NER) and
mention detection (MD) in natural language processing. Instead of treating NER
as a sequence labeling problem, we propose a new local detection approach,
which relies on the recent fixed-size ordinally forgetting encoding (FOFE)
method to fully encode each sentence fragment and its left/right contexts into
a fixed-size representation. Subsequently, a simple feedforward neural network
(FFNN) is learned to either reject or predict entity label for each individual
text fragment. The proposed method has been evaluated in several popular NER
and MD tasks, including CoNLL 2003 NER task and  TAC-KBP2015 and TAC-KBP2016
Tri-lingual Entity Discovery and Linking (EDL) tasks. Our method has yielded
pretty strong performance in all of these examined tasks. This local detection
approach has shown many advantages over the traditional sequence labeling
methods.",23 Apr 2017 01:39:30 GMT,Theoretical,"Information extraction, text mining, and question answering",,Minxxxx,Xx,xxxxxxxxxx@gmail.com,York University,No,Hxx,Jixxx,xxxxxxxyorku.ca,York University,No,Sedxxxxx,Watchaxxxxxxxxxxxx,xxxxxxxxxxse.yorku.ca,York University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minxxxx,Xx,York University,,,,,,xxxxxxxxxx@gmail.com,,,,,Canada,,Minxxxx Xx;Hxx Jixxx;Sedxxxxx Watchaxxxxxxxxxxxx,xxxxxxxxxx@gmail.com;xxxxxxxxyorku.ca;xxxxxxxxxxxse.yorku.ca,A Local Detection Approach for Named Entity Recognition and Mention Detection,A Local Detection Approach for Named Entity Recognition and Mention Detection,11,Mingbin Xu,Research Assistant,"Lassonde School of Engineering, York University. 
4700 Keele Street, Toronto, Ontario, Canada",on,,No. Do not include my submission in this dataset.,No,None,None
269,269X-C2J7D6G8A6,Taking a Closed-book Examination: Decoupling KB-based Inference by Virtual Hypothesis for Answering Real-world Questions,Zhxxxx Wxx;Cxx Lxx;Xiaxxxxxx Zexx;Kaxx Lxx;Shxxxx Hx and Jxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Complex question answering in real world is a comprehensive and challenging
task due to its demand for deeper question understanding and deeper inference.
Information retrieval is a common solution and easy to implement, but it cannot
answer questions which need long-distance dependency across multiple documents.
Knowledge base (KB) organizes information as a graph, and KB-based inference
can employ logic formulas or knowledge embeddings to capture such long-distance
semantic associations. However, KB-based inference has not been applied to
real-world question answering well, for it is difficult to transform a complex
question into an accurate and appropriate hypothesis for inference. We propose
to decouple KB-based inference from question answering by mapping each pair of
question and answer to a virtual hypothesis on KB, and then estimate relevance
among hypotheses in a joint objective. Therefore, our method is a solution for
overcoming the barrier of question understanding and directly utilizes
long-distance evidence for inference. We create a specialized question
answering dataset only for inference, and our method is proved to be effective
by conducting experiments on both AI2 Science Questions dataset and ours.",7 Feb 2017 08:59:32 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",graph-based algorithms;  context-aware question answering;  collaborative methods for question answering;  open-domain question answering;  semantic knowledge induction,Zhxxxx,Wxx,xxxxxxxxxi@ia.ac.cn,Chinese Academy of Sciences,No,Cxx,Lxx,xxxxxxxxxxpr.ia.ac.cn,Chinese Academy of Sciences,No,Xiaxxxxxx,Zexx,xxxxxxxxxxxxxx@nlpr.ia.ac.cn,Chinese Academy of Sciences,No,Kaxx,Lxx,xxxxxxxxx.ia.ac.cn,Chinese Academy of Sciences,No,Shxxxx,Hx,xxxxxxxxxxxlpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Jxx,Zhxx,xxxxxxxxxr.ia.ac.cn,Chinese Academy of Sciences,No,,,,,,,,,,,,,,,,,,,,,,Zhxxxx,Wxx,Microsoft Inc.,,,,,,xxxxxxxxxxxxicrosoft.com,,,,,China,,Zhxxxx Wxx;Cxx Lxx;Xiaxxxxxx Zexx;Kaxx Lxx;Shxxxx Hx;Jxx Zhxx,xxxxxxxxxi@ia.ac.cn;xxxxxxxxxxxpr.ia.ac.cn;xxxxxxxxxxxxxxg@nlpr.ia.ac.cn;xxxxxxxxxr.ia.ac.cn;xxxxxxxxxxxxlpr.ia.ac.cn;xxxxxxxxxxr.ia.ac.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
270,270X-F8D7E6E6J9,Enhanced LSTM for Natural Language Inference,Qixx Chxx;Xiaxxxx Zxx;Zhexxxxx Lixx;Sx Wxx;Hxx Jixxx and Dixxx Inxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"Reasoning and inference are central to human and artificial intelligence.
Modeling inference in human language is very challenging. With the availability
of large annotated data (Bowman et al., 2015), it has recently become feasible
to train neural network based inference models, which have shown to be very
effective. In this paper, we present a new state-of-the-art result, achieving
the accuracy of 88.6% on the Stanford Natural Language Inference Dataset.
Unlike the previous top models that use very complicated network architectures,
we first demonstrate that carefully designing sequential inference models based
on chain LSTMs can outperform all previous models. Based on this, we further
show that by explicitly considering recursive architectures in both local
inference modeling and inference composition, we achieve additional
improvement. Particularly, incorporating syntactic parsing information
contributes to our best result---it further improves the performance even when
added to the already very strong model.",23 Apr 2017 05:28:00 GMT,Empirical/Data-Driven,Semantics,,Qixx,Chxx,xxxxxxxxxxx.ustc.edu.cn,University of Science and Technology of China,No,Xiaxxxx,Zxx,xxxxxxxxgmail.com,National Research Council Canada,No,Zhexxxxx,Lixx,xxxxxxxxxtc.edu.cn,University of Science and Technology of China,No,Sx,Wxx,xxxxxxxxlytek.com,iFLYTEK Research,No,Hxx,Jixxx,xxxxxxxyorku.ca,York University,No,Dixxx,Inxxxx,xxxxxxxxxx.uottawa.ca,University of Ottawa,No,,,,,,,,,,,,,,,,,,,,,,Qixx,Chxx,University of Science and Technology of China,,,+86 1xxxxxxxxxx,,,xxxxxxxxxxx.ustc.edu.cn,,Hefei,Anhui,,China,,Qixx Chxx;Xiaxxxx Zxx;Zhexxxxx Lixx;Sx Wxx;Hxx Jixxx;Dixxx Inxxxx,xxxxxxxxxxx.ustc.edu.cn;xxxxxxxxxgmail.com;xxxxxxxxxstc.edu.cn;xxxxxxxxxlytek.com;xxxxxxxxyorku.ca;xxxxxxxxxxx.uottawa.ca,Enhanced LSTM for Natural Language Inference,Enhanced LSTM for Natural Language Inference,12,Qian Chen,,"University of Science and Technology of China, 443 Huangshan Road, Hefei, Anhui, China 230027",on,on,Only include my submission if it is accepted.,No,None,None
271,271X-F3E2F2C7F8,Detecting Changed-Hands Online Review Accounts,Gexx Fxx;Shxxx Waxx;Bixx Lxx and Lexxx Akxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Having a reputable social media or review account can be a good cover for
spamming activities. It has become prevalent that spammers buy/sell such
accounts openly on the Web. We call these sold/bought accounts changed-hands
(CH) accounts. They are hard to detect by existing spam detection algorithms as
their spamming activities are under the disguise of clean histories or high
reputation scores. In this paper, we study the detection of CH accounts from a
linguistic perspective. We propose a novel algorithm to determine if an account
has changed hands and to pinpoint the change point. Experimental results with
review accounts demonstrate the effectiveness of our approach.",6 Feb 2017 17:46:55 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,text mining,Gexx,Fxx,xxxxxxuic.edu,University of Illinois at Chicago,No,Shxxx,Waxx,xxxxxxxxxxi@gmail.com,University of Illinois at Chicago,No,Bixx,Lxx,xxxxxxic.edu,University of Illinois at Chicago,No,Lexxx,Akxxxx,xxxxxxxxxs.cmu.edu,H. John Heinz III College,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gexx,Fxx,University of Illinois at Chicago,,,,,,xxxxxxxxxgmail.com,,,,,United States,,Gexx Fxx;Shxxx Waxx;Bixx Lxx;Lexxx Akxxxx,xxxxxxuic.edu;xxxxxxxxxxxi@gmail.com;xxxxxxuic.edu;xxxxxxxxxcs.cmu.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
272,272X-H4F7G8E3G2,An Unsupervised Hierarchical Framework for Authorship-based Segmentation of a Multi-Author Document,Khxxxx Aldxxxx;Nixx Yaxx;Wenxxxx Jxx and Xiaxxxxxx H,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Segmenting a document collaboratively written by multiple authors into distinct
authorial components plays an increasingly important role in many applications
and has great significance on security and forensic investigation. In this
paper, we propose an effective, unsupervised two-level hierarchical learning
framework based on Naive-Bayesian approach. The key novelty and benefit of the
two-level hierarchical learning framework lies in two main aspects. We start
from estimating the writing styles reflected by segments and produce the
initial class information of data. Then, we take advantage of the difference
in the posterior probabilities of the Naive-Bayesian model and create
meaningful training datasets with a high precision for the use of accurate
supervised learning. We evaluate the performance of the proposed approach on
three benchmark datasets widely used for authorship analysis. A scientific
paper is also used to demonstrate the performance of the approach on authentic
documents. Experimental results show the superior performance of the proposed
approach over the state-of-the-arts.",7 Feb 2017 05:45:14 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  text classification,Khxxxx,Aldxxxx,xxxxxxxxxxxxei@uts.edu.au,"Global Big Data Technologies Centre, University of Technology Sydney, Australia",No,Nixx,Yaxx,xxxxxxxxxxwpu.edu.cn,"Northwestern Polytechnical University, School of Automation",No,Wenxxxx,Jxx,xxxxxxxxxxx@uts.edu.au,"Global Big Data Technologies Centre, University of Technology Sydney, Australia",No,Xiaxxxxxx,Hx,xxxxxxxxxxxe@uts.edu.au,"Global Big Data Technologies Centre, University of Technology Sydney, Australia",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Khxxxx,Aldxxxx,"Global Big Data Technologies Centre, University of Technology Sydney, Australia",,,+61 4xxxxxxxxx,,,xxxxxxxxxxxxei@uts.edu.au,,Sydney,New South Wales,,Australia,,Khxxxx Aldxxxx;Nixx Yaxx;Wenxxxx Jxx;Xiaxxxxxx Hx,xxxxxxxxxxxxei@uts.edu.au;xxxxxxxxxxnwpu.edu.cn;xxxxxxxxxxxa@uts.edu.au;xxxxxxxxxxxxe@uts.edu.au,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
273,273X-B7E6C3C7J5,Slim Embedding Layers for Recurrent Neural Language Models,Zhoxxxxxxx Lx;Rayxxxx Kulxxxxx;Shaxxxx Waxx;Yuxxxx Zhxx and Shxxxx W,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Recurrent neural language models are the state-of-the-art models for language
modeling. When the vocabulary size is large, the space taken to store the model
parameters becomes the bottleneck for the use of recurrent neural language
models. In this paper, we introduce a simple space efficient compression method
that randomly shares the structured parameters at both the input and output
embedding layers of the recurrent neural language models to significantly
reduce the size of model parameters but still compactly represent the original
input and output embedding layers. The method is easy to implement and tune.
Experiments on several data sets show that the new compression method could get
similar performance while only using a very tiny fraction of
parameters.",6 Feb 2017 21:53:22 GMT,Empirical/Data-Driven,Machine learning,NLP applications;  scalability/efficiency of ML methods;  MT post-editing,Zhoxxxxxxx,Lx,xxxxxxxxright.edu,Wright State University,No,Rayxxxx,Kulxxxxx,xxxxxxxxxx@wright.edu,Wright State University,No,Shaxxxx,Waxx,xxxxxxxxx@gmail.com,Wright State University,No,Yuxxxx,Zhxx,xxxxxxxxxsouri.edu,University of Missouri,No,Shxxxx,Wx,xxxxxxxxx@gmail.com,Yitu Inc.,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhoxxxxxxx,Lx,Wright State University,,,,,,xxxxxxxxright.edu,,,,,United States,,Zhoxxxxxxx Lx;Rayxxxx Kulxxxxx;Shaxxxx Waxx;Yuxxxx Zhxx;Shxxxx Wx,xxxxxxxxright.edu;xxxxxxxxxxx@wright.edu;xxxxxxxxxx@gmail.com;xxxxxxxxxssouri.edu;xxxxxxxxxx@gmail.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
274,274X-H3F2H5C3D9,An Unsupervised Neural Attention Model for Aspect Extraction,Ruxxxx Hx;Wxx Sxx;Hwxx Txx and Daxxxx Dahxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Oral Monday,,Undecided (Sentiment Analysis Opinion Mining),"Aspect extraction is an important and challenging task in aspect-based
sentiment analysis. Existing works tend to apply variants of topic models on
this task. While fairly successful, these methods usually do not produce highly
coherent aspects. In this paper, we present a novel neural approach with the
aim of discovering coherent aspects. The model improves coherence by exploiting
the distribution of word co-occurrences through the use of neural word
embeddings. Unlike topic models which typically assume independently generated
words, word embedding models encourage words that appear in similar contexts to
be located close to each other in the embedding space. In addition, we use an
attention mechanism to de-emphasize irrelevant words during training, further
improving the coherence of aspects. Experimental results on real-life datasets
demonstrate that our approach discovers more meaningful and coherent aspects,
and substantially outperforms baseline methods on several evaluation tasks.",23 Apr 2017 07:37:36 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Ruxxxx,Hx,xxxxxxxxxxxxp.nus.edu.sg,National University of Singapore,No,Weexxxx,Lxx,xxxxxxxxxx.nus.edu.sg,National University of Singapore,No,Hwexxxxx,Nx,xxxxxxxxxxnus.edu.sg,National University of Singapore,No,Daxxxx,Dahxxxxxx,xxxxxxxxxxxxooglemail.com,SAP,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ruxxxx,Hx,National University of Singapore,,,,,,xxxxxxxxxxxxp.nus.edu.sg,,,,,Singapore,,Ruxxxx Hx;Wxx Sxx;Hwxx Txx;Daxxxx Dahxxxxxx,xxxxxxxxxxxxp.nus.edu.sg;xxxxxxxxxxx.nus.edu.sg;xxxxxxxxxx.nus.edu.sg;xxxxxxxxxxxxxooglemail.com,An Unsupervised Neural Attention Model for Aspect Extraction,An Unsupervised Neural Attention Model for Aspect Extraction,10,Ruidan He,,"Department of Computer Science, National University of Singapore
Computing 1, 13 Computing Drive, Singapore 117417",,on,No. Do not include my submission in this dataset.,No,None,None
276,276X-J7P2A3E6F9,Semi-supervised Multitask Learning for Sequence Labeling,Maxxx Rxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Poster Tuesday,,Undecided (Tagging Chunking Syntax Parsing),"We propose a sequence labeling framework with a secondary training objective,
learning to predict surrounding words for every word in the dataset.
This language modeling objective incentivises the system to learn
general-purpose patterns of semantic and syntactic composition, which are also
useful for improving accuracy on different sequence labeling tasks.
The architecture was evaluated on a range of datasets, covering the tasks of
error detection in learner texts, named entity recognition, chunking and
POS-tagging.
The novel language modeling objective provided consistent performance
improvements on every benchmark, without requiring any additional annotated or
unannotated data.",23 Apr 2017 03:34:12 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Maxxx,Rxx,xxxxxxxxxxxl.cam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxx,Rxx,University of Cambridge,,,,,,xxxxxxxxxxxl.cam.ac.uk,,Cambridge,,,United Kingdom,,Maxxx Rxx,xxxxxxxxxxxl.cam.ac.uk,Semi-supervised Multitask Learning for Sequence Labeling,Semi-supervised Multitask Learning for Sequence Labeling,10,Marek Rei,,"Computer Laboratory, University of Cambridge, 15 JJ Thomson Avenue, Cambridge CB3 0FD",on,on,Only include my submission if it is accepted.,No,None,None
277,277X-H2D6J4G2F6,Document Level Sentiment Classification with Hierarchical Multiway Attentions,Dexxxx Mx;Suxxxx Lx;Xiaxxxxx Zhxxx and Houxxxx Wxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Document-level sentiment classification aims to assign the user reviews a
sentiment polarity.
 Previous methods utilized just the document content without consideration of
user and product information,
 or did not comprehensively consider what roles the three kinds of information
play in text modeling.
In this paper, to reasonably use all the information, we present the idea that 
user, product and their combination
can all influence the generation of attentions to words and sentences, when
judging the sentiment of a document. 
With this idea, we propose a hierarchical multiway attention (HMA) model, where
the hierarchical structure is
used to uniformly model the attentions on the word and sentence layers, and on
each layer multiple ways of
using user and product information are designed to influence the generation of
attentions.
Then, sentences and documents are well modeled by multiple representation
vectors,
 which provide rich information for sentiment classification.
Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our
model.",7 Feb 2017 07:33:42 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis,Dexxxx,Mx,xxxxxxxu.edu.cn,Peiking University,No,Suxxxx,Lx,xxxxxxxxxpku.edu.cn,Peking University,No,Xiaxxxxx,Zhxxx,xxxxxxxxgmail.com,Peking University,No,Houxxxx,WAxx,xxxxxxxxku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dexxxx,Mx,Peiking University,,,,,,xxxxxxxxxpku.edu.cn,,,,,China,,Dexxxx Mx;Suxxxx Lx;Xiaxxxxx Zhxxx;Houxxxx WAxx,xxxxxxxu.edu.cn;xxxxxxxxxxpku.edu.cn;xxxxxxxxxgmail.com;xxxxxxxxxku.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
278,278X-C7P7B3H2P3,Sentence compression by deletion using joint sequence to sequence models,Vixx Lxx;Truxxxxxxx Ngxxxx and Minxxxx Ngxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"We propose combined models of enhanced bidirectional LSTM as feature extractor
and well-known classifiers such as conditional random field and support vector
machine for compressing sentence. The task is to classify each word into two
categories: to be retained or to be removed from the original sentence. Facing
the lack of reliable feature generating techniques in many languages, we employ
the obtainable word embedding as the exclusive feature. Our models are trained
and evaluated on both pubic English and Vietnamese data set, which shows their
state-of-the-art performance. We report our models’ results in a new
Vietnamese sentence compression data set as a reference.",6 Feb 2017 07:15:20 GMT,Applications/Tools,Summarization,information extraction;  multilingual applications;  answer extraction;  document summarization,Vixx,Laixxxx,xxxxxxxxxist.ac.jp,Japan Advanced Institute of Science and Technology (JAIST),No,Truxxxxxxx,Ngxxxx,xxxxxxxxxxxjaist.ac.jp,Japan Advanced Institute of Sciene and Technology,No,Minxxxx,Ngxxxx,xxxxxxxxxxaist.ac.jp,Japan Advanced Institute of Science and Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vixx,Laixxxx,Japan Advanced Institute of Science and Technology (JAIST),,,505xxxxxxx,,,xxxxxxxxxist.ac.jp,,Nomi,Ishikawa,,Japan,,Vixx Lxx;Truxxxxxxx Ngxxxx;Minxxxx Ngxxxx and  Tecxxxxxxxx,xxxxxxxxxist.ac.jp;xxxxxxxxxxx@jaist.ac.jp;xxxxxxxxxxjaist.ac.jp,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
279,279X-C4F8A7E8H8,A Semantic Multi-field Clinical Search for Patient Medical Records,UMAMxxxxxxxx VASAxxxxxxxxx and Fraxxxx Chaxxxx,Biomedical,Aurxxxxx Néxxxxx;Kaxxx Verxxxxx,Reject,,Undecided (Biomedical),"A semantic based search engine for clinical data would be a substantial aid for
hospitals to provide support for clinical practitioners. Since electronic
medical records of patients contain a variety of information, there is a need
to extract meaningful patterns from the Patient Medical Records (PMR). The
proposed work matches patients to relevant clinical practical guidelines (CPGs)
by matching their medical records with the CPGs. However in both PMR and CPG,
the information pertaining to symptoms, diseases, diagnosis procedures and
medicines is not structured and there is a need to pre-process and index the
information in a meaningful way. In order to reduce manual efforts to match to
the clinical guidelines, this work automatically extracts the clinical
guidelines from the PDF documents using Regular Expression based approach and
indexes them with a multi-field index using Lucene. We have attempted a simple
multi-field Lucene search and an ontology based advanced search, where the PMR
is mapped to SNOMED core subset to find the important concepts. We found that
ontology based search engine gave more meaningful results for specific queries,
when compared to term based search.",6 Feb 2017 15:35:13 GMT,Applications/Tools,Biomedical,NLP applications;  information extraction;  NLP on noisy unstructured text;  information retrieval;  ontological semantics,UMAMxxxxxxxx,VASAxxxxxxxxx,xxxxxxxxxxx8@gmail.com,Research Fellow NTU,No,Fraxxxx,Charxxxxxxxx,xxxxxxeee.org,Associate Professor,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,UMAMxxxxxxxx,VASAxxxxxxxxx,Research Fellow NTU,,,815xxxxx,,,xxxxxxxxxxx8@gmail.com,,Boonlay Place,Boonlay Place,,Singapore,"Umamaheswari recently joined as a Research Fellow at NTU Singapore. Before joining NTU Singapore, she had been working as  Project Associate II and a part-time research scholar at Anna University Chennai and contributed in projects funded by DIT, New Delhi since 2006. Her research interests are semantic based Natural Language Processing, Information Retrieval, Machine Learning, Text mining and processing. In her research, she had published six research papers in international journals and presented five papers in various NLP based international conferences.",UMAMxxxxxxxx VASAxxxxxxxxx;Fraxxxx Chaxxxx,xxxxxxxxxxx8@gmail.com;xxxxxxxeee.org,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
280,280X-B3J4P4P9D3,Deep Residual Learning with Bi-directional LSTM-CRF for Multilingual Named Entity Recognition,Yinxxxx Xxx;Etxxx Haxx and Jexx Daxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"State-of-the-art named entity recognition systems traditionally require large
amounts of domain-specific knowledge in the form of hand-crafted features and
lexicons to learn from limited, supervised training corpora. In this paper, we
introduce a novel neural network architecture that automatically learns word-
and character-level features using a hybrid residual architecture with a
convolutional neural network, bi-directional LSTM, and CRF. Our neural network
can capture both orthographic and context information, eliminating the need for
feature engineering and allowing end-to-end learning from scratch. Our model
obtains state-of-the-art results in NER on multiple languages without resorting
to any language-specific knowledge (such as gazetteers) or hand-crafted
features.",7 Feb 2017 07:51:08 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",cross-lingual approaches;  information extraction;  named entity recognition;  cross-language information extraction;  text mining;  text classification;  term extraction;  temporal/spatial information extraction,Yinxxxx,Xxx,xxxxxxxbay.com,eBay Inc.,No,Etxxx,Haxx,xxxxxxxebay.com,ebay,No,Jeaxxxxxxx,Ruxxxx,xxxxxxxxebay.com,eBay,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yinxxxx,Xxx,eBay Research,,,408xxxxxxx,,,xxxxxxxbay.com,,San Jose,CA,,United States,Research Scientist on Deep Learning and NLP.,Yinxxxx Xxx;Etxxx Haxx;Jexx Daxxx,xxxxxxxbay.com;xxxxxxxxebay.com;xxxxxxxx@ebay.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
281,281X-F4F7B6A7H6,Sentiment Analysis using Relative Prosody Features,Haxxxx Abxxxx;K N;Anxx Kuxxx;Maxxxx Shrixxxxxxx and Surxxxxxxx V,Speech,Chxxxx Hoxx;Chixxxxxx Lxx,Reject,,Reject (Speech),"Recent improvement in usage of digital media has led people to share their
opinions about specific entity through audio. In this paper, an approach to
detect the sentiment of an online spoken reviews based on relative prosody
features is presented. Most of the existing systems for audio based sentiment
analysis use conventional audio features, but they are not problem specific
features to extract the sentiment. In this work, relative prosody features are
extracted from normal and stressed regions of audio signal to detect the
sentiment. Stressed regions are identified using the strength of excitation.
Support Vector Machine (SVM) and Gaussian Mixture Model (GMM) classifiers are
used to build the sentiment models. MOUD database is used for the proposed
study. Experimental results show that, the rate of detecting the sentiment is
improved with relative prosody features compared with the prosody and Mel
Frequency Cepstral Coefficients (MFCC) because the relative prosody features
has more sentiment specific discrimination compared to prosody features.",6 Feb 2017 06:01:59 GMT,Empirical/Data-Driven,Speech,sentiment analysis,Haxxxx,Abxxxx,xxxxxxxxxxxxxxxxsearch.iiit.ac.in,International Institute of Information Technology,No,K N xxxxxxxx,Alxxxx,xxxxxxxxxxxxxxxearch.iiit.ac.in,International Institute of Information Technology,No,Anixxxxxxx,Vupxxxx,xxxxxxxxxxxa@iiit.ac.in,IIIT Hyderabad,No,Maxxxx,Shrixxxxxxx,xxxxxxxxxxxxa@iiit.ac.in,International Institute of Information Technology Hyderabad,No,Suryxxxxxxxx,Gangxxxxxxx,xxxxxxxt.ac.in,"IIIT Hyderabad -  500 032, Telangana, India",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Haxxxx,Abxxxx,International Institute of Information Technology,,,,,,xxxxxxxxxxxxxxxxsearch.iiit.ac.in,,,,,India,,Haxxxx Abxxxx;K N;Anxx Kuxxx;Maxxxx Shrixxxxxxx;Surxxxxxxx V,xxxxxxxxxxxxxxxxsearch.iiit.ac.in;xxxxxxxxxxxxxxxxearch.iiit.ac.in;xxxxxxxxxxxxa@iiit.ac.in;xxxxxxxxxxxxva@iiit.ac.in;xxxxxxxit.ac.in,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
282,282X-G3P8B6B5A6,Detecting and Understanding Satirical Fake News with Neural Networks and Linguistic Features,Fxx Yaxx;Arxxx Mukxxxxxx and Edxxxx Drxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Social media has enabled the spreading of news faster than ever before. While
this enhances situational awareness, it also increases the potential damage
carried by deceptive information. Although fake news can be detected based on
the source website, the fundamental features have not been fully revealed in
the literature of computational linguistics. In this work, we focus on one
specific type of fake news - satirical fake news, and tackle the problem of
detecting fake news by leveraging neural networks and linguistic features. By
observing that satirical fake information is usually reflected in certain
paragraphs rather than the whole document, we implement attention mechanism to
spot the most satirical paragraph with only document-level labeling. To further
investigate paragraph-level satirical fake information, we collect satirical
news articles from multiple websites and analyze both document-level and
paragraph-level linguistic features. The evaluation suggests the proposed model
detects satirical fake news effectively and reveals what features are important
under which level.",7 Feb 2017 09:20:49 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",NLP applications;  text classification,Fxx,Yaxx,xxxxxxx@uh.edu,University of Houston,No,Arxxx,Mukxxxxxx,xxxxxxxxx@gmail.com,University of Houston,No,Edxxxx,Drxxxx,xxxxxxxxxemple.edu,Temple University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fxx,Yaxx,University of Houston,,,,,,xxxxxxx@uh.edu,,,,,United States,,Fxx Yaxx;Arxxx Mukxxxxxx;Edxxxx Drxxxx,xxxxxxx@uh.edu;xxxxxxxxxx@gmail.com;xxxxxxxxxtemple.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
283,283X-D3E7B9J4J7,Between Reading Time and Information Structure,Masxxxxx Asaxxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"This paper presents a contrastive analysis between the reading time and
information structure in Japanese.
We overlaid the reading time annotation BCCWJ-EyeTrack and an information
structure annotation on the Balanced Corpus of Contemporary Written Japanese.
Statistical analysis based on a mixed linear model showed that the
``specificity,'' ``sentience,'' and ``commonness'' of the Japanese information
structure affect the reading time.
These three characteristics produce different patterns of delay in the reading
time.
Especially, the reading time patterns differ depending on the
 commonness such as new information or bridging.
The results suggest that new information and bridging can be classified by the
reading time pattern.",6 Feb 2017 13:07:58 GMT,Empirical/Data-Driven,Discourse and pragmatics,corpus development;  discourse;  anaphora resolution;  pragmatics,Masxxxxx,Asaxxxx,xxxxxxxxxxinjal.ac.jp,National Institute for Japanese Language and Linguistics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Masxxxxx,Asaxxxx,National Institute for Japanese Language and Linguistics,,,81-42xxxxxxxxx,,,xxxxxxxxxxinjal.ac.jp,,,,,Japan,"2012-current  National Institute for Japanese Language and Linguistics, Japan
Project Assoc. Prof.
2004-2011   Nara Institute of Science and Technology, Japan  Assist. Prof.",Masxxxxx Asaxxxx and  Linxxxxxxxxx,xxxxxxxxxxinjal.ac.jp,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
284,284X-G5F2G7P6E6,"Toward Abstractive Multi-Document Summarization Using Submodular Function-Based Framework, Sentence Compression and Merging",Ylxxxx Chxxx;Moxx Maxxxx and Mxx Tafxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"We propose a submodular function-based summarization system which integrates
three important measures namely importance, coverage, and non-redundancy to
detect the important sentences for the summary. Our designed functions are
monotone and submodular which allow us to apply an efficient and scalable
greedy algorithm to obtain informative and well-covered summaries. In addition,
we integrate two abstraction-based methods namely sentence compression and
sentence merging for generating new concise sentence set. We design our
summarization models for both generic and query-focused summarization.
Experimental results on DUC-2004 and DUC-2007 dataset show that our generic and
query-focused summarizers have outperformed the state-of-the-art summarization
systems in terms of ROUGE-1 and ROUGE-2 recall and F-measure.",6 Feb 2017 06:20:45 GMT,Theoretical,Summarization,document summarization;  multi-document summarization,Ylxxxx,Chxxx,xxxxxxxxxxli@uleth.ca,University of Lethbridge,No,Moinxxxxxxx,Taxxxx,xxxxxxxuleth.ca,University of Lethbridge,No,Mir xxxxxxx,Naxxxx,xxxxxxxxxm@uleth.ca,University of Lethbridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Moxx,Taxxxx,University of Lethbridge,,,,,,xxxxxxxuleth.ca,,,,,Canada,,Ylxxxx Chxxx;Moxx Maxxxx;Mxx Tafxxxx,xxxxxxxxxxli@uleth.ca;xxxxxxxxuleth.ca;xxxxxxxxxxm@uleth.ca,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
285,285X-A5B8J3D3C3,Interaction-based feature selection method for text categorization,Xiaxxxxxx Taxx;Yuaxxxxx Dxx and Yanxxxx Xixx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In many text categorization tasks, feature selection is crucial for reducing
the high dimensionality of text data. Many feature selection approaches have
been proposed for text categorization, such as Chi-square and Information Gain.
However, most of the previous approaches assume that features are independent
with each other, whereas the feature interactions are not well understood. In
this paper, we propose a new feature selection approach that takes into account
feature interactions. Feature interactions are evaluated by an information
theoretic measure, i.e., the interaction information among features and the
class label. By maximizing the four-way joint mutual information between
features and the class label, we can select the most representative features
with highest feature interactions. Extensive experiments on benchmark text data
sets have shown that the proposed approach outperforms the state-of-the-art
feature selection approaches for text categorization.",6 Feb 2017 06:51:42 GMT,Theoretical,"Document analysis including text categorization, topic models, and retrieval",text mining;  text classification,Xiaxxxxxx,Taxx,xxxxxxxxxxxc@gmail.com,University of Electronic Science and Technology of China,No,Yuaxxxxx,Dxx,xxxxxxxxtc.edu.cn,"School of Computer Science and Engineering, University of Electronic Science and Technology of China, China",No,Yanxxxx,Xixxx,xxxxxxxxxxxg@gmail.com,"School of Computer Science and Engineering, University of Electronic Science and Technology of China, China",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxxxx,Taxx,"School of Computer Science and Engineering, University of Electronic Science and Technology of China, China",,,,,,xxxxxxxxxxxc@gmail.com,,,,,China,,Xiaxxxxxx Taxx;Yuaxxxxx Dxx;Yanxxxx Xixxx and Engixxxxxxx Unixxxxxxx,xxxxxxxxxxxc@gmail.com;xxxxxxxxxtc.edu.cn;xxxxxxxxxxxng@gmail.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
286,286X-D5A9A5J9C8,Sparse and Multi-scale Meta-words for Document Categorization,Nicxxxx Rey-Vxxxxxxxxx;Thxxxx Solxxxx;Huxx Jaxx and Maxxxx Moxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We propose a methodology for representing documents as bags of meta-words for
text classification. Meta-words are obtained by grouping semantically related
words in an embedded space. These meta-words capture semantic differences at
the word level to create a new representation at the document level. In order
to improve such document representation, we propose the extraction of
meta-words at different semantic scales, thus expanding the document
representation based on these multi-scale meta-words. Additionally, we propose
to use recently introduced ideas in the NLP community of sparse representation
in order to find more meaningful meta-words in the word embedding space. We
demonstrate that the meta-words, or centroids, based representation is more
inherently related to the underlying topic of a document than the single words
representation. The results of this methodology in traditional text
categorization problems outperform similarly proposed approaches for the same
set of tasks.",7 Feb 2017 00:50:56 GMT,Theoretical,"Document analysis including text categorization, topic models, and retrieval",semantic relations;  text classification,Nicxxxx,Rey-Vxxxxxxxxx,xxxxxxxxxxv@gmail.com,University of Houston,No,Thxxxx,Solxxxx,xxxxxxxxxxxxio@gmail.com,UH,No,Hugxxxxxx,Escxxxxxx,xxxxxxxxx@gmail.com,INAOE,No,Maxxxx,Moxxxx,xxxxxxxxxinaoep.mx,INAOE,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nicxxxx,Rey-Vxxxxxxxxx,University of Houston,,,,,,xxxxxxxxxxv@gmail.com,,,,,United States,,Nicxxxx Rey-Vxxxxxxxxx;Thxxxx Solxxxx;Huxx Jaxx;Maxxxx Moxxxx,xxxxxxxxxxv@gmail.com;xxxxxxxxxxxxrio@gmail.com;xxxxxxxxxx@gmail.com;xxxxxxxxx@inaoep.mx,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
287,287X-G5A5F8C9B6,Neural Dynamic Feedback Connections with Hyper-Networks for Question Answering,Taxxxx Kxx;Rodxxxx Nogxxxxx and Yoxxxx Bexxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this work, we present a question answering model that has distinct
questioning and answering modules with neural dynamic feedback connections
between them. The answering module is considered as a main network that does
reading comprehension over a document  to extract answer information and we
propose in addition the questioning module to control it as a hyper-network. It
is all processed through neural dynamic feedback connections that allows to
update question information as well and subsequently use it to dynamically
generate some parameters of the answering module. Our models are evaluated on
question answering dataset SQuAD and we show competitive results with
visualizations on how the question information properly controls the answering
module",6 Feb 2017 13:59:16 GMT,Applications/Tools,"Information extraction, text mining, and question answering",context-aware question answering,Taxxxx,Kxx,xxxxxxxxxxxumontreal.ca,Université de Montréal,No,Rodxxxx,Nogxxxxx,xxxxxxxxxxxeira@nyu.edu,New York University,No,Yoxxxx,Bexxxx,xxxxxxxxxxxxx@umontreal.ca,Université de Montréal,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Taxxxx,Kxx,Université de Montréal,,,,,,xxxxxxxxxxxumontreal.ca,,,QC,,Canada,,Taxxxx Kxx;Rodxxxx Nogxxxxx;Yoxxxx Bexxxx,xxxxxxxxxxxumontreal.ca;xxxxxxxxxxxxeira@nyu.edu;xxxxxxxxxxxxxo@umontreal.ca,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
288,288X-J7B3G3A3D9,The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task,Rxx Schxxxxx;Maaxxxx Sxx;Ioaxxxx Konxxxx;Lexxx Zixxxx;Yexxx Chxx and Noxx Ax,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"A writer's style depends not just on personal traits but also on her intent and
mental state.
In this paper, we show how variants of the same writing task can lead to
measurable differences in writing style. We present a case study based on the 
story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned
similar writing tasks with different constraints: (1) writing an entire story,
(2) adding a story ending for a given story context, and (3) adding an
incoherent ending to a story. We show that a simple linear classifier informed
with stylistic features is able to successfully distinguish between the three
cases, without even looking at the story context. In addition, our style-based
classifier establishes a new state-of-the-art result on the story cloze
challenge, substantially higher than previous results based on deep learning
models. Our results demonstrate that different task framings can dramatically
affect the way people write.",10 Feb 2017 03:16:47 GMT,Empirical/Data-Driven,Discourse and pragmatics,collaborative methods for question answering,Rxx,Schxxxxx,xxxxxxxxxxxxshington.edu,University of Washington and The Allen Institute for Artificial Intelligence,No,Maaxxxx,Sxx,xxxxxxxxxxxhington.edu,University of Washington,No,Ioaxxxx,Konxxxx,xxxxxxxxxxxxxashington.edu,University of Washington,No,Lexxx,Zixxxx,xxxxxxxxxxxxxshingtone.edu,University of Washington,No,Yexxx,Chxx,xxxxxxxxxxxshington.edu,University of Washington,No,Noaxxxx,Smxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,Rxx,Schxxxxx,University of Washington and The Allen Institute for Artificial Intelligence,,,,,,xxxxxxxxxxxxshington.edu,,Seattle,WA,,United States,,Rxx Schxxxxx;Maaxxxx Sxx;Ioaxxxx Konxxxx;Lexxx Zixxxx;Yexxx Chxx;Noxx Ax,xxxxxxxxxxxxshington.edu;xxxxxxxxxxxshington.edu;xxxxxxxxxxxxxwashington.edu;xxxxxxxxxxxxxashingtone.edu;xxxxxxxxxxxxshington.edu;xxxxxxxxxxxxxashington.edu,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
289,289X-P7A8E7F7H6,A Meaning-based Statistical Framework for Solving English Math Word Problems,Chaxxxxxx Lixxx;Shixxxxxx Tsxx;Yu-xxxxxx Woxx;Yi-xxxxx Lxx and Kehxxxx S,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"A meaning-based statistical framework for solving English math word problems
(MWPs) via understanding and reasoning is presented in this paper. It first
analyzes the text and transforms both body and question parts into their logic
forms, and then performs inference on them. The associated context of each
quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.),
which provides the flexibility for annotating the extracted math quantity with
its associated syntactic and semantic information (indicating quantity physical
meaning). Those role-tags are then used to identify the desired operand and
filter out irrelevant quantities under the proposed statistical framework. The
problem solving process thus resembles the human cognitive understanding of
MWPs and produces a more meaningful interpretation. We learn the model
parameters and automatically construct related tables (e.g., lexical entailment
and verb-category tables) via performing weakly supervised learning on the
given MWPs. Experimental results show that the proposed approach outperforms
existing systems on benchmark data-sets.",10 Feb 2017 01:17:29 GMT,Applications/Tools,Semantics,NLP applications;  educational applications,Chaxxxxxx,Lixxx,xxxxxxxxxxxxsinica.edu.tw,"Institute of Information Science, Academia Sinica",No,Shixxxxxx,Tsxx,xxxxxxxxxxxxxx.sinica.edu.tw,"Institute of Information Science, Academia Sinica",No,Yu-xxxxxx,Woxx,xxxxxxxxxxxxxxs.sinica.edu.tw,"Institute of Information Science, Academia Sinica",No,Yi-xxxxx,Lxx,xxxxxxxxxxnica.edu.tw,"Institute of Information Science, Academia Sinica",No,Kehxxxx,Sx,xxxxxxxxxxxnica.edu.tw,"Institute of Information Science, Academia Sinica",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Chaxxxxxx,Lixxx,"Institute of Information Science, Academia Sinica",,,,,,xxxxxxxxxxxxsinica.edu.tw,,,,,Taiwan,,Chaxxxxxx Lixxx;Shixxxxxx Tsxx;Yu-xxxxxx Woxx;Yi-xxxxx Lxx;Kehxxxx Sx,xxxxxxxxxxxxsinica.edu.tw;xxxxxxxxxxxxxxs.sinica.edu.tw;xxxxxxxxxxxxxxxs.sinica.edu.tw;xxxxxxxxxxxnica.edu.tw;xxxxxxxxxxxinica.edu.tw,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
290,290X-C8E3C6J3E4,A Path Reasoning Network for Multi-Relation Question Answering,Manxxxx Zhxx;Mixxxx Huxxx and xiaxxxx zx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"It is challenging to answer multi-relation questions, which requires to reason
with a question and multiple fact triples in knowledge base. In this paper, we
present a novel neural network called Path Reasoning Network
(PRN). PRN makes reasoning on a question through iterative analysis: a
reasoning module dynamically decides which part of an input question should be
analyzed at each iteration; the representation of the question is iteratively
updated according to the previous output of the reasoning module; and the final
answer will be predicted at the end of the reasoning process. Experiments show
that our model yields competitive results on answering multi-relation
questions. Further, the model can offer traceable and observable intermediate
predictions for reasoning analysis and failure diagnosis.",7 Feb 2017 08:56:35 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP applications;  question interpretation;  open-domain question answering,Manxxxx,Zhxx,xxxxxxxxxgmail.com,Tsinghua University,No,Mixxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,xiaxxxx,zxx,xxxxxxxxxxxnghua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Manxxxx,Zhxx,Tsinghua University,,,+86 1xxxxxxxxxx,,,xxxxxxxxxgmail.com,,,,,China,,Manxxxx Zhxx;Mixxxx Huxxx;xiaxxxx zxx,xxxxxxxxxgmail.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxxxxnghua.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
291,291X-E3H9C6A2A4,RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems,Choxxxxxx Txx;Lixx Mxx;Donxxxx Zhxx and Rxx Yx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"Open-domain human-computer conversation has been attracting increasing
attention over the past few years. However, there does not exist a standard
automatic evaluation metric for open-domain dialog systems; researchers usually
resort to human annotation for model evaluation, which is time- and
labor-intensive. In this paper, we propose RUBER, a Referenced metric and
Unreferenced metric Blended Evaluation Routine, which evaluates a reply by
taking into consideration both a groundtruth reply and a query (previous
user-issued utterance). Our metric is learnable, but its training does not
require labels of human satisfaction. Hence, RUBER is flexible and extensible
to different datasets and languages. Experiments on both retrieval and
generative dialog systems show that RUBER has a high correlation with human
annotation.",7 Feb 2017 04:24:50 GMT,Resources/Evaluation,Resources and evaluation,evaluation methods for dialogues,Choxxxxxx,Txx,xxxxxxxxxxxo@pku.edu.cn,Peking University,No,Lixx,Mxx,xxxxxxxxxxxxmou@gmail.com,Peking University,No,Donxxxx,Zhxx,xxxxxxxxku.edu.cn,Peking University,No,Rxx,Yxx,xxxxxxxxxxu@gmail.com,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Choxxxxxx,Txx,Peking University,,,,,,xxxxxxxxxxxo@pku.edu.cn,,,,,China,,Choxxxxxx Txx;Lixx Mxx;Donxxxx Zhxx;Rxx Yxx,xxxxxxxxxxxo@pku.edu.cn;xxxxxxxxxxxxxmou@gmail.com;xxxxxxxxxku.edu.cn;xxxxxxxxxxxu@gmail.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
293,293X-B7B8P8J8B8,Idiom-Aware Compositional Distributed Semantics,Penxxxx Lxx;Xixxxx Qxx;Kaxxx Qixx and Xuaxxxxx Huxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Idioms are especially common in natural languages. Different from its literal
meaning, an idiom has a figurative meaning. This phenomenon imposes great
challenges for representing the semantics of language, especially in current
prevailing end-to-end neural models, which assume that the semantics of a
phrase or sentence is literally composed from its constitutive words. In this
paper, we proposed an idiom-aware distributed semantic model to build
representation of sentences while understanding idioms they contains. Our model
adaptively addresses semantic compositionality of a phrase literally or
idiomatically. To better evaluate our model, we also construct an
idiom-enriched sentiment classification dataset with considerable scale and
abundant peculiarities of idioms. The qualitative and quantitative experimental
analyses demonstrate the efficacy of our models.",7 Feb 2017 09:52:57 GMT,Empirical/Data-Driven,Semantics,multiword semantics/compositionality,Penxxxx,Lxx,xxxxxxxxxxdan.edu.cn,,No,Xixxxx,Qxx,xxxxxxxxxan.edu.cn,Fudan University,No,Kaxxx,Qixx,xxxxxxxxxdan.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Penxxxx,Lxx,,,,,,,xxxxxxxxxxdan.edu.cn,,,,,China,,Penxxxx Lxx;Xixxxx Qxx;Kaxxx Qixx;Xuaxxxxx Huxxx,xxxxxxxxxxdan.edu.cn;xxxxxxxxxdan.edu.cn;xxxxxxxxxxdan.edu.cn;xxxxxxxxxxudan.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
294,294X-G9C3H6H9C8,Part-of-Speech Tagging for Twitter with Adversarial Neural Networks,Qx Zhxxx;Txx Gxx;Haxxxx Huxxx and Xuaxxxxx Huxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"In this work, we propose a novel neural network to perform the task of
part-of-speech tagging for Tweets. In contrast to newswire articles, Tweets are
usually informal and contain numerous out-of-vocabulary words. Moreover, there
is lacking of labeled dataset for this domain. To tackle these challenges, in
this work, we introduced a method which extends bi-directional long short-term
memory recurrent neural network with an adversarial predictor. It can make
better use of labeled data from other resource-rich domains, unlabeled
in-domain data, and labeled in-domain data. To evaluate the performance of the
proposed method, three different datasets are used in this work. Experimental
results  demonstrated that the unlabeled in-domain data and labeled
out-of-domain data were make good use of by the proposed method. It achieved
better performance than state-of-the-art methods in most cases.",6 Feb 2017 08:03:51 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",part-of-speech tagging;  NLP in social networking media,Qx,Zhxxx,xxxxxxxn.edu.cn,Fudan University,No,Txx,Gxx,xxxxxxxxxxxxfudan.edu.cn,Fudan University,No,Haxxxx,Huxxx,xxxxxxxxxxxudan.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qx,Zhxxx,Fudan University,,,,,,xxxxxxxn.edu.cn,,,,,China,,Qx Zhxxx;Txx Gxx;Haxxxx Huxxx;Xuaxxxxx Huxxx,xxxxxxxn.edu.cn;xxxxxxxxxxxx@fudan.edu.cn;xxxxxxxxxxxfudan.edu.cn;xxxxxxxxxxudan.edu.cn,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
295,295X-D9H3J3A6P6,An Investigation into the Pedagogical Features of Documents,Emxxx Shxxx and Prxx Natxxxxxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),"We present a novel annotated corpus and the results of our initial
investigations into estimating the usefulness of a document to an individual
who seeks to learn about specific concepts described in the document.  In this
work, we refer to this learning utility as the ``pedagogical value'' of the
document to the learner. Pedagogical value is an important concept for
education and has been studied extensively within that domain, but there has
been little work exploring this concept from a natural language processing
(NLP) perspective.  In this study, we have created the first annotated corpus
for the study of pedagogical value. We have also used this corpus to train a
classifier to automatically tag the ``pedagogical roles'' of documents, e.g.,
Tutorial and Survey, using techniques that represent each document as a
distribution over sentence embedding clusters. This work introduces the pursuit
of pedagogically-motivated NLP and presents experimental methods in this
context.",6 Feb 2017 07:37:12 GMT,Resources/Evaluation,Other,corpus development;  educational applications;  text classification,Emxxx,Shxxx,xxxxxxx@isi.edu,USC Information Sciences Institute,No,Prxx,Natxxxxxx,xxxxxxxx@isi.edu,USC Information Sciences Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Emxxx,Shxxx,USC Information Sciences Institute,,,,,,xxxxxxx@isi.edu,,,CA,,United States,,Emxxx Shxxx;Prxx Natxxxxxx,xxxxxxx@isi.edu;xxxxxxxxj@isi.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
296,296X-E6B6P2D6E4,Found in Translation: Reconstructing Phylogenetic Language Trees from Translations,Elxx Rabxxxxxxx;Noxx Orxxx and Shxxx Wixxxx,Multilingual,Omxx Abxxx;Moxx Dixx,Accept - Oral Monday,,Undecided (Multilingual),"Translation has played an important role in trade, law, commerce, politics, and
literature for thousands of years. Translators have always tried to be
invisible; ideal translations should look as if they were written originally in
the target language. We show that traces of the source language remain in the
translation product to the extent that it is possible to uncover the history of
the source language by looking only at the translation. Specifically, we
automatically reconstruct phylogenetic language trees from monolingual texts
(translated from several source languages). The signal of the source language
is so powerful that it is retained even after two phases of translation. This
strongly indicates that source language interference is the most dominant
characteristic of translated texts, overshadowing the more subtle signals of
universal properties of translation.",20 Apr 2017 09:21:27 GMT,Empirical/Data-Driven,Multilinguality,,Elxx,Rabxxxxxxx,xxxxxxxxxgmail.com,University of Haifa,No,Noxx,Orxxx,xxxxxxxxxx@gmail.com,Univeristy of Haifa,No,Shxxx,Winxxxx,xxxxxxxxxxaifa.ac.il,University of Haifa,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Elxx,Rabxxxxxxx,IBM Research and University of Haifa,,,,,,xxxxxxxxxgmail.com,,,,,Israel,,Elxx Rabxxxxxxx;Noxx Orxxx;Shxxx Winxxxx,xxxxxxxxxgmail.com;xxxxxxxxxxn@gmail.com;xxxxxxxxxxhaifa.ac.il,Found in Translation: Reconstructing Phylogenetic Language Trees from Translations,Found in Translation: Reconstructing Phylogenetic Language Trees from Translations,11,Ella Rabinovich,,,on,on,No. Do not include my submission in this dataset.,No,None,None
297,297X-F3A5J6C3H4,Parsing Images into Dependency Graphs,Yuxxxx Mixxx;Saxx Phxx;Suxxxx Uemxxxx and Akxxx Miyxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"This paper proposes a method for predicting syntactic/semantic
dependency graphs that represent the content of a given image.
A significant obstacle to
this goal is in obtaining training data, i.e., pairs of an image and its
dependency graph.
Our proposal is to obtain dependencies from image caption texts
by applying off-the-shelf parsers.
This resource allows us to train visual recognition models to detect
dependencies for a given image, by
reducing the task into multilabel classification.
Experimental results demonstrate that our method can predict
dependencies at high accuracy that human-generated and auto-generated
captions do not necessarily mention explicitly.",7 Feb 2017 05:27:29 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",multimodal representations and processing,Yuxxxx,Mixxx,xxxxxxxxii.ac.jp,National Instutite of Informatics,No,Saxx,Phxx,xxxxxxxxii.ac.jp,National Institute of Informatics,No,Suxxxx,Uemxxxx,xxxxxxxxnii.ac.jp,National Institute of Informatics,No,Akxxx,Miyxxxxx,xxxxxxxxxx@nii.ac.jp,National Institute of Informatics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxxx,Mixxx,University of Tokyo,,,,,,xxxxxxxxxxxxu-tokyo.ac.jp,,Tokyo,,,Japan,,Yuxxxx Mixxx;Saxx Phxx;Suxxxx Uemxxxx;Akxxx Miyxxxxx,xxxxxxxxii.ac.jp;xxxxxxxxnii.ac.jp;xxxxxxxxxnii.ac.jp;xxxxxxxxxxa@nii.ac.jp,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
298,298X-D4H3F4E6H6,Sentiment Lexicon Expansion using a New Neural PU Learning Approach,Yasxxxx Waxx;Yaxx Zhxxx and Bixx Lx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Although many sentiment lexicons in different languages exist, many are not
comprehensive. In a recent sentiment analysis application, we used a popular
Chinese sentiment lexicon and found that it missed a large number of sentiment
words in social media. This prompted us to make a new attempt to study
sentiment lexicon expansion. In this paper, we first propose a new formulation
of the problem by modeling it as a PU learning problem (learning from positive
and unlabeled examples). We then adapt an existing PU learning method to an
neural network-based method and show enhanced accuracy. To improve further, we
propose a brand new PU learning approach. Experimental results show that the
proposed approach outperforms baseline methods greatly.",7 Feb 2017 06:57:23 GMT,Theoretical,Sentiment analysis and opinion mining,sentiment analysis;  domain adaptation;  lexicon development;  NLP in social networking media;  adaptation to noisy data,Yasxxxx,Waxx,xxxxxxxxxxx@huawei.com,"Shannon Cognitive Computing Laboratory, Huawei Technologies Co. Ltd.",No,Yaxx,Zhxxx,xxxxxxxxxxx@huawei.com,"Shannon Cognitive Computing Laboratory, Huawei Technologies Co. Ltd.",No,Bixx,Lxx,xxxxxxic.edu,"Department of Computer Science, University of Illinois at Chicago",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yasxxxx,Waxx,"Noah's Ark Laboratory, Huawei Technologies Co. Ltd.",,,,,,xxxxxxxxxxx@huawei.com,,Shenzhen,,,China,,Yasxxxx Waxx;Yaxx Zhxxx;Bixx Lxx,xxxxxxxxxxx@huawei.com;xxxxxxxxxxx6@huawei.com;xxxxxxuic.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
300,300X-D3F9B6D3P3,A Constrained Sequence-to-Sequence Neural Model for Sentence Simplification,Yaoxxxx Zhxxx;Zhxxxx Yx;Yanxxxx Fexx;Donxxxx Zhxx and Rxx Yx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Sentence simplification reduces semantic complexity to benefit people with
language impairments. Previous simplification studies on the sentence level and
word level have achieved promising results but also meet great challenges. For
sentence-level studies, sentences after simplification are fluent but sometimes
are not really simplified. For word-level studies, words are simplified but
also have potential grammar errors due to different usages of words before and
after simplification. In this paper, we propose a two-step simplification
framework by combining both the word-level and the sentence-level
simplifications, making use of their corresponding advantages. Based on the
two-step framework, we implement a novel constrained neural generation model to
simplify sentences given simplified words. The final results on Wikipedia and
Simple Wikipedia aligned datasets indicate that our method yields better
performance than various baselines.",6 Feb 2017 15:37:19 GMT,Applications/Tools,Generation,sentence simplification,Yaoxxxx,Zhxxx,xxxxxxxxxxxxg@pku.edu.cn,Peking University,No,Zhxxxx,Yx,xxxxxxxxxpku.edu.cn,Peking Universiy,No,Yanxxxx,Fexx,xxxxxxxxxxx@pku.edu.cn,Peking University,No,Donxxxx,Zhxx,xxxxxxxxku.edu.cn,Peking University,No,Rxx,Yxx,xxxxxxxxxxu@gmail.com,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaoxxxx,Zhxxx,Peking University,,,,,,xxxxxxxxxxxxg@pku.edu.cn,,Beijing,Beijing,,China,,Yaoxxxx Zhxxx;Zhxxxx Yx;Yanxxxx Fexx;Donxxxx Zhxx;Rxx Yxx,xxxxxxxxxxxxg@pku.edu.cn;xxxxxxxxxxpku.edu.cn;xxxxxxxxxxxg@pku.edu.cn;xxxxxxxxxku.edu.cn;xxxxxxxxxxxu@gmail.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
301,301X-B7A3H6H5B8,Density Estimation for Geolocation via Convolutional Mixture Density Network,Haxxxx Ixx;Shxxx Wakxxxxx and Eixx ARxxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Nowadays, geographic information related to Twitter is crucially important for
fine grained applications.
However, the amount of geographic information available on Twitter is low,
which makes the pursuit of many applications challenging.
Under such circumstances, estimating the location of a tweet is an important
goal of study in this area. Unlike most previous studies that directly estimate
the location,
this study employs a probability distribution to represent richer information
of the tweet, not only the location but also its ambiguity.
To realize this modeling, we propose the convolutional mixture density network
(CMDN), which uses text data to estimate the mixture model parameters.
Experimentally obtained results reveal that CMDN achieved higher prediction
performance than previous approaches. It also provides a quantitative
representation of the location ambiguity for each tweet that properly works for
extracting the reliable location estimations.",7 Feb 2017 11:54:11 GMT,Applications/Tools,Social media,NLP applications;  experimental evaluation/comparison of ML methods;  NLP in social networking media;  adaptation to noisy data;  social network,Haxxxx,Ixx,xxxxxxxxx@gmail.com,NAIST,No,Shxxx,Wakxxxxx,xxxxxxxxxx@gmail.com,NAIST,No,Eixx,ARAxxxx,xxxxxxxxxxxi@gmail.com,Kyoto University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Haxxxx,Ixx,Nara Institute of Science and Technology,,,,,,xxxxxxxxx@gmail.com,,,,,Japan,,Haxxxx Ixx;Shxxx Wakxxxxx;Eixx ARAxxxx,xxxxxxxxx@gmail.com;xxxxxxxxxxa@gmail.com;xxxxxxxxxxxki@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
302,302X-E3P2D7H9H5,Reasons for Lexical Memorization and Two Alleviating Method in Supervised Hypernymy Detection,Koxx Waxxxx and Tsuxxxxx Kaxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Automatic hypernymy detection has an important role in natural language
processing. In this area, supervised approaches using distributional
representations have been dominant lately. However, these approaches suffer
from a type of overfitting, called lexical memorization. This is the problem
that a classifier does not learn relations between two words, but learns only a
prototypical hypernym in training data.  In this paper, we clarify the problem
and investigate why a classifier tends to overfit hypernyms in training data.
Moreover, to resolve this problem we propose two methods: adjusting frequencies
of each word in the training data and adding unsupervised measures to the
features of supervised approaches. Our experiment demonstrates that these
methods successfully alleviate lexical memorization.",6 Feb 2017 08:36:32 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  distributional similarity;  semantic relations;  ontological semantics,Koxx,Waxxxx,xxxxxxxxxxxxxxxc.u-tokyo.ac.jp,The University of Tokyo,No,Tsuxxxxx,Kaxx,xxxxxxxxxxxx-tokyo.ac.jp,The University of Tokyo,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Koxx,Waxxxx,The University of Tokyo / RIKEN AIP,,,,,,xxxxxxxxxxxxxxxc.u-tokyo.ac.jp,,,,,Japan,,Koxx Waxxxx;Tsuxxxxx Kaxx,xxxxxxxxxxxxxxxc.u-tokyo.ac.jp;xxxxxxxxxxxxu-tokyo.ac.jp,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
304,304X-C2E3G3C4J5,Efficient Exploration for Dialogue Policy Learning with BBQ-Networks,Zacxxxx Lixxxx;Jiaxxxxx Gxx;Lixxxx Lx;Lx Dexx;Xixxxx Lx and Faxxxx Ahxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"When rewards are sparse and action spaces large, Q-learning with epsilon-greedy
exploration can be inefficient. This poses problems for applications such as
task-oriented dialogue systems, where the primary reward signal, indicating
successful completion of a task, requires a long sequence of appropriate
actions. We present a new algorithm to significantly improve the efficiency of
exploration for deep Q-learning agents in dialogue systems. Our agents explore
via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop
neural network. Our algorithm improves over common exploration strategies such
as epsilon-greedy and  Boltzmann exploration. Additionally, we show that
spiking the replay buffer with experiences from a small number of successful
episodes, as are easy to harvest for dialogue tasks, can make Q-learning
feasible when it might otherwise fail.",7 Feb 2017 09:10:12 GMT,Empirical/Data-Driven,Dialog and interactive systems,dialogue control;  dialogue;  on-line learning;  reinforcement learning,Zacxxxx,Lixxxx,xxxxxxxxxxxse@gmail.com,UCSD,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,Microsoft Research,No,Lixxxx,Lx,xxxxxxxxxxs@gmail.com,Microsoft Research,No,Lx,Dexx,xxxxxxxxxosoft.com,Microsoft Research,No,Xixxxx,Lx,xxxxxxxxxosoft.com,Microsoft Research,No,Faxxxx,Ahxxx,xxxxxxxxxxrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,Zacxxxx,Lixxxx,UCSD,,,,,,xxxxxxxxxxxse@gmail.com,,,,,United States,,Zacxxxx Lixxxx;Jiaxxxxx Gxx;Lixxxx Lx;Lx Dexx;Xixxxx Lx;Faxxxx Ahxxx,xxxxxxxxxxxse@gmail.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxs@gmail.com;xxxxxxxxxrosoft.com;xxxxxxxxxrosoft.com;xxxxxxxxxxcrosoft.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
305,305X-F2D3E8A8P8,Controlling Linguistic Style Aspects in Neural Language Generation,Jesxxxx Fixxxx and Yoxx Golxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Most work on neural natural language generation (NNLG) focus on controlling the
content of the generated text. 
We propose an NNLG framework that allows the user to individually control
several stylistic aspects of the generated text, in addition to its content. 
The method is based on conditioned RNN language model, where the desired
content as well as the stylistic parameters serve as conditioning contexts.
We demonstrate our approach on the movie reviews domain and show that it is
successful in generating coherent sentences corresponding to the required
linguistic style and content.",6 Feb 2017 08:19:26 GMT,Empirical/Data-Driven,Generation,language generation,Jesxxxx,Fixxxx,xxxxxxxxxxxxer@gmail.com,Bar Ilan Univerdity,No,Yoxx,Golxxxxx,xxxxxxxxxxxrg@gmail.com,Bar Ilan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jesxxxx,Fixxxx,Bar Ilan University,,,,,,xxxxxxxxxxxxer@gmail.com,,,,,Israel,,Jesxxxx Fixxxx;Yoxx Golxxxxx,xxxxxxxxxxxxer@gmail.com;xxxxxxxxxxxxrg@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
308,308X-J6H6A2B6J3,Named Entity Disambiguation for Noisy Text,Yoxxx Esxxx;Noxx Coxxx;Kixx Radxxxxx;Shxxx Marxxxxxxx;Ikxxx Yaxxxx and Omxx Lxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"We address the task of Named Entity Disambiguation (NED) for noisy text. 
We present WikilinksNED, a large-scale NED dataset of fragments taken from
web-pages, that is significantly noisier and more challenging then existing
news-based datasets.
We propose a model based on Attention-RNNs to model the sequential nature of
text, and attend to the useful signals in it.
We describe novel methods for sampling the training and for initializing word
and entity embeddings, and demonstrate their importance for model performance.
We evaluate both on WikilinksNED and on a standard, smaller, news-based dataset
and find our model significantly outperforms existing state-of-the-art methods
on WikilinksNED while achieving comparable performance on the smaller
less-noisy dataset.",7 Feb 2017 07:26:57 GMT,Empirical/Data-Driven,Semantics,named entity disambiguation;  entity disambiguation;  word sense disambiguation,Yoxxx,Esxxx,xxxxxxxxxxy@gmail.com,Technion,No,Noxx,Coxxx,xxxxxxxxgmail.com,Technion,No,Kixx,Radxxxxx,xxxxxxxxxxxy@gmail.com,,No,Shxxx,Marxxxxxxx,xxxxxxxxxxxxchnion.ac.il,Technion,No,Ikxxx,Yaxxxx,xxxxxxxusia.jp,Studio Ousia,No,Omxx,Lexx,xxxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,Noxx,Coxxx,Technion,,,,,,xxxxxxxxgmail.com,,,,,Israel,,Yoxxx Esxxx;Noxx Coxxx;Kixx Radxxxxx;Shxxx Marxxxxxxx;Ikxxx Yaxxxx;Omxx Lexx,xxxxxxxxxxy@gmail.com;xxxxxxxxxgmail.com;xxxxxxxxxxxky@gmail.com;xxxxxxxxxxxxechnion.ac.il;xxxxxxxousia.jp;xxxxxxxxxxxxxwashington.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
309,309X-F8H3J2E2B6,A Factored Neural Network Model for Characterizing Online Discussions in Vector Space,Hxx Chxxx;Hxx Faxx and Maxx Ostxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We develop a novel factored neural model
that learns multi-sentence embeddings in
an unsupervised setting. The model cap-
tures latent global context, local content,
and response characteristics of comments
intree-structureddiscussions. With alarge
collection of topic-varying Reddit discus-
sions, we observe that comment embed-
dings learned from the model bring con-
sistent improvement in a community en-
dorsement prediction task. Our qualitative
study shows that the model learns latent
modes of style and topic, as well as dimen-
sions related to response patterns.",7 Feb 2017 07:59:12 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  NLP applications;  distributional similarity;  text classification;  NLP in social networking media,Hxx,Chxxx,xxxxxxxo@uw.edu,University of Washington,No,Hxx,Faxx,xxxxxxuw.edu,University of Washington,No,Maxx,Ostxxxxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hxx,Chxxx,University of Washington,,,,,,xxxxxxxo@uw.edu,,Seattle,WA,,United States,,Hxx Chxxx;Hxx Faxx;Maxx Ostxxxxxx,xxxxxxxo@uw.edu;xxxxxx@uw.edu;xxxxxxxxxxxxxashington.edu,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
310,310X-B6A3E3C6C3,Cross-language Article Linking using Cross-Encyclopedia Entity Embedding,Chuxxxxx Wx and Ricxxxx Tzoxxxxxx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"Cross-language article linking (CLAL) is the task of finding corresponding
article pairs of different languages across encyclopedias.
This task is a difficult disambiguation problem that one article must be
selected among several candidate articles with similar titles and contents.
Existing works focus on engineering text-based or link-based features for this
task, which is time-consuming job and some are only applicable within the same
encyclopedia. In this paper, we address these problems by proposing
cross-encyclopedia entity embedding. Unlike other works, the proposed
method does not rely on known cross-language pairs. We apply the proposed
method to CLAL between English Wikipedia and Chinese Baidu Baike. Our
features improve performance relative to the baseline by 2.76% more than the
current best system, a statistically significant gain.",7 Feb 2017 04:00:16 GMT,Applications/Tools,Multilinguality,cross-language information retrieval;  multilingual applications;  entity disambiguation;  multilingual resources;  NLP on Wikipedia and other collaboratively constructed resources,Chuxxxxx,Wx,xxxxxxxxxxxxx02.nthu.edu.tw,"Institute of Information Systems and Applications, National Tsing Hua University.",No,Richarxxxxxxxxxxx,Tsxx,xxxxxxxxxxx.ncu.edu.tw,National Central University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chuxxxxx,Wx,"Department of Computer Science, National Tsing Hua University.",,,,,,xxxxxxxxxxxxx06.nthu.edu.tw,,,,,Taiwan,,Chuxxxxx Wx;Ricxxxx Tzoxxxxxx,xxxxxxxxxxxxx02.nthu.edu.tw;xxxxxxxxxxxe.ncu.edu.tw,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
311,311X-J8J2C6B6C5,Automatic Generation of Related Work through Summarizing Citations,Jinxxxxxx Chxx and Hxx Zhxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Automatically generating the related work section for a writing paper is useful
for researchers as it can save a lot of time and avoid missing related works.
The related work section of scientific paper usually introduces other
researchers’ work published before and makes comparisons with the current
author’s work. This paper proposes an approach to automatically generating
related work by comparing the main text of the writing paper with the citations
to the references from other papers. Our ap-proach firstly collects the papers
that cite the reference papers of the writing paper and extracts the
corresponding citation sentences to form a citation document. It then makes a
summarization that makes a comparison between the citation document and the
writing paper’s abstract, introduction and conclusion. It extracts the
representative keywords from the citation document and the writing paper, and
constructs a graph of the keywords. The discriminated nodes in the graph are
figured out, and then the minimum Steiner tree that covers the nodes is
extracted. A summary is generated by extracting the sentences covering the
Steiner tree. The experiments show that our approach outperforms three
baselines MEAD, ReWoS and ARWG according to ROUGE evaluations.",6 Feb 2017 08:45:37 GMT,Theoretical,Summarization,document summarization;  multi-document summarization,Jinxxxxxx,Chxx,xxxxxxxxt.edu.cn,,No,Hxx,Zhxxx,xxxxxxxct.ac.cn,"Insititute of Computing Technology, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jinxxxxxx,Chxx,Nanjing University of Posts and Telecommunications,,,,,,xxxxxxxxt.edu.cn,,,,,China,,Jinxxxxxx Chxx;Hxx Zhxxx,xxxxxxxxt.edu.cn;xxxxxxxxct.ac.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
312,312X-F3J4D3E3E6,"Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs",Juxxxx Cxx;Shxxx Huxxx;Wexxxx Sxx and Xiaxxxx Wx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Poster Tuesday,,Undecided (Tagging Chunking Syntax Parsing),"We study the Maximum Subgraph problem in deep dependency parsing. We consider
two restrictions to deep dependency graphs: (a) 1-endpoint-crossing and (b)
pagenumber-2. Our main contribution is an exact algorithm that ob-
tains maximum subgraphs satisfying both restrictions simultaneously in time
O(n5).
Moreover, ignoring one linguistically-rare structure descreases the complexity
to
O(n4). We
also extend our quartic-time algorithm into
a practical parser with a discriminative disambiguation model and evaluate its
performance on four linguistic data sets used in
semantic dependency parsing.",22 Apr 2017 14:38:51 GMT,Theoretical,"Tagging, chunking, syntax, and parsing",,Juxxxx,Cxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,Shxxx,Huxxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,Wexxxx,Sxx,xxxxxx.edu.cn,Peking University,No,Xiaxxxx,Wxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wexxxx,Sxx,Peking University,,,8601xxxxxxxxx,,,xxxxxx.edu.cn,,,,,China,,Juxxxx Cxx;Shxxx Huxxx;Wexxxx Sxx;Xiaxxxx Wxx,xxxxxxxxxx@pku.edu.cn;xxxxxxxxxxx@pku.edu.cn;xxxxxxx.edu.cn;xxxxxxxxxxx@pku.edu.cn,"Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs","Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs",11,Weiwei Sun,,"Institute of Computer Science and Technology, Peking University. Zhongguancun North Street 128, Haidian District, Beijing, China.",on,,No. Do not include my submission in this dataset.,No,None,None
313,313X-A8C7J8P4E9,Identifying Semantic Edit Intentions from Revisions in Wikipedia,Dixx Yaxx;Aaxxx Halxxxxx;Roxxxx Krxxx and Edxxxx Hxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Extracting the intention behind changes to documents can bring deep insights to
collaborative writing processes. In this work, we developed in collaboration
with Wikipedia editors a 13-category taxonomy of the semantic intention behind
edits to Wikipedia articles.  Using labeled article edits, we build a
computational model that achieved a micro-averaged F1 score of 0.621 across
edit intentions. We then used this model to investigate how different types of
edits predict the retention of newcomers and the changes in the quality of
articles, two key concerns for English Wikipedia today. Our analysis shows that
the types of edits that users make in their first session predict their
subsequent survival as Wikipedia editors. In addition, edit intention predicts
improvements in article quality, and articles seem to need different types of
edits depending upon their initial quality.",7 Feb 2017 09:35:26 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",NLP applications;  NLP on noisy unstructured text;  text classification;  NLP on Wikipedia and other collaboratively constructed resources;  document mining,Dixx,Yaxx,xxxxxxxx.cmu.edu,Carnegie Mellon University,No,Aaxxx,Halxxxxx,xxxxxxxxxxxikimedia.org,Wikimedia Foundation,No,Roxxxx,Krxxx,xxxxxxxxxxxt@cs.cmu.edu,Carnegie Mellon University,No,Edxxxx,Hoxx,xxxxxxmu.edu,CMU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dixx,Yaxx,Carnegie Mellon University,,,,,,xxxxxxxx.cmu.edu,,,AL,,United States,,Dixx Yaxx;Aaxxx Halxxxxx;Roxxxx Krxxx;Edxxxx Hoxx,xxxxxxxx.cmu.edu;xxxxxxxxxxxxikimedia.org;xxxxxxxxxxxxt@cs.cmu.edu;xxxxxxcmu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
315,315X-F2A4C5J5P4,A Neural Transition-Based Chinese Semantic Dependency Graph Parser,Yuxxxx Waxx;Wanxxxxx Cxx;Jixxx Gxx and Tixx Lx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Chinese semantic dependency graph has been recently proposed as an extension of
semantic dependency tree, which captures richer latent semantics of a sentence.
An intuitive approach to obtain graph structured output is to utilize a
conventional dependency tree parser to produce trees first, following with a
classification procedure to recover additional dependency arcs. This two-stage
approach requires manually designed linguistic rules and typically suffers from
error propagation. To overcome these problems, we propose a one-stage
transition-based system for dependency graph parsing, which parses graph
directly. Technically, we introduce two LSTM-based modules, namely Bi-LSTM
Subtraction and Incremental Tree-LSTM, to better represent each transition
configuration. Experiments on SemEval-2016 Task 9 dataset show that our parser
achieves the state-of-the-art performance.",7 Feb 2017 11:59:45 GMT,Empirical/Data-Driven,Semantics,semantic relations;  parsing,Yuxxxx,Waxx,xxxxxxxxxxhit.edu.cn,Harbin Institute of Technology,No,Wanxxxxx,Cxx,xxxxxxxxxgmail.com,Harbin Institute of Technology,No,Jixxx,Gxx,xxxxxxxxxxx24@gmail.com,Harbin Institute of Technology,No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxxx,Waxx,Harbin Institute of Technology,,,,,,xxxxxxxxxxhit.edu.cn,,,,,China,,Yuxxxx Waxx;Wanxxxxx Cxx;Jixxx Gxx;Tixx Lxx,xxxxxxxxxxhit.edu.cn;xxxxxxxxx@gmail.com;xxxxxxxxxxxx24@gmail.com;xxxxxxxxxip.126.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
316,316X-J8J8F9G5P8,Modeling Contexts for Entity Disambiguation with Type-gating Attentions,Fexx Nxx;Yuxxx Cxx;Xiaxxxxxx Huxxx;Chixxxxx Lxx and Roxx Px,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we propose a type-gating attention model for entity
disambiguation, which integrates a joint attention mechanism and a type-gating
mechanism into a Neural-Network-based framework. The first mechanism is
designed to identify most discriminative words from mention contexts and most
relevant sentences from corresponding entity descriptions. And the second
mechanism is designed to take the advantage of an observation that descriptive
words of an entity can be used to enrich the description of another entity of
same type. Our evaluation shows that the proposed model outperforms the
state-of-the-arts on three public datasets. The further study also confirms
that both the joint attention mechanism and the type-gating mechanism are
effective.",7 Feb 2017 12:01:49 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",entity disambiguation,Fexx,Nxx,xxxxxxxxxxe@gmail.com,Sun-yat-sen University,No,Yuxxx,Cxx,xxxxxxxxxxxicrosoft.com,Microsoft Research Asia,No,Xiaxxxxxx,Huxxx,xxxxxxxxxxcrosoft.com,Microsoft,No,Chixxxxx,Lxx,xxxxxxxxosoft.com,Microsoft Research,No,Roxx,Pxx,xxxxxxxxu.edu.cn,Sun-Yat-Sen University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Fexx,Nxx,Sun Yat-Sen University,,,,,,xxxxxxxxxxu@gmail.com,,,,,China,,Fexx Nxx;Yuxxx Cxx;Xiaxxxxxx Huxxx;Chixxxxx Lxx;Roxx Pxx,xxxxxxxxxxe@gmail.com;xxxxxxxxxxxxicrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxosoft.com;xxxxxxxxsu.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
317,317X-C4A3A5F5A5,Fine-Grained Neural Entity Typing with Knowledge Attention,Jx Xxx;Yaxxxx Lxx;Zhixxxx Lxx and Maoxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Entity typing aims to identify the semantic type of an entity in a specific
plain text, which is important for named entity disambiguation and benefits
lots of NLP applications such as question answering. Most existing methods,
including recent state-of-the-art neural models, typically classify entity
types according to their sentences. These methods, however, fail to model
complicated correlations between entities and context, and also neglect rich
background information in knowledge graphs (KGs). To address these issues, we
take information from KG into consideration, and propose Neural Entity Typing
(NET) with knowledge attention. We conduct experiments on a real-world dataset,
and the results demonstrate that our model significantly outperforms other
state-of-the-art methods, and case studies further demonstrate the
effectiveness of incorporating KG information into the NET model.",7 Feb 2017 12:38:41 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  mention detection;  named entity disambiguation;  entity disambiguation;  NLP on Wikipedia and other collaboratively constructed resources;  semantic knowledge induction,Jx,Xxx,xxxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Yaxxxx,Lxx,xxxxxxxxxxxxxxtsinghua.edu.cn,Tsinghua University,No,Zhixxxx,Lxx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jx,Xxx,Tsinghua University,,,,,,xxxxxxxxxxxxxxsinghua.edu.cn,,,,,China,,Jx Xxx;Yaxxxx Lxx;Zhixxxx Lxx;Maoxxxx Sxx,xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxxxxxxtsinghua.edu.cn;xxxxxxxxxxxghua.edu.cn;xxxxxxxxxxhua.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
318,318X-E5H6A5P8A3,Improved Word Representation Learning with Sememes,Yixxx Nxx;Ruoxxxx Xxx;Zhixxxx Lxx and Maoxxxx Sx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"Sememes are minimum semantic units of word meanings, and the meaning of each
word sense is typically composed by several sememes. Since sememes are not
explicit for each word, people manually annotate word sememes and form
linguistic common-sense knowledge bases. In this paper, we present that, word
sememe information can improve word representation learning (WRL), which maps
words into a low-dimensional semantic space and serves as a fundamental step
for many NLP tasks. The key idea is to utilize word sememes to capture exact
meanings of a word within specific contexts accurately. More specifically, we
follow the framework of Skip-gram and present three sememe-encoded models to
learn representations of sememes, senses and words, where we apply the
attention scheme to detect word senses in various contexts. We conduct
experiments on two tasks including word similarity and word analogy, and our
models significantly outperform baselines. The results indicate that WRL can
benefit from sememes via the attention scheme, and also confirm our models
being capable of correctly modeling sememe information.",20 Apr 2017 06:46:07 GMT,Empirical/Data-Driven,Semantics,,Yixxx,Nxx,xxxxxxxxxgmail.com,Tsinghua University,No,Ruoxxxx,Xxx,xxxxxxxxxg@163.com,Tsinghua University,No,Zhixxxx,Lxx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yixxx,Nxx,Tsinghua University,,,,,,xxxxxxxxxgmail.com,,,,,China,,Yixxx Nxx;Ruoxxxx Xxx;Zhixxxx Lxx;Maoxxxx Sxx,xxxxxxxxxgmail.com;xxxxxxxxxng@163.com;xxxxxxxxxxxghua.edu.cn;xxxxxxxxxxhua.edu.cn,Improved Word Representation Learning with Sememes,Improved Word Representation Learning with Sememes,10,Yilin Niu,,"Tsinghua University
Tsinghua University, Haidian District, Beijing, 100084, China",on,on,Only include my submission if it is accepted.,No,None,None
319,319X-D5B2J2G8B5,Learning to Generate Market Comments from Stock Prices,Soixxxxx Murxxxxx;Akixxxx Watxxxxx;Akxxx Miyxxxxx;Keixxxx Gosxxxx;Tosxxxxxx Yaxxxx;Hixxxx Takxxxxx and Yuxxxx Mixx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Poster Monday,,Undecided (Generation Summarization),"This paper presents a novel encoder-decoder model for automatically generating
market comments from stock prices. The model first encodes both short- and
long-term series of stock prices so that it can mention short- and long-term
changes in stock prices. In the decoding phase, our model can also generate a
numerical value by selecting an appropriate arithmetic operation such as
subtraction or rounding, and applying it to the input stock prices. Empirical
experiments show that our best model generates market comments at the fluency
and the informativeness approaching human-generated reference texts.",22 Apr 2017 06:30:43 GMT,Empirical/Data-Driven,Generation,,Soixxxxx,Murxxxxx,xxxxxxxxxxxxxi.titech.ac.jp,Tokyo Institute of Technology,No,Akixxxx,Watxxxxx,xxxxxxxxxxxxxi.titech.ac.jp,Tokyo Institute of Technology,No,Akxxx,Miyxxxxx,xxxxxxxxxx@nii.ac.jp,the Graduate University for Advanced Studies,No,Keixxxx,Gosxxxx,xxxxxxxxxxxxxxxx.dis.titech.ac.jp,Tokyo Institute of Technology,No,Tosxxxxxx,Yaxxxx,xxxxxxxxxxxxxxxe.gm@hitachi.com,"Research & Development Group, Hitachi, Ltd.",No,Hixxxx,Takxxxxx,xxxxxxxxxxxxtitech.ac.jp,Tokyo Institute of Technology,No,Yuxxxx,Mixxx,xxxxxxxxii.ac.jp,National Instutite of Informatics,No,,,,,,,,,,,,,,,,,Soixxxxx,Murxxxxx,NTT docomo,,,,,,xxxxxxxxxxxxxi.titech.ac.jp,,,,,Japan,,Soixxxxx Murxxxxx;Akixxxx Watxxxxx;Akxxx Miyxxxxx;Keixxxx Gosxxxx;Tosxxxxxx Yaxxxx;Hixxxx Takxxxxx;Yuxxxx Mixxx,xxxxxxxxxxxxxi.titech.ac.jp;xxxxxxxxxxxxxxi.titech.ac.jp;xxxxxxxxxxa@nii.ac.jp;xxxxxxxxxxxxxxxxx.dis.titech.ac.jp;xxxxxxxxxxxxxxxxe.gm@hitachi.com;xxxxxxxxxxxx.titech.ac.jp;xxxxxxxxnii.ac.jp,Learning to Generate Market Comments from Stock Prices,Learning to Generate Market Comments from Stock Prices,11,Soichiro Murakami,,"Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama, JAPAN, 226-8503",on,,No. Do not include my submission in this dataset.,No,None,None
320,320X-G3F4A9B3H5,Word Embedding Clustering for Taxonomy Construction,Axx Tuxx;Sxx Chxxxx and Sxx Kixxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Taxonomy plays an important role in many applications by organizing domain
knowledge into a hierarchy of ""is-a"" relations between terms. In this paper, we
present a novel approach to construct taxonomies based on word embedding
clustering, using the following three word embedding measures: semantic
clusters, taxonomic centroids, and relative distances from root, to identify
the semantic relationships between terms and their hypernyms. The three
measures were inspired by our empirical observations of the word embeddings of
terms in WordNet taxonomies.  The experimental results show that our proposed
approach significantly outperformed the state-of-the-art methods in terms of
Recall and F-measure.",7 Feb 2017 01:40:47 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  semantic relations;  relation/event extraction,Anhxxxxx,Lxx,xxxxxxxxxxxx-star.edu.sg,,No,Siuxxxxxxx,Hxx,xxxxxxxxxtu.edu.sg,NTU,No,Seexxxxxx,Nx,xxxxxxxxxnus.edu.sg,NUS,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anhxxxxx,Lxx,Institute for Infocomm Research,,,,,,xxxxxxxxxxxx-star.edu.sg,,,,,Singapore,,Axx Tuxx;Sxx Chxxxx;Sxx Kixxx,xxxxxxxxxxxx-star.edu.sg;xxxxxxxxxntu.edu.sg;xxxxxxxxxxnus.edu.sg,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
321,321X-H3B2G6E8J6,A Hierarchy-to-Sequence Attentional Neural Machine Translation Model,jinxxxx sx;Jixxx Zexx;Dexx Xixxx;Yaxx Lxx and Yonxxxxx Yx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Although end-to-end attentional neural machine translation (NMT) has achieved
great progress recently, it still suffers from performance degradation when
translating long sentences. In this paper, partially inspired by the idea of
segmenting a long sentence into short clauses, each of which can be easily
translated by NMT, we propose a hierarchy-to-sequence attentional neural
machine translation model. Our encoder takes the segmented clause sequence as
input and arranges words, clauses and sentences in a hierarchical structure,
with two layers of recurrent neural networks modeling semantic compositionality
at the word and clause levels. Correspondingly, our decoder sequentially
translates clauses and simultaneously applies two types of attention models to
respectively capture contexts of inter-clause and intra-clause for translation
prediction. In doing so, our model not only reduces the difficulties of
parameter learning when translating long sentences, but also better
distinguishes and explores the effects of different scopes of contexts.
Experimental results on Chinese-English translation demonstrate the
superiorities of the proposed model over the conventional NMT model.",7 Feb 2017 07:10:45 GMT,Empirical/Data-Driven,Machine translation,Hierarchical SMT,jinxxxx,sx,xxxxxxxu.edu.cn,Xiamen university,No,Jixxx,Zexx,xxxxxxxxxxxmu.edu.cn,Xiamen university,No,Dexx,Xixxx,xxxxxxxxxx@gmail.com,Soochow University,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Yonxxxxx,Yxx,xxxxxxxxxxxu.xmu.edu.cn,Xiamen University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,jinxxxx,sx,Xiamen university,,,,,,xxxxxxxu.edu.cn,,Xiamen,,,China,,jinxxxx sx;Jixxx Zexx;Dexx Xixxx;Yaxx Lxx;Yonxxxxx Yxx,xxxxxxxu.edu.cn;xxxxxxxxxx.xmu.edu.cn;xxxxxxxxxxg@gmail.com;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxxxu.xmu.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
322,322X-A9P6B7D9A5,Event Ordering with a Generalized Model for Sieve Prediction Ranking,Bixx McDxxxxx;Natxxxxxx Chaxxxxx; Axxx and ex Oroxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper improves on several aspects of
a sieve-based event ordering architecture,
CAEVO (Chambers et al., 2014), which
creates globally consistent temporal relations
between events and time expressions.
First, we examine the usage of word embeddings
and semantic role features. With
the incorporation of these new features, we
demonstrate a 5% relative F1 gain over our
replicated version of CAEVO. Second, we
reframe the architecture’s sieve-based inference
algorithm as a prediction reranking
method that approximately optimizes a
scoring function computed using classifier
precisions. Within this prediction reranking
framework, we propose an alternative
scoring function, showing an 8.8% relative
gain over the original CAEVO. Further
analysis of these methods on alternative
splits of the data suggests that they have a
tendency to overfit on our relatively small
corpus, but the high performance gains of
an oracle prediction reranker suggest that
there is significant room for improvement
using alternative reranking methods.",6 Feb 2017 09:44:26 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  experimental evaluation/comparison of ML methods;  temporal/spatial information extraction,Bixx,McDxxxxx,xxxxxxxxxx@gmail.com,The Pennsylvania State University,No,Natxxxxxx,Chaxxxxx,xxxxxxxx@usna.edu,US Naval Academy,No,Alexxxxxx,Oroxxxxxxx,xxxxxxxpsu.edu,The Pennsylvania State University,No,Daxxx,Reixxxx,xxxxxxx@psu.edu,Penn State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bixx,McDxxxxx,The Pennsylvania State University,,,,,,xxxxxxxxxx@gmail.com,,,,,United States,,Bixx McDxxxxx;Natxxxxxx Chaxxxxx;Alexxxxxx Oroxxxx;Daxxx Reixxxx,xxxxxxxxxx@gmail.com;xxxxxxxxx@usna.edu;xxxxxxx@psu.edu;xxxxxxxx@psu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
323,323X-H3A4J7H6J3,A Neural Local Coherence Model,Dxx Tixx and Shxxxx Joxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Poster Monday,,Undecided (Discourse Pragmatics),"We propose a local coherence model based on a convolutional neural network that
operates over the entity grid representation of a text. The model captures long
range en- tity transitions along with entity-specific features without loosing
generalization, thanks to the power of distributed representation. We present a
pairwise ranking method to train the model in an end-to-end fashion on a task
and learn task-specific high level features. Our evaluation on three different
coherence assessment tasks demonstrates that our model achieves state of the
art results outperforming existing models by a good margin.",22 Apr 2017 22:10:12 GMT,Empirical/Data-Driven,Discourse and pragmatics,,Dxx,Tienxxxxxxx,xxxxxxxxen@uva.nl,University of Amsterdam,No,Shxxxx,Joxx,xxxxxxxxku.edu.qa,Qatar Computing Research Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dxx,Tienxxxxxxx,University of Amsterdam,,,8494xxxxxxx,,,xxxxxxxxen@uva.nl,,Doha,Doha,,Netherlands,,Dxx Tixx;Shxxxx Joxx,xxxxxxxxen@uva.nl;xxxxxxxxxku.edu.qa,A Neural Local Coherence Model,A Neural Local Coherence Model,11,Dat,,"Informatics Institute, University of Amsterdam",on,on,Only include my submission if it is accepted.,No,None,None
325,325X-D2E3F2C4B4,Leveraging Auxiliary Tasks for Document-Level Cross-Domain Sentiment Classification,Jiaxxxx Yx and Jixx Jixxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In this paper, we study domain adaptation with a state-of-the-art hierarchical
neural network for document-level sentiment classification. We first designed a
new auxiliary task based on sentiment scores of domain-independent words,
followed by proposing two neural network architectures to respectively induce
document embeddings and sentence embeddings that work well for different
domains. When these document and sentence embeddings are used for sentiment
classification, we find that with both pseudo and external sentiment lexicons,
our proposed methods can perform similar to or better than several highly
competitive domain adaptation methods on a benchmark dataset of product
reviews.",7 Feb 2017 02:06:50 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  domain adaptation,Jiaxxxx,Yx,xxxxxxxxxxxxxis.smu.edu.sg,Singapore Management University,No,Jixx,Jixxx,xxxxxxxxxxsmu.edu.sg,Singapore Management University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jiaxxxx,Yx,Singapore Management University,,,658xxxxxxx,,,xxxxxxxxxxxxxis.smu.edu.sg,,Singapore,,,Singapore,,Jiaxxxx Yx;Jixx Jixxx,xxxxxxxxxxxxxis.smu.edu.sg;xxxxxxxxxx@smu.edu.sg,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
326,326X-P6P3G3A2P9,Adversarial Multi-Criteria Learning for Chinese Word Segmentation,Xixxxx Chxx;Zhxx Sxx;Xixxxx Qxx and Xuaxxxxx Huxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Accept - Oral Wednesday,,,"Different linguistic perspectives causes many diverse segmentation criteria for
Chinese word segmentation (CWS). Most existing methods focus on improve the
performance for each single criterion. However, it is interesting to exploit
these different criteria and mining their common underlying knowledge. In this
paper, we propose adversarial multi-criteria learning for CWS by integrating
shared knowledge from multiple heterogeneous segmentation criteria. 
Experiments on eight corpora with heterogeneous segmentation criteria show that
the performance of each corpus obtains a significant improvement, compared to
single-criterion learning. Source codes of this paper are available on Github.",22 Apr 2017 16:34:02 GMT,Applications/Tools,"Phonology, morphology, and word segmentation",,Xixxxx,Chxx,xxxxxxxxxxn@gmail.com,Fudan University,No,Zhxx,Sxx,xxxxxxxxxh@163.com,Fudan University,No,Xixxxx,Qxx,xxxxxxxxxan.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixxxx,Chxx,Fudan University,,,(+86)xxxxxxxxxxx,,,xxxxxxxxxxn@gmail.com,,Shanghai,,,China,,Xixxxx Chxx;Zhxx Sxx;Xixxxx Qxx;Xuaxxxxx Huxxx,xxxxxxxxxxn@gmail.com;xxxxxxxxxzh@163.com;xxxxxxxxxdan.edu.cn;xxxxxxxxxxudan.edu.cn,Adversarial Multi-Criteria Learning for Chinese Word Segmentation,Adversarial Multi-Criteria Learning for Chinese Word Segmentation,11,Xinchi Chen,,"Shanghai Key Laboratory of Intelligent Information Processing, Fudan University",on,on,Only include my submission if it is accepted.,No,None,None
327,327X-J3A9B2E9D2,"MAG: A Multilingual, Knowledge-Based Agnostic and Deterministic Entity Linking Approach",Dixxx Mouxxxxxxx;Ricxxxx Usxxxx;Micxxxx R�xxxx and Axelxxxxxxxx Ngxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Entity Linking (EL) has recently been the subject of a significant body of
research. Commonly, the best performing approaches rely on trained mono-lingual
models. Porting these approaches to other languages is consequently a difficult
endeavor as it demands devising corresponding training data and retraining the
models. We address this drawback by presenting a novel multilingual,
knowledgebase agnostic and deterministic approach to EL, dubbed MAG. MAG is
based on sophisticated indexing allowing for context-based retrieval on
structured Knowledge Base (KB) and graph algorithms. We evaluate MAG on 25 data
sets and 7 languages. Our results show that MAG can be used across languages
without any retraining. Our approach is able to achieve state-of-the-art
performance on the English data sets while outperforming the state of the art
on all non-English data sets.",6 Feb 2017 12:29:14 GMT,Applications/Tools,"Information extraction, text mining, and question answering",cross-lingual approaches;  cross-language information retrieval;  information extraction;  cross-language information extraction;  multilingual applications;  graph-based algorithms;  named entity disambiguation;  information retrieval;  entity disambiguation;  multilingual resources,Dixxx,Mouxxxxxxx,xxxxxxxxxxxxxxxxxxtik.uni-leipzig.de,University of Leipzig,No,Ricxxxx,Usxxxx,xxxxxxxxxxxxxxxxk.uni-leipzig.de,Leipzig University,No,Micxxxx,R�xxxx,xxxxxxxxxxxxxcher@gmail.com,Leipzig University,No,Axelxxxxxxxx,Ngonxxxxxxxx,xxxxxxxxxxxxxxxxk.uni-leipzig.de,University of Leipzig,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dixxx,Mouxxxxxxx,University of Leipzig,,,4901xxxxxxxxx,,,xxxxxxxxxxxxxxxxxxtik.uni-leipzig.de,,Leipzig,Saxony,,Germany,,Dixxx Mouxxxxxxx;Ricxxxx Usxxxx;Micxxxx R�xxxx;Axelxxxxxxxx Ngxxxx,xxxxxxxxxxxxxxxxxxtik.uni-leipzig.de;xxxxxxxxxxxxxxxxik.uni-leipzig.de;xxxxxxxxxxxxxxcher@gmail.com;xxxxxxxxxxxxxxxxik.uni-leipzig.de,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
328,328X-B5J9J6B8F7,Towards Generating Product Reviews from Aspect-Sentiment Scores,Hoxxxx Zaxx and Xiaxxxx Wxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Data-to-text generation is very essential and important in machine writing
applications. The recent deep learning models, like Recurrent Neural Networks
(RNNs), have shown a bright future for relevant text-to-text generation tasks.
However, rare work has been done in data-to-text generation tasks, especially
the generation of long reviews from user opinions. In this paper, we introduce
a deep neural network model to generate long Chinese reviews from
aspect-sentiment scores representing users’ opinions. We conduct our study
within the framework of encoder-decoder networks, and we propose a hierarchical
structure with aligned attention in the Long-Short Term Memory (LSTM) decoder.
Experiments show that our model outperforms retrieval based baseline methods,
and also beats the sequential generation models in qualitative evaluations.",6 Feb 2017 10:44:13 GMT,Empirical/Data-Driven,Generation,NLP applications;  language generation,Hoxxxx,Zaxx,xxxxxxxxku.edu.cn,Peking University,No,Xiaxxxx,Wxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Wxx,Peking University,,,,,,xxxxxxxxxx@pku.edu.cn,,,,,China,,Hoxxxx Zaxx;Xiaxxxx Wxx,xxxxxxxxku.edu.cn;xxxxxxxxxxx@pku.edu.cn,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
329,329X-G5H6C7P9B2,Deep vs. Shallow: Designing Language Independent Part-of-Speech Taggers,Toxxxx Horxxxxx and Torxxxx Zexxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"We present an evaluation of CRF and LSTM PoS tagger architectures in order to
find a general valid architecture to train models for any language.
We determine a minimalistic feature set for CRF taggers which uses word
distributional information as well as word and character ngrams, and compare
this setup to various LSTM tagger architectures.
We find that LSTMs are only superior to CRF and off-the-shelf taggers when
training and testing on data of the same corpus.
Furthermore, we find that CRF and off-the-shelf taggers are usually a magnitude
faster than the most accurate LSTM taggers.
Language-fitted taggers remain necessary when training models of corpora with
extremely fine tagsets.",10 Feb 2017 05:31:25 GMT,Resources/Evaluation,"Tagging, chunking, syntax, and parsing",part-of-speech tagging,Toxxxx,Horxxxxx,xxxxxxxxxxxxxnn@uni-due.de,"Language Technology Lab, University of Duisburg-Essen",No,Torxxxx,Zexxx,xxxxxxxxxxxxh@uni-due.de,"Language Technology Lab, University of Duisburg-Essen",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Toxxxx,Horxxxxx,"Language Technology Lab, University of Duisburg-Essen",,,,,,xxxxxxxxxxxxxnn@uni-due.de,,,,,Germany,,Toxxxx Horxxxxx;Torxxxx Zexxx,xxxxxxxxxxxxxnn@uni-due.de;xxxxxxxxxxxxch@uni-due.de,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
330,330X-F9B3E7B8A9,Modelling and measuring suspense in narrative,Ricxxxx Doxxx and Paxx Pixxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Most work on automatic generation of narratives, and more specifically
suspenseful narrative, has focused on detailed domain-specific modelling of
character motivation and plot structure. Recent work in computational
linguistics on automatic learning of narrative schemas suggests an alternative
approach that exploits such schemas as a starting point for measuring suspense.
The contribution of this paper is a method for modelling and automatically
measuring suspensefulness. The proposal is evaluated against ratings by human
judges.",6 Feb 2017 11:47:45 GMT,Theoretical,Discourse and pragmatics,discourse;  language generation;  pragmatics,Ricxxxx,Doxxx,xxxxxxxxxxxst@gmail.com,Independent researcher,No,Paxx,Pixxx,xxxxxxxxxx@open.ac.uk,The Open University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Paxx,Pixxx,The Open University,,,,,,xxxxxxxxxx@open.ac.uk,,,,,United Kingdom,,Ricxxxx Doxxx;Paxx Pixxx,xxxxxxxxxxxst@gmail.com;xxxxxxxxxxx@open.ac.uk,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
331,331X-A4A8D6P4A6,Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps,Toxxxx Faxxx and Irxxx Gurxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Concept maps can be used to concisely represent important information and bring
structure into large document collections. Therefore, we study a variant of
multi-document summarization that produces summaries in the form of concept
maps. However, suitable evaluation datasets for this task are currently
missing. To close this gap, we present a newly created corpus of concept maps
that summarize heterogeneous collections of web documents on educational
topics. It was created using a novel crowdsourcing approach that allows us to
efficiently determine important elements in large document collections. We
release the corpus along with a baseline system and proposed evaluation
protocol to enable further research on this variant of summarization.",7 Feb 2017 08:21:42 GMT,Resources/Evaluation,Summarization,corpus development;  corpus annotation methods;  document summarization;  multi-document summarization,Toxxxx,Faxxx,xxxxxxxxxxxxxxu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,Irxxx,Gurxxxxx,xxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Toxxxx,Faxxx,"UKP Lab, Technische Universität Darmstadt",,,,,,xxxxxxxxxxxxxxu-darmstadt.de,,,,,Germany,,Toxxxx Faxxx;Irxxx Gurxxxxx,xxxxxxxxxxxxxxu-darmstadt.de;xxxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
332,332X-J3G7F6P6H7,Iterative Co-Attention for Text Comprehension,Jixx Fx;Xixxxx Qxx and Xuaxxxxx Huxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Recently, attention mechanisms play a key role in end-to-end neural text
comprehension tasks. Typically, these attention mechanisms are elaborate
tailored to capture the complicated interactions between the question and the
document. In this paper, we propose a novel dynamic memory-based co-attention
network to tackle text comprehension tasks. Unlike previous models, our
iterative co-attention mechanism can not only capture bidirectional attentions
simultaneously, but also infer the relation between query, document and answer
with multi-turn reasoning. Our model achieves state-of-the-art result on the
large text comprehension benchmarks SQuAD.",7 Feb 2017 06:57:55 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",context-aware question answering,Jixx,Fx,xxxxxxxxxan.edu.cn,Fudan University,No,Xixxxx,Qxx,xxxxxxxxxan.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixxxx,Qxx,Fudan University,,,8621xxxxxxxx,,,xxxxxxxxxan.edu.cn,,,,,China,,Jixx Fx;Xixxxx Qxx;Xuaxxxxx Huxxx,xxxxxxxxxan.edu.cn;xxxxxxxxxdan.edu.cn;xxxxxxxxxxudan.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
333,333X-D8G7C3H6E7,Selective Encoding for Abstractive Sentence Summarization,Qixxxx Zhxx;Nxx Yaxx;Fuxx Wxx and Mixx Zxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Tuesday,,Undecided (Generation Summarization),"We propose a selective encoding model to extend the sequence-to-sequence
framework for abstractive sentence summarization. It consists of a sentence
encoder, a selective gate network, and an attention equipped decoder. The
sentence encoder and decoder are built with recurrent neural networks. The
selective gate network constructs a second level sentence representation by
controlling the information flow from encoder to decoder. The second level
representation is tailored for sentence summarization task, which leads to
better performance. We evaluate our model on the English Gigaword, DUC 2004 and
MSR abstractive sentence summarization datasets. The experimental results show
that the proposed selective encoding model outperforms the state-of-the-art
baseline models.",22 Apr 2017 06:38:03 GMT,Empirical/Data-Driven,Summarization,,Qixxxx,Zhxx,xxxxxxxxmail.com,Harbin Institute of Technology,No,Nxx,Yaxx,xxxxxxxxxrosoft.com,Microsoft Research Asia,No,Fuxx,Wxx,xxxxxxxxxrosoft.com,Microsoft Research Asia,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixxxx,Zhxx,Harbin Institute of Technology,,,,,,xxxxxxxxmail.com,,Harbin,Heilongjiang,,China,,Qixxxx Zhxx;Nxx Yaxx;Fuxx Wxx;Mixx Zhxx,xxxxxxxxmail.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com,Selective Encoding for Abstractive Sentence Summarization,Selective Encoding for Abstractive Sentence Summarization,10,Qingyu Zhou,,"Harbin Institute of Technology, 92 West Dazhi Street,Nan Gang District, Harbin, China",,,Only include my submission if it is accepted.,No,None,None
334,334X-H8C2D9C7F8,Document-level Neural Machine Translation with an Inter-Sentence Gate Model,Shaxxxx Kuxxx;Dexx Xixxx;Wexxxx Lxx and Guoxxxx Zxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation systems are usually trained on a large amount of
bilingual sentence pairs and translate one sentence at a time, ignoring
inter-sentence information. This may make the translation of a sentence
ambiguous or even inconsistent with the translations of neighboring sentences.
In order to handle this issue, we propose an Inter-Sentence Gate model that
uses the same encoder to encode two adjacent sentences and controls the amount
of information flowing from the preceding sentence to the translation of the
current sentence with an inter-sentence gate. In this way, our proposed model
can capture the connection between sentences and use the captured information
to help document-level neural machine translation. On several NIST
Chinese-English translation tasks, our experiments demonstrate that the
proposed inter-sentence gate model achieves substantial improvements over the
baseline.",7 Feb 2017 07:12:05 GMT,Empirical/Data-Driven,Machine translation,MT quality control,Shaxxxx,Kuxxx,xxxxxxxxxx405@qq.com,Soochow University,No,Dexx,Xixxx,xxxxxxxxxx@gmail.com,Soochow University,No,Wexxxx,Lxx,xxxxxxxxxxxxxxlibaba-inc.com,Alibaba Group,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shaxxxx,Kuxxx,Soochow University,,,,,,xxxxxxxxxx405@qq.com,,,,,China,,Shaxxxx Kuxxx;Dexx Xixxx;Wexxxx Lxx;Guoxxxx Zhxx,xxxxxxxxxx405@qq.com;xxxxxxxxxxg@gmail.com;xxxxxxxxxxxxxxalibaba-inc.com;xxxxxxxxxuda.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
335,335X-P5E5C6D6A3,Gated Self-Matching Networks for Reading Comprehension and Question Answering,Wexxxx Waxx;Nxx Yaxx;Fuxx Wxx;Baxxxx Chxxx and Mixx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"In this paper, we present the gated self-matching networks for reading
comprehension style question answering, which aims to answer questions from a
given passage. We first match the question and passage with gated
attention-based recurrent networks to obtain the question-aware passage
representation. Then we propose a self-matching attention mechanism to refine
the representation by matching the passage against itself, which effectively
encodes information from the whole passage. We finally employ the pointer
networks to locate the positions of answers from the passages. We conduct
extensive experiments on the SQuAD dataset. The single model achieves 71.3% on
the evaluation metrics of exact match on the hidden test set, while the
ensemble model further boosts the results to 75.9%. At the time of submission
of the paper, our model holds the first place on the SQuAD leaderboard for both
single and ensemble model.",22 Apr 2017 04:37:14 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Wexxxx,Waxx,xxxxxxxxxx@pku.edu.cn,"Institute of Computational Linguistics Dept of Computer Science & Technology, Peking University",No,Nxx,Yaxx,xxxxxxxxxrosoft.com,Microsoft Research Asia,No,Fuxx,Wxx,xxxxxxxxxrosoft.com,Microsoft Research Asia,No,Baxxxx,Chxxx,xxxxxxxu.edu.cn,Peking University,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Wexxxx,Waxx,"Institute of Computational Linguistics Dept of Computer Science & Technology, Peking University",,,,,,xxxxxxxxxx@pku.edu.cn,,,,,China,,Wexxxx Waxx;Nxx Yaxx;Fuxx Wxx;Baxxxx Chxxx;Mixx Zhxx,xxxxxxxxxx@pku.edu.cn;xxxxxxxxxxrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxu.edu.cn;xxxxxxxxxxxicrosoft.com,Gated Self-Matching Networks for Reading Comprehension and Question Answering,Gated Self-Matching Networks for Reading Comprehension and Question Answering,10,Wenhui Wang,,"Key Laboratory of Computational Linguistics, Peking University, MOE, China",,,Only include my submission if it is accepted.,No,None,None
336,336X-A9H7C3B8B9,Neural Joint Model for Transition-based Chinese Syntactic Analysis,Shxxxx Kuxxxx;Daixxxx Kawxxxxx and Saxxx Kurxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Wednesday,,Undecided (Tagging Chunking Syntax Parsing),"We present neural network-based joint models for Chinese word segmentation, POS
tagging and dependency parsing. Our models are the first neural approaches for
fully joint Chinese analysis that is known to prevent the error propagation
problem of pipeline models. Although word embeddings play a key role in
dependency parsing, they cannot be applied directly to the joint task in the
previous work. To address this problem, we propose embeddings of character
strings, in addition to words. Experiments show that our models outperform
existing systems in Chinese word segmentation and POS tagging, and perform
preferable accuracies in dependency parsing. We also explore bi-LSTM models
with fewer features.",23 Apr 2017 05:46:33 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Shxxxx,Kuxxxx,xxxxxxxxxxxxxxxi.kyoto-u.ac.jp,Kyoto University,No,Daixxxx,Kawxxxxx,xxxxxxxxxo-u.ac.jp,Kyoto University,No,Saxxx,Kurxxxxxx,xxxxxxxxxxto-u.ac.jp,Kyoto University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Kuxxxx,Kyoto University,,,,,,xxxxxxxxxxxxxxxi.kyoto-u.ac.jp,,,,,Japan,,Shxxxx Kuxxxx;Daixxxx Kawxxxxx;Saxxx Kurxxxxxx,xxxxxxxxxxxxxxxi.kyoto-u.ac.jp;xxxxxxxxxto-u.ac.jp;xxxxxxxxxxoto-u.ac.jp,Neural Joint Model for Transition-based Chinese Syntactic Analysis,Neural Joint Model for Transition-based Chinese Syntactic Analysis,11,Shuhei Kurita,,Kyoto University,on,,No. Do not include my submission in this dataset.,No,None,None
337,337X-H3P6C8E4D6,Coreference Resolution for Swedish and German using Distant Supervision,Alxx and ex Waxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Coreference resolution is the identification of phrases that refer to the same
entity in a text. Current techniques to solve coreferences use machine-learning
algorithms, which require large annotated data sets. Such annotated resources
are not available for most languages today.
In this paper, we describe a method for solving coreferences for Swedish and
German using distant supervision that does not use manually annotated texts.

We generate a weakly labelled training set using parallel corpora,
English-Swedish and English-German, where we solve the coreference for English
using CoreNLP and transfer it to Swedish and German using word alignments. To
carry this out, we identify mentions from dependency graphs in both target
languages using hand-written rules.
Finally, we evaluate the end-to-end results using the evaluation script from
the CoNLL 2012 shared task for which we obtain a score of 34.98 for Swedish and
13.16 for German and, respectively, 46.73 and 36.98 using gold mentions.",6 Feb 2017 13:07:23 GMT,Empirical/Data-Driven,Semantics,coreference resolution,Alexxxxxx,Waxxxx,xxxxxxxxxxxxxxxndevelopment.se,Lund University,No,Pixxxx,Nuxxxx,xxxxxxxxxxxes@cs.lth.se,Lund University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Pixxxx,Nuxxxx,Lund University,,,,,,xxxxxxxxxxxes@cs.lth.se,,,,,Sweden,,Alexxxxxx Waxxxx;Pixxxx Nuxxxx,xxxxxxxxxxxxxxxndevelopment.se;xxxxxxxxxxxxes@cs.lth.se,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
338,338X-P8B4G6H7C2,Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,Xuexxxx Waxx;Kaxx Lxx and Jxx Zxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Oral Monday,,Undecided (Sentiment Analysis Opinion Mining),"Solving cold-start problem in review spam detection is an urgent and
significant task.
It can help the on-line review websites to relieve the damage of spammers in
time, but has never been investigated by previous work.
This paper proposes a novel neural network model to detect review spam for
cold-start problem, by learning to represent the new reviewers' review with
jointly embedded textual and behavioral information.
Experimental results prove the proposed model achieves an effective performance
and possesses preferable domain-adaptability.
It is also applicable to a large scale dataset in an unsupervised way.",21 Apr 2017 18:35:38 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Xuexxxx,Waxx,xxxxxxxxxxr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Kaxx,Lxx,xxxxxxxxx.ia.ac.cn,Chinese Academy of Sciences,No,Jxx,Zhxx,xxxxxxxxx3@163.com,"NLPR, Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xuexxxx,Waxx,"Institute of Automation, Chinese Academy of Sciences",,,+86 1xxxxxxxxxx,,,xxxxxxxxxxr.ia.ac.cn,,Beijing,Beijing,,China,,Xuexxxx Waxx;Kaxx Lxx;Jxx Zhxx,xxxxxxxxxxr.ia.ac.cn;xxxxxxxxxr.ia.ac.cn;xxxxxxxxx23@163.com,Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,11,Xuepeng Wang,,"Institute of Automation,
Chinese Academy of Sciences",on,on,Only include my submission if it is accepted.,No,None,None
339,339X-B8H8D6G4E7,Selective Attention Memory Network for Chinese Implicit Relation Recognition,yaxx lxx and Jixxxx Zhxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Recently, Chinese implicit discourse relation recognition has drawn more and
more attention, since it is a huge challenge and requires the deep
understanding of language. In this paper, we propose a novel memory augmented
model that can not only capture the semantics of the arguments in logic order
but also preserve the crucial information in the memory network. Furthermore,
we design a selective attention mechanism which enables our model to directly
benefit from additional resources (e.g. explicit relation samples) and
alleviates the shortage of labeled implicit data. Extensive experiments
demonstrate that our proposed model can achieve the new state-of-the-art result
on Chinese Discourse Treebank.",7 Feb 2017 00:38:32 GMT,Survey Papers,Discourse and pragmatics,discourse,yaxx,lxx,xxxxxxxxxxxxxnlpr.ia.ac.cn,"NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China",No,Jixxxx,Zhxxx,xxxxxxxxxxng@ia.ac.cn,Institute of Automation Chinese Academy of Sciences,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,yaxx,lxx,"NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China",,,,,,xxxxxxxxxxxxxnlpr.ia.ac.cn,,,,,China,,yaxx lxx;Jixxxx Zhxxx,xxxxxxxxxxxxxnlpr.ia.ac.cn;xxxxxxxxxxxng@ia.ac.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
340,340X-D6B2B9H2G8,A Deep Convolutional Neural Model for Character-Based Chinese Word Segmentation,Zhixxxx Xxx and Junxxxx Hx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"In this paper, we propose a deep convolutional neural model for character-based
Chinese word segmentation. It first constructs position embeddings to encode
unigram and bigram features that are directly related to single positions in
input sentence, and then adaptively builds up hierarchical position
representations with a deep convolutional net.                          In addition,
a
multi-task
learning strategy is used to enhance this deep neural model by treating
multiple supervised CWS datasets as different tasks. Experimental results have
shown that our neural model outperforms the existing neural ones, and the model
equipped with multi-task learning has successfully achieved state-of-the-art
F-score performance for standard benchmarks: 0.964 on PKU dataset and 0.978 on
MSR dataset. To the best of our knowledge, this is the first time that deep
convolutional networks have been applied to Chinese word segmentation.",7 Feb 2017 10:49:04 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",word segmentation,Zhixxxx,Xxx,xxxxxxxxxan.edu.cn,Fudan University,No,Junxxxx,Hx,xxxxxxxxxxxxfudan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhixxxx,Xxx,Fudan University,,,,,,xxxxxxxxxan.edu.cn,,,,,China,,Zhixxxx Xxx;Junxxxx Hx,xxxxxxxxxan.edu.cn;xxxxxxxxxxxx@fudan.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
341,341X-A5J9P2B6J2,Modeling Source Syntax for Neural Machine Translation,Juxxxx Lx;Dexx Xixxx;Zhaxxxxx Tx;Muxxx Zxx;Mxx Zhxxx and Guoxxxx Zxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Oral Tuesday,,Undecided (Machine Translation),"Even though a linguistics-free sequence to sequence model in neural machine
translation (NMT) has certain capability of implicitly learning syntactic
information of source sentences, this paper shows that source syntax can be
explicitly incorporated into NMT effectively to provide further improvements.
Specifically, we linearize parse trees of source sentences to obtain structural
label sequences. On the basis, we propose three different sorts of encoders to
incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word
and label annotation vectors parallelly; 2) Hierarchical RNN encoder that
learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed
RNN encoder that stitchingly learns word and label annotation vectors over
sequences where words and labels are mixed. Experimentation on
Chinese-to-English translation demonstrates that all the three proposed
syntactic encoders are able to improve translation accuracy. It is interesting
to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best
performance with an significant improvement of 1.4 BLEU points. Moreover, an
in-depth analysis from several perspectives is provided to reveal how source
syntax benefits NMT.",23 Apr 2017 02:40:04 GMT,Empirical/Data-Driven,Machine translation,,Juxxxx,Lx,xxxxxxxxxx@gmail.com,"Soochow University, Suzhou",No,Dexx,Xixxx,xxxxxxxxxx@gmail.com,Soochow University,No,Zhaxxxxx,Tx,xxxxxxxxxx@gmail.com,Tencent AI Lab,No,Muxxx,Zxx,xxxxxxxxxgmail.com,"Tencent Co., Ltd",No,Mxx,Zhxxx,xxxxxxxxxxuda.edu.cn,Suda,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,Juxxxx,Lx,"Soochow University, Suzhou",,,,,,xxxxxxxxxx@gmail.com,,Suzhou,Jiangsu,,China,,Juxxxx Lx;Dexx Xixxx;Zhaxxxxx Tx;Muxxx Zxx;Mxx Zhxxx;Guoxxxx Zhxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxg@gmail.com;xxxxxxxxxxg@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxxxsuda.edu.cn;xxxxxxxxxuda.edu.cn,Modeling Source Syntax for Neural Machine Translation,Modeling Source Syntax for Neural Machine Translation,10,Junhui Li,,,on,,No. Do not include my submission in this dataset.,No,None,None
342,342X-C3D7B6H6F6,Jointly Extracting Relations with Class Ties via Effective Deep Ranking,Hxx Yx;Wexxxx Chxx;Zhuxxxxx Lxx and Zhoxxxx L,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"Connections between relations in relation extraction, which we call class ties,
are common. In distantly supervised scenario, one entity tuple may have
multiple relation facts. Exploiting class ties between relations of one entity
tuple will be promising for distantly supervised relation extraction. However,
previous models are not effective or ignore to model this property. In this
work, to effectively leverage class ties, we propose to make joint relation
extraction with a unified model that integrates convolutional neural network
(CNN) with a general pairwise ranking framework, in which three novel ranking
loss functions are introduced. Additionally, an effective method is presented
to relieve the severe class imbalance problem from NR (not relation) for model
training. Experiments on a widely used dataset show that leveraging class ties
will enhance extraction and demonstrate the effectiveness of our model to learn
class ties. Our model outperforms the baselines significantly, achieving
state-of-the-art performance.",21 Apr 2017 03:08:24 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Hxx,Yx,xxxxxxxxaa.edu.cn,Beihang University,No,Wexxxx,Chxx,xxxxxxxxxxxbuaa.edu.cn,Beihang University,No,Zhuxxxxx,Lxx,xxxxxxxxxxo@gmail.com,China Defense Science and Technology Information Center,No,Zhoxxxx,Lx,xxxxxxxxa.edu.cn,Beihang University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hxx,Yx,Beihang University,,,818xxxxxxx,,,xxxxxxxxxtlook.com,,,,,China,,Hxx Yx;Wexxxx Chxx;Zhuxxxxx Lxx;Zhoxxxx Lx,xxxxxxxxaa.edu.cn;xxxxxxxxxxx@buaa.edu.cn;xxxxxxxxxxxo@gmail.com;xxxxxxxxaa.edu.cn,Jointly Extracting Relations with Class Ties via Effective Deep Ranking,Jointly Extracting Relations with Class Ties via Effective Deep Ranking,11,Hai Ye,,"School of Computer Science and Engineering, Beihang University, 
Beijing 100191, China",on,on,No. Do not include my submission in this dataset.,No,None,None
343,343X-F8G2G6P3B8,Neural Word Segmentation with Rich Pretraining,Jxx Yaxx;Yxx Zhxxx and Fxx Dxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Tuesday,,Undecided (Tagging Chunking Syntax Parsing),"Neural word segmentation research has benefited from large-scale raw texts by
leveraging them for pretraining character and word embeddings. On the other
hand, statistical segmentation research has exploited richer sources of
external information, such as punctuation, automatic segmentation and POS. We
investigate the effectiveness of a range of external training sources for
neural word segmentation by building a modular segmentation model, pretraining
the most important submodule using rich external sources. Results show that
such pretraining significantly improves the model, leading to accuracies
competitive to the best methods on six benchmarks.",22 Apr 2017 13:59:12 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Jxx,Yaxx,xxxxxxxxxxxxxil.sutd.edu.sg,Singapore University of Technology and Design,No,Yxx,Zhxxx,xxxxxxxxxxsutd.edu.sg,Singapore University of Technology and Design,No,Fxx,Doxx,xxxxxxxxxxxxxil.sutd.edu.sg,Singapore University of Technology & Design,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jxx,Yaxx,Singapore University of Technology and Design,,,986xxxxx,,,xxxxxxxxgmail.com,,,Singapore,,Singapore,"I am a final year Ph.D. candidate majoring in Computer Science at Singapore University of Technology and Design (SUTD). 
My research interests are mainly focused on Natural language processing (Word Segmentation, Named Entity Recognition, Automatic Essay Scoring .etc) with machine learning approaches (especially deep learning models).",Jxx Yaxx;Yxx Zhxxx;Fxx Doxx,xxxxxxxxxxxxxil.sutd.edu.sg;xxxxxxxxxxxsutd.edu.sg;xxxxxxxxxxxxxxil.sutd.edu.sg,Neural Word Segmentation with Rich Pretraining,Neural Word Segmentation with Rich Pretraining,11,,,,on,on,Only include my submission if it is accepted.,No,None,None
344,344X-C4E4F3G8P6,Artificial Intelligence or Asset Intelligence? Acronym Disambiguation for Enterprises,Yaxx Lx;Bx Zhxx;Arxxx Fuxxxx and Faxxxx Tx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Acronyms are abbreviations formed from the initial components of words or
phrases. In enterprises, people often use acronyms to make communications more
efficient. However, acronyms could be difficult to understand for people who
are not familiar with the subject matter (new employees, etc.), thereby
affecting productivity. To alleviate such troubles, we study how to
automatically resolve the true meanings of acronyms in a given context. Acronym
disambiguation for enterprises is challenging for several reasons. First,
acronyms may be highly ambiguous since an acronym used in the enterprise could
have multiple internal and external meanings. Second, there are usually no
comprehensive knowledge bases such as Wikipedia available in enterprises.
Finally, the system should be generic to work for any enterprise. In this work
we propose an end-to-end framework to tackle all these challenges. The
framework takes the enterprise corpus as input and produces a high-quality
acronym disambiguation system as output. Our disambiguation models are trained
via distant supervised learning, without requiring any manually labeled
training examples. Therefore, our proposed framework can be deployed to any
enterprise to support high-quality acronym disambiguation. Experimental results
on real world data justified the effectiveness of our system.",7 Feb 2017 07:45:13 GMT,Applications/Tools,"Information extraction, text mining, and question answering",unsupervised and semi-supervised learning;  NLP applications;  information extraction;  NLP on noisy unstructured text;  named entity disambiguation;  entity disambiguation;  text mining;  NLP in business application;  document mining,Yaxx,Lx,xxxxxxxxx.ucsb.edu,"University of California, Santa Barbara",No,Bx,Zhxx,xxxxxxxxxxxc@gmail.com,Pinterest,No,Arxxx,Fuxxxx,xxxxxxxxxoogle.com,Google Inc.,No,Faxxxx,Txx,xxxxxxxxxx@gmail.com,UIUC,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Lx,"University of California, Santa Barbara",,,,,,xxxxxxxxx.ucsb.edu,,,,,United States,,Yaxx Lx;Bx Zhxx;Arxxx Fuxxxx;Faxxxx Txx,xxxxxxxxx.ucsb.edu;xxxxxxxxxxxuc@gmail.com;xxxxxxxxxgoogle.com;xxxxxxxxxxo@gmail.com,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
345,345X-G2E3J3H2F5,An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation,Rapxxxx Sxx and Hixxxx Nakxxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Recently, the attention mechanism plays a key role to achieve high performance
for Neural Machine Translation models. However, as it computes a score function
for the encoder states in all positions at each decoding step, the attention
model greatly increases the computational complexity. In this paper, we
investigate the adequate vision span of attention models in the context of
machine translation, by proposing a novel attention framework that is capable
of reducing redundant score computation dynamically. The term ""vision span""
means a window of the encoder states considered by the attention model in one
step. In our experiments, we found that the average window size of vision span
can be reduced by over 50% with modest loss in accuracy on English-Japanese and
German-English translation tasks.",7 Feb 2017 12:13:47 GMT,Empirical/Data-Driven,Machine translation,subjectivity analysis,Rapxxxx,Sxx,xxxxxxxxxxxxx.u-tokyo.ac.jp,The University of Tokyo,No,Hixxxx,Nakxxxxx,xxxxxxxxxxxxxxxx.i.u-tokyo.ac.jp,The University of Tokyo,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rapxxxx,Sxx,The University of Tokyo,,,,,,xxxxxxxxxxxxx.u-tokyo.ac.jp,,,,,Japan,,Rapxxxx Sxx;Hixxxx Nakxxxxx,xxxxxxxxxxxxx.u-tokyo.ac.jp;xxxxxxxxxxxxxxxxi.i.u-tokyo.ac.jp,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
346,346X-A3A6B3D6H9,Pre-Trained Word Vectors in RNN-based Text Classifiers,Paxx Texx and Ilxx Zaixxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"We describe a straightforward approach to training GRU/LSTM based text
classifiers. The approach shows state-of-the-art results on 5 well-known NLP
datasets. We pro- vide provable and reproducible empirical evidence that
unsupervised pre-trained word vectors significantly improve test ac- curacy and
are less computationally ex- pensive and so decrease training time.",6 Feb 2017 12:50:12 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  unsupervised and semi-supervised learning;  domain adaptation;  learning with small datasets;  opinion mining and extraction;  experimental evaluation/comparison of ML methods;  text classification,Paxx,Texx,xxxxxxxxxions.tech,Emotions Tech,No,Ilxx,Zaixxxx,xxxxxxxxxions.tech,Emotions Tech,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Paxx,Texx,Emotions Tech,,,,,,xxxxxxxxxions.tech,,,,,United Kingdom,,Paxx Texx;Ilxx Zaixxxx,xxxxxxxxxions.tech;xxxxxxxxxtions.tech,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
347,347X-A6D7H5P3F8,Flexible and Creative Chinese Poetry Generation Using Neural Memory,Jixxxx Zhxxx;Yaxx Fexx;Doxx Waxx;Yaxx Waxx;Anxxxx Abxx;Shxxxx Zhxxx and Anxx Zhxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Poster Monday,,Undecided (Generation Summarization),"It has been shown that Chinese poems can be successfully generated by
sequence-to-sequence neural models, particularly with the attention mechanism.
A potential problem of this approach, however, is that neural models can only
learn abstract rules, while poem generation is a highly creative process that
involves not only rules but also innovations for which pure statistical models
are not appropriate in principle. This work proposes a memory augmented neural
model for Chinese poem generation, where the neural model and the augmented
memory work together to balance the requirements of linguistic accordance and
aesthetic innovation, leading to innovative generations that are still
rule-compliant. In addition, it is found that the memory mechanism provides
interesting flexibility that can be used to generate poems with different
styles.",21 Apr 2017 19:38:34 GMT,Empirical/Data-Driven,Generation,,Jixxxx,Zhxxx,xxxxxxxxxxxxxxxxt.tsinghua.edu.cn,"CSLT, RIIT, Tsinghua University, China",No,Yaxx,Fexx,xxxxxxxxxxxxxxxxxt.tsinghua.edu.cn,"Huilan Limited, Beijing, China",No,Doxx,Waxx,xxxxxxxxxxxxxxxx.tsinghua.edu.cn,"CSLT, RIIT, Tsinghua University, China",No,Yaxx,Waxx,xxxxxxxxxxxxxxxxxt.tsinghua.edu.cn,"CSLT, RIIT, Tsinghua University, China",No,Anxxxx,Abxx,xxxxxxxxxxxxxjtlu.edu.cn,"Department of Computer Science & Software Engineering, Xi'an Jiaotong-Liverpool University",No,Shxxxx,Zhxxx,xxxxxxxxxxxxxxxxt.tsinghua.edu.cn,"CSLT, RIIT, Tsinghua University, China",No,Anxx,Zhxxx,xxxxxxxxxxxxxxxxxit.tsinghua.edu.cn,"CSLT, RIIT, Tsinghua University, China",No,,,,,,,,,,,,,,,,,Jixxxx,Zhxxx,"CSLT, RIIT, Tsinghua University, China",,,,,,xxxxxxxxxxxxxxxxt.tsinghua.edu.cn,,,,,China,,Jixxxx Zhxxx;Yaxx Fexx;Doxx Waxx;Yaxx Waxx;Anxxxx Abxx;Shxxxx Zhxxx;Anxx Zhxxx,xxxxxxxxxxxxxxxxt.tsinghua.edu.cn;xxxxxxxxxxxxxxxxxit.tsinghua.edu.cn;xxxxxxxxxxxxxxxxs.tsinghua.edu.cn;xxxxxxxxxxxxxxxxxit.tsinghua.edu.cn;xxxxxxxxxxxx@xjtlu.edu.cn;xxxxxxxxxxxxxxxxxt.tsinghua.edu.cn;xxxxxxxxxxxxxxxxxxit.tsinghua.edu.cn,Flexible and Creative Chinese Poetry Generation Using Neural Memory,Flexible and Creative Chinese Poetry Generation Using Neural Memory,10,Jiyuan Zhang,,"CSLT, RIIT, Tsinghua University, China",,,No. Do not include my submission in this dataset.,No,None,None
348,348X-P4E3G3H4D7,Neural Relation Extraction with Multi-lingual Attention,Yaxxxx Lxx;Zhixxxx Lxx and Maoxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"Relation extraction has been widely used for finding unknown relational facts
from plain text. Most existing methods focus on exploiting mono-lingual data
for relation extraction, ignoring massive information from the texts in various
languages. To address this issue, we introduce a multi-lingual neural relation
extraction framework, which employs mono-lingual attention to utilize the
information within mono-lingual texts and further proposes cross-lingual
attention to consider the information consistency and complementarity among
cross-lingual texts. Experimental results on real-world datasets show that, our
model can take advantage of multi-lingual texts and consistently achieve
significant improvements on relation extraction as compared with baselines.",22 Apr 2017 12:22:02 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Yaxxxx,Lxx,xxxxxxxxxxxxxxtsinghua.edu.cn,Tsinghua University,No,Zhixxxx,Lxx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxxxx,Lxx,Tsinghua University,,,8615xxxxxxxxx,,,xxxxxxxxxxxxxxtsinghua.edu.cn,,Beijing,Beijing,,China,,Yaxxxx Lxx;Zhixxxx Lxx;Maoxxxx Sxx,xxxxxxxxxxxxxxtsinghua.edu.cn;xxxxxxxxxxxghua.edu.cn;xxxxxxxxxxhua.edu.cn,Neural Relation Extraction with Multi-lingual Attention,Neural Relation Extraction with Multi-lingual Attention,10,Lin Yankai,,"Tsinghua University, Haidian District, Beijing, China",on,on,No. Do not include my submission in this dataset.,No,None,None
349,349X-B3J6P4P8H3,Monolingual Phrase Alignment on Parse Forests,Yuxx Arxxx and Junxxxxx Tsxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Different from previous studies focused on paraphrases of simply word
sequences, we propose a method to detect paraphrases with syntactic tree
structures. Such syntactic information has been widely recognized its
importance in various NLP tasks. We achieve this goal by aligning phrases in
parse forests of sentential paraphrases. 
A dataset that provides gold parse trees and their phrase alignments are
created. The experiment results show that our method has achieved 83.64% and
78.91% recall and precision in terms of alignment pairs, which are 92% and 89%
of human performance.",10 Feb 2017 02:54:04 GMT,Theoretical,"Tagging, chunking, syntax, and parsing",textual entailment and paraphrasing;  alignment,Yuxx,Arxxx,xxxxxxxxxxxsaka-u.ac.jp,Osaka University,No,Junxxxxx,Tsxxxx,xxxxxxxxxaist.go.jp,Aritificial Intelligence Research Centre at AIST,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxx,Arxxx,Osaka University,,,+81-6xxxxxxxxxx,,,xxxxxxxxxxxsaka-u.ac.jp,,Osaka,,,Japan,,Yuxx Arxxx;Junxxxxx Tsxxxx,xxxxxxxxxxxsaka-u.ac.jp;xxxxxxxxxxaist.go.jp,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
350,350X-J9D3A6P3F8,Automatically Labeled Data Generation for Large Scale Event Extraction,Yuxx Chxx;Shxxxx Lxx;Xixxx Zhxxx;Kaxx Lxx and Jxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"Modern models of event extraction for tasks like ACE are based on supervised
learning of events from small hand-labeled data. However, hand-labeled training
data is expensive to produce, in low coverage of event types, and limited in
size, which makes supervised methods hard to extract large scale of events for
knowledge base population. To solve the data labeling problem, we propose to
automatically label training data for event extraction via world knowledge and
linguistic knowledge, which can detect key arguments and trigger words for each
event type and employ them to label events in texts automatically. The
experimental results show that the quality of our large scale automatically
labeled data is competitive with elaborately human-labeled data. And our
automatically labeled data can incorporate with human-labeled data, then
improve the performance of models learned from these data.",22 Apr 2017 09:29:50 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Yuxx,Chxx,xxxxxxxxxxxlpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Shxxxx,Lxx,xxxxxxxxxxxxlpr.ia.ac.cn,Chinese Academy of Sciences,No,Xixxx,Zhxxx,xxxxxxxxxxutlook.com,University of Chinese Academy of Sciences,No,Kaxx,Lxx,xxxxxxxxx.ia.ac.cn,Chinese Academy of Sciences,No,Jxx,Zhxx,xxxxxxxxx3@163.com,"NLPR, Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxx,Chxx,"Institute of Automation, Chinese Academy of Sciences",,,8613xxxxxxxxx,,,xxxxxxxxxxxlpr.ia.ac.cn,,,,,China,,Yuxx Chxx;Shxxxx Lxx;Xixxx Zhxxx;Kaxx Lxx;Jxx Zhxx,xxxxxxxxxxxlpr.ia.ac.cn;xxxxxxxxxxxxnlpr.ia.ac.cn;xxxxxxxxxxoutlook.com;xxxxxxxxxr.ia.ac.cn;xxxxxxxxx23@163.com,Automatically Labeled Data Generation for Large Scale Event Extraction,Automatically Labeled Data Generation for Large Scale Event Extraction,11,Yubo Chen,,"Institute of Automation Chinese Academy of Sciences 
95 Zhongguancun East Road, 100190, BEIJING, CHINA",on,on,Only include my submission if it is accepted.,No,None,None
351,351X-C5J7E7B6G9,Neural Machine Translation Model with a Large Vocabulary Selected by Branching Entropy,Zx Loxx;Takxxxxx Utxxxx;Tomxxxxx Mitxxxxxxx and Mixxx Yamxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation (NMT), a new approach to machine
translation, has achieved promising results comparable to those of
traditional approaches such as statistical machine translation
(SMT). Despite its recent success, NMT cannot handle a larger
vocabulary because the training complexity and decoding complexity
proportionally increase with the number of target words. This problem
becomes even more serious when translating patent documents, which
contain many technical terms that are observed infrequently. In this
paper, we propose to select phrases that contain out-of-vocabulary
words using the statistical approach of branching entropy. This allows
the proposed NMT system to be applied to a translation task of any
language pair without any language-specific knowledge about technical
term identification. The selected phrases are then replaced with tokens
during training and post-translated by the phrase translation table of
SMT. Experiments on Japanese-Chinese patent sentences proved the
effectiveness of phrases selected with branching entropy, as the
proposed NMT system achieves a substantial improvement over an
equivalent NMT system without our proposed technique.",6 Feb 2017 15:14:43 GMT,Empirical/Data-Driven,Machine translation,hybrid MT;  phrase-based SMT;  statistical machine translation,Zx,Loxx,xxxxxxxxxgmail.com,University of Tsukuba,No,Takxxxxx,Utxxxx,xxxxxxxxxgmail.com,University of Tsukuba,No,Tomxxxxx,Mitxxxxxxx,xxxxxxxxxgmail.com,Japan Patent Information Organization,No,Mixxx,Yamxxxxx,xxxxxxxxxgmail.com,University of Tsukuba,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Takxxxxx,Utxxxx,University of Tsukuba,,,+81-2xxxxxxxxxx,,,xxxxxxxxxgmail.com,,,,,Japan,,Zx Loxx;Takxxxxx Utxxxx;Tomxxxxx Mitxxxxxxx;Mixxx Yamxxxxx,xxxxxxxxxgmail.com;xxxxxxxxx@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxx@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
352,352X-D6B3C3J3H9,Adversarial Multi-task Learning for Text Classification,Penxxxx Lxx;Xixxxx Qxx and Xuaxxxxx Huxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"Neural network models have shown their promising opportunities for multi-task
learning, which focus on learning the shared layers to extract the common and
task-invariant features. However, in most existing approaches, the extracted
shared features are prone to be contaminated by task-specific features or the
noise brought by other tasks.
In this paper, we propose an adversarial multi-task learning framework,
alleviating the shared and private latent feature spaces from interfering with
each other.
We conduct extensive experiments on 16 different text classification tasks,
which demonstrates the benefits of our approach. Besides, we show that the
shared knowledge learned by our proposed model can be regarded as off-the-shelf
knowledge and easily transferred to new tasks.
The datasets of all 16 tasks are publicly available at
\url{http://nlp.fudan.edu.cn/data/}",19 Apr 2017 14:20:26 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Penxxxx,Lxx,xxxxxxxxxxdan.edu.cn,,No,Xixxxx,Qxx,xxxxxxxxxan.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Penxxxx,Lxx,,,,,,,xxxxxxxxxxdan.edu.cn,,,,,China,,Penxxxx Lxx;Xixxxx Qxx;Xuaxxxxx Huxxx,xxxxxxxxxxdan.edu.cn;xxxxxxxxxdan.edu.cn;xxxxxxxxxxudan.edu.cn,Adversarial Multi-task Learning for Text Classification,Adversarial Multi-task Learning for Text Classification,10,Pengfei Liu,,"Pengfei Liu
Fudan University",on,on,Only include my submission if it is accepted.,No,None,None
353,353X-D5G7F6B8E2,Clustering of Named Entities using Bag-of-Vectors (BoV): Experiments and Results on Telugu Newspaper Corpus,SaiKxxxxxxx Goxxx;Adxxxx Chanxxxxxxxxx;Bhxxx Muxxxx and  Arxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Semantic similarity plays an important role in many of Natural Language
Processing and Information Retrieval applications. Most Information retrieval
methodologies represent the documents using Vector Space Model traidionally
known as Bag of Word (BoW) model. This notation is widely acceptable. Since
most similarity functions use BoW for finding the similarity. In this paper, we
have attempted to cluster Named Entities (NEs) extracted from Telugu newspaper
corpus. We contend that for this sort of work, more suited vector
representation beyond BoW model and similarity measures can be utilized.
Specifically, we investigate the alternative representation for entities called
Bag-of-Vectors (BoV) (or Bag-of-Bag-of-Features). In this method, every entity
is defined as set of vectors, in which each vector is represented based on
contextual features (Words, Part-of-Speech, prefix and suffix) for each
occurrence of the entity. For clustering, we need to generalize the similarity
functions to Sum-of-Sum and Sum-of-Max, for Cosine similarity, Scalar product
and Jaccard co-efficient. Experimentally, we demonstrate that the BoV
representation enhance the clustering results than compared to traditional BoW
representation.",6 Feb 2017 14:10:31 GMT,Resources/Evaluation,"Information extraction, text mining, and question answering",information extraction;  named entity recognition;  term extraction,SaiKxxxxxxx,Goxxx,xxxxxxxxx@gmail.com,Birla Institute of Technology and Science Pilani Hyderabad Campus,No,Adxxxx,Chanxxxxxxxxx,xxxxxxxxx@gmail.com,Birla Institute of Technology and Science Pilani Hyderabad Campus,No,Bhanxxxxxxxx,Nxx,xxxxxxxxxxxxxxxxbits-pilani.ac.in,Birla Institute of Technology and Science Pilani Hyderabad Campus,No,Arxxx,M,xxxxxxxxxxxxxxxxxbits-pilani.ac.in,Birla Institute of Technology and Science Pilani Hyderabad Campus,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SaiKxxxxxxx,Goxxx,Birla Institute of Technology and Science Pilani Hyderabad Campus,,,,,,xxxxxxxxx@gmail.com,,Hyderabad,Telangana,,India,,SaiKxxxxxxx Goxxx;Adxxxx Chanxxxxxxxxx;Bhxxx Muxxxx;Arxxx M and Scixxxx Pixxxx,xxxxxxxxx@gmail.com;xxxxxxxxxx@gmail.com;xxxxxxxxxxxxxxxxxbits-pilani.ac.in;xxxxxxxxxxxxxxxxx.bits-pilani.ac.in,,,,,,,,,Only include my submission if it is accepted.,No,None,None
354,354X-G7J3F7G6G3,"POS, ANA and LEM: Word embeddings built from annotated corpora perform better",Atxxxx Noxxxx and Borxxxxx Sikxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Word embedding models have been popular and quite efficient tools for
representing lexical semantics in different languages. Nevertheless, there is
no standard for the direct evaluation of such models. Moreover, the
applicability of word embedding models is still a research question for less
resourced and morphologically complex languages. In this paper, we present and
evaluate different corpus preprocessing methods that make the creation of
high-quality word embedding models for Hungarian (and other morphologically
complex languages) possible. We use a crowd-sourcing-based intrinsic evaluation
scenario, and a detailed comparison of our models is presented. The results
show that models built from analyzed corpora are of better quality than raw
models.",6 Feb 2017 14:18:32 GMT,Resources/Evaluation,Semantics,part-of-speech tagging;  lexical semantics;  morphology;  distributional similarity,Atxxxx,Noxxxx,xxxxxxxxxxxx@itk.ppke.hu,"MTA-PPKE Hungarian Language Technology Research Group, Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, Budapest",No,Borxxxxx,Sikxxxxx,xxxxxxxxxxxxxla@itk.ppke.hu,Pázmány Péter Catholic University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Atxxxx,Noxxxx,"MTA-PPKE Hungarian Language Technology Research Group, Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, Budapest",,,,,,xxxxxxxxxxxx@itk.ppke.hu,,,,,Hungary,,Atxxxx Noxxxx;Borxxxxx Sikxxxxx,xxxxxxxxxxxx@itk.ppke.hu;xxxxxxxxxxxxxxla@itk.ppke.hu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
355,355X-F7F3P3A9G3,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,Hixxxx Ouxxx;Hirxxxxx Shxxxx and Yuxx Matxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"The performance of Japanese predicate argument structure (PAS) analysis has
improved in recent years thanks to the joint modeling of interactions between
multiple predicates. However, this approach relies heavily on syntactic
information predicted by parsers, and suffers from errorpropagation. To remedy
this problem, we
introduce a model that uses grid-type recurrent neural networks. The proposed
model automatically induces features sensitive to multi-predicate interactions
from
the word sequence information of a sentence. Experiments on the NAIST Text
Corpus demonstrate that without syntactic information, our model outperforms
previous syntax-dependent models.",22 Apr 2017 03:07:24 GMT,Empirical/Data-Driven,Semantics,,Hixxxx,Ouxxx,xxxxxxxxxxxxxxt6@is.naist.jp,Nara Institute of Science and Technology,No,Hirxxxxx,Shxxxx,xxxxxxxxx.naist.jp,Nara Institute of Science and Technology,No,Yuxx,Matxxxxxx,xxxxxxxx.naist.jp,Nara Institute of Science and Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hixxxx,Ouxxx,Riken AIP,,,,,,xxxxxxxxxxhi@riken.jp,,,,,Japan,,Hixxxx Ouxxx;Hirxxxxx Shxxxx;Yuxx Matxxxxxx and  Tecxxxxxxxx,xxxxxxxxxxxxxxt6@is.naist.jp;xxxxxxxxxs.naist.jp;xxxxxxxxx.naist.jp,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,10,Hiroki Ouchi,,Nara Institute of Science and Technology,on,on,Only include my submission if it is accepted.,No,None,None
356,356X-E5G8J6P6C7,Multilevel Attention Models for Dialog Generation,Yanxxxx Waxx;Juxxxx Fexx;Faxxx Mexx;Wexxx Roxx and Zhxxx Xixx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"One key challenge for creating a chat bot is to find an effective way to learn 
      from human-human conversation data. Recently Recently a few neural
network based dialog models, including the RNN language model(RNNLM) and the
hierarchical
recurrent encoder-decoder (HRED), model have shown promising results on dialog
response generation. However there is a critical problem that most of responses
generated by these models are of chit-chat style instead of being informative
even if the training data are goal-oriented dialogs. In this paper, we approach
this problem by two means. First, we propose six novel attention mechanisms to
improve on HRED. We apply attention on multiple levels of HRED. Experiments
show the bidirectional HRED with dynamic attention model obtains the best
performance, significantly higher than the HRED baseline. We also visualize how
the attention works in dialog context to verify the attention mechanisms.
Second. we design experiments to quantify how the size and diversity of the
training data influences the performance of the HRED model and our proposed
attention models.",7 Feb 2017 05:11:57 GMT,Theoretical,Dialog and interactive systems,language generation;  dialogue;  question answering in restricted domains,Yanxxxx,Waxx,xxxxxxxxxuaa.edu.cn,"School of Computer Science and Engineering, Beihang University",No,Juxxxx,Fexx,xxxxxxxxxxxxxinamobile.com,The Research Institution of China Mobile,No,Faxxx,Mexx,xxxxxxxxxxxxinamobile.com,The Research Institution of China Mobile,No,Wexxx,Roxx,xxxxxxxxxaa.edu.cn,"School of Computer Science and Engineering, Beihang University",No,Zhxxx,Xixxx,xxxxxxxxxaa.edu.cn,"School of Computer Science and Engineering, Beihang University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yanxxxx,Waxx,"School of Computer Science and Engineering, Beihang University",,,8618xxxxxxxxx,,,xxxxxxxxxuaa.edu.cn,,Beijing,,,China,,Yanxxxx Waxx;Juxxxx Fexx;Faxxx Mexx;Wexxx Roxx;Zhxxx Xixxx and Engixxxxxxx Beixxxx,xxxxxxxxxuaa.edu.cn;xxxxxxxxxxxxxhinamobile.com;xxxxxxxxxxxxxinamobile.com;xxxxxxxxxuaa.edu.cn;xxxxxxxxxuaa.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
357,357X-D6C4D9H6A7,Cross-domain Chinese Word Segmentation with Domain-Adversarial Training,Qx Zhxxx;Luxxx Zhxx;Haxxxx Huxxx and Xuaxxxxx Huxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"We propose a novel neural network for domain adaptation on a Chinese word
segmentation task with an adversarial training paradigm. It requires labelled
data from resource-rich domains and a large amount of unlabelled data from the
target domain. The proposed method extends the bi-directional long short-term
memory recurrent neural network with a label predictor, a domain classifier,
and character level language models. Compared to most of the existing Chinese
word segmentation methods, whose performances are usually highly impacted by
the domain, the proposed method can easily shift from different domains.
Experimental results demonstrate that the method performs well on the Chinese
word segmentation task in a series of domains.",6 Feb 2017 15:14:59 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",domain adaptation;  word segmentation,Qx,Zhxxx,xxxxxxxn.edu.cn,Fudan University,No,Luxxx,Zhxx,xxxxxxxxxxxxfudan.edu.cn,Fudan University,No,Haxxxx,Huxxx,xxxxxxxxxxxudan.edu.cn,Fudan University,No,Xuaxxxxx,Huxxx,xxxxxxxxxxdan.edu.cn,Fudan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qx,Zhxxx,Fudan University,,,,,,xxxxxxxn.edu.cn,,,,,China,,Qx Zhxxx;Luxxx Zhxx;Haxxxx Huxxx;Xuaxxxxx Huxxx,xxxxxxxn.edu.cn;xxxxxxxxxxxx@fudan.edu.cn;xxxxxxxxxxxfudan.edu.cn;xxxxxxxxxxudan.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
359,359X-G7A3D3F6P8,The Thing Recognizer: Making sense of word embeddings,Atxxxx Noxxxx and Borxxxxx Sikxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Neural word embedding models trained on sizable corpora have proved to be a
very efficient means of representing meaning. However, the abstract vectors
representing words and phrases in these models are not interpretable for humans
by themselves. In this paper we present the Thing Recognizer, a method that
assigns explicit symbolic semantic features from a finite list of terms to
words present in an embedding model, making the model interpretable for humans
and covering the semantic space by a controlled vocabulary of semantic
features. We do this in a cross-lingual manner, applying semantic tags taken
form lexical resources in one language (English) to the embedding space of
another (Hungarian).",6 Feb 2017 14:28:36 GMT,Resources/Evaluation,Semantics,cross-lingual approaches;  lexical semantics;  lexicon development;  distributional similarity;  semantic knowledge induction,Atxxxx,Noxxxx,xxxxxxxxxxxx@itk.ppke.hu,"MTA-PPKE Hungarian Language Technology Research Group, Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, Budapest",No,Borxxxxx,Sikxxxxx,xxxxxxxxxxxxxla@itk.ppke.hu,Pázmány Péter Catholic University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Atxxxx,Noxxxx,"MTA-PPKE Hungarian Language Technology Research Group, Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, Budapest",,,,,,xxxxxxxxxxxx@itk.ppke.hu,,,,,Hungary,,Atxxxx Noxxxx;Borxxxxx Sikxxxxx,xxxxxxxxxxxx@itk.ppke.hu;xxxxxxxxxxxxxxla@itk.ppke.hu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
360,360X-D6B2H8G3J2,Discourse Mode Identification in Essays,Wxx Soxx;Doxx Waxx;Ruxxx Fx;Lixxxx Lxx;Tixx Lxx and Guoxxxx H,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Oral Monday,,Undecided (Discourse Pragmatics),"Discourse modes play an important role in writing composition and evaluation.
This paper presents a study on the manual and automatic identification of
narration,exposition, description, argument and emotion expressing sentences in
narrative essays. We annotate a corpus to study the characteristics of
discourse modes
and describe a neural sequence labeling model for identification. Evaluation
results show that discourse modes can be identified automatically with an
average F1-score of 0.7. We further demonstrate that discourse modes can be
used as features that improve automatic essay scoring (AES). The impacts of
discourse modes for AES are also discussed.",23 Apr 2017 11:54:37 GMT,Applications/Tools,Discourse and pragmatics,,Wxx,Soxx,xxxxxxxxu.edu.cn,Capital Normal University,No,Doxx,Waxx,xxxxxxxxxxiflytek.com,iFlytek Research,No,Ruxxx,Fx,xxxxxxxxytek.com,iFlytek Research,No,Lixxxx,Lxx,xxxxxxxxu.edu.cn,Capital Normal University,No,Tixx,Lxx,xxxxxxxxxit.edu.cn,Harbin Institute of Technology,No,Guoxxxx,Hx,xxxxxxxxytek.com,iFlytek Research,No,,,,,,,,,,,,,,,,,,,,,,Wxx,Soxx,Capital Normal University,,,,,,xxxxxxxxu.edu.cn,,,,,China,"I am an associate professor in Capital Normal University, China.
I got my Ph.d from Harbin Institute of Technology, China.",Wxx Soxx;Doxx Waxx;Ruxxx Fx;Lixxxx Lxx;Tixx Lxx;Guoxxxx Hx,xxxxxxxxu.edu.cn;xxxxxxxxxxxiflytek.com;xxxxxxxxlytek.com;xxxxxxxxnu.edu.cn;xxxxxxxxxhit.edu.cn;xxxxxxxxlytek.com,Discourse Mode Identification in Essays,Discourse Mode Identification in Essays,11,Wei Song,,,,,No. Do not include my submission in this dataset.,No,None,None
361,361X-D2P9P8P2B5,A Modified Annotation Scheme For Semantic Textual Similarity,Darxxxx Agaxxxx;Vaxxxx Mujxxxx;Dixxx Mixxx and Radxxxx Maxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Semantic Similarity plays an important role in several natural language related
tasks such as evaluation of machine translation, textual entailment, entity
linking, document clustering etc. To be able to make machines do such tasks
automatically, one must need to have well annotated textual semantic similarity
corpus. Therefore as an initial stepping step,                    we present a
modified
annotation scheme for measuring semantic similarity between two Hindi
sentences. Given two sentences, the goal of the annotation is to give the
similarity score between two sentences on a scale of 0 to 5. We observed
several difficulties in assigning similarity score by following the (Agirre et
al., 2012) annotation scheme. To overcome those difficulties, we developed new
scoring scheme and observed considerable inter annotator agreement compared to
(Agirre et al., 2012) annotation scheme. Using our annotation scheme we also
annotated the degree of semantic relatedness on 750 pair of Hindi sentences. In
future, we will release this dataset as an open source.",6 Feb 2017 14:59:57 GMT,Resources/Evaluation,Semantics,MT evaluations;  multiword semantics/compositionality;  NLP applications;  information extraction;  sentence simplification;  lexical semantics;  lexical paraphrasing;  information retrieval;  textual entailment and paraphrasing;  distributional similarity;  semantic relations;  document summarization;  alignment;  semantic role labelling;  semantic knowledge induction;  document clustering,Darxxxx,Agaxxxx,xxxxxxxxxxxxxan95@gmail.com,IIIT Hyderabad,No,Vaxxxx,Mujxxxx,xxxxxxxxxgmail.com,student,No,Dixxx,Misrxxxxxxxx,xxxxxxxxit.ac.in,IIIT Hyderabad,No,Radxxxx,Maxxxx,xxxxxxxxxxxxdi@iiit.ac.in,IIIT Hyderabad,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Darxxxx,Agaxxxx,IIIT Hyderabad,,,,,,xxxxxxxxxxxxxan95@gmail.com,,,,,India,,Darxxxx Agaxxxx;Vaxxxx Mujxxxx;Dixxx Mixxx;Radxxxx Maxxxx,xxxxxxxxxxxxxan95@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxiit.ac.in;xxxxxxxxxxxxxdi@iiit.ac.in,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
362,362X-J8G3J9A5J2,Semi-Supervised Learning of a Pronunciation Dictionary from Disjoint Phonemic Transcripts and Text,Takxxxxx Shixxxxxx;Shxxxx Watxxxxx;Daxxxx Mocxxxxxxx and Grxxxx Nexxx,Speech,Chxxxx Hoxx;Chixxxxxx Lxx,Rejected - Withdrawn,,Rejected - Withdrawn (Speech),"While the performance of automatic speech recognition systems has recently
approached human 
levels in some tasks, application is still limited to specific domains.
This is because system development relies on extensive supervised training and
expert tuning
in the target domain.
To solve the problem, systems must become more self-sufficient, having the
ability to learn
directly from speech and adapt to new tasks.
One open question in this area is how to learn a pronunciation dictionary
containing the appropriate
vocabulary.
Humans are able to recognize words, even ones they have never heard before, 
by reading text and understanding the context in which a word is used.
However, this ability is missing in current speech recognition systems.
In this work, we propose a new paradigm that automatically expands 
an initial pronunciation dictionary using independently sampled acoustic and
textual data.
Experiments using the WSJ data set 
demonstrate that one instance of this paradigm using a model based on Bayesian
learning
of Dirichlet processes can acquire word pronunciations from unaligned phonetic
and word transcriptions effectively.",7 Feb 2017 09:48:56 GMT,Empirical/Data-Driven,Speech,language acquisition;  generative models;  unsupervised and semi-supervised learning;  domain adaptation;  language modeling for spoken language;  lexicon development;  spoken language understanding;  Bayesian learning;  speech recognition,Takxxxxx,Shixxxxxx,xxxxxxxxxxxitech.ac.jp,Tokyo Institute of Technology,No,Shxxxx,Watxxxxx,xxxxxxxxieee.org,MERL,No,Daxxxx,Mocxxxxxxx,xxxxxxxxsm.ac.jp,The Institute of Statistical Mathematics,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Takxxxxx,Shixxxxxx,Tokyo Institute of Technology,,,,,,xxxxxxxxxxxx.titech.ac.jp,,,,,Japan,,Takxxxxx Shixxxxxx;Shxxxx Watxxxxx;Daxxxx Mocxxxxxxx;Grxxxx Nexxxx,xxxxxxxxxxxitech.ac.jp;xxxxxxxx@ieee.org;xxxxxxxxism.ac.jp;xxxxxxxxxcs.cmu.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
363,363X-C9A4J6H9C6,Automatic Selection of Context Configurations for Improved Class-Specific Word Representations,Ivxx Vuxxxx;Rxx Schxxxxx;Axx Rapxxxxxx;Rxx Reixxxxx and Anxx Korxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"This paper is concerned with identifying contexts useful for training word
representation models for different word classes such as adjectives (A), verbs
(V), and nouns (N). We introduce a simple yet effective framework for an
automatic selection of class-specific context configurations. We construct a
context configuration space based on universal dependency relations between
words, and efficiently search this space with an A*-style algorithm. In word
similarity tasks for each word class, we show that our framework is both
effective and efficient. Particularly, it improves the Spearman's correlation
with human scores on SimLex-999 over the best previously proposed
class-specific contexts by 6 (A), 6 (V) and 5 (N) rho points. With our selected
context configurations, we train on only 14% (A), 26.2% (V), and 33.6% (N) of
all dependency-based contexts, resulting in a reduced training time. Our
results generalise: we show that the configurations our algorithm learns for
one English training setup outperform previously proposed context types in
another training setup for English. Moreover, basing the configuration space on
universal dependencies, it is possible to import the learned configurations to
German and Italian. We also demonstrate improved per-class results over other
context types in these two languages.",6 Feb 2017 20:04:05 GMT,Empirical/Data-Driven,Semantics,cross-lingual approaches;  lexical semantics;  distributional similarity;  semantic relations,Ivxx,Vuxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Rxx,Schxxxxx,xxxxxxxxxxxxshington.edu,University of Washington and The Allen Institute for Artificial Intelligence,No,Axx,Rapxxxxxx,xxxxxxxxxuji.ac.il,The Hebrew University,No,Rxx,Reixxxxx,xxxxxxxxxxt@gmail.com,Technion - Israel Institute of Technology,No,Anxx,Korxxxxx,xxxxxxxxxxxxx@cl.cam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Ivxx,Vuxxxx,University of Cambridge,,,4474xxxxxxxx,,,xxxxxxxam.ac.uk,,Cambridge,,,United Kingdom,,Ivxx Vuxxxx;Rxx Schxxxxx;Axx Rapxxxxxx;Rxx Reixxxxx;Anxx Korxxxxx,xxxxxxxam.ac.uk;xxxxxxxxxxxxashington.edu;xxxxxxxxxhuji.ac.il;xxxxxxxxxxxt@gmail.com;xxxxxxxxxxxxxn@cl.cam.ac.uk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
365,365X-A3F6F2F5H6,Learning attention for historical text normalization by learning to pronounce,Maxxxx Bolxxxxx;Joaxxxx Bixxxx and Anxxxx Søxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Monday,,Undecided (Machine Learning),"Automated processing of historical texts often relies on pre-normalization to
modern word forms. Training encoder-decoder architectures to solve such
problems typically requires a lot of training data, which is not available for
the named task. We address this problem by using several novel encoder-decoder
architectures, including a multi-task learning (MTL) architecture using a
grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art
by an absolute 2% increase in performance. We analyze the induced models across
44 different texts from Early New High German. Interestingly, we observe that,
as previously conjectured, multi-task learning can learn to focus attention
during decoding, in ways remarkably similar to recently proposed attention
mechanisms. This, we believe, is an important step toward understanding how MTL
works.",21 Apr 2017 19:28:33 GMT,Empirical/Data-Driven,Machine learning,,Maxxxx,Bolxxxxx,xxxxxxxxxxxxxuistics.rub.de,"Department of Linguistics, Ruhr-Universität Bochum",No,Joaxxxx,Bixxxx,xxxxxxxxx@gmail.com,University of Copenhagen,No,Anxxxx,Søxxxxx,xxxxxxxx@di.ku.dk,University of Copenhagen,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,Bolxxxxx,University of Copenhagen,,,,,,xxxxxxxdi.ku.dk,,,,,Denmark,,Maxxxx Bolxxxxx;Joaxxxx Bixxxx;Anxxxx Søxxxxx,xxxxxxxxxxxxxuistics.rub.de;xxxxxxxxxx@gmail.com;xxxxxxxxx@di.ku.dk,Learning attention for historical text normalization by learning to pronounce,Learning attention for historical text normalization by learning to pronounce,13,Marcel Bollmann,,,on,,Only include my submission if it is accepted.,No,None,None
366,366X-H3B5E6P8G9,Argument Mining with Structured SVMs and RNNs,Vlxx Nicxxxx;Jooxxxx Paxx and Clxxxx Caxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Oral Tuesday,,Undecided (Discourse Pragmatics),"We propose a novel factor graph model for argument mining, designed for
settings in which the argumentative relations in a document do not necessarily
form a tree structure. (This is the case in over 20% of the web comments
dataset we release.) Our model jointly learns elementary unit type
classification and argumentative relation prediction. Moreover, our model
supports SVM and RNN parametrizations, can enforce structure constraints (e.g.,
transitivity), and can express dependencies between adjacent relations and
propositions. Our approaches outperform unstructured baselines in both web
comments and argumentative essay datasets.",23 Apr 2017 00:53:53 GMT,Empirical/Data-Driven,Discourse and pragmatics,,Vlxx,Nicxxxx,xxxxxxxxxornell.edu,Cornell University,No,Jooxxxx,Paxx,xxxxxxxxxxilliams.edu,Williams College,No,Clxxxx,Caxxxx,xxxxxxxxxxcornell.edu,Cornell University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vlxx,Nicxxxx,Instituto de Telecomunicações,,,,,,xxxxxxene.ro,,,NY,,Portugal,,Vlxx Nicxxxx;Jooxxxx Paxx;Clxxxx Caxxxx,xxxxxxxxxornell.edu;xxxxxxxxxxxilliams.edu;xxxxxxxxxxxcornell.edu,Argument Mining with Structured SVMs and RNNs,Argument Mining with Structured SVMs and RNNs,11,Vlad Niculae,,"Cornell University
Department of Computer Science
Ithaca, NY 14853-7501",on,on,No. Do not include my submission in this dataset.,No,None,None
367,367X-F3B6H7B6P8,From BLEU to RAINBOW: Why We Need New Metrics for NLG.,Jekxxxxxxx Novxxxxx;Amxxxx Cexxxx;Ondxxxx Duxxxx and Vexxxx Rixxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"The majority of NLG evaluation relies on automatic metrics, such as BLEU. In
this paper, we investigate a wide range of these metrics, including
state-of-the-art word-based and novel grammar-based ones, and demonstrate that
they only weakly reflect human judgements of system outputs as generated by
data-driven, end-to-end NLG.
A detailed error analysis shows that automatic metrics are particularly bad in
distinguishing outputs of medium and good quality, which can be partially
attributed to the fact that human judgements and metrics are given on different
scales. We also show that metric performance is data and system specific. 
We then suggest an alternative metric, called RAINBOW, combining the individual
strengths of different automatic scores. This new metric achieves up to rho=.81
correlation with human judgements on the sentence level (compared to a maximum
of rho=.33  for existing metrics) and achieves stable results across systems
and data sets.",7 Feb 2017 11:44:22 GMT,Resources/Evaluation,Generation,language generation;  evaluation metrics,Jekxxxxxxx,Novxxxxx,xxxxxxxxxxxxxxerina@gmail.com,Heriot Watt University,No,Amxxxx,Cercxxxxxxxx,xxxxxxxw.ac.uk,Heriot-Watt University,No,Ondxxxx,Duxxxx,xxxxxxxxhw.ac.uk,Heriot-Watt University,No,Vexxxx,Rixxxx,xxxxxxxxxr@hw.ac.uk,Heriot-Watt University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vexxxx,Rixxxx,Heriot-Watt University,,,,,,xxxxxxxxxr@hw.ac.uk,,,,,United Kingdom,,Jekxxxxxxx Novxxxxx;Amxxxx Cexxxx;Ondxxxx Duxxxx;Vexxxx Rixxxx,xxxxxxxxxxxxxxerina@gmail.com;xxxxxxxhw.ac.uk;xxxxxxxx@hw.ac.uk;xxxxxxxxxxr@hw.ac.uk,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
368,368X-A3J3H6J5A7,Semantic Textual Similarity For Hindi,Darxxxx Agaxxxx;Vaxxxx Mujxxxx;Dixxx Mixxx and Radxxxx Maxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Semantic textual similarity is the degree of equivalence between the two
sentences semantically. We may also say, it is the ability to substitute one
text for the other without changing its meaning.  In this paper, we propose
rule based and supervised systems which measure the semantic relatedness
between two Hindi sentences on the scale of 0 (least similar) to 5 (most
similar).  Both systems make use of several syntactico-semantic features such
as language specific linguistic characteristics, distributional semantics and
dependency clusters. With several constraints on these features, our rule based
system is able to achieve around 75.23\% accuracy on Hindi news similarity
corpus. In supervised approach, we use support vector machine (SVM) with above
mentioned features and euclidean distance between dependency clusters to derive
word level alignments. Later, we use these alignments to assign similarity
score between two sentences. With this approach we are able to achieve
considerable accuracy on a small set of our corpus.",7 Feb 2017 08:06:53 GMT,Applications/Tools,Semantics,MT evaluations;  multiword semantics/compositionality;  unsupervised and semi-supervised learning;  NLP applications;  information extraction;  automatic MT evaluation metrics;  lexical semantics;  information retrieval;  rule-based/symbolic learning methods;  textual entailment and paraphrasing;  distributional similarity;  semantic relations;  evaluation metrics;  alignment;  semantic role labelling;  semantic knowledge induction;  document clustering,Darxxxx,Agaxxxx,xxxxxxxxxxxxxan95@gmail.com,IIIT Hyderabad,No,Vaxxxx,Mujxxxx,xxxxxxxxxgmail.com,student,No,Dixxx,Misrxxxxxxxx,xxxxxxxxit.ac.in,IIIT Hyderabad,No,Radxxxx,Maxxxx,xxxxxxxxxxxxdi@iiit.ac.in,IIIT Hyderabad,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Darxxxx,Agaxxxx,IIIT Hyderabad,,,,,,xxxxxxxxxxxxxan95@gmail.com,,,,,India,,Darxxxx Agaxxxx;Vaxxxx Mujxxxx;Dixxx Mixxx;Radxxxx Maxxxx,xxxxxxxxxxxxxan95@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxiit.ac.in;xxxxxxxxxxxxxdi@iiit.ac.in,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
369,369X-J5P3A8J7H4,Morphology Generation for Statistical Machine Translation using Deep Learning Techniques,Maxxx Rx and Caxxxx Escxxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Morphology in unbalanced languages remains a big challenge in the context of
machine translation. In this paper, we propose to de-couple machine translation
from morphology generation in order to better deal with the problem. We
investigate the morphology simplification with a reasonable trade-off between
expected gain and generation complexity. For the Chinese-Spanish task, optimum
morphological simplification is in gender and number. For this purpose, we
design a new classification architecture which, compared to other standard
machine learning techniques, obtains the best results. This proposed
neural-based architecture consists of several layers: an embedding, a
convolutional followed by a recurrent neural network and, finally, ends with
sigmoid and softmax layers. We obtain classification results over 98% accuracy
in gender classification, over 93% in number classification, and an overall
translation improvement of 0.7 METEOR.",6 Feb 2017 15:10:07 GMT,Empirical/Data-Driven,Machine translation,phrase-based SMT;  morphology,Marxxxxx,Costxxxxxxxx,xxxxxxxxxz@upc.edu,Universitat Politècnica de Catalunya,No,Caxxxx,Escxxxxx,xxxxxxxxxxxxxno@tsc.upc.edu,Universitat Politècnica de Catalunya,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Marxxxxx,Costxxxxxxxx,Universitat Politècnica de Catalunya,,,,,,xxxxxxxxxz@upc.edu,,Barcelona,Catalonia,,Spain,,Maxxx Rx;Caxxxx Escxxxxx,xxxxxxxxxz@upc.edu;xxxxxxxxxxxxxxno@tsc.upc.edu,,,,,,,on,,"Yes, include my submission even if the paper is rejected.",No,None,None
371,371X-G5F6B6A3F6,Phrasal Recurrent Neural Network,Daxx Zhxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"We propose a new, simple, yet effective
framework, phrasal recurrent neural net-
works (pRNNs) for phrase representation. Different from previous
RNN based models, pRNNs do not model
a sentence as surface word sequences. In-
stead, pRNNs model all candidate phrases with arbitrary lengths. To represent
phrases as fix-length real-valued vectors,
we build the RNN pyramid, which is composed of shifted parallel RNN sequences.
To utilize potential phrases, we employ
the attention mechanism to compare and
combine them. We test our model on
language model and machine translation
tasks. Our model leads to an improvement
of over 10 points in perplexity both on standard Penn Treebank and FBIS English
data set over a state-of-the-art LSTM language modeling baseline. It
outperforms both the Moses (phrase-based statistical
model) and a strong sequence-to-sequence
baseline in the Chinese-English machine
translation task.",7 Feb 2017 13:12:08 GMT,Empirical/Data-Driven,Machine translation,MT applications;  multiword semantics/compositionality;  generative models;  tree-based SMT;  structured input/output;  chunking;  parsing;  multimodal representations and processing,Daxx,Zhxxx,xxxxxxxxx@gmail.com,"Institute of Computing Technology, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxx,Zhxxx,"Institute of Computing Technology, Chinese Academy of Sciences",,,8615xxxxxxxxx,,,xxxxxxxxx@gmail.com,,Beijing,Beijing,,China,,Daxx Zhxxx,xxxxxxxxx@gmail.com,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
372,372X-A2E7A3P7P6,Creating Domain-Specific Sentiment Lexicons via Text Mining,Kexxx Labxxxx;Suxxxx Alfxxxxxx and Suxxx Gaxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Our work focuses on generating a domain-specific lexicon using probabilities
and information theoretic techniques. By employing text mining, we overcome the
poor performance of transferred supervised machine learning techniques and
remove the need to adapt an existing lexicon while maintaining accuracy. We
show that text mining techniques performs as well as traditional approaches and
we demonstrate that domain specific lexicons perform better than general
lexicons in a sentiment analysis task. We further review and compare the
generated lexicons.",6 Feb 2017 15:16:33 GMT,Theoretical,Sentiment analysis and opinion mining,sentiment analysis;  lexicon development;  opinion mining and extraction;  text mining,Kexxx,Labxxxx,xxxxxxxx@uark.edu,University of Arkansas,No,Suxxxx,Alfxxxxxx,xxxxxxxx@uark.edu,University of Arkansas,No,Suxxx,Gaxxx,xxxxxxxuark.edu,University of Arkansas,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kexxx,Labxxxx,University of Arkansas,,,,,,xxxxxxxx@uark.edu,,,,,United States,,Kexxx Labxxxx;Suxxxx Alfxxxxxx;Suxxx Gaxxx,xxxxxxxx@uark.edu;xxxxxxxxx@uark.edu;xxxxxxxxuark.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
373,373X-F6D7D9J6C3,An Ensemble of Retrieval- and Generation-Based Human-Computer Conversation Systems,Yixxxx Soxx;Rxx Yxx;Mixx Zhxxx;Jiaxxxxx Nxx;Chexxxxx Lx;Donxxxx Zhxx and  Wexxxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Human-computer conversation systems have attracted much attention in Natural
Language Processing. Conversation systems can be roughly divided into two
categories: retrieval-based and generation-based ones. Retrieval systems search
a user-issued utterance (namely a query) in a large conversational repository,
and return a reply that best matches the query. Generative approaches
synthesize new replies. Both ways have certain advantages, but suffer from
their own disadvantages. We propose a novel ensemble of retrieval-based and
generation-based conversation system. The retrieved candidates, in addition to
the original query, are fed to a reply generator via a neural network, so that
the model is aware of more information. The generated replies together with the
retrieved ones then participate in a re-ranking process to find the final reply
to output. Experimental results show that such an ensemble system outperforms
each single module by a large margin.",7 Feb 2017 09:58:59 GMT,Applications/Tools,Dialog and interactive systems,dialogue,Yixxxx,Soxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,Rxx,Yxx,xxxxxxxxxxu@gmail.com,Peking University,No,Mixx,Zhxxx,xxxxxxxxxx.pku.edu.cn,Peking University,No,Jiaxxxxx,Nxx,xxxxxxxxxxontreal.ca,University of Montreal,No,Chexxxxx,Lx,xxxxxxxxxxxx.ncku.edu.tw,National Cheng Kung University,No,Donxxxx,Zhxx,xxxxxxxxku.edu.cn,Peking University,No,Wexxxx,E,xxxxxxxxxxx.pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,Yixxxx,Soxx,Peking University,,,,,,xxxxxxxxxx@pku.edu.cn,,,,,China,,Yixxxx Soxx;Rxx Yxx;Mixx Zhxxx;Jiaxxxxx Nxx;Chexxxxx Lx;Donxxxx Zhxx;Wexxxx E,xxxxxxxxxx@pku.edu.cn;xxxxxxxxxxxu@gmail.com;xxxxxxxxxxx.pku.edu.cn;xxxxxxxxxxmontreal.ca;xxxxxxxxxxxxl.ncku.edu.tw;xxxxxxxxxku.edu.cn;xxxxxxxxxxxh.pku.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
374,374X-C2P6J9J3F5,Learning to Rank and Re-Rank (R&R): A Context-Aware Approach for Human-Computer Conversation,M.xx Soxxxx and Rxx Yxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"To implement an automatic conversation system is regarded as one of the most
hardcore problems in Natural Language Processing, especially when refers to a
multi-turn conversation between human and computer. In this paper, we conduct
an investigation of the multi-turn human-computer conversation: given a
sequence of human messages, our system could return a corresponding sequence of
replies to the issued messages. We design a series of pointwise and pairwise
features, passing information from previous message(s) to the on-going
conversation. Formally, we formulate the problem into a learning to rank and
re-rank paradigm given the information passed along. The system is
fundamentally an information retrieval framework based on a vast conversation
resource. We run experiments to compare several rival algorithms on a massive
data repository (nearly 10 million conversation pairs of postings-replies).
Performance comparisons against baselines indicate the proposed method yields a
good trade-off between effectiveness and efficiency, both of which are
important for a practical, online system.",7 Feb 2017 07:36:37 GMT,Empirical/Data-Driven,Dialog and interactive systems,NLP applications;  dialogue;  contex modeling for dialogues,M.K.xxxxxxx,Chxx,xxxxxxxxxxxxn@hotmail.com,Peking University,No,Rxx,Yxx,xxxxxxxxxxu@gmail.com,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rxx,Yxx,Peking University,,,,,,xxxxxxxxxxu@gmail.com,,,,,China,Assistant professor in Peking University,M.xx Soxxxx;Rxx Yxx,xxxxxxxxxxxxn@hotmail.com;xxxxxxxxxxxu@gmail.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
375,375X-E7A8H4C9H9,CANE: Context-Aware Network Embedding for Relation Modeling,Cunxxxx Tx;Hxx Lxx;Zhixxxx Lxx and Maoxxxx Sx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Accept - Poster Monday,,Undecided (Social Media),"Network embedding (NE) is playing a critical role in network analysis, due to
its ability to represent vertices with efficient low-dimensional embedding
vectors. However, existing NE models aim to learn a fixed context-free
embedding for each vertex and neglect the diverse roles when interacting with
other vertices. In this paper, we assume that one vertex usually shows
different aspects when interacting with different neighbor vertices, and should
own different embeddings respectively. Therefore, we present Context-Aware
Network Embedding (CANE), a novel NE model to address this issue. CANE learns
context-aware embeddings for vertices with mutual attention mechanism and is
expected to model the semantic relationships between vertices more precisely.
In experiments, we compare our model with existing NE models on three
real-world datasets. Experimental results show that CANE achieves significant
improvement than state-of-the-art methods on link prediction and comparable
performance on vertex classification. The source code and datasets can be
obtained from \url{https://github.com/thunlp/CANE}.",22 Apr 2017 07:29:36 GMT,Empirical/Data-Driven,Social media,,Cunxxxx,Tx,xxxxxxxxx@gmail.com,Tsinghua University,No,Hxx,Lxx,xxxxxxxxxxxxxxail.neu.edu.cn,Northeastern University,No,Zhixxxx,Lxx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cunxxxx,Tx,Tsinghua University,,,,,,xxxxxxxxx@gmail.com,,,,,China,,Cunxxxx Tx;Hxx Lxx;Zhixxxx Lxx;Maoxxxx Sxx,xxxxxxxxx@gmail.com;xxxxxxxxxxxxxxmail.neu.edu.cn;xxxxxxxxxxxghua.edu.cn;xxxxxxxxxxhua.edu.cn,CANE: Context-Aware Network Embedding for Relation Modeling,CANE: Context-Aware Network Embedding for Relation Modeling,10,Cunchao Tu,,"Tsinghua University, Haidian District, Beijing, 100084, China",on,on,Only include my submission if it is accepted.,No,None,None
376,376X-G7C9D8B8A4,"Event-based, Recursive Neural Networks for the Extraction and Aggregation of International Alliance Relations",Xaxxxx Tanxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this article, we explore how text mining can help producing reliable,
high-level numerical data for multi-document relation extraction applications.
We combine neural network techniques, information aggregation and visualization
methods to extract event-based relations with a good tolerance to noise. We
describe a search engine based on these methods, identifying the evolution of
alliance and opposition relations between countries.",6 Feb 2017 15:22:17 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP applications;  information extraction;  text mining;  relation/event extraction,Xaxxxx,Tanxxxx,xxxxxxxx@limsi.fr,"LIMSI, CNRS, Univ. Paris-Sud, Université Paris-Saclay",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xaxxxx,Tanxxxx,"Sorbonne Université, Inserm, LIMICS",,,,,,xxxxxxxxxxxxxxxxxxbonne-universite.fr,,,,,France,,Xaxxxx Tanxxxx,xxxxxxxx@limsi.fr,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
377,377X-H4B7E6B4A7,Acquiring Chains of Positive Effects between Entities,Juxxxx Kloxxxxx;Kenxxxx Torxxxxx;Rxx Iixx and Jonxxxxxx O,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this work, we attempt to acquire effect chains, which are instantiated
chains of such binary lexicosyntactic patterns as “A requires B”. An
example is “Pokemon Go requires extra smartphone batteries” ^ “Elecom
Co. sells extra smartphone batteries”. This chain implies a potential
positive effect to Elecom Co. by Pokemon Go in the sense that the latter may
contribute to a boost of battery sales of the former. Actually, many
stockbrokers predicted
such positive effect after the big hit of Pokemon Go and Elecom Co.’s stock
price surged immediately. We propose a method to generate such effect chains
automatically, exploiting the new concept of effect direction. This method can
acquire 65,000, 44,000, and 95,000 chains related to companies, social issues,
and technologies, with a high-precision of 80% using 4 billion Web pages in
Japanese.",7 Feb 2017 11:57:58 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relation discovery;  relation/event extraction,Juxxxx,Kloxxxxx,xxxxxxxxict.go.jp,National Institute of Information and Communications Technology,No,Kenxxxx,Torxxxxx,xxxxxxxxxnict.go.jp,NICT,No,Rxx,Iixx,xxxxxxxxxnict.go.jp,National Institute of Information and Communications Technology,No,Jonxxxxxx,Ox,xxxxxxxxxnict.go.jp,NICT,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Juxxxx,Kloxxxxx,National Institute of Information and Communications Technology,,,,,,xxxxxxxxict.go.jp,,,,,Japan,,Juxxxx Kloxxxxx;Kenxxxx Torxxxxx;Rxx Iixx;Jonxxxxxx Ox,xxxxxxxxict.go.jp;xxxxxxxxxxnict.go.jp;xxxxxxxxxxnict.go.jp;xxxxxxxxxxnict.go.jp,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
378,378X-A3A2E6H3J3,A Continuously Growing Dataset of Sentential Paraphrases,Wuxxx Lxx;Sixx Qxx;Hxx Hx and Wxx X,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"We study the efficacy of obtaining paraphrases from Twitter by linking tweets
that refer to the same URLs. We present a new paraphrase corpus of 44,365
sentence pairs annotated with gold labels. This is the largest annotated
paraphrase corpus to date, several times bigger than the two other existing
sentential paraphrase corpora. We show that this new dataset is also more
unbiased and reliable through one of the first systematic studies of automatic
paraphrase identification models across multiple datasets. Our method also
avoids the topic detection or sentence selection steps used in previous work
and thus can easily be applied to Twitter to extract paraphrases continuously.
Our dataset grows by more than 30,000 new sentential paraphrases per month at
about 80% precision. We will make our data available with the publication of
this paper.",7 Feb 2017 05:19:46 GMT,Resources/Evaluation,Semantics,textual entailment and paraphrasing,Wuxxx,Lxx,xxxxxxx@osu.edu,The Ohio State University,No,Sixx,Qxx,xxxxxxxxgmail.com,University of Pennsylvania,No,Hxx,Hx,xxxxxxx.umd.edu,"University of Maryland, College Park",No,Wxx,Xx,xxxxxxx@osu.edu,The Ohio State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wuxxx,Lxx,The Ohio State University,,,,,,xxxxxxx@osu.edu,,,,,United States,,Wuxxx Lxx;Sixx Qxx;Hxx Hx;Wxx Xx,xxxxxxx@osu.edu;xxxxxxxxxgmail.com;xxxxxxxx.umd.edu;xxxxxxxx@osu.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
379,379X-G3F9C2H8H9,What is the Essence of a Claim? Cross-Domain Claim Identification,Johxxxxx Daxexxxxxxx;Stexxxx Egxx;Ivxx Habxxxxx;Chrxxxxxx Stxx and Irxxx Gurxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Argument mining has become a popular research area in NLP. It typically
includes the identification of argumentative components, e.g. claims, as the
central component of an argument. We perform a qualitative analysis across six
different datasets and show that these appear to conceptualize claims quite
differently. To learn about the consequences of such different
conceptualizations of claim for practical applications, we carried out
extensive experiments using state-of-the-art feature-rich and deep learning
systems, to identify claims in a cross-domain fashion. We show that despite
differences in the perception of claims in different datasets, there are shared
properties on the lexical level as well as system configurations that help to
overcome the gaps in cross-domain scenarios.",6 Feb 2017 22:56:42 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",domain adaptation;  learning with small datasets;  NLP on noisy unstructured text;  experimental evaluation/comparison of ML methods;  text mining;  text classification,Johxxxxx,Daxexxxxxxx,xxxxxxxxxxxxxxxxxxxxxmatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,Stexxxx,Egxx,xxxxxxxxxxxn@gmail.com,"UKP Lab, TU Darmstadt",No,Ivxx,Habxxxxx,xxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,Chrxxxxxx,Stxx,xxxxxxxxxxxxxxxxxik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,Irxxx,Gurxxxxx,xxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Johxxxxx,Daxexxxxxxx,"UKP Lab, Technische Universität Darmstadt",,,4961xxxxxxxxx,,,xxxxxxxxxxxxxxxxxxxxxmatik.tu-darmstadt.de,,,,,Germany,,Johxxxxx Daxexxxxxxx;Stexxxx Egxx;Ivxx Habxxxxx;Chrxxxxxx Stxx;Irxxx Gurxxxxx,xxxxxxxxxxxxxxxxxxxxxmatik.tu-darmstadt.de;xxxxxxxxxxxen@gmail.com;xxxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de;xxxxxxxxxxxxxxxxxxik.tu-darmstadt.de;xxxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
380,380X-J5J8H8P3A7,Real-time Scientific Impact Prediction in Twitter,Xixx Lxx;Zhuxxxxx Lxx and Hexxx Huxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"There is a tremendous unrevealed information about scientiﬁc papers in
Twitter. Many tweets are posted to express excitement of accepted papers in
Twitter and we call them scholarly tweets (ST tweets). Traditional scientiﬁc
impact prediction is based on long time accumulated citation networks, meta
data and the whole text of papers without considering the information in
Twitter. We propose a new approach to predict scientiﬁc impact in Twitter in
real time before the paper content is published. After ﬁltering ST tweets and
extracting Tweet Scholar Blocks indicating meta data of papers to help predict
scientiﬁc impact in real time, author social features, venue popularity
features and title features are exploited to predict whether the paper will
increase h-index of its ﬁrst author after ﬁve years. Our model achieve an
outstanding result that its best accuracy is 80.95%. The best feature
conjunction consists of the sum of friends and followers of all the co-authors,
followers count of the ﬁrst author and title embeddings. And sum of followers
of all the co-authors is the most important feature. Hope real-time scientiﬁc
impact prediction in Twitter can help researchers to expand their inﬂuences
and more conveniently “stand on the shoulders of giants”.",6 Feb 2017 16:10:35 GMT,Applications/Tools,Social media,NLP applications;  NLP in social networking media,Xixx,Lxx,xxxxxxxxxxx@hotmail.com,Beijing Institute of Technology,No,Zhuxxxxx,Lxx,xxxxxxxxxxo@gmail.com,China Defense Science and Technology Information Center,No,Hexxx,Huxxx,xxxxxxxxt.edu.cn,Beijing Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixx,Lxx,Beijing Institute of Technology,,,8618xxxxxxxxx,,,xxxxxxxxxit.edu.cn,,Beijing,Beijing,,China,,Xixx Lxx;Zhuxxxxx Lxx;Hexxx Huxxx,xxxxxxxxxxx@hotmail.com;xxxxxxxxxxxo@gmail.com;xxxxxxxxit.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
381,381X-E7G5F3E6E8,Context-Aware Answer Sentence Selection with Hierarchical Gated Recurrent Neural Networks,Chuxxxx Txx;Fuxx Wxx;Weixxxx Lx and Mixx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we study the task of reading comprehension style answer sentence
selection that aims to select the best sentence from a given passage to answer
a question. Unlike most previous works that match the question and each
candidate sentence separately, we observe that the context information among
sentences in the same passage plays a vital role in this task. We propose
modeling context information with hierarchical gated recurrent neural networks.
Specifically, we first apply a word level recurrent neural network to model the
context independent matching between the question and each candidate sentence.
We then employ a sentence level recurrent neural network to incorporate the
context information among all candidate sentences. Moreover, we introduce the
gate mechanism to select matching information before feeding into recurrent
neural networks at both word and sentence level. Experiments on the WikiQA and
SQuAD datasets show that our model achieves state-of-the-art results.",7 Feb 2017 07:08:20 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",open-domain question answering,Chuxxxx,Txx,xxxxxxxxxxxxxxde.buaa.edu.cn,Beihang University,No,Fuxx,Wxx,xxxxxxxxxrosoft.com,Microsoft Research Asia,No,Weixxxx,Lx,xxxxxxxa.edu.cn,Beihang University,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chuxxxx,Txx,Beihang University,,,,,,xxxxxxxxxxxxxxde.buaa.edu.cn,,,,,China,,Chuxxxx Txx;Fuxx Wxx;Weixxxx Lx;Mixx Zhxx,xxxxxxxxxxxxxxde.buaa.edu.cn;xxxxxxxxxxrosoft.com;xxxxxxxxa.edu.cn;xxxxxxxxxxxicrosoft.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
382,382X-C6P6D7J7P9,Creating Training Corpora for NLG Micro-Planners,Clxxxx Garxxxx;Anaxxxxxx Shixxxxxx;Shxxxx Narxxxx and Laxxx Perezxxxxxxxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Monday,,Undecided (Generation Summarization),"In this paper, we present a novel framework for semi-automatically
creating linguistically challenging micro-planning data-to-text
corpora from existing Knowledge Bases. Because our method pairs data
of varying size and shape with texts ranging from simple clauses to
short texts, a dataset created using this framework provides a
challenging benchmark for microplanning. Another feature of this
framework is that it can be applied to any large scale knowledge base
and can therefore be used to train and learn KB verbalisers.  We apply
our framework to DBpedia data and compare the resulting dataset with
Wen et al. 2016's. We show that while Wen et al.'s dataset is
more than twice larger than ours, it is less diverse both in terms of
input and in terms of text. We thus propose our corpus generation
framework as a novel method for creating challenging data sets from
which NLG models can be learned which are capable of handling the
complex interactions occurring during in micro-planning between
lexicalisation, aggregation, surface realisation, referring expression
generation and sentence segmentation. To encourage researchers to take
up this challenge, we made available a dataset of 21,855 data/text
pairs created using this framework in the context of the
WebNLG shared task.",23 Apr 2017 10:27:17 GMT,Resources/Evaluation,Generation,,Clxxxx,Garxxxx,xxxxxxxxxxxent@loria.fr,"CNRS/LORIA, Nancy",No,Anaxxxxxx,Shixxxxxx,xxxxxxxxxxxxxxorina@loria.fr,CNRS/LORIA Nancy (France),No,Shxxxx,Narxxxx,xxxxxxxxxxxyan@ed.ac.uk,"School of Informatics,The University of Edinburgh",No,Laxxx,Perez-xxxxxxxxxxx,xxxxxxxxxxxed.ed.ac.uk,"School of Informatics,The University of Edinburgh",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Clxxxx,Garxxxx,"CNRS/LORIA, Nancy",,,,,,xxxxxxxxxxxent@loria.fr,,,,,France,,Clxxxx Garxxxx;Anaxxxxxx Shixxxxxx;Shxxxx Narxxxx;Laxxx Perez-xxxxxxxxxxx,xxxxxxxxxxxent@loria.fr;xxxxxxxxxxxxxxmorina@loria.fr;xxxxxxxxxxxxyan@ed.ac.uk;xxxxxxxxxxxeed.ed.ac.uk,Creating Training Corpora for NLG Micro-Planners,Creating Training Corpora for NLG Micro-Planners,10,Claire Gardent,,CNRS/LORIA Nancy,,on,Only include my submission if it is accepted.,No,None,None
383,383X-A3E8D4A3C9,Neural Knowledge Acquisition via Joint Representation of Knowledge Graph and Text,Xx Hxx;Zhixxxx Lxx and Maoxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we propose a novel joint representation learning framework for
knowledge acquisition on two tasks, knowledge graph completion (KGC) and
relation extraction (RE) from text. The joint learning mechanism enables us to
take both knowledge graphs (KGs) and text into consideration and perform better
KGC and RE. In this framework, representations of KGs and text are learned
within a unified semantic space and share parameters via alignments between KGs
and text. Due to the joint mechanism, structural KG features and flexible
textual features are successfully incorporated together to enhance the
representations. In experiments, we evaluate our joint model on relation
extraction, entity prediction and relation prediction, and the results show
that our joint model can significantly and consistently improve the performance
as compared with other baselines.",7 Feb 2017 07:26:19 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  text mining;  relation/event extraction,Xx,Hxx,xxxxxxxxxx3@gmail.com,Tsinghua University,No,Zhixxxx,Lxx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xx,Hxx,Tsinghua University,,,+86-1xxxxxxxxxx,,,xxxxxxxxxx3@gmail.com,,Beijing,Beijing,,China,,Xx Hxx;Zhixxxx Lxx;Maoxxxx Sxx,xxxxxxxxxx3@gmail.com;xxxxxxxxxxxghua.edu.cn;xxxxxxxxxxhua.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
384,384X-D9B4B9A2E9,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,Elxxx Pavxxxx and Maxxxx Paxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"We present a method for populating fine-grained classes (e.g., “1950s
American jazz musicians”) with instances (e.g., Charles Mingus ). While
state-of-the-art methods tend to treat class labels as single lexical units,
the proposed method considers each of the individual modifiers in the class
label relative to the head. An evaluation on the task of reconstructing
Wikipedia category pages demonstrates a >10 point increase in AUC, over a
strong baseline relying on widely-used Hearst patterns.",30 Apr 2017 23:06:56 GMT,Empirical/Data-Driven,Semantics,,Elxxx,Pavxxxx,xxxxxxxxxxxk@gmail.com,University of Pennsylvania,No,Maxxxx,Paxxx,xxxxxxxogle.com,Google Inc.,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Elxxx,Pavxxxx,Brown University,,,,,,xxxxxxxxxbrown.edu,,,PA,,United States,,Elxxx Pavxxxx;Maxxxx Paxxx,xxxxxxxxxxxk@gmail.com;xxxxxxxxogle.com,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,11,Marius Pasca,,"Google Inc.,
1600 Amphitheatre Parkway,
Mountain View, CA 94043",,on,Only include my submission if it is accepted.,No,None,None
385,385X-P6P9A8F4D4,Predicting Native Language from Gaze,Yevxxxx Bexxxx;Chxx Nakxxxxx;Suzxxxx Flxxx and Boxxx Kxx,Multilingual,Omxx Abxxx;Moxx Dixx,Accept - Oral Monday,,Undecided (Multilingual),"A fundamental question in language learning concerns the role of a speaker's
first language in second language acquisition. We present a novel methodology
for studying this question: analysis of eye-movement patterns in second
language reading of free-form text. Using this methodology, we demonstrate for
the first time that the native language of English learners can be predicted
from their gaze fixations when reading English. We provide analysis of
classifier uncertainty and learned features, which indicates that differences
in English reading are likely to be rooted in linguistic divergences across
native languages. The presented framework complements production studies and
offers new ground for advancing research on multilingualism.",29 Jul 2017 15:54:01 GMT,Empirical/Data-Driven,Multilinguality,,Yevxxxx,Bexxxx,xxxxxxxmit.edu,CSAIL MIT,No,Chxx,Nakxxxxx,xxxxxxx@mit.edu,MIT Linguistics,No,Suzxxxx,Flxxx,xxxxxxxmit.edu,MIT Linguistics,No,Boxxx,Kaxx,xxxxxxmit.edu,CSAIL MIT,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yevxxxx,Bexxxx,CSAIL MIT,,,,,,xxxxxxxmit.edu,,,,,United States,,Yevxxxx Bexxxx;Chxx Nakxxxxx;Suzxxxx Flxxx;Boxxx Kaxx,xxxxxxxmit.edu;xxxxxxxx@mit.edu;xxxxxxx@mit.edu;xxxxxxxmit.edu,Predicting Native Language from Gaze,Predicting Native Language from Gaze,11,Yevgeni Berzak,,MIT,on,on,No. Do not include my submission in this dataset.,No,None,None
386,386X-P3E9P2D6F8,Entity Matching via Word Embedding,Anxxxx Schxxxxxx;Edxxxx Drxxxx and Arxxx Mukxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Many data-intensive applications collect data from a variety of (Web) sources.
A key task in this process is entity matching (EM), which is the problem of
determining the records from these sources that refer to the same real-world
entities. Traditional approaches use the record representation of entities to
accomplish this task. With the nascence of social media, entities on the Web
are now accompanied by user generated content. We present a method for EM that
uses this hitherto untapped source of entity information. We use document-based
distances, with an emphasis on word embedding document distances, to determine
if two entities match. We cast the EM problem as an instance of the stable
marriage problem. Experimental results using real-world reviews demonstrate the
high effectiveness of our approach. To our knowledge, this is the first work
exploring the use of user generated content about entities in the EM task.",7 Feb 2017 08:09:52 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  entity disambiguation,Anxxxx,Schxxxxxx,xxxxxxxxxtemple.edu,Temple University,No,Edxxxx,Drxxxx,xxxxxxxxxemple.edu,Temple University,No,Arxxx,Mukxxxxxx,xxxxxxxxx@gmail.com,University of Houston,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxxxx,Schxxxxxx,Temple University,,,,,,xxxxxxxxxtemple.edu,,,,,United States,,Anxxxx Schxxxxxx;Edxxxx Drxxxx;Arxxx Mukxxxxxx,xxxxxxxxxtemple.edu;xxxxxxxxxtemple.edu;xxxxxxxxxx@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
387,387X-E3C2J4H8C7,Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,Abhxxxx Mixxxx;Kuxxxx Dxx and Pusxxxx Bhatxxxxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Oral Monday,,Undecided (Sentiment Analysis Opinion Mining),"Cognitive NLP systems- i.e., NLP systems that make use of behavioral data -
augment traditional text-based features with cognitive features extracted from
eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of
features is typically manual. We contend that manual extraction of features may
not be the best way to tackle text subtleties that characteristically prevail
in complex classification tasks like Sentiment Analysis and Sarcasm Detection,
and that even the extraction and choice of features should be delegated to the
learning system.  We introduce a framework to automatically extract cognitive
features from the eye-movement/gaze data of human readers reading the text and
use them as features along with textual features for the tasks of sentiment
polarity and sarcasm detection. Our proposed framework is based on
Convolutional Neural Network (CNN). The CNN learns features from both gaze and
text and uses them to classify the input text. We test our technique on
published sentiment and sarcasm labeled datasets, enriched with gaze
information, to show that using a combination of automatically learned text and
gaze features often yields better classification performance over (i)  CNN
based systems that rely on text input alone and (ii) existing systems that rely
on handcrafted gaze and textual features.",22 Apr 2017 22:30:50 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Abhxxxx,Mixxxx,xxxxxxxxxxxxx.530@gmail.com,IBM Research India,No,Kuxxxx,Dxx,xxxxxxxxxin.ibm.com,IBM Research India,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,"CSE Department, IIT Bombay",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Abhxxxx,Mixxxx,IBM Research,,,9188xxxxxxxx,,,xxxxxxxxxxxxx.530@gmail.com,,,,,India,,Abhxxxx Mixxxx;Kuxxxx Dxx;Pusxxxx Bhatxxxxxxxxx,xxxxxxxxxxxxx.530@gmail.com;xxxxxxxxxxin.ibm.com;xxxxxxxxxx@gmail.com,Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,11,Abhijit Mishra,,"IBM Research, Bangalore, India",on,on,Only include my submission if it is accepted.,No,None,None
388,388X-A7A9P8C9A2,Universal Semantic Parsing,Sixx Rexxx;Osxxx Täcxxxxxxx;Slxx Pexxxx;Maxx Stexxxxx and Mirxxxx Laxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Universal Dependencies (UD) provides a cross-linguistically uniform syntactic
representation, with the aim of advancing multilingual applications of parsing
and natural language understanding. Reddy et al. (2016) recently developed a
semantic interface for (English) Stanford Dependencies, based on the lambda
calculus.  In this work, we introduce UDepLambda, a similar semantic interface
for UD, which allows mapping natural language to logical forms in an almost
language-independent framework. We evaluate our approach on semantic parsing
for the task of question answering against Freebase.  To facilitate
multilingual evaluation, we provide German and Spanish translations of the
WebQuestions and GraphQuestions datasets. Results show that UDepLambda
outperforms strong baselines across languages and datasets.  For English, it
achieves the strongest result to date on GraphQuestions, with competitive
results on WebQuestions.",7 Feb 2017 01:22:41 GMT,Empirical/Data-Driven,Semantics,grammatical formalisms;  NLP applications;  cross-lingual approaches;  natural language interfaces to databases;  formal semantics and logic;  rule-based/symbolic learning methods;  cross-language question answering;  answer extraction;  semantic relations;  question answering in restricted domains,Sixx,Rexxx,xxxxxxxxxy@ed.ac.uk,University of Edinburgh,No,Osxxx,Täcxxxxxxx,xxxxxxxxxxxxrom@gmail.com,Google,No,Slxx,Pexxxx,xxxxxxxtrovi.de,Google,No,Maxx,Stexxxxx,xxxxxxxxxxnf.ed.ac.uk,University of Edinburgh,No,Mirxxxx,Laxxxx,xxxxxxxx.ed.ac.uk,"School of Informatics, University of Edinburgh",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Sixx,Rexxx,Stanford University,,,+1 66xxxxxxxxxx,,,xxxxxxxxxnford.edu,,Edinburgh,,,United States,,Sixx Rexxx;Osxxx Täcxxxxxxx;Slxx Pexxxx;Maxx Stexxxxx;Mirxxxx Laxxxx,xxxxxxxxxy@ed.ac.uk;xxxxxxxxxxxxxrom@gmail.com;xxxxxxxxtrovi.de;xxxxxxxxxxxnf.ed.ac.uk;xxxxxxxxx.ed.ac.uk,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
389,389X-J5D3A6F9P2,Macro-Events: A Paradigm for Structured Summarization,Anxxxx Hxx;Hanxxxx Lxx;Yixxxx Yaxx and Jaxxx Carxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Event mention extraction has been well studied for more than two decades,
primarily through the lens of the MUC and ACE programs.  However, extracting
sentence-level events fails to offer a clear solution to providing structured
summaries of events mentioned across multiple sentences in  news articles.  We
propose a new framework for event extraction called macro-events that provides
a document-level focus of events, unifying together aspects of information
extraction, question answering, and summarization.  We show empirically that
existing methods are insufficient to solve this problem, and introduce a method
based on the Learning to Search structured prediction framework that can
achieve higher performance on this task.",7 Feb 2017 04:41:45 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  structured input/output;  experimental evaluation/comparison of ML methods;  document summarization;  reinforcement learning;  relation/event extraction,Anxxxx,Hxx,xxxxxxxxxxx0@gmail.com,Carnegie Mellon University,No,Hanxxxx,Lxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Yixxxx,Yaxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Jaxxx,Carxxxxxx,xxxxxxxcmu.edu,CMU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxxxx,Hxx,Carnegie Mellon University,,,,,,xxxxxxxxxxx0@gmail.com,,,PA,,United States,,Anxxxx Hxx;Hanxxxx Lxx;Yixxxx Yaxx;Jaxxx Carxxxxxx,xxxxxxxxxxx0@gmail.com;xxxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu;xxxxxxx.cmu.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
390,390X-A5F9A6E2G6,Estimating rule quality for knowledge base completion with the reduction in coverage assumption,Kaxx Zuxxxx and Jexxx Daxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Currently, there are many large, automatically constructed knowledge bases
(KBs). One interesting task is learning from a KB to generate new knowledge
either in the form of inferred facts or rules that define regularities. One
challenge for learning is that KBs are necessarily open world: we cannot assume
anything about the truth values of facts not included in the KB. From a
learning perspective, this means we lack negative examples. To address this
problem, we propose a novel score function for evaluating the quality of
first-order definite clause learned from a KB. Our metric attempts to include
information about the facts not in the KB when evaluating the quality of a
potential rule. Empirically, we find that our metric results in more precise
predictions than previous approaches.",7 Feb 2017 09:39:01 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",relational Learning;  rule-based/symbolic learning methods;  evaluation metrics;  semantic knowledge induction,Kaxx,Zuxxxx,xxxxxxxxxxxxfri.uni-lj.si,University of Ljubljana,No,Jexxx,Daxxx,xxxxxxxxxxxxxs.kuleuven.be,KU Leuven,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxx,Zuxxxx,University of Ljubljana,,,,,,xxxxxxxxxxxxfri.uni-lj.si,,,,,Slovenia,,Kaxx Zuxxxx;Jexxx Daxxx,xxxxxxxxxxxxfri.uni-lj.si;xxxxxxxxxxxxxcs.kuleuven.be,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
391,391X-H7J3E6J8F7,Conceptual Dependency Networks based Open Information Extraction,Shexxxx Soxx;Yisxxxx Lxx;Qixxx Dx and Roxx L,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"The rapid growth of web and documents makes the efficient information
extraction to construct a knowledge base more urgent, especially in open
domain. Open Information Extraction (OIE), which is especial for the open
domain, is a new challenge on complete extraction of various unstructured texts
contained in webpages and other documents. We propose a novel method,
Conceptual Dependency Networks based Information Extraction (CDNIE), featured
with conceptual dependency networks and multiple order semantic parsing, to
extract the semantic information from unstructured texts. The proposed method
automatically processes the multiple order semantic information from natural
sentences. Semantic information will be converted into relation triples as the
format like <subject, relational phrase, object>. Then conceptual dependency
networks are constructed by using the WordNet and Wikipedia to discover the
hierarchy structure and expand the implicit information. Horizontal expansion
by synonym and vertical expansion by hyponymy greatly increase the number of
knowledge, which is practical beneficial for building knowledge base. We
evaluate the proposed method using three datasets manually labelled with some
extraction from different source. Experimental results show that our method
extracts a large amount of information with a higher precision, achieving 34%
increase on quantity and 15% promotion on precision than the best-performing
alternative method on same datasets.",7 Feb 2017 09:43:55 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction,Shexxxx,Soxx,xxxxxxxxxxdian.edu.cn,"Software Engineering Institute, Xidian University",No,Yisxxxx,Lxx,xxxxxxxxxian.edu.cn,"Software Engineering Institute, Xidian University",No,Qixxx,Dx,xxxxxxxxxxxxxidian.edu.cn,"School of Computer Science and Technology, Xidian University",No,Roxx,Lx,xxxxxxxxxxxxidian.edu.cn,"School of Computer Science and Technology, Xidian University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shexxxx,Soxx,"Software Engineering Institute, Xidian University",,,,,,xxxxxxxxxxdian.edu.cn,,Xi'an,Shaanxi,,China,,Shexxxx Soxx;Yisxxxx Lxx;Qixxx Dx;Roxx Lx and Tecxxxxxxx Xixxxx,xxxxxxxxxxdian.edu.cn;xxxxxxxxxxian.edu.cn;xxxxxxxxxxxxxxidian.edu.cn;xxxxxxxxxxxxxidian.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
392,392X-E2B8C6E6D3,Language Independent Acquisition of Abbreviations,Micxxxx Glxxx;Mxx Faxxxx and Alxxx Glxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper addresses automatic extraction
of abbreviations (encompassing acronyms
and initialisms) and corresponding longform
expansions from text. We create
and are going to release a multilingual resource
for abbreviations and their corresponding
expansions, built automatically
using Wikipedia redirect and disambiguation
pages. We address a shortcoming
of previous work where only the redirect
pages were used, and so every abbreviation
had only a single expansion, even
though multiple different expansions are
possible for many of the abbreviations.
We develop a principled machine learning
based approach to scoring expansion
candidates using different techniques such
as indicators of near synonymy, topical
relatedness, and surface similarity. We
show improved performance over seven
languages, including two with a non-Latin
alphabet, relative to strong baselines.",12 Feb 2017 06:50:10 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  lexical semantics;  lexicon development;  multilingual resources;  term extraction;  NLP on Wikipedia and other collaboratively constructed resources,Micxxxx,Glxxx,xxxxxxxxxxxss@gmail.com,IBM,No,Md. Faxxxxxxxxxxx,Choxxxxxx,xxxxxxxxxxy@gmail.com,IBM T. J. Watson Research Center,No,Alxxx,Glixxxx,xxxxxxxxgmail.com,IBM T.J. Watson Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Micxxxx,Glxxx,IBM,,,,,,xxxxxxxxxxxss@gmail.com,,,,,United States,,Micxxxx Glxxx;Mxx Faxxxx;Alxxx Glixxxx,xxxxxxxxxxxss@gmail.com;xxxxxxxxxxxy@gmail.com;xxxxxxxxxgmail.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
393,393X-B6C3F6B4E6,Neural Character-level Dependency Parsing for Chinese,Haxxxx Lx;Zhixxxx Zhxxx;Yuxx Jx and Hxx Zxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"This paper presents a truly full character-level neural dependency parsing for
Chinese, which has suffered a lot from the dilemma of defining word or not to
model character interactions. Integrating full character-level dependencies
with character embedding and human annotated character-level part-of-speech and
dependency labels for the first time, we show an extra performance enhancement
from the evaluation on Chinese Penn Treebank and SJTU Character Dependency
Treebank and the potential of better understanding deeper structure of Chinese
sentences.",7 Feb 2017 08:27:39 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",part-of-speech tagging;  parsing,Haxxxx,Lx,xxxxxxxxl@163.com,Shanghai Jiao Tong University,No,Zhixxxx,Zhxxx,xxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong University,No,Yuxx,Jx,xxxxxxxx2@qq.com,Shanghai Jiao Tong University,No,Hxx,Zhxx,xxxxxxxxxxxsjtu.edu.cn,Shanghai Jiao Tong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Haxxxx,Lx,Shanghai Jiao Tong University,,,,,,xxxxxxxxl@163.com,,Shanghai,,,China,,Haxxxx Lx;Zhixxxx Zhxxx;Yuxx Jx;Hxx Zhxx,xxxxxxxxl@163.com;xxxxxxxxxxjtu.edu.cn;xxxxxxxx52@qq.com;xxxxxxxxxxx.sjtu.edu.cn,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
394,394X-E5H9E3F4C6,To Compute Syntagmatic Associations by Distributional Semantics,Xiaxxxx Lxx and Dexxx Huxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Two kinds of relations exist between words, syntagmatic and paradigmatic. Word
embedding as a state-of-the-art model of distributional semantics has been used
to discover the paradigmatic relations between words and have been widely used
in natural language processing tasks. Based on a hypothesis that at sentence
level, except for words in paradigmatic relations, two words in certain
syntagmatic relation are more similar than those not in any syntagmatic
relations, we propose to discover words in syntagmatic relations in a sentence
using word embedding based similarity computation. The experiments prove that
word embedding based similarity between words in syntagmatic relations is
higher than that between words not in any syntagmatic relations. And word
em-bedding based measure is competitive to the best measures in literature and
can be a good complement to those measures. This discover can be conducive to
many syntagmatic related natural language processing tasks such as parsing,
text generation, machine translation, collocation extraction and multiword
expression recognition. Further experiment in collocation extraction shows that
the proposed word embedding based association measure is effective in filtering
the noisy collocation candidates at sentence level and it outperforms the
existing well-known association measures in all precision, recall and
F-measure.",6 Feb 2017 16:07:16 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",multiword semantics/compositionality;  NLP applications;  information extraction;  distributional similarity,Xiaxxxx,Lxx,xxxxxxxxxxxxl.dlut.edu.cn,"School of Computer Science and Technology, Dalian University of Technology, Dalian, Liaoning Province, China",No,Dexxx,Huxxx,xxxxxxxxxlut.edu.cn,"School of Computer Science and Technology, Dalian University of Technology, Dalian, Liaoning Province, China",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Lxx,"School of Computer Science and Technology, Dalian University of Technology,",,,,,,xxxxxxxxxxxxl.dlut.edu.cn,,,,,China,,Xiaxxxx Lxx;Dexxx Huxxx and Tecxxxxxxx Daxxxx,xxxxxxxxxxxxl.dlut.edu.cn;xxxxxxxxxxlut.edu.cn,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
395,395X-B6J7B3H3H4,DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations,Guaxxxxx Lxx and Yunxxxxx Chxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"This paper proposes DRL-Sense--a multi-sense word representation learning
model, to address the word sense ambiguity issue, where a sense selection
module and a sense representation module are jointly learned in a reinforcement
learning fashion. A novel reward passing procedure is proposed to enable joint
training on the selection and representation modules. The modular design
implements pure sense-level representation learning with linear time sense
selection (decoding). We further develop a non-parametric learning algorithm
and a sense exploration mechanism for better flexibility and robustness. The
experiments on benchmark data show that the proposed approach achieves the
state-of-the-art performance on contextual word similarities and comparable
performance with Google's word2vec while using much less training data.",7 Feb 2017 12:45:56 GMT,Empirical/Data-Driven,Semantics,unsupervised and semi-supervised learning;  lexical semantics;  reinforcement learning;  word sense induction,Guaxxxxx,Lxx,xxxxxxxxxxxx304@gmail.com,National Taiwan University,No,Yunxxxxx,Chxx,xxxxxxxx@ieee.org,National Taiwan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Guaxxxxx,Lxx,Massachusetts Institute of Technology,,,,,,xxxxxxx@mit.edu,,,,,United States,,Guaxxxxx Lxx;Yunxxxxx Chxx,xxxxxxxxxxxx304@gmail.com;xxxxxxxxx@ieee.org,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
396,396X-P5G9J6B5A4,Streaming Sub-Story Detection with a Cognitive Model,Lexx Derxxxxxxx and Kaxxxx Bonxxxxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Increasingly people are following real world events, e.g. a shooting, through
social media -- a task referred to as story detection. The streaming, high
volume and velocity nature of this data makes this very challenging. Moreover,
there tend to be several different stories on the same event (e.g. the identity
of the perpetrator, their family background), which we refer to as sub-stories
and the corresponding task of their automatic detection -- as sub-story
detection. Current automatic methods (e.g. LSH, oHDP), however, tend to
struggle with the short and noisy text in social streams. This paper
demonstrates how cognitive models of episodic event clustering, coupled with a
neural network, can improve automatic sub-story detection in Twitter streams on
a given event.",7 Feb 2017 11:51:54 GMT,Empirical/Data-Driven,Social media,NLP on noisy unstructured text;  NLP in social networking media;  temporal/spatial information extraction;  document clustering;  social network,Lexx,Derxxxxxxx,xxxxxxxxxxxxxxx@sheffield.ac.uk,University of Sheffield,No,Kaxxxx,Bonxxxxxx,xxxxxxxxxxxxxcs.shef.ac.uk,University of Sheffield,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lexx,Derxxxxxxx,IT University of Copenhagen,,,,,,xxxxxxxxxxxxki@gmail.com,,,,,Denmark,,Lexx Derxxxxxxx;Kaxxxx Bonxxxxxx,xxxxxxxxxxxxxxx@sheffield.ac.uk;xxxxxxxxxxxxxdcs.shef.ac.uk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
397,397X-F6E4C7G4P3,Bridging Neural Machine Translation and Bilingual Dictionaries,Jixxxx Zhxxx and Chexxxxxx Zoxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural Machine Translation (NMT) has become the new state-of-the-art in several
language pairs. However, it remains a challenging problem how to integrate NMT
with a bilingual dictionary which mainly contains words rarely or never seen in
the bilingual training data. In this paper, we propose two methods to bridge
NMT and the bilingual dictionaries. The core idea behind is to design novel
models that transform the bilingual dictionaries into adequate sentence pairs,
so that NMT can distil latent bilingual mappings from the ample and repetitive
phenomena. One method leverages a mixed word/character model and the other
attempts at synthesizing parallel sentences guaranteeing massive occurrence of
the translation lexicon. Extensive experiments demonstrate that the proposed
methods can remarkably improve the translation quality, and most of the rare
words in the test sentences can obtain correct translations if they are covered
by the dictionary.",7 Feb 2017 08:12:59 GMT,Empirical/Data-Driven,Machine translation,MT applications;  hybrid MT;  statistical machine translation,Jixxxx,Zhxxx,xxxxxxxxxxng@ia.ac.cn,Institute of Automation Chinese Academy of Sciences,No,Chexxxxxx,Zoxx,xxxxxxxxxxr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixxxx,Zhxxx,Institute of Automation Chinese Academy of Sciences,,,1342xxxxxxx,,,xxxxxxxxxxxxing@gmail.com,,beijing,beijing,,China,,Jixxxx Zhxxx;Chexxxxxx Zoxx,xxxxxxxxxxng@ia.ac.cn;xxxxxxxxxxpr.ia.ac.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
399,399X-E4A3C6A9B9,Extracting User-Reported Mobile App Defects from Online Reviews,Yxx Waxx;Honxxxxx Waxx and Hxx Fxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"User-generated mobile app reviews have become a gold mine for timely
identifying implementation defects in this type of software artifacts. In this
work, we extract detailed defect descriptions from user reviews at a sentence
level via a hidden structural SVM model. Structured features and constraints
are developed to reduce exhaustive annotation and enable accurate model
estimation on partially annotated review data. Extensive empirical evaluations
on a large collection of mobile app reviews demonstrate the effectiveness of
our proposed solution, especially when only partial annotation is available.",7 Feb 2017 08:23:47 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",unsupervised and semi-supervised learning;  Web mining;  text mining,Yxx,Waxx,xxxxxxxxudel.edu,University of Delaware,No,Honxxxxx,Waxx,xxxxxxxxginia.edu,University of Virginia,No,Hxx,Faxx,xxxxxxxdel.edu,University of Delaware,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yxx,Waxx,University of Delaware,,,,,,xxxxxxxxudel.edu,,,,,United States,,Yxx Waxx;Honxxxxx Waxx;Hxx Faxx,xxxxxxxxudel.edu;xxxxxxxxxginia.edu;xxxxxxxudel.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
400,400X-F6D6D2E5H8,Concept Alignment and Entailment in Educational Systems,Flxxxx Bulxxxxx and Roxxxx Niexxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Existing educational systems are generally focused only on one aspect of the
learning process. Typically, they only offer feedback either to the teacher or
the student, and they require important human interventions when new questions
are added. In this paper, we address these issues by splitting student and
instructor responses into simpler constituents, and analyze them individually,
at a finer-grained level. Such detailed analysis will enable automated
educational systems to become highly scalable, domain-independent and to enrich
the classroom experience. We propose a novel sentence alignment algorithm based
on collapsed typed dependencies. The system's performance is promising,
achieving a 172% better F1-score compared to the best performing baseline.
Second, we propose 2 methods for determining the entailment relation between a
reference response statement and a student answer. Both methods outperform the
baseline and a state-of-the-art system.",7 Feb 2017 11:36:50 GMT,Empirical/Data-Driven,Machine learning,NLP applications;  textual entailment and paraphrasing;  educational applications;  alignment,Flxxxx,Bulxxxxx,xxxxxxxxxxxxov@my.unt.edu,University of North Texas,No,Roxxxx,Niexxxx,xxxxxxxxxxxsen@unt.edu,University of North Texas ; University of Colorado,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Flxxxx,Bulxxxxx,University of North Texas,,,469xxxxxxx,,,xxxxxxxxxxxxov@my.unt.edu,,,,,,,Flxxxx Bulxxxxx;Roxxxx Niexxxx;Unixxxxxxx ox,xxxxxxxxxxxxov@my.unt.edu;xxxxxxxxxxxlsen@unt.edu,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
402,402X-C8D3H8D8D7,Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning,Matxxxx Rixxxx;Elxxx Khaxxxx and Ricxxxx Goxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Although neural networks are well suited for sequential transfer learning
tasks, the catastrophic forgetting problem hinders proper integration of prior
knowledge. In this work, we propose a solution to this problem by using a
multi-task objective based on the idea of distillation and a mechanism that
directly penalizes forgetting at the shared representation layer during the
knowledge integration phase of training. We demonstrate our approach on a
Twitter domain sentiment analysis task with sequential knowledge transfer from
four related tasks. We show that our technique outperforms networks fine-tuned
to the target task. Additionally, we show both through empirical evidence and
examples that it does not forget useful knowledge from the source task that is
forgotten during standard fine-tuning. Our experiments demonstrate the power of
multi-source transfer techniques in practical text analytics problems when
paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A
(Nakov et al., 2016) dataset we surpass the state of the art established during
the competition with a comparatively simple model architecture that is not even
competitive when trained on only the labeled task specific data.",7 Feb 2017 00:55:16 GMT,Theoretical,Machine learning,sentiment analysis;  unsupervised and semi-supervised learning;  domain adaptation;  text classification;  NLP in social networking media;  theoretical aspects of machine learning,Matxxxx,Rixxxx,xxxxxxxxxus.ibm.com,IBM,No,Elxxx,Khaxxxx,xxxxxxxxxus.ibm.com,IBM,No,Ricxxxx,Gooxxxx,xxxxxxxxxus.ibm.com,IBM,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Matxxxx,Rixxxx,IBM,,,,,,xxxxxxxxxus.ibm.com,,,,,United States,,Matxxxx Rixxxx;Elxxx Khaxxxx;Ricxxxx Gooxxxx,xxxxxxxxxus.ibm.com;xxxxxxxxxxus.ibm.com;xxxxxxxxxxus.ibm.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
403,403X-D6G3H4E3P2,Incorporating Causal Transitivity for Medical Causality Mining from Text: A Unified Framework,Senxxxx Zhxx;Bixx Qxx and Tixx Lx,Biomedical,Aurxxxxx Néxxxxx;Kaxxx Verxxxxx,Reject,,Undecided (Biomedical),"Causality in medical text including medical literature, patients' records and
posts in online health community is of great value for discovering new
knowledge, diagnosis assistance, treatment decision-making and etc. Existing
studies frame causal knowledge mining into relation extraction framework which
greatly limits the power of causal transitivity and the intrinsic
characteristics of medical text data, leading to poor quality of causality and
low recall. We formally define the task of medical causality mining and propose
a unified framework which incorporates causal association, causal transitivity
rules and contextual causal similarity. The proposed unified framework is
flexible to obtain those causal pair expressed in complicated sentences even in
multiple sentences. Furthermore, it can determine those causal pairs with low
frequency but obey causal transitivity rules, which is a big advantage compared
to other data-driven approaches.",7 Feb 2017 05:48:19 GMT,Empirical/Data-Driven,Biomedical,biomedical text mining,Senxxxx,Zhxx,xxxxxxxxxxhit.edu.cn,Harbin Institute of Technology,No,Bixx,Qxx,xxxxxxxxxit.edu.cn,Harbin Institute of Technology,No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Senxxxx,Zhxx,Harbin Institute of Technology,,,,,,xxxxxxxxxxhit.edu.cn,,,,,China,,Senxxxx Zhxx;Bixx Qxx;Tixx Lxx,xxxxxxxxxxhit.edu.cn;xxxxxxxxxhit.edu.cn;xxxxxxxxxip.126.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
404,404X-A3C5A2P4B9,Modeling Discrimination Awareness and Predicting Policy Changes,Dixx Yaxx;Scxxx Coxxxx and Edxxxx Hxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"We use Twitter as a sensor to investigate social awareness of discrimination in
the United States. Specifically, we examine demographic patterns in the amount
of discussion of different types of discrimination, as expressed in tweets.
Results show significant differences across geographic, political, gender, and
economic dimensions. To understand how these differences may be manifested in
society, we then show that linguistic features from these tweets can predict
changes in discrimination-related policy at the U.S. state level, and can
predict the passing or failing of legislation with accuracies between 70-85\%
up to three months in advance. Implications for how our analyses can augment
traditional political science techniques and help policy analysts understand
critical social issues from social media are discussed.",7 Feb 2017 09:18:30 GMT,Empirical/Data-Driven,Social media,NLP applications;  text mining;  NLP in social networking media;  social network,Dixx,Yaxx,xxxxxxxx.cmu.edu,Carnegie Mellon University,No,Scxxx,Coxxxx,xxxxxxxxxxrosoft.com,Microsoft Research,No,Edxxxx,Hoxx,xxxxxxmu.edu,CMU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dixx,Yaxx,Carnegie Mellon University,,,,,,xxxxxxxx.cmu.edu,,,AL,,United States,,Dixx Yaxx;Scxxx Coxxxx;Edxxxx Hoxx,xxxxxxxx.cmu.edu;xxxxxxxxxxcrosoft.com;xxxxxxcmu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
405,405X-H4A5D5J4G3,Leveraging Word Embeddings in Detecting Cyberbully,Taxxxx Bxx and Laxxxx Soxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Detecting cyberbullying text is a challenging task due to its wide range of
profane words and irregular grammar. Different methods have been used over the
years, ranging from sentiment analysis to using social network features, for
cyberbullying detection. However, most of these methods are complex and are
usually accompanied by a static list of limited profane words. In this paper,
the concept of transfer learning is applied by training a word embedding model
on an unlabeled corpus and leveraging its features for three different data
transformation methods on a labeled dataset. Evaluation showed that two of our
methods outperform the baseline method. As a result, the classifiers used can
be further reinforced by training on more labeled data or by means of
fine-tuning.",6 Feb 2017 17:49:02 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",sentiment analysis;  text mining;  text classification;  NLP in social networking media;  social network,Taxxxx,Bin Axxxxxxxxxx,xxxxxxxxxxxb@gmail.com,Multimedia University,No,Laxxxx,Soxx,xxxxxxxxmu.edu.my,Multimedia University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Laxxxx,Soxx,Multimedia University,,,,,,xxxxxxxxmu.edu.my,,,,,Malaysia,,Taxxxx Bxx;Laxxxx Soxx,xxxxxxxxxxxb@gmail.com;xxxxxxxxxmu.edu.my,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
407,407X-D8C4J3A8P7,LSTM-Based Pooling for Improved Representation Learning in Non-Factoid Answer Selection,Andxxxx Rüxxxxx and Irxxx Gurxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present LSTM-based pooling, an extension to existing representation
learning models for non-factoid answer selection that is able to focus on the
most important segments of the input in order to create a meaningful
representation. In contrast to recent attention-based models, LSTM-based
pooling does not rely on a dependency between question and candidate
answer. Experimental results show the effectiveness of our approach, which
substantially outperforms several strong baselines and achieves
state-of-the-art results on the non-factoid answer selection datasets
InsuranceQA version 1 and version 2. Furthermore, we show that our proposed
model is also highly effective when applied to factoid answer selection with
considerably shorter answers.",6 Feb 2017 16:57:01 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information retrieval;  open-domain question answering,Andxxxx,Rüxxxxx,xxxxxxxxxxxxxxxxxxxtik.tu-darmstadt.de,"Ubiquitous Knowledge Processing Lab (UKP-TUDA), Department of Computer Science, Technische Universität Darmstadt",No,Irxxx,Gurxxxxx,xxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Andxxxx,Rüxxxxx,"UKP Lab, Technische Universität Darmstadt",,,,,,xxxxxxxxxxxxxxxxxxxtik.tu-darmstadt.de,,,,,Germany,,Andxxxx Rüxxxxx;Irxxx Gurxxxxx,xxxxxxxxxxxxxxxxxxxtik.tu-darmstadt.de;xxxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
408,408X-F3A6J6A5H2,Convolutional Neural Networks with Ranking Loss for Distant Supervised Relation Extraction,Daoxxxx Zexx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In distant supervised relation extraction, it assumes that if two entities have
a relation in a known knowledge base, then all sentences that mention these two
entities will express that relation in some way.
The distant supervision strategy is an effective method for automatically
labeling training data.
It is well known that distant supervision assumption is too strong and causes
the wrong label problem.
In addition, it has class imbalance problem in the automatically labeled
training data and the artificial class NA is very noisy since it groups many
different infrequent relation types.

In this paper, we propose Convolutional Neural Networks (CNNs) with ranking
loss to address distant supervised relation extraction. 
In the framework, a CNN with selective attention over instances is used to
embed the semantics of sentences.
To address the class imbalance problem and the noise in artificial class, we
adopt a pairwise ranking loss function with cost sensitive in the training
stage.
Experiments show that our method is effective and ranking loss function with
cost sensitive can effectively deal with class imbalance problem and the noise
in artificial class.",7 Feb 2017 11:41:32 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",relation/event extraction,Daoxxxx,Zexx,xxxxxxxx6@163.com,Changsha University of Science & Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daoxxxx,Zexx,Changsha University of Science & Technology,,,8618xxxxxxxxx,,,xxxxxxxx6@163.com,,,,,China,,Daoxxxx Zexx,xxxxxxxx6@163.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
409,409X-G6P6D8D3B6,Multi-task Learning using Neural Attention Models,Jxx Niexxxx and Euxxx Cxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"In statistical NLP pipeline, the use of linguistic annotations like
part-of-speech (POS)-tags or Named Entity (NE) tags has been very successful.
In contrast, in end-to-end system the integration of this information is more
complicated.

In this work, we show that multi-task learning is a successful and easy
approach to introduce this additional knowledge into the model. By jointly
training several NLP tasks, we are able to facilitate common information and
improve the quality on the individual task. In the experiments, we analyze the
amount of sharing between the different tasks and investigate the influence of
adaptation towards special tasks.

We perform experiments on the example tasks of machine translation, POS tagging
and NE tagging. Especially in the low resourced condition used in the
experiments, we are able to improve the translation quality by up to 1.5 BLEU
point.        It is notable that the improvements are achieved using a significantly
small out-of-domain data.",7 Feb 2017 08:40:12 GMT,Empirical/Data-Driven,Machine translation,part-of-speech tagging;  named entity recognition;  learning with small datasets;  statistical machine translation,Jxx,Niexxxx,xxxxxxxxxes@kit.edu,Karlsruhe Institute of Technology,No,Euxxx,Cxx,xxxxxxxxo@kit.edu,Karlsruhe Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jxx,Niexxxx,Karlsruhe Institute of Technology,,,,,,xxxxxxxxxes@kit.edu,,,,,Germany,,Jxx Niexxxx;Euxxx Cxx,xxxxxxxxxes@kit.edu;xxxxxxxxxo@kit.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
410,410X-C8G7P3F3G2,Seq2Sense: Harnessing Sequence Learning for Supervised Word Sense Disambiguation,Alexxxxxxx Ragxxxxx;Claxxxx Dexxx and Robxxxx Naxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Word Sense Disambiguation models exist in many flavors. Even though supervised
ones tend to perform best in terms of accuracy, they often lose ground to more
flexible knowledge-based solutions, which do not require training a word expert
for every disambiguation target. To bridge this gap, we adopt a different
perspective and rely on sequence-to-sequence learning to frame the
disambiguation problem. We propose Seq2Sense, a supervised all-words approach
to disambiguation viewed as translation of a sequence of words into a sequence
of potentially sense-tagged tokens. Our extensive evaluation over standard
benchmarks and across different tasks shows that Seq2Sense consistently
achieves results in line with the state of the art, even against word experts
with engineered features.",7 Feb 2017 11:51:01 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  word sense disambiguation,Alexxxxxxx,Ragxxxxx,xxxxxxxxxxx.uniroma1.it,Sapienza University of Rome,No,Claxxxx,Delxxxxxxx,xxxxxxxxxxxx.uniroma1.it,Sapienza University of Rome,No,Robxxxx,Navxxxx,xxxxxxxxxxxuniroma1.it,Sapienza University of Rome,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alexxxxxxx,Ragxxxxx,University of Helsinki,,,,,,xxxxxxxxxxxxxxxnato@helsinki.fi,,,,,Italy,,Alexxxxxxx Ragxxxxx;Claxxxx Dexxx;Robxxxx Navxxxx,xxxxxxxxxxx.uniroma1.it;xxxxxxxxxxxxi.uniroma1.it;xxxxxxxxxxx.uniroma1.it,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
411,411X-D4G6D4D6D3,Assessing Objective Comment Quality through Political Forecasting,Hx Anxxxx;Maxxxx Rouxxxxxxx;Micxxxx Bixxxx;Phxxxx Tetxxxx;Barxxxx Melxxxx and Lyxx Unxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Comments are often rated for their subjective quality, but few researchers have
studied comment quality in terms of objective utility. We explore comment
quality assessment with respect to both subjective (i.e. users’ ratings) and
objective (i.e., did it influence? did it improve decisions?) metrics in a
massive online geopolitical forecasting system, ultimately comparing linguistic
characteristics of each quality metric. Using a variety of features, we predict
all types of quality with better accuracy than the simple yet strong baseline
of comment length. Looking at the most predictive content illustrates rater
biases; for example, forecasters are subjectively biased in favor of comments
mentioning business transactions or dealings as well as material things, even
though such comments do not indeed prove any more useful objectively.
Additionally, more complex sentence constructions, as evidenced by subordinate
conjunctions, are characteristic of comments leading to objective improvements
in forecasting.",7 Feb 2017 00:07:19 GMT,Applications/Tools,Sentiment analysis and opinion mining,NLP applications;  user studies;  opinion mining and extraction;  NLP in social networking media,H. xxxxxx,Schxxxxx,xxxxxxxxxxnybrook.edu,Stony Brook University,No,Maxxxx,Rouxxxxxxx,xxxxxxxxxxh@gmail.com,Stony Brook Univ. / Univ. of Pennsylvania,No,Micxxxx,Bixxxx,xxxxxxxxxxh@gmail.com,University of Pennsylvania,No,Phxxxx,Tetxxxx,xxxxxxxxxxxxton.upenn.edu,University of Pennsylvania,No,Barxxxx,Melxxxx,xxxxxxxxxxxch.upenn.edu,University of Pennsylvania,No,Lyxx,Unxxx,xxxxxxxxx.upenn.edu,University of Pennsylvania,No,,,,,,,,,,,,,,,,,,,,,,H. xxxxxx,Schxxxxx,Stony Brook University,,,,,,xxxxxxxxxxnybrook.edu,,,NY,,United States,"I study large and scalable computational linguistics for health and social sciences. This includes novel natural language processing and machine learning techniques for: 
(1) discovering new links with health and well-being as manifest through language in social media, (2) understanding people and personality, and (3) developing language-based metrics of psychological variables. I also develop algorithms in lexical semantics for word sense disambiguation, concept similarity, and automatic knowledge acquisition from the Web.",Hx Anxxxx;Maxxxx Rouxxxxxxx;Micxxxx Bixxxx;Phxxxx Tetxxxx;Barxxxx Melxxxx;Lyxx Unxxx,xxxxxxxxxxnybrook.edu;xxxxxxxxxxxh@gmail.com;xxxxxxxxxxxh@gmail.com;xxxxxxxxxxxxxton.upenn.edu;xxxxxxxxxxxxch.upenn.edu;xxxxxxxxxx.upenn.edu,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
412,412X-J6D4B6B4C6,Detecting and assessing contextual change in diachronic text documents using context volatility,Chrxxxxxx Kahxxxx;Andxxxx Niexxxx and Gerxxxx Hexx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper we describe and evaluate a novel approach for modeling and
quantifying dynamics in the vocabulary context of diachronic corpora. The
foundation for this work is context volatility, a measurement for contextual
change, that we use as our main  measuring unit. The computation of context
volatility for a word relies on the significance-values of its co-occurrent
terms and the corresponding co-occurrence ranks in sequential time spans.
Besides the original idea for context volatility we show alternative
calculation schemes to quantify the contextual change in order to overcome
problems regarding numerical issues with the data structure. Additionally, we
present an evaluation to justify context volatility. Since we cannot rely on a
widely accepted evaluation dataset, we judge on the models with a synthetic
document set that simulates the contextual changes. Along with the evaluation
we present a real data example to prove the potential of context volatility for
different tasks.",7 Feb 2017 10:31:35 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",discourse;  language generation;  lexical semantics;  text mining;  word sense induction;  term extraction,Chrxxxxxx,Kahxxxx,xxxxxxxxxxxxxxxxik.uni-leipzig.de,Leipzig University,No,Andxxxx,Niexxxx,xxxxxxxxxxxxxxxxxik.uni-leipzig.de,Leipzig University,No,Gerxxxx,Hexxx,xxxxxxxxxxxxxxxk.uni-leipzig.de,University of Leipzig,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Andxxxx,Niexxxx,Leipzig University,,,,,,xxxxxxxxxxxxxxxxxik.uni-leipzig.de,,,,,Germany,,Chrxxxxxx Kahxxxx;Andxxxx Niexxxx;Gerxxxx Hexxx,xxxxxxxxxxxxxxxxik.uni-leipzig.de;xxxxxxxxxxxxxxxxxtik.uni-leipzig.de;xxxxxxxxxxxxxxxxk.uni-leipzig.de,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
413,413X-B6G6C9P6G7,Sequence-to-Sequence Models for Reasoning over Knowledge Graphs,Wenxxxx Yxx;Yadxxxxx Yaghxxxxxxxx and Hinxxxx Schxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Large scale knowledge graphs (KGs) such as Freebase are generally incomplete
and have missing facts.  Reasoning over KGs is thus an important capability
that is needed for question answering or other NLP tasks that require knowledge
about the world.  KG reasoning includes diverse scenarios, e.g., given a head
entity and a relation path, predict the tail entity; or given two entities
connected by some
relation paths, predict the unknown relation between them. We present novel
sequence-to-sequence modeling of KG paths by vector representation of entities
and relations with two benefits: (i) modeling paths of arbitrary lengths while
updating the entity and relation representations by the training signal at each
step; (ii) handling different types of KG reasoning in a unified framework. 
Extensive experiments show that our models are state-of-the-art for Path Query
Answering and Knowledge Base Completion.",7 Feb 2017 10:43:04 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relational Learning;  experimental evaluation/comparison of ML methods;  relation/event extraction;  NLP on Wikipedia and other collaboratively constructed resources,Wenxxxx,Yxx,xxxxxxxxxxxng@gmail.com,University of Munich,No,Yadxxxxx,Yaghxxxxxxxx,xxxxxxxxxcis.lmu.de,"Center for Information and Language Processing, University of Munich",No,Hinxxxx,Schxxxxx,xxxxxxxxxxcislmu.org,"Center for Information and Language Processing, University of Munich",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wenxxxx,Yxx,University of Pennsylvania,,,,,,xxxxxxxxxxxs.upenn.edu,,,,,United States,"Postdoc at UPenn, phd from LMU Munich.",Wenxxxx Yxx;Yadxxxxx Yaghxxxxxxxx;Hinxxxx Schxxxxx and Lanxxxxx Proxxxxxxx,xxxxxxxxxxxng@gmail.com;xxxxxxxxxxcis.lmu.de;xxxxxxxxxx@cislmu.org,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
414,414X-J2A8H7J4G3,An Ensemble Based Iterative Transfer Learning Technique for Automatic Short Answer Grading,Shoxxxx Rxx;Himxxxxx Shxxxx and Y Narxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Supervised automatic short answer grading (ASAG) techniques, while have been
demonstrated to be effective, rely significantly on instructor provided
reference answers and also need labeled training data in the form of graded
student answers for every assessment task. To overcome these limitations,  we
introduce in this paper, an ASAG technique with two novel features. First, we
propose an ensemble of a text classifier based on student answers and a
classifier using features derived from various similarity measures with respect
to reference answers. Second, we employ canonical correlation analysis based
transfer learning to build a common feature representation for training the
classifier ensemble for assessment task having \textit{no} labeled data. The
proposed technique outperforms all winning supervised entries on the SciEnts
Bank dataset from the ``Student Response Analysis'' task of SemEval 2013. 
Additionally, we demonstrate generalizability and benefits of the proposed
technique through evaluation on multiple ASAG datasets from different subject
topics and standards.",7 Feb 2017 07:48:49 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",unsupervised and semi-supervised learning;  domain adaptation;  educational applications;  open-domain question answering;  question answering in restricted domains,Shoxxxx,Rxx,xxxxxxxxxxy@xerox.com,Xerox,No,Himanxxxxxxxxxx,Bhxxx,xxxxxxxxxxxu@gmail.com,Xerox Research Center India,No,Y,Narxxxxx,xxxxxxxxxxxsc.ernet.in,Indian Institute of Science,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Himanxxxxxxxxxx,Bhxxx,Xerox Research Center India,,,,,,xxxxxxxxxxxu@gmail.com,,,,,India,,Shoxxxx Rxx;Himxxxxx Shxxxx;Y Narxxxxx,xxxxxxxxxxy@xerox.com;xxxxxxxxxxxhu@gmail.com;xxxxxxxxxxxisc.ernet.in,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
415,415X-C3C2G3B3P3,A Comparative Quality Evaluation of SMT and NMT using Professional Translators,Shxxxx Casxxxxx;Joxx Mooxxxxx;Rixx Senxxxxx;Pixxx Loxxx;Fedxxxxx Gasxxxx;Anxx Wxx;Yoxx Georgxxxxxxxxx;Antxxxx Valxxxx;Vilxxxxxx Soxxxx and Maxxx Gixxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"The use of machine translation (MT) has become widespread since statistical MT
(SMT) established itself as the dominant paradigm. However, there is growing
interest in the research community in the possibilities of neural MT (NMT)
based largely on impressive results in automatic evaluation. This paper reports
on a comparative human evaluation of phrase-based SMT and NMT for four language
pairs, using PET (Post-Editing Tool) to compare educational domain output from
both systems using a variety of metrics, including automatic evaluation, human
rankings of adequacy and fluency, error-type markup, and post-editing effort
(technical and temporal effort). Our results show a preference for NMT in
side-by-side ranking for all language pairs, texts, and segment lengths. In
addition, perceived fluency is improved and annotated errors are fewer in the
NMT output. Results are mixed for perceived accuracy and for errors of
omission, addition, and mistranslation. Despite far fewer segments requiring
post-editing, document-level post-editing performance was not found to have
significantly improved in NMT compared to SMT. This evaluation was conducted as
part of the (anonymised) project, which aims to create a replicable
semi-automated methodology for high-quality machine translation of educational
data.",6 Feb 2017 17:37:47 GMT,Resources/Evaluation,Machine translation,MT evaluations;  automatic MT evaluation metrics;  phrase-based SMT;  human judgments of MT;  MT post-editing,Shxxxx,Casxxxxx,xxxxxxxxxxxxxxx@adaptcentre.ie,Dublin City University,No,Joxx,Mooxxxxx,xxxxxxxxmail.com,Dublin City University,No,Rixx,Senxxxxx,xxxxxxxxxxxch@ed.ac.uk,University of Edinburgh,No,Pixxx,Loxxx,xxxxxxxxxxxxxdaptcentre.ie,Dublin City University,No,Fedxxxxx,Gasxxxx,xxxxxxxxxxxxputing.dcu.ie,Dublin City University,No,Anxx,Wxx,xxxxxxxxxxxaptcentre.ie,Dublin City University,No,Yoxx,Georgxxxxxxxxx,xxxxxxxxxxxxxxxxlou@bydeluxe.com,Deluxe Media,No,Antonxxxxxxxxxx,Micexxxxxxxxx,xxxxxxxxx.unipi.it,The University of Edinburgh,No,Vilxxxxxx,Soxxxx,xxxxxxxxxxhotmail.com,Ionian University,No,Maxxx,Giaxxxx,xxxxxxxxxxxxx@bydeluxe.com,Deluxe Media,No,,Shxxxx,Casxxxxx,Dublin City University,,,+353xxxxxxxxx,,,xxxxxxxxxxxxxxx@adaptcentre.ie,,Dublin,,,Ireland,,Shxxxx Casxxxxx;Joxx Mooxxxxx;Rixx Senxxxxx;Pixxx Loxxx;Fedxxxxx Gasxxxx;Anxx Wxx;Yoxx Georgxxxxxxxxx;Antxxxx Valxxxx;Vilxxxxxx Soxxxx;Maxxx Giaxxxx,xxxxxxxxxxxxxxx@adaptcentre.ie;xxxxxxxxgmail.com;xxxxxxxxxxxich@ed.ac.uk;xxxxxxxxxxxxxadaptcentre.ie;xxxxxxxxxxxxxputing.dcu.ie;xxxxxxxxxxxxaptcentre.ie;xxxxxxxxxxxxxxxxulou@bydeluxe.com;xxxxxxxxxi.unipi.it;xxxxxxxxxxxhotmail.com;xxxxxxxxxxxxxa@bydeluxe.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
416,416X-G7E3P6P3A6,Sentence Complexity: A Comparative Study Humans vs. Machine,Domxxxxxx Bruxxxx;Fexxxx Dellxxxxxxxx;Lorxxxx Dx and Gixxxx Vexxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"This work presents an innovative study addressing linguistic complexity from a
new perspective that compares judgments given by humans on sentence complexity
with the performance of automatic syntactic analysis. To our knowledge, this is
the first study aims at investigating in this comparative perspective which
morpho-syntactic and syntactic features are involved in the assessment of
sentence complexity. All analyses have been carried out on two
typologically--different languages and using two parsers based on two different
parsing strategies.",6 Feb 2017 17:42:17 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,syntax;  parsing,Domxxxxxx,Bruxxxx,xxxxxxxxxxxxxxato@ilc.cnr.it,"Institute of Computational Linguistics ""A. Zampolli"" (ILC-CNR), Pisa",No,Fexxxx,Dellxxxxxxxx,xxxxxxxxxxxxxxetta@ilc.cnr.it,"Institute of Computational Linguistics ""A. Zampolli""",No,Lorxxxx,De xxxxxx,xxxxxxxxxxxxxtei@gmail.com,University of Pisa,No,Gixxxx,Venxxxx,xxxxxxxxxxxxri@ilc.cnr.it,"Istituto di Linguistica Computazionale ""Antonio Zampolli""",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gixxxx,Venxxxx,"Istituto di Linguistica Computazionale ""Antonio Zampolli""",,,,,,xxxxxxxxxxxxri@ilc.cnr.it,,,,,Italy,,Domxxxxxx Bruxxxx;Fexxxx Dellxxxxxxxx;Lorxxxx Dx;Gixxxx Venxxxx,xxxxxxxxxxxxxxato@ilc.cnr.it;xxxxxxxxxxxxxxxetta@ilc.cnr.it;xxxxxxxxxxxxxttei@gmail.com;xxxxxxxxxxxxxri@ilc.cnr.it,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
417,417X-B3G9B8P7F8,"Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts",Chexxxx Txx;Daxxxx Caxx and Noxx Ax,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Accept - Oral Tuesday,,Undecided (Multidisciplinary),"Understanding how ideas relate to each other is a fundamental question in many
domains, ranging from intellectual history to public communication. Because
ideas are naturally embedded in texts, we propose the first framework to
systematically characterize the relations between ideas based on their
occurrence in a corpus of documents, independent of how these ideas are
represented. Combining two statistics—cooccurrence within documents and
prevalence correlation over time—our approach reveals a number of different
ways in which ideas can cooperate and compete. For instance, two ideas can
closely track each other’s prevalence over time, and yet rarely cooccur,
almost like a “cold war” scenario. We observe that pairwise cooccurrence
and prevalence correlation exhibit different distributions. We further
demonstrate that our approach is able to uncover intriguing relations between
ideas through in-depth case studies on news articles and research papers.",22 Apr 2017 17:57:21 GMT,Empirical/Data-Driven,Multidisciplinary,,Chexxxx,Txx,xxxxxxxxxxenhaot.com,University of Washington,No,Daxxxx,Caxx,xxxxxxxxxxew.cmu.edu,Carnegie Mellon University,No,Noaxxxx,Smxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chexxxx,Txx,University of Colorado Boulder,,,,,,xxxxxxxxxxenhaot.com,,,WA,,United States,,Chexxxx Txx;Daxxxx Caxx;Noxx Ax,xxxxxxxxxxenhaot.com;xxxxxxxxxxrew.cmu.edu;xxxxxxxxxxxxxashington.edu,"Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts","Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts",11,CT,,"Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, 98195, USA",on,on,No. Do not include my submission in this dataset.,No,None,None
418,418X-E8A3G2G6A3,Character-Based Text Classification using Top Down Semantic Model for Sentence Representation,Zhexxxxx Wx;Xxx Zhxxx and Daxxxx Dahxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present a model known as TDSM (Top Down Semantic Model) for extracting a
sentence representation that considers both the word-level semantics by
linearly combine the words with attention weights and the sentence-level
semantics with BiLSTM and use it on text classification. We apply the model on
characters and our results show that our model is better than all the other
character-based and word-based convolutional neural network models by
\cite{zhang15} across seven different datasets with only 1\% of their
parameters. We also demonstrate that this model beats the traditional
state-of-the-art TF-IDF on small and polished datasets like news article in
which typically deep learning models surrender.",7 Feb 2017 08:04:18 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",discriminative learning methods;  text classification;  document mining;  semantic knowledge induction,Zhexxxxx,Wx,xxxxxxxxxwu@sap.com,"SAP Innovation Center, Singapore",No,Xxx,Zhxxx,xxxxxxxxg@sap.com,"Nanyang Technological University, Singapore and SAP Innovation Center, Singapore",No,Daxxxx,Dahxxxxxx,xxxxxxxxxer@sap.com,"SAP Innovation Center, Singapore",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhexxxxx,Wx,"SAP Innovation Center, Singapore",,,,,,xxxxxxxxxwu@sap.com,,,,,Singapore,,Zhexxxxx Wx;Xxx Zhxxx;Daxxxx Dahxxxxxx,xxxxxxxxxwu@sap.com;xxxxxxxxxg@sap.com;xxxxxxxxxxer@sap.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
419,419X-A5J9P3E6C7,One-Shot Neural Cross-Lingual Transfer for Paradigm Completion,Katxxxxxx Kaxx;Ryxx Cotxxxxxx and Hinxxxx Schxxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Accept - Poster Tuesday,,,"We present a novel cross-lingual transfer method for paradigm completion, the
task of mapping a lemma to its inflected forms, using a neural encoder-decoder
model, the state of the art for the monolingual task. We use labeled data from
a high-resource language to increase performance on a low-resource language. In
experiments on 21 language pairs from four different language families, we
obtain up to 58% higher accuracy than without transfer and show that even
zero-shot and one-shot learning are possible. We further find that the degree
of language relatedness strongly influences the ability to transfer
morphological knowledge.",20 Apr 2017 17:38:57 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",,Katxxxxxx,Kaxx,xxxxxxxs.lmu.de,LMU Munich,No,Ryxx,Cotxxxxxx,xxxxxxxxxxxxll@gmail.com,Johns Hopkins University,No,Hinxxxx,Schxxxxx,xxxxxxxxxxcislmu.org,"Center for Information and Language Processing, University of Munich",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Katxxxxxx,Kaxx,NYU,,,,,,xxxxxxyu.edu,,,,,United States,,Katxxxxxx Kaxx;Ryxx Cotxxxxxx;Hinxxxx Schxxxxx and Lanxxxxx Proxxxxxxx,xxxxxxxs.lmu.de;xxxxxxxxxxxxell@gmail.com;xxxxxxxxxx@cislmu.org,One-Shot Neural Cross-Lingual Transfer for Paradigm Completion,One-Shot Neural Cross-Lingual Transfer for Paradigm Completion,11,Katharina Kann,PhD student/researcher,"CIS, LMU Munich
Oettingenstraße 67, 80538 Munich, Germany",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
420,420X-C2P4G7P6D8,User-Factor Adaptation for Processing Social Media,Verxxxxx Lyxx;Youxxxxx Sxx;Nirxxxxx Balasxxxxxxxxxx and Hx Anxxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Language use in social media is as diverse as its user base, varying in
demographics, personality, and many other human factors. We propose a general
task of user-factor adaptation --- adapting supervised learning models using a
background of information about the users (in our case, previous tweets). We
explore a standard domain adaptation technique which assumes discrete domains,
an incompatible assumption for user factors. Instead we propose a new
continuous adaptation technique, better suited for real-valued user factors. We
apply this technique with known user factors including age, gender, and
personality traits, as well as language-based latent factors, evaluating over
five tasks: POS tagging, PP-attachment, sentiment analysis, sarcasm detection,
and stance detection. Adaptation provides statistically significant benefits
for 3 of the 5 tasks: up to +1.2 points for PP-attachment, +3.1 points for
sarcasm, and +2.8 points for stance.",7 Feb 2017 09:21:46 GMT,Empirical/Data-Driven,Social media,domain adaptation;  NLP in social networking media,Verxxxxx,Lyxx,xxxxxxxxxxxxonybrook.edu,Stony Brook University,No,Youxxxxx,Sxx,xxxxxxxxxxxxxstonybrook.edu,Stony Brook University,No,Nirxxxxx,Balasxxxxxxxxxx,xxxxxxxxxxxxxtonybrook.edu,Stony Brook University,No,H. xxxxxx,Schxxxxx,xxxxxxxxxxnybrook.edu,Stony Brook University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Verxxxxx,Lyxx,Stony Brook University,,,,,,xxxxxxxxxxxxonybrook.edu,,Stony Brook,NY,,United States,,Verxxxxx Lyxx;Youxxxxx Sxx;Nirxxxxx Balasxxxxxxxxxx;Hx Anxxxx,xxxxxxxxxxxxonybrook.edu;xxxxxxxxxxxxxxstonybrook.edu;xxxxxxxxxxxxxstonybrook.edu;xxxxxxxxxxxnybrook.edu,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
421,421X-D5C6P8B2D3,Reference-Aware Language Models,Zixxxx Yaxx;Phxx Bluxxxx;Chxxx Dyxx and Waxx Lxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"We propose a general class of language models that treat reference as explicit
stochastic latent variables. This architecture allows models to create mentions
of entities and their attributes by accessing external databases (required by,
e.g., dialogue generation and recipe generation) and internal state (required
by, e.g. language models which are aware of coreference). This facilitates the
incorporation of information that can be accessed in predictable locations in
databases or discourse context, even when the targets of the reference may be
rare words. Experiments on three representative applications show our model
variants outperform models based on deterministic attention.",7 Feb 2017 02:55:14 GMT,Applications/Tools,Machine learning,,Zixxxx,Yaxx,xxxxxxxxxs.cmu.edu,CMU,No,Phxx,Bluxxxx,xxxxxxxxxgoogle.com,Google,No,Chxxx,Dyxx,xxxxxxxxogle.com,Google,No,Waxx,Lixx,xxxxxxxxxgoogle.com,Google,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zixxxx,Yaxx,CMU,,,,,,xxxxxxxxxs.cmu.edu,,,,,United States,,Zixxxx Yaxx;Phxx Bluxxxx;Chxxx Dyxx;Waxx Lixx,xxxxxxxxxs.cmu.edu;xxxxxxxxxxgoogle.com;xxxxxxxxoogle.com;xxxxxxxxxxgoogle.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
423,423X-G9B4J8B7J5,Building Wordnet: A Survey,Laxxx Nadxxxxx and Yasxxxxxxx Harxxxxxxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),"Development of first wordnet (Princeton Wordnet) was started in 1985 and it has
gained lots of attention by linguists of all over the world. Mostly all
languages have their own wordnet built using different methods till date. The
available resources like bilingual, monolingual dictionaries, corpora in
respective languages have significant influence on the method used for creation
of wordnet. This survey discusses the methods used for development of different
wordnets which having low resources.",7 Feb 2017 10:22:47 GMT,Survey Papers,Other,cross-lingual approaches;  cross-language information retrieval;  cross-language information extraction;  lexicon development;  ontology development,Laxxx,Nadxxxxx,xxxxxxxxxxxmi@gmail.com,"College of Engineering, Pune",No,Yasxxxxxxx,Harxxxxxxx,xxxxxxxxxcoep.ac.in,Assistant Professor,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Laxxx,Nadxxxxx,"College of Engineering, Pune",,,,,,xxxxxxxxxxxmi@gmail.com,,,,,India,,Laxxx Nadxxxxx;Yasxxxxxxx Harxxxxxxx,xxxxxxxxxxxmi@gmail.com;xxxxxxxxxxcoep.ac.in,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
424,424X-F9B6E9F3F9,Automatic Extraction of Typological Linguistic Features from Descriptive Grammars,Shaxxxx Muxxxx;Laxx Boxxx;Anxx Saxxxx and Haxxxx Hammxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"The present paper describes experiments on automatically extracting
typological linguistic features of natural languages from traditional
written descriptive grammars. The feature-extraction task has high
potential value in typological, genealogical, historical, and other
related areas of linguistics that make use of databases of structural
features of languages. Until now, the task of extracting such features
from grammars has been done manually, which is highly time and labor
consuming and becomes prohibitive when extended to the thousands of
languages for which linguistic descriptions are available. The system
we describe here starts from semantically parsed text over which a set
of rules are applied to extract feature values. We evaluate the
system's performance on the manually curated Grambank database as the
gold standard and report the first measures of precision and recall
for this problem.",6 Feb 2017 19:43:37 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  information extraction;  answer extraction,Shafqxxxxxxxxx,Vixx,xxxxxxxxxxxt@gmail.com,"University of Gothenburg, Sweden",No,Laxx,Boxxx,xxxxxxxxxxxxvenska.gu.se,"Språkbanken, University of Gothenburg",No,Anxx,Saxxxx,xxxxxxxxxxxxlingfil.uu.se,"Department of Linguistics and Philology, Uppsala University, Uppsala Sweden",No,Haxxxx,Hammxxxxxxxx,xxxxxxxxxxxxxxxxom@lingfil.uu.se,"Department of Linguistics and Philology, Uppsala University, Uppsala Sweden",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shafqxxxxxxxxx,Vixx,"University of Gothenburg, Sweden",,,,,,xxxxxxxxxxxt@gmail.com,,,,,Sweden,,Shaxxxx Muxxxx;Laxx Boxxx;Anxx Saxxxx;Haxxxx Hammxxxxxxxx and Phixxxxxx Uppxxxx,xxxxxxxxxxxt@gmail.com;xxxxxxxxxxxxsvenska.gu.se;xxxxxxxxxxxxxlingfil.uu.se;xxxxxxxxxxxxxxxxrom@lingfil.uu.se,,,,,,,,,Only include my submission if it is accepted.,No,None,None
426,426X-E6A3J3E7G4,A Quadratic Assignment Decipherment and Graduated Assignment Solution for Ontology Matching,Kexxxxx Waxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Ontology matching aims at seizing semantically similar correspondences between
different ontologies. Once ontology matching problem is successfully solved,
the heterogeneity problem of large-scale ontologies from multiple sources can
be well handled. The problem that we deal with in this paper is an ontology
matching problem that includes the one-versus-one matching constraint.
Different from previous work, we propose a generalized quadratic assignment
decipherment for ontology matching and assign the compatibility matrix in a
customizable way to include both local and global information of ontologies.
Besides, we refine the original graduated assignment algorithm with
personalized disturbance and matrix padding to solve the quadratic assignment
problem. Experiments on Ontology Alignment Evaluation Initiative datasets have
proved our method's competitive performance when compared with the
state-of-the-art systems, even though additional resources are not incorporated
into our system. Meanwhile, the formulation as a quadratic assignment problem
provides ontology matching with the ready-made algorithms rooted in
optimization theory besides graduated assignment.",10 Feb 2017 03:52:37 GMT,Applications/Tools,Semantics,graph-based algorithms;  ILP-based approaches (ILP=Integer Linear Programming);  ontological semantics;  semantic knowledge induction,Kexxxxx,Waxx,xxxxxxx.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kexxxxx,Waxx,Peking University,,,,,,xxxxxxx.edu.cn,,,,,China,,Kexxxxx Waxx,xxxxxxx.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
427,427X-D5C6C3F7C9,Authorship Attribution with Topically Biased Training Data,Magxxxxxx Janxxxxxx;Evaxxxxxx Mixxxx and Vlxxx Kexxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Authorship attribution is a task of identifying an author of a written document
given samples of writing of candidate authors. Traditionally it was studied in
an intra-topic setting, when all documents are on the same topic; recently it
was also analyzed in a cross-topic setting, when the known samples are on a
different topic than the questioned document. We investigate the effect of a
special cross-topic setting, namely the situation when writing samples for some
authors are more topically similar to the questioned document than samples from
other authors. We show that classifiers are biased towards attributing a text
to a candidate with training data on the same topic. We present a method of
removing characteristic topical n-grams from a feature set of a classifier in
order to lessen the classifiers' bias.",7 Feb 2017 02:12:52 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",scalability/efficiency of ML methods;  domain adaptation;  text mining;  text classification;  NLP in the blogosphere;  document mining,Magxxxxxx,Janxxxxxx,xxxxxxxxxcs.dal.ca,Dalhousie University,No,Evaxxxxxx,Mixxxx,xxxxxx.dal.ca,Dalhousie University,No,Vlxxx,Kexxxx,xxxxxxxs.dal.ca,Dalhousie University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Magxxxxxx,Janxxxxxx,Dalhousie University,,,,,,xxxxxxxxxcs.dal.ca,,,,,Canada,,Magxxxxxx Janxxxxxx;Evaxxxxxx Mixxxx;Vlxxx Kexxxx,xxxxxxxxxcs.dal.ca;xxxxxxx.dal.ca;xxxxxxxxs.dal.ca,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
428,428X-H2A3D2F4E9,Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization,Maxxxx Peyxxxx and Juxxxx Ecklxxxxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Tuesday,,Undecided (Generation Summarization),"We present a  new supervised framework that learns to estimate automatic
Pyramid scores and uses them for optimization-based extractive multi-document
summarization. For learning automatic Pyramid scores, we developed a method for
automatic training data generation which is based on a genetic algorithm using
automatic Pyramid as the fitness function. Our experimental evaluation shows
that  our new framework significantly outperforms strong baselines regarding
automatic Pyramid, and that there is much room for improvement in comparison
with the upper-bound for automatic Pyramid.",22 Apr 2017 18:24:25 GMT,Empirical/Data-Driven,Summarization,,Maxxxx,Peyxxxx,xxxxxxxxxxxxxxxtu-darmstadt.de,Technische Universität Darmstadt,No,Juxxxx,Ecklxxxxxxxx,xxxxxxxxxxxxxxxxxxxxxrmatik.tu-darmstadt.de,Technische Universität Darmstadt,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,Peyxxxx,Technische Universität Darmstadt,,,,,,xxxxxxxxxxxxxxxtu-darmstadt.de,,,,,Germany,,Maxxxx Peyxxxx;Juxxxx Ecklxxxxxxxx,xxxxxxxxxxxxxxxtu-darmstadt.de;xxxxxxxxxxxxxxxxxxxxxxrmatik.tu-darmstadt.de,Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization,Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization,11,Maxime Peyrard,,"Ubiquitous Knowledge Processing Lab
Hochschulstraße 10 
64289 Darmstadt
Germany",on,on,No. Do not include my submission in this dataset.,No,None,None
429,429X-C3D3C4P9P7,Affect-LM: A Neural Language Model for Customizable Affective Text Generation,Saxxx Ghxxx;Matxxxx Choxxxx;Euxxxx Lakxxxx;Louisxxxxxxxxx Morxxxx and Stxxxx Scxxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Monday,,Undecided (Dialog Interactive Systems),"Human verbal communication includes affective messages which are conveyed
through use of emotionally colored words. There has been a lot of research
effort in this direction but the problem of integrating state-of-the-art neural
language models with affective information remains an area ripe for
exploration. In this paper, we propose an extension to an LSTM (Long Short-Term
Memory) language model for generation of conversational text, conditioned on
affect categories. Our proposed model, Affect-LM enables us to customize the
degree of emotional content in generated sentences through an additional design
parameter. Perception studies conducted using Amazon Mechanical Turk show that
Affect-LM can generate naturally looking emotional sentences without
sacrificing grammatical correctness. Affect-LM also learns
affect-discriminative word representations, and perplexity experiments show
that additional affective information in conversational text can improve
language model prediction.",23 Apr 2017 06:45:42 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Saxxx,Ghxxx,xxxxxxxxxt.usc.edu,USC Institute for Creative Technologies,No,Matxxxx,Choxxxx,xxxxxxxxxxct.usc.edu,USC Institute for Creative Technologies,No,Euxxxx,Lakxxxx,xxxxxxxxxxct.usc.edu,USC Instute for Creative Technologies,No,Louisxxxxxxxxx,Morxxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Stxxxx,Schxxxx,xxxxxxxxxct.usc.edu,University of Southern California,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Saxxx,Ghxxx,USC Institute for Creative Technologies,,,,,,xxxxxxxxxt.usc.edu,,,,,United States,,Saxxx Ghxxx;Matxxxx Choxxxx;Euxxxx Lakxxxx;Louisxxxxxxxxx Morxxxx;Stxxxx Schxxxx,xxxxxxxxxt.usc.edu;xxxxxxxxxxict.usc.edu;xxxxxxxxxxict.usc.edu;xxxxxxxxxcs.cmu.edu;xxxxxxxxxxct.usc.edu,Affect-LM: A Neural Language Model for Customizable Affective Text Generation,Affect-LM: A Neural Language Model for Customizable Affective Text Generation,9,Sayan Ghosh,,"USC Institute for Creative Technologies
12015 E Waterfront Drive
Playa Vista, Los Angeles
California 90094",on,,Only include my submission if it is accepted.,No,None,None
430,430X-A4F5A6G5D2,Neural Models of Social Media Post Structure for Persuasive Influence Detection,Chrixxxxxxx Hixxx and Kaxxx McKxxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Automatic detection of persuasion in online
discussion is important for understanding
how social media is used. Predicting
persuasiveness is difficult, however,
due to the need to model world
knowledge, dialogue interaction, and discourse
structure. We focus on using the
structure of social media posts to predict
persuasiveness. Using neural models combined
with word embeddings, discourse
relations, and semantic frames, we demonstrate
statistically significant improvement
over prior work in detecting when a persuasive
argument is successful.",7 Feb 2017 08:32:14 GMT,Empirical/Data-Driven,Social media,discourse;  NLP on noisy unstructured text;  semantic relations;  text classification;  NLP in social networking media;  social network,Chrixxxxxxx,Hixxx,xxxxxxxxxlumbia.edu,Columbia University,No,Kaxxx,McKxxxx,xxxxxxxxxxolumbia.edu,Columbia University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chrixxxxxxx,Hixxx,Columbia University,,,978-xxxxxxxx,,,xxxxxxxxxlumbia.edu,,,,,United States,,Chrixxxxxxx Hixxx;Kaxxx McKxxxx,xxxxxxxxxlumbia.edu;xxxxxxxxxxxolumbia.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
431,431X-A5E4P4P6E4,Knowledge Adaptation: Teaching to Adapt,Sebxxxxxx Ruxxx;Paxxx Ghaxxxxx and Joxx Gx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Domain adaptation is crucial in many real-world applications where the
distribution of the training data differs from the distribution of the test
data. Previous Deep Learning-based approaches to domain adaptation need to be
trained jointly on source and target domain data and are therefore unappealing
in scenarios where models need to be adapted to a large number of domains or
where a domain is evolving, e.g. spam detection where attackers continuously
change their tactics.

To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge
Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain
adaptation scenario. We show how a student model achieves state-of-the-art
results on unsupervised domain adaptation from multiple sources on a standard
sentiment analysis benchmark by taking into account the domain-specific
expertise of multiple teachers and the similarities between their domains.

When learning from a single teacher, using domain similarity to gauge
trustworthiness is inadequate. To this end, we propose a simple metric that
correlates well with the teacher's accuracy in the target domain. We
demonstrate that incorporating high-confidence examples selected by this metric
enables the student model to achieve state-of-the-art performance in the
single-source scenario.",6 Feb 2017 18:24:49 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  domain adaptation;  opinion mining and extraction;  text classification,Sebxxxxxx,Ruxxx,xxxxxxxxxxxxian@gmail.com,"National University of Ireland, Galway",No,Paxxx,Ghaxxxxx,xxxxxxxxlien.com,"Aylien, Ltd.",No,Johxxxx,Brexxxx,xxxxxxxxxxxxxxxsight-centre.org,Insight Centre for Data Analytics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sebxxxxxx,Ruxxx,"National University of Ireland, Galway",,,,,,xxxxxxxxxxxxian@gmail.com,,,,,Ireland,,Sebxxxxxx Ruxxx;Paxxx Ghaxxxxx;Joxx Gx,xxxxxxxxxxxxian@gmail.com;xxxxxxxxylien.com;xxxxxxxxxxxxxxxxsight-centre.org,,,,,,,,,Only include my submission if it is accepted.,No,None,None
432,432X-D4C2B3C8J8,Exploring Deep Neural Networks for Multi-Target Stance Detection,Parxxxx Sobxxxx;Xiaxxxx Zxx and Dixxx Inxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Detecting subjectivity expressed towards concerned targets is an interesting
problem and has received intensive study. Previous work often treated each
target independently, ignoring the potential (sometimes very strong) dependency
that could exist among targets (e.g., the subjectivity expressed towards two
products or two political candidates in an election). 

In this paper, we relieve such an independence assumption in order to jointly
model the subjectivity expressed towards multiple targets. We propose and show
that an attention-based encoder-decoder framework is very effective for this
problem, outperforming several alternatives that jointly learn dependent
subjectivity through cascading classification or multi-task learning, as well
as models that independently predict subjectivity towards individual targets.
We will make available our dataset to facilitate future research on
multi-target subjectivity detection.",6 Feb 2017 18:25:58 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,corpus development;  sentiment analysis;  subjectivity analysis;  text classification;  NLP in social networking media,Parxxxx,Sobxxxx,xxxxxxxxxuottawa.ca,"School of Electrical Engineering and Computer Science, University of Ottawa",No,Xiaxxxx,Zxx,xxxxxxxxgmail.com,National Research Council Canada,No,Dixxx,Inxxxx,xxxxxxxxxx.uottawa.ca,University of Ottawa,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Parxxxx,Sobxxxx,"School of Electrical Engineering and Computer Science, University of Ottawa",,,,,,xxxxxxxxxuottawa.ca,,,,,Canada,,Parxxxx Sobxxxx;Xiaxxxx Zxx;Dixxx Inxxxx,xxxxxxxxxuottawa.ca;xxxxxxxxxgmail.com;xxxxxxxxxxx.uottawa.ca,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
433,433X-B6J3E4P9B3,Universal Dependencies Parsing for Colloquial Singaporean English,Honxxxx Waxx;Yxx Zhxxx;Guaxxxxxx Leoxxxx;Jxx Yaxx and Hxx Lexxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Poster Monday,,Undecided (Tagging Chunking Syntax Parsing),"Singlish can be interesting to the ACL community both linguistically as a major
creole based on English, and computationally for information extraction and
sentiment analysis of regional social media. We investigate dependency parsing
of Singlish by constructing a dependency treebank under the Universal
Dependencies scheme, and then training a neural network model by integrating
English syntactic knowledge into a state-of-the-art parser trained on the
Singlish treebank. Results show that English knowledge can lead to 25% relative
error reduction, resulting in a parser of 84.47% accuracies. To the best of our
knowledge, we are the first to use neural stacking to improve cross-lingual
dependency parsing on low-resource languages. We make both our annotation and
parser available for further research.",23 Apr 2017 04:36:05 GMT,Resources/Evaluation,"Tagging, chunking, syntax, and parsing",,Honxxxx,Waxx,xxxxxxxxxx@gmail.com,Singapore University of Design and Technology,No,Yxx,Zhxxx,xxxxxxxxxxsutd.edu.sg,Singapore University of Technology and Design,No,GuangYxxxxxxxxxxx,Chxx,xxxxxxxxxdso.org.sg,DSO National Laboratories,No,Jxx,Yaxx,xxxxxxxxxxxxxil.sutd.edu.sg,Singapore University of Technology and Design,No,Haixxxxxx,Chxxx,xxxxxxxxxdso.org.sg,DSO National Laboratories,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Honxxxx,Waxx,University of California Santa Barbara,,,,,,xxxxxxxxxx@gmail.com,,,,,United States,,Honxxxx Waxx;Yxx Zhxxx;Guaxxxxxx Leoxxxx;Jxx Yaxx;Hxx Lexxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxxsutd.edu.sg;xxxxxxxxxxdso.org.sg;xxxxxxxxxxxxxxil.sutd.edu.sg;xxxxxxxxxxdso.org.sg,Universal Dependencies Parsing for Colloquial Singaporean English,Universal Dependencies Parsing for Colloquial Singaporean English,13,HONGMIN WANG,,"Hongmin Wang, Singapore University of Technology and Design, 8 Somapah Rd, Singapore 487372",on,on,Only include my submission if it is accepted.,No,None,None
434,434X-D9F7D3B9B7,Dynamic Conditional Decoder for Response Generation,Qixxxx Zxx;Weixxxx Zhxxx and Tixx Lx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"External keywords information is crucial for response generation models to
address the vague output problem, such as ``Me, too.'', ``I don't konw.''.
Previous single keyword models are not adapted for generating complicated and
variable-length response.
Existing multi-keyword models typically focus on the composite representation
of external keywords, which indeed leads to the loss of semantic information of
the original keywords.
To address these issues, in this paper, we propose a novel decoding approach to
fully adopt the semantic information of multi-keyword. Concretely, dynamic
number keywords are conditioned to the hidden state of the decoder to supervise
the response generation process. A unified keyword gating function is thus
proposed to jointly model the encoder, hidden state of the decoder and the
keywords information. 
Experimental results show that the proposed approach outperforms the
state-of-the-art models in both the automatic and human evaluations.",7 Feb 2017 11:59:00 GMT,Empirical/Data-Driven,Dialog and interactive systems,language generation;  dialogue,Qixxxx,Zxx,xxxxxxxxxhit.edu.cn,Harbin Institute of Technology,No,Weixxxx,Zhxxx,xxxxxxxxxx.hit.edu.cn,Harbin Institute of Technology,No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixxxx,Zxx,Harbin Institute of Technology,,,,,,xxxxxxxxxhit.edu.cn,,,,,China,,Qixxxx Zxx;Weixxxx Zhxxx;Tixx Lxx,xxxxxxxxxhit.edu.cn;xxxxxxxxxxx.hit.edu.cn;xxxxxxxxxip.126.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
435,435X-D3P3J6J6E6,Neural Disambiguation of Causal Lexical Markers based on Context,Eugxxxx Martíxxxxxxxxxxx;Vexxx Shwxxxx;Irxxx Gurxxxxx and Ixx Daxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Causation is a psychological tool of humans to understand the world and it is
projected in natural language. Causation relates two events, so in order to
understand the causal relation of those events and the causal reasoning of
humans, the study of causality classification is required. Herein, we propose a
neural network architecture for the task of causality classification. We claim
that the encoding of the meaning of a sentence is required for the
disambiguation of its causal meaning. Our results show that our claim holds,
and we outperform the state-of-the-art.",6 Feb 2017 18:39:44 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse;  semantic relations;  text classification,Eugxxxx,Martíxxxxxxxxxxx,xxxxxxxx@ujaen.es,Technische Universität Darmstadt,No,Vexxx,Shwxxxx,xxxxxxxxx@gmail.com,Bar-Ilan University,No,Irxxx,Gurxxxxx,xxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,Ixx,Daxxx,xxxxxxxxxbiu.ac.il,Bar-Ilan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Eugxxxx,Martíxxxxxxxxxxx,Universidad de Granada,,,3495xxxxxxx,,,xxxxxxxx@ujaen.es,,Darmstadt,Hesse,,Spain,"Sentiment Analysis,
Deep Learning,
Causal language classification,
Knowledge representation",Eugxxxx Martíxxxxxxxxxxx;Vexxx Shwxxxx;Irxxx Gurxxxxx;Ixx Daxxx,xxxxxxxx@ujaen.es;xxxxxxxxxx@gmail.com;xxxxxxxxxxxxxxxxxxxxatik.tu-darmstadt.de;xxxxxxxxx.biu.ac.il,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
436,436X-E6J7B2G6F5,WiNER: A Wikipedia annotated corpus for Named Entity Recognition,Abxxx Ghaxxxx and Phixxxxxx Lanxxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"We revisit the idea of mining Wikipedia in order to generate automatically
named entity annotations. We propose a new methodology that we applied to
English Wikipedia to build WiNER, a large, high quality, annotated corpus. We
evaluate its usefulness on 6 NER tasks, comparing 4 state-of-the art popular
approaches. We show that LSTM-CRF is the approach that benefits the most from
our corpus. We report impressive gains with this model when using a small
portion of WiNER on top the CONLL training material. Last, we propose a simple
but efficient method for exploiting the full range of WiNER, leading to further
improvements.",7 Feb 2017 02:53:53 GMT,Empirical/Data-Driven,Resources and evaluation,corpus development;  named entity recognition;  NLP on Wikipedia and other collaboratively constructed resources,Abxxx,Ghaxxxx,xxxxxxxxxxxxx@umontreal.ca,Université de Montréal,No,Phixxxxxx,Lanxxxxx,xxxxxxxxxxxumontreal.ca,Université de Montréal,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Abxxx,Ghaxxxx,Université de Montréal,,,514xxxxxxx,,,xxxxxxxxxxxxx@umontreal.ca,,,,,Canada,,Abxxx Ghaxxxx;Phixxxxxx Lanxxxxx,xxxxxxxxxxxxx@umontreal.ca;xxxxxxxxxxxxumontreal.ca,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
437,437X-F3C4H3E6H3,Incorporating Global Visual Features into Attention-Based Neural Machine Translation,Iaxxx Calxxxx;Qxx Lxx and Nixx Camxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"We introduce multi-modal, attention-based neural machine translation (NMT)
models which incorporate visual features into different parts of both the
encoder and the decoder. We utilise global image features extracted using a
pre-trained convolutional neural network and incorporate them (i) as words in
the source sentence, (ii) to initialise the encoder hidden state, and (iii) as
additional data to initialise the decoder hidden state. In our experiments, we
evaluate how these different strategies to incorporate global image features
compare and which ones perform best. We also study the impact that adding
synthetic multi-modal, multilingual data brings and find that the additional
data have a positive impact on multi-modal models. We report new
state-of-the-art results and our best models also significantly improve on a
comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k
data set according to all metrics evaluated. To the best of our knowledge, it
is the first time a purely neural model significantly improves over a PBSMT
model on all metrics evaluated on this data set.",6 Feb 2017 18:54:02 GMT,Empirical/Data-Driven,Machine translation,MT applications;  hybrid MT;  learning with small datasets;  statistical machine translation;  multimodal representations and processing,Iaxxx,Calxxxx,xxxxxxxxxxxer@gmail.com,Dublin City University,No,Qxx,Lxx,xxxxxxx@dcu.ie,Dublin City University,No,Nixx,Camxxxxx,xxxxxtcd.ie,Trinity College Dublin,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Iaxxx,Calxxxx,University of Amsterdam,,,,,,xxxxxxxxxxxer@gmail.com,,,,,Netherlands,,Iaxxx Calxxxx;Qxx Lxx;Nixx Camxxxxx,xxxxxxxxxxxer@gmail.com;xxxxxxxu@dcu.ie;xxxxxxtcd.ie,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
438,438X-C8A6C8A4P5,Perspective Identification in Informal Arabic Text,Hexx Elfxxxx and Moxx Dixx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"In this paper we describe our work on identifying the ideological perspective
from which a given Egyptian discussion forum comment was written. We base our
work on the hypothesis that the most two important elements governing
perspective in the Egyptian community are the role religion/Islam should play
in politics and one's position on political reform versus stability. 
We build computational systems that try to uncover these two elements.
Specifically, we identify a comment’s priority as well as its stance on
different political entities governed by these two elements.
We explore the use of standard ngrams–with and without morphological
preprocessing, code-switching between Dialectal and Standard Arabic, sentiment
and latent semantics as features within a logistic regression model. The best
setup significantly beats the F-score of the ngram baseline by 9.2% and 10.5%
on the tuning and held-out test sets, respectively.",7 Feb 2017 01:38:00 GMT,Empirical/Data-Driven,Social media,NLP on noisy unstructured text;  code-switching;  NLP in social networking media;  NLP in the blogosphere;  pragmatics;  NLP for Web 2.0,Hexx,Elfxxxx,xxxxxxxxxxlumbia.edu,Columbia University,No,Moxx,Dixx,xxxxxxxgwu.edu,GWU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hexx,Elfxxxx,Amazon,,,,,,xxxxxxxxxxlumbia.edu,,,,,United States,,Hexx Elfxxxx;Moxx Dixx,xxxxxxxxxxlumbia.edu;xxxxxxx@gwu.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
439,439X-G5F8A5H3D3,Learning Semantic Correspondences in Technical Documentation,Kyxx Ricxxxxxxx and Joxxx Kuxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"We consider the problem of translating high-level textual descriptions to
formal representations in technical documentation as part of an effort to model
the meaning of such documentation.  We focus specifically on the problem of
learning translational correspondences between text descriptions and grounded
representations in the target documentation, such as formal representation of
functions or code templates.  Our approach exploits the parallel nature of such
documentation, or the tight coupling between high-level text and the low-level
representations we aim to learn. Data is collected by mining technical
documents for such parallel text-representation pairs, which we use to train a
simple semantic parsing model. We report new baseline results on sixteen novel
datasets, including the standard library documentation for nine popular
programming languages across seven natural languages, and a small collection of
Unix utility manuals.",22 Apr 2017 16:55:29 GMT,Empirical/Data-Driven,Semantics,,Kyxx,Ricxxxxxxx,xxxxxxxxxxxx-stuttgart.de,University of Stuttgart,No,Joxxx,Kuxx,xxxxxxxxxxxxxxxuni-stuttgart.de,University of Stuttgart,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kyxx,Ricxxxxxxx,University of Stuttgart,,,,,,xxxxxxxxxxxx-stuttgart.de,,,,,Germany,,Kyxx Ricxxxxxxx;Joxxx Kuxx,xxxxxxxxxxxx-stuttgart.de;xxxxxxxxxxxxxxxxuni-stuttgart.de,Learning Semantic Correspondences in Technical Documentation,Learning Semantic Correspondences in Technical Documentation,11,Kyle Richardson,PhD Student,"University of Stuttgart Institute 
for Machine Language Processing 
Pfaffenwaldring 5
D-70569 Stuttgart, Germany",on,on,No. Do not include my submission in this dataset.,No,None,None
440,440X-A3E7D4C3J8,A* CCG Parsing with a Supertag and Dependency Factored Model,Masxxxx Yosxxxxxx;Hirxxxx Noxx and Yuxx Matxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Monday,,Undecided (Tagging Chunking Syntax Parsing),"We propose a new A* CCG parsing model in which the probability of a tree is
decomposed into factors of CCG categories and its syntactic dependencies both
defined on bi-directional LSTMs. Our factored model allows the precomputation
of all probabilities and runs very efficiently, while modeling sentence
structures explicitly via dependencies. Our model achieves the state-of-the-art
results on English and Japanese CCG parsing.",23 Apr 2017 10:12:53 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Masxxxx,Yosxxxxxx,xxxxxxxxxxxxxxxxi.yh8@is.naist.jp,Nara Institute of Science and Technology,No,Hirxxxx,Noxx,xxxxxxxxnaist.jp,Nara Institute of Science and Technology,No,Yuxx,Matxxxxxx,xxxxxxxx.naist.jp,Nara Institute of Science and Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Masxxxx,Yosxxxxxx,Nara Institute of Science and Technology,,,,,,xxxxxxxxxxxxxxxxi.yh8@is.naist.jp,,,,,Japan,,Masxxxx Yosxxxxxx;Hirxxxx Noxx;Yuxx Matxxxxxx and  Tecxxxxxxxx,xxxxxxxxxxxxxxxxi.yh8@is.naist.jp;xxxxxxxx.naist.jp;xxxxxxxxx.naist.jp,A* CCG Parsing with a Supertag and Dependency Factored Model,A* CCG Parsing with a Supertag and Dependency Factored Model,11,Masashi Yoshikawa,,"Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara, 630-0192, Japan",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
442,442X-P2D6G8H6D7,A Part-of-Speech Tagset for Albanian,Bexxx Kabxxxx and Thxxxx Prxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"In this paper we present a medium-sized tagset for Albanian that can adequately
represent the syntagmatic aspects of the language. We provide mappings both to
the original Google Universal Part-of-Speech Tags and the variant used in the
Universal Dependencies project. We use a manually tagged corpus of 31,527
tokens to evaluate our tagset with three different statistical taggers and get
promising results of up to 94.33% accuracy on the coarser tagsets.",7 Feb 2017 11:36:19 GMT,Resources/Evaluation,"Tagging, chunking, syntax, and parsing",part-of-speech tagging,Bexxx,Kabxxxx,xxxxxxxxxxshi@fau.de,FAU Erlangen-Nürnberg,No,Thxxxx,Prxxxx,xxxxxxxxxxisl@fau.de,FAU Erlangen-Nürnberg,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thxxxx,Prxxxx,FAU Erlangen-Nürnberg,,,,,,xxxxxxxxxxisl@fau.de,,,,,Germany,,Bexxx Kabxxxx;Thxxxx Prxxxx,xxxxxxxxxxshi@fau.de;xxxxxxxxxxoisl@fau.de,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
443,443X-J5B6B8J4F7,Generating Contrastive Referring Expressions,Maxxxx Vilxxxxx;Chrxxxxxx Teixxxxxx and  Axxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Tuesday,,Undecided (Dialog Interactive Systems),"The referring expressions (REs) produced by a natural language generation (NLG)
system can be misunderstood by the hearer, even when they are semantically
correct. In an interactive setting, the NLG system can try to recognize such
misunderstandings and correct them. We present an algorithm for generating
corrective REs that use contrastive focus (“no, the BLUE button”) to
emphasize the information the hearer most likely misunderstood. We show
empirically that these contrastive REs are preferred over REs without contrast
marking.",26 Apr 2017 02:16:26 GMT,Theoretical,Dialog and interactive systems,,Maxxxx,Vilxxxxx,xxxxxxxxxxxxxxuni-saarland.de,Universität des Saarlandes,No,Chrxxxxxx,Teixxxxxx,xxxxxxxxxxxxxooglemail.com,Saarland University,No,Alexxxxxx,Koxxxx,xxxxxxxxxxxxxni-saarland.de,Saarland University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,Vilxxxxx,Universität des Saarlandes,,,,,,xxxxxxxxxxxxxxuni-saarland.de,,Saarbrücken,Saarland,,Germany,,Maxxxx Vilxxxxx;Chrxxxxxx Teixxxxxx; Axxx and ex Koxxxx,xxxxxxxxxxxxxxuni-saarland.de;xxxxxxxxxxxxxgooglemail.com;xxxxxxxxxxxxxxni-saarland.de,Generating Contrastive Referring Expressions,Generating Contrastive Referring Expressions,10,Christoph Teichmann,,"Universität des Saarlandes
Fachrichtung 4.7 Allgemeine Linguistik
Building C 7.2
Postfach 15 11 50
66041 Saarbrücken
Germany",on,on,No. Do not include my submission in this dataset.,No,None,None
444,444X-G2P3C4P9F3,Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting,Pexxx Poxxxx;Alxxxx Romxxxx and Anxx Rumxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Language generation tasks that seek to mimic human ability to use language
creatively are difficult to evaluate, since one must consider creativity,
style, and other non-trivial aspects of the generated text. The goal of this
paper is to develop evaluation methods for one such task, ghostwriting of rap
lyrics, and to provide an explicit, quantifiable foundation for the goals and
future directions of this task. Ghostwriting must produce text that is similar
in style to the emulated artist, yet distinct in content. We develop a novel
evaluation methodology that addresses several complementary aspects of this
task, and illustrate how such evaluation can be used to meaningfully analyze
system performance. We provide a corpus of lyrics for 13 rap artists, annotated
for stylistic similarity, which allows us to assess the feasibility of manual
evaluation for generated verse.",6 Feb 2017 19:07:05 GMT,Resources/Evaluation,Generation,language generation;  evaluation metrics,Pexxx,Poxxxx,xxxxxxxxxs.uml.edu,University of Massachusetts Lowell,No,Alxxxx,Romxxxx,xxxxxxxxxtlook.com,UMass Lowell,No,Anxx,Rumxxxxxx,xxxxxxx.uml.edu,University of Massachusetts Lowell,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Pexxx,Poxxxx,Microsoft Research Montreal,,,415xxxxxxx,,,xxxxxxxxxxxxxmicrosoft.com,,,,,Canada,,Pexxx Poxxxx;Alxxxx Romxxxx;Anxx Rumxxxxxx,xxxxxxxxxs.uml.edu;xxxxxxxxxutlook.com;xxxxxxxx.uml.edu,,,,,,,,on,"Yes, include my submission even if the paper is rejected.",No,None,None
445,445X-H8B6D3F5H9,What is so novel about novelty?Experiments with Document Level Novelty Detection,Tirxxxxxxx Ghxxxx;Shuxxxx Vexxx;Asxx Ekxxx;Pusxxxx Bhatxxxxxxxxx;Srixxxxxx Saxxx;Geoxxxxx Tsatxxxxxxx;Paxxxx Coxxxx and Micxxxxx Grxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"The rapid growth of web documents in
the internet has necessitated finding means
of discarding redundant information and
identifying as well as retaining novel ones.
Detecting novelty is a fundamental re-
quirement in a variety of text processing
tasks ranging from update summarization
to plagiarism detection. Though novelty
detection at the document level is of im-
mense importance, this has been a less
investigated frontier. In this paper we
first define document novelty in terms of
two perspectives, viz style and content.
We deem text reuse, be it verbatim or
rewrite, to be a primary factor counter-
ing novelty. We then propose two novel
schemes exploiting deep semantic features
with traditional Support Vector Machines
and Deep Neural Networks for document
level novelty detection. We also create
a resource, reckoning human cognition of
content novelty at the document level and
propose a benchmark framework for it’s
evaluation. Finally, we come up with an
intuitive measure to quantify the amount
of novel content in a document with re-
spect to a set of source documents. Ex-
periments show that our methods achieve
state-of-the-art performance over the best
reported results on text reuse, and attain
encouraging performance on our gener-
ated gold standard.",7 Feb 2017 11:49:38 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",corpus development;  information retrieval;  NLP applications;  text classification;  document mining,Tirxxxxxxx,Ghxxxx,xxxxxxxxxxxxlg@gmail.com,Indian Institute of Technology Patna,No,Shuxxxx,Vexxx,xxxxxxxxxxx0@gmail.com,Indian Institute of Technology Patna,No,Asxx,Ekxxx,xxxxxxxxxx@gmail.com,Indian Institute of Technology Patna,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,Indian Institute of Technology Patna,No,Srinivasaxxxxxxxxxxxxxxxxxxx,Chixxxxxx,xxxxxxxxxxxxelsevier.com,Elsevier Inc.,No,Geoxxxxx,Tsatxxxxxxx,xxxxxxxxxxxxx@elsevier.com,Elsevier Inc.,No,Paxxxx,Coxxxx,xxxxxxxxxxlsevier.com,Elsevier Inc.,No,Micxxxxx,Grexxxx,xxxxxxxxxxxlsevier.com,Elsevier Inc.,No,,,,,,,,,,,,Tirxxxxxxx,Ghxxxx,Indian Institute of Technology Patna,,,,,,xxxxxxxxxxxxlg@gmail.com,,,,,India,,Tirxxxxxxx Ghxxxx;Shuxxxx Vexxx;Asxx Ekxxx;Pusxxxx Bhatxxxxxxxxx;Srixxxxxx Saxxx;Geoxxxxx Tsatxxxxxxx;Paxxxx Coxxxx;Micxxxxx Grexxxx,xxxxxxxxxxxxlg@gmail.com;xxxxxxxxxxx10@gmail.com;xxxxxxxxxxl@gmail.com;xxxxxxxxxx@gmail.com;xxxxxxxxxxxx@elsevier.com;xxxxxxxxxxxxxs@elsevier.com;xxxxxxxxxxxlsevier.com;xxxxxxxxxxxelsevier.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
446,446X-B7G7B3A3H5,“You are no Jack Kennedy”: On Media Highlights of Presidential Debates,Chexxxx Txx;Hxx Pexx and Noxx Ax,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),"Media outlets select bits of political communication to quote. To
quantitatively explore this selection process, we build a three-decade dataset
of presidential debate transcripts and post-debate coverage. We first examine
the effect of wording and propose a binary classification framework that
controls for the speaker and debate situations. We find that humans can only
achieve an accuracy of 60% in this task, indicating that media choices are not
entirely obvious. Our classifiers outperform humans on average, mainly in
primary debates. Interesting differences arise when we compare important
factors from humans’ free responses with those from data-driven methods: few
humans mentioned that “context matters”, whereas our data show that
well-quoted sentences are more distinct from the previous utterance by the same
speaker than less-quoted sentences. Using quoting patterns in our data, we also
present a preliminary analysis on the extent of fragmentation in the media.
Consistent with existing studies, we observe a decreasing trend in bipartisan
coverage.",6 Feb 2017 23:07:38 GMT,Empirical/Data-Driven,Multidisciplinary,NLP applications;  text mining,Chexxxx,Txx,xxxxxxxxxxenhaot.com,University of Washington,No,Hxx,Pexx,xxxxxxxxxxxxshington.edu,University of Washington,No,Noaxxxx,Smxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chexxxx,Txx,University of Colorado Boulder,,,,,,xxxxxxxxxxenhaot.com,,,WA,,United States,,Chexxxx Txx;Hxx Pexx;Noxx Ax,xxxxxxxxxxenhaot.com;xxxxxxxxxxxxashington.edu;xxxxxxxxxxxxxashington.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
447,447X-A6B3F6D6D3,Neural Discourse Structure for Text Categorization,Yanxxxxx Jx and Noxx Ax,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Oral Tuesday,,Undecided (Discourse Pragmatics),"We show that discourse structure, as defined by Rhetorical Structure Theory and
provided by an existing discourse parser, benefits text categorization.  Our
approach uses a recursive neural network and a newly proposed attention
mechanism to compute a representation of the text that focuses on salient
content, from the perspective of both RST and the task.  Experiments consider
variants of the approach and illustrate its strengths and weaknesses.",21 Apr 2017 23:55:40 GMT,Empirical/Data-Driven,Discourse and pragmatics,,Yanxxxxx,Jx,xxxxxxxxgmail.com,"CSE, University of Washington",No,Noaxxxx,Smxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yanxxxxx,Jx,"CSE, University of Washington",,,,,,xxxxxxxxgmail.com,,Seattle,WA,,United States,,Yanxxxxx Jx;Noxx Ax,xxxxxxxxgmail.com;xxxxxxxxxxxxxashington.edu,Neural Discourse Structure for Text Categorization,Neural Discourse Structure for Text Categorization,10,Yangfeng Ji,Research Associate,University of Washington,on,,Only include my submission if it is accepted.,No,None,None
448,448X-B4C4G3H6A7,Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks,Wilxxxx Fxx and  ,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Monday,,Undecided (Semantics),"We present a system which parses sentences into Abstract Meaning
Representations, improving state-of-the-art results for this task by more than
5%.  AMR graphs represent semantic content using linguistic properties such
as semantic roles, coreference, negation, and more.  The AMR parser does not
rely on a syntactic pre-parse, or heavily engineered features, and uses five
recurrent neural networks as the key architectural components for inferring AMR
graphs.",23 Apr 2017 02:19:50 GMT,Applications/Tools,Semantics,,Wilxxxx,Foxxxx,xxxxxxxxxxxxxd@colorado.edu,University of Colorado,No,Jamxxxxx,Maxxxx,xxxxxxxxxxxx@colorado.edu,University of Colorado Boulder,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wilxxxx,Foxxxx,University of Colorado,,,,,,xxxxxxxxxxxxxd@colorado.edu,,Boulder,CO,,United States,,Wilxxxx Foxxxx;Jaxxx Hx,xxxxxxxxxxxxxd@colorado.edu;xxxxxxxxxxxxx@colorado.edu,Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks,Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks,10,william r foland jr.,,,,on,No. Do not include my submission in this dataset.,No,None,None
449,449X-P3C6B7C6J6,Unsupervised Keyphrase Extraction from Scholar Texts,Vixxx Daudxxxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper we present a method for keyphrase extraction from scholar texts
and introduce a new  approach to evaluation of such keyphrase extraction
methods. We compare keyphrase extraction methods by considering their
performance with variable number of extracted keyphrases. Our results show that
phrasal segmentation of texts is a more significant factor for accuracy of
extracted keyphrases than a term-weight measuring system. The proposed method,
based on collocation segmentation and NTF-PIDF weight, outperform the
widely-used traditional unsupervised keyphrase extraction method and achieves
15.6 F-score compared to 13.3 F-score. We also extend compilation of keyphrase
lists from single documents to collections of documents and achieve acceptance
ratio 92.7% for so compiled keyphrases. We use the presented method to
demonstrate yearly research topics, their dynamics, and characterize
specificity of scholar article collections.",7 Feb 2017 10:14:22 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  NLP applications;  NLP on noisy unstructured text;  chunking;  multi-document summarization;  term extraction,Vixxx,Daudxxxxxxxxx,xxxxxxxxxxxxxvicius@vtex.lt,VTEX,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vixxx,Daudxxxxxxxxx,VTEX,,,,,,xxxxxxxxxxxxxvicius@vtex.lt,,Vilnius,,,Lithuania,,Vixxx Daudxxxxxxxxx,xxxxxxxxxxxxxvicius@vtex.lt,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
450,450X-G6A7E3E2G3,Tensor-based Learning for Rumor-aware Stock Movements,Jinxxxx Txx;Qixx Lx;Jxx Waxx and Roxx Xxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Recent studies in behavioral finance dis-cover that emotional investors are
affected by rumors, especially in the era of so-cial media which fertilizes
vibrant rumor creation and sharing. The challenge lies in how to quantify the
influence of rumor on stock markets along with other market information. A
common strategy in pre-vious studies has been to concatenate the features of
different information sources into one compound feature vector. In this
article, we introduce a tensor-based learning approach to capture the
rumor-based stock movements considering the joint impact of rumors and
historical transaction data. It provides an efficient way to mingle the
structure and unstructured data, and study their joint influence on market
movements. Experiments on three-year stock data of Growth Enterprise Market in
China show that our ap-proach outperforms the state-of-the-art vector-based
methods.",6 Feb 2017 19:46:25 GMT,Applications/Tools,Machine learning,NLP applications;  Web mining;  text mining;  NLP in business application,Jinxxxx,Txx,xxxxxxxx5@qq.com,Southwestern University of Finance and Economics,No,Qixx,Lx,xxxxxxxxxufe@qq.com,Southwestern University of Finance and Economics,No,Jxx,Waxx,xxxxxxxx2@qq.com,Southwestern University of Finance and Economics,No,Roxx,Xixx,xxxxxxxx5@qq.com,Southwestern University of Finance and Economics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jinxxxx,Txx,Southwestern University of Finance and Economics,,,,,,xxxxxxxx5@qq.com,,,,,China,,Jinxxxx Txx;Qixx Lx;Jxx Waxx;Roxx Xixx and  Ecoxxxxxxx,xxxxxxxx5@qq.com;xxxxxxxxxxufe@qq.com;xxxxxxxx32@qq.com;xxxxxxxx85@qq.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
451,451X-H7F9B3A9G3,KnowNER: Incremental Knowledge in Named Entity Recognition,Domxxxx Sexxxx;Lucxxxx Dxx;Johxxxxx Hofxxxx and Gerxxxx Wexxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"KnowNER is a Named Entity Recognition (NER) system that leverages different
degrees of external knowledge. A novel modular framework divides
knowledge into four categories according to the depth of knowledge they
convey.  Each category consists of a specific set of features generated from
different information sources and used to train a conditional random field
(CRF). We show that the incorporation of deeper knowledge systematically
boosts accuracy across datasets and entity types, with
improvements of up to 15 F1 points. KnowNER performs among state-of-the-art NER
approaches and achieves human-level performance (more than 95 F1 points) for
named entities like persons or locations.",6 Feb 2017 19:43:22 GMT,Applications/Tools,"Information extraction, text mining, and question answering",named entity recognition,Domxxxx,Sexxxx,xxxxxxxxxxllinois.edu,"University of Illinois, Urbana-Champaign",No,Lucxxxx,Delxxxxxx,xxxxxxxxxxx-inf.mpg.de,Max Planck Institute for Informatics,No,Johxxxxx,Hofxxxx,xxxxxxxxxxxi-inf.mpg.de,Max Planck Institute for Informatics,No,Gerxxxx,Wexxxx,xxxxxxxxxx-inf.mpg.de,Max Planck Institute for Informatics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Domxxxx,Sexxxx,University of Illinois at Urbana-Champaign,,,,,,xxxxxxxxxxllinois.edu,,,,,United States,,Domxxxx Sexxxx;Lucxxxx Dxx;Johxxxxx Hofxxxx;Gerxxxx Wexxxx,xxxxxxxxxxllinois.edu;xxxxxxxxxxxi-inf.mpg.de;xxxxxxxxxxxxi-inf.mpg.de;xxxxxxxxxxx-inf.mpg.de,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
452,452X-B3F5B7B3C2,The State of the Art in Semantic Representation,Omxx Abxxx and Axx Rapxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Monday,,Undecided (Semantics),"Semantic representation is receiving growing attention in NLP in the past few
years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have
been put forth. Yet, little has been done to assess the achievements and the
shortcomings of these new contenders, compare them with syntactic schemes, and
clarify the general goals of research on semantic representation. We address
these gaps by critically surveying the state of the art in the field.",24 Apr 2017 07:29:59 GMT,Survey Papers,Semantics,,Omxx,Abxxx,xxxxxxxxxxxxxil.huji.ac.il,The Hebrew University of Jerusalem,No,Axx,Rapxxxxxx,xxxxxxxxxuji.ac.il,The Hebrew University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Omxx,Abxxx,The Hebrew University of Jerusalem,,,,,,xxxxxxxxxxxxxil.huji.ac.il,,,,,Israel,,Omxx Abxxx;Axx Rapxxxxxx,xxxxxxxxxxxxxil.huji.ac.il;xxxxxxxxxhuji.ac.il,The State of the Art in Semantic Representation,The State of the Art in Semantic Representation,13,Omri Abend,,"School of Computer Science and Engineering
Edmond Safra Campus, Givat Ram
The Hebrew University
Jerusalem, 9190401, Israel

e-mail: oabend at cs dot huji dot ac dot il",,,No. Do not include my submission in this dataset.,No,None,None
453,453X-G4F8C4E4D3,Answering SAT Algebra Questions with a Cascade of Tree Transducers,Maxx Hopxxxx;Crixxxxx Petrexxxxxxxxxxx;Roxx Lexxx;Roxxx Lexxxx;Alxxxx Herxxxxx and Vixxx Joxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We build a question answering (QA) system for the Math SAT, a standardized exam
of complex questions spanning multiple sentences and exhibiting sophisticated
anaphoric phenomena. We architect the QA system as a cascade of tree
transducers, allowing us to propagate uncertainty until it can be confidently
resolved. Experiments show the first-ever results (43% recall and 91%
precision) on SAT closed-vocabulary algebra questions, which constitute
approximately 45% of a typical math SAT. We also apply our system to the public
Dolphin question set, and improve the state-of-the-art F1-score from 73.9% to
77.0% on this dataset.",7 Feb 2017 01:37:17 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",discourse;  anaphora resolution;  mathematical linguistics;  question interpretation;  coreference resolution;  question answering in restricted domains,Maxx,Hopxxxx,xxxxxxxxlenai.org,Allen Institute for Artificial Intelligence,No,Crixxxxx,Petrexxxxxxxxxxx,xxxxxxxxlenai.org,Allen Institute for Artificial Intelligence,No,Roxx,Lexxx,xxxxxxxxlenai.org,Allen Institute for Artificial Intelligence,No,Roxxx,Lexxxx,xxxxxxxxlenai.org,Allen Institute for Artificial Intelligence,No,Alxxxx,Herxxxxx,xxxxxxxxlenai.org,Allen Institute for Artificial Intelligence,No,Vixxx,Joxxx,xxxxxxxxlenai.org,Allen Institute for Artificial Intelligence,No,,,,,,,,,,,,,,,,,,,,,,Maxx,Hopxxxx,Reed College,,,,,,xxxxxxxx@reed.edu,,Seattle,WA,,United States,,Maxx Hopxxxx;Crixxxxx Petrexxxxxxxxxxx;Roxx Lexxx;Roxxx Lexxxx;Alxxxx Herxxxxx;Vixxx Joxxx,xxxxxxxxlenai.org;xxxxxxxxxlenai.org;xxxxxxxxxlenai.org;xxxxxxxxxlenai.org;xxxxxxxxxlenai.org;xxxxxxxxxlenai.org,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
454,454X-P6H4C2E6D3,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,Iaxxx Calxxxx;Qxx Lxx and Nixx Camxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Tuesday,,Undecided (Machine Translation),"We introduce a Multi-modal Neural Machine Translation model in which a
doubly-attentive decoder naturally incorporates spatial visual features
obtained using pre-trained convolutional neural networks, bridging the gap
between image description and translation. Our decoder learns to attend to
source-language words and parts of an image independently by means of two
separate attention mechanisms as it generates words in the target language. We
find that our model can efficiently exploit not just back-translated in-domain
multi-modal data but also large general-domain text-only MT corpora. We also
report state-of-the-art results on the Multi30k data set.",22 Apr 2017 15:19:36 GMT,Empirical/Data-Driven,Machine translation,,Iaxxx,Calxxxx,xxxxxxxxxxxer@gmail.com,Dublin City University,No,Qxx,Lxx,xxxxxxx@dcu.ie,Dublin City University,No,Nixx,Camxxxxx,xxxxxtcd.ie,Trinity College Dublin,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Iaxxx,Calxxxx,University of Amsterdam,,,,,,xxxxxxxxxxxer@gmail.com,,,,,Netherlands,,Iaxxx Calxxxx;Qxx Lxx;Nixx Camxxxxx,xxxxxxxxxxxer@gmail.com;xxxxxxxu@dcu.ie;xxxxxxtcd.ie,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,12,Iacer Calixto,PhD student,"ADAPT Centre, Dublin City University, Glasnevin, Dublin 9, Dublin, Ireland.",on,on,No. Do not include my submission in this dataset.,No,None,None
455,455X-B6B7C5P6D8,Story Comprehension for Predicting What Happens Next,Snixxxx Chaxxxxxxx;Haxxxx Pexx and Dxx Rxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Automatic story comprehension is a fundamental challenge in Natural Language
Understanding, and can enable computers to understand social norms, human
behavior and commonsense. In this paper, we present a model to understand a
story on three semantic axes: (i) the sequence of events described in the
story, (ii) its emotional trajectory, and (iii) its plot consistency. We judge
the model's understanding of real-world stories by inquiring if, like humans,
it can develop an expectation of what will happen next in a given story. In
particular, we use it to predict the correct ending to a given short story from
possible alternatives. The model addresses this by using a hidden variable to
weigh the above semantic aspects in the context of the story. Our experiments
demonstrate the potential of our features in characterizing these semantic
aspects, and the strength of our hidden variable based approach. Our model
achieves an absolute improvement of 19% accuracy above the best existing
state-of-the-art approaches on a publicly available dataset.",7 Feb 2017 08:53:39 GMT,Empirical/Data-Driven,Semantics,sentiment analysis;  NLP applications;  discriminative learning methods;  text classification;  pragmatics;  semantic knowledge induction,Snixxxx,Chaxxxxxxx,xxxxxxxxxgmail.com,University of Illinois at urbana-champaign,No,Haxxxx,Pexx,xxxxxxxxxlinois.edu,UIUC,No,Dxx,Roxx,xxxxxxxxinois.edu,University of Illinois,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Snixxxx,Chaxxxxxxx,"University of California, Santa Cruz",,,301-xxxxxxxx,,,xxxxxxxxxgmail.com,,,IL,,United States,,Snixxxx Chaxxxxxxx;Haxxxx Pexx;Dxx Roxx,xxxxxxxxxgmail.com;xxxxxxxxxxlinois.edu;xxxxxxxxxinois.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
456,456X-A7P3A5F8E3,Pronunciation Disambiguation of Anusvara/Anunasika for Hindi Grapheme to Phoneme (G2P) Conversion,Somxxxx Rxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"The pronunciation disambiguation of
anusvara and anunasika in Hindi is a
long-standing issue particularly for the
grapheme to phoneme (G2P) conversion.
A G2P module is an essential component
of an Automatic Speech Recogni-
tion(ASR) and a Text-to-Speech(TTS)
synthesis system. This paper proposes
a novel rule-based approach for pronun-
ciation disambiguation of anusvara and
anunasika in Hindi. The proposed method
uses both segmental and supra-segmental
phonological constraint for pronunciation
disambiguation of anusvara/anunasika.
A software module is implemented in
Python for G2P conversion in Hindi.
The accuracy of proposed G2P module
is 99.18% for anusvara/anunasika when
tested on a corpus of size 1450 words.",6 Feb 2017 20:01:15 GMT,Applications/Tools,"Phonology, morphology, and word segmentation",phonology;  rule-based/symbolic learning methods,Somxxxx,Rxx,xxxxxxxxxxx6@gmail.com,"Jawaharlal Nehru University, Delhi",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Somxxxx,Rxx,Melvault Software Solutions Pvt Ltd,,,,,,xxxxxxxxxxxxmelvault.com,,,,,India,,Somxxxx Rxx,xxxxxxxxxxx6@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
457,457X-C6F6J9A4B8,A text summarization technique based on a semantic approach,Fraxxxxxx Coxxxx;Flxxx Amxxx;Vinxxxxx Mosxxxx;Antxxxx Picxxxxxxx;Antxxxx D'Axxxxxx and Maxxx Caxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"In this paper, we present a general framework
for summarizing news articles from
different web sources that exploits a novel
summarization algorithm based on a semantic
analysis of the text. A set of subjects,
predicates and objects (i.e. triples) is
extracted for each document and then analyzed
to build a summary using an unsupervised
clustering algorithm. Then, the
centroids of the clusters are used to extract
significant sentences for building an
effective summary. Several experiments
are carried out using the ROUGE evaluation
software. The experimental results
show that the proposed method effectively
extracts relatively important information
by removing redundant and irrelevant data
with respect to other summarizers.",6 Feb 2017 20:06:31 GMT,Applications/Tools,Summarization,text classification;  multi-document summarization;  document mining,Fraxxxxxx,Coxxxx,xxxxxxxxunisa.it,DIIn - Unisa,No,Flxxx,Amxxx,xxxxxxxxxxo@unina.it,"DIETI - Università di Napoli ""Federico II""",No,Vinxxxxx,Mosxxxx,xxxxxxxxxxxxcato@unina.it,"DIETI - Università di Napoli ""Federico II""",No,Antxxxx,Picxxxxxxx,xxxxxxxxxxxxxiello@unina.it,"DIETI - Università di Napoli ""Federico II""",No,Antxxxx,D'Axxxxxx,xxxxxxxxxx@isa.cnr.it,Istituto di Scienze dell' Alimentazione - CNR,No,Maxxx,Casxxxx,xxxxxxxxxxxlo@unina.it,"DIIn - Università di Napoli ""Federico II""",No,,,,,,,,,,,,,,,,,,,,,,Fraxxxxxx,Coxxxx,DIIn - Unisa,,,,,,xxxxxxxxunisa.it,,,,,Italy,,Fraxxxxxx Coxxxx;Flxxx Amxxx;Vinxxxxx Mosxxxx;Antxxxx Picxxxxxxx;Antxxxx D'Axxxxxx;Maxxx Casxxxx,xxxxxxxxunisa.it;xxxxxxxxxxto@unina.it;xxxxxxxxxxxxxcato@unina.it;xxxxxxxxxxxxxxiello@unina.it;xxxxxxxxxxx@isa.cnr.it;xxxxxxxxxxxllo@unina.it,,,,,,,,,Only include my submission if it is accepted.,No,None,None
458,458X-J4E3F8H5C8,Identifying Multilingual Markers of Parkinson's Disease Through Morphological Segmentation,Elxx Eyxxxx;Adxxxx Mx;Katxxxxxx Roxx;Juxx Raxxxx;Saxxxx Skxxxx;Natxxxx Truxxxxx;Maxxx Rodxxxxxxx;Eugxxxx Hexxx;Aguxxxxx Ib�xxxxx and Guixxxxxx Cexxx,Biomedical,Aurxxxxx Néxxxxx;Kaxxx Verxxxxx,Reject,,Undecided (Biomedical),"Computer-aided detection of aging-related neurological diseases, such as
Parkinson's disease,  has gained a lot of interest as the population ages.  Our
work is the first study that uses morphological analysis for  automatic
detection of Parkinson's disease. We propose a novel measure of complexity of a
speaker's language through morphological segmentation.        We tested our method on
Czech, German, and Spanish, and  obtained significant classification accuracy
across three languages.",7 Feb 2017 01:56:25 GMT,Empirical/Data-Driven,Biomedical,morphology;  NLP applications;  multilingual applications,Elxx,Eyxxxx,xxxxxxxxxx@gmail.com,IBM Research,No,Adoxxxxxx,Garxxxx,xxxxxxxxxxxxxxrcia@gmail.com,Favaloro University,No,Katxxxxxx,Roxx,xxxxxxxxxxxxxx@googlemail.com,University of Würzburg,No,Juanxxxxxxx,Orozcxxxxxxxxxx,xxxxxxxxxxo@gmail.com,University of Antioquia,No,Saxxxx,Skxxxx,xxxxxxxxxxxxx@kk-bochum.de,Friedrich-Alexander-Universität,No,Natxxxx,Truxxxxx,xxxxxxxxx@gmail.com,University of Antioquia,No,Maxxx,Rodxxxxxxx,xxxxxxxxxxxguez@nudz.cz,"National Institute of Mental Health, Prague",No,Eugxxxx,Hexxx,xxxxxxxxxx@gmail.com,Favaloro University,No,Aguxxxxx,Ib�xxxxx,xxxxxxxxxxeco.org.ar,Universidad Autónoma del Caribe,No,Guixxxxxx,Cexxxx,xxxxxxxxxs.ibm.com,IBM Research,No,,Elxx,Eyxxxx,IBM Research,,,,,,xxxxxxxxxx@gmail.com,,,,,United States,,Elxx Eyxxxx;Adxxxx Mx;Katxxxxxx Roxx;Juxx Raxxxx;Saxxxx Skxxxx;Natxxxx Truxxxxx;Maxxx Rodxxxxxxx;Eugxxxx Hexxx;Aguxxxxx Ib�xxxxx;Guixxxxxx Cexxxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxxxxxarcia@gmail.com;xxxxxxxxxxxxxxx@googlemail.com;xxxxxxxxxxxo@gmail.com;xxxxxxxxxxxxxa@kk-bochum.de;xxxxxxxxxx@gmail.com;xxxxxxxxxxxxguez@nudz.cz;xxxxxxxxxxe@gmail.com;xxxxxxxxxxneco.org.ar;xxxxxxxxxus.ibm.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
459,459X-E5D7D2H8B3,Towards a Semiotic Textology Framework for Multimodal and Distributional Semantic Models,Boxxx Navxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"In the last few years, Distributional Semantic Models have received much
attention in the Natural Language Processing community, including its extension
to multimodal distributional semantic representations. However, it seems that
little attention has been paid to the theoretical aspects of the multimodal and
distributional meaning and its vector-based representation. In this paper we
will try to solve in part this situation presenting the Semiotic Textology as a
theoretical framework suitable for Multimodal Distributional Semantic Models.
Unlike other linguistic frameworks, Semiotic Textology defines the text as a
multimodal device. Therefore, it can contribute to the development of
Multimodal Distributional Semantic Models setting up not only a wide conceptual
framework for multimodal communication, but its main challenges and problems.
Specifically, in this paper we address two topics: first, how distributional
semantics models fit in this theoretical framework as an inference process, and
second two issues for a unified vector-based representation of the multimodal
and distributional meaning: the kind of meaning that images and texts share,
and the distributional meaning of images.",6 Feb 2017 20:10:07 GMT,Theoretical,Semantics,formal semantics and logic;  mathematical linguistics;  distributional similarity;  multimodal representations and processing,Boxxx,Navxxxx,xxxxxxxxsi.ua.es,University of Alicante,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Boxxx,Navarxxxxxxxxxxx,University of Alicante,,,,,,xxxxxxxxsi.ua.es,,,,,Spain,,Boxxx Navxxxx,xxxxxxxxsi.ua.es,,,,,,,,,Only include my submission if it is accepted.,No,None,None
461,461X-B2C5H6F6A3,Building a Neural Machine Translation System Using Only Synthetic Parallel Data,Jaexxxx Paxx;Byuxxxxxx Nx and Sunxxxx Yxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Recent works have proved that synthetic parallel data generated by existing
translation models can be an effective solution to various neural machine
translation (NMT) issues. In this study, we construct NMT systems using only
synthetic parallel data. As an effective alternative to real parallel data, we
also present a new type of synthetic parallel corpus. The proposed pseudo
parallel data are distinct from previous approaches in that real and synthetic
sentences are mixed on both sides of sentence pairs. Experiments on
Czech-German and French-German translations demonstrate the efficacy of the
proposed pseudo parallel corpus that guarantees not only both balanced and
competitive performance for bidirectional translation but also substantial
improvement with the aid of a real parallel corpus.",7 Feb 2017 09:16:51 GMT,Empirical/Data-Driven,Machine translation,corpus development;  MT applications;  NLP applications;  cross-lingual approaches;  interlingual MT;  machine-aided translation;  multilingual resources,Jaexxxx,Paxx,xxxxxxxxgmail.com,Seoul National University,No,Byuxxxxxx,Nx,xxxxxxxxxxxk@gmail.com,Seoul National University,No,Sunxxxx,Yoxx,xxxxxxxxnu.ac.kr,Seoul National University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sunxxxx,Yoxx,Seoul National University,,,,,,xxxxxxxxnu.ac.kr,,,,,Republic of Korea,,Jaexxxx Paxx;Byuxxxxxx Nx;Sunxxxx Yoxx,xxxxxxxxgmail.com;xxxxxxxxxxxbk@gmail.com;xxxxxxxxsnu.ac.kr,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
462,462X-E5F6A6C4G2,Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models,Naxxx Rekxxxxx;Mixxx Luxx;Arxxx Bakxxxxx;Alexxxxxx D�xx;Lixxx Andxxxxxx and Alxxx Haxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Poster Monday,,Undecided (Sentiment Analysis Opinion Mining),"Volatility prediction—an essential concept in financial markets—has
recently been addressed using sentiment analysis methods. We investigate the
sentiment of annual disclosures of companies in stock markets to forecast
volatility. We specifically explore the use of recent Information Retrieval
(IR) term weighting models that are effectively extended by related terms using
word embeddings. In parallel to textual information, factual market data have
been widely used as the mainstream approach to forecast market risk. We
therefore study different fusion methods to combine text and market data
resources. Our word embedding-based approach significantly outperforms
state-of-the-art methods. In addition, we investigate the characteristics of
the reports of the companies in different financial sectors.",19 Apr 2017 15:14:14 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Naxxx,Rekxxxxx,xxxxxxxxxxxx.tuwien.ac.at,TU WIEN,No,Mixxx,Luxx,xxxxxxxxxxuwien.ac.at,Vienna University of Technology,No,Arxxx,Bakxxxxx,xxxxxxxxxxiasa.ac.at,International Institute for Applied Systems Analysis (IIASA),No,Alexxxxxx,D�xx,xxxxxxxxxxxxxr@tuwien.ac.at,TU WIEN,No,Lixxx,Andxxxxxx,xxxxxxxxxxxxx.tuwien.ac.at,Vienna University of Technology,No,Alxxx,Hanxxxx,xxxxxxxxxxxxtuwien.ac.at,TU Wien,No,,,,,,,,,,,,,,,,,,,,,,Naxxx,Rekxxxxx,TU WIEN,,,,,,xxxxxxxxxxxx.tuwien.ac.at,,Vienna,,,Austria,,Naxxx Rekxxxxx;Mixxx Luxx;Arxxx Bakxxxxx;Alexxxxxx D�xx;Lixxx Andxxxxxx;Alxxx Hanxxxx,xxxxxxxxxxxx.tuwien.ac.at;xxxxxxxxxxxuwien.ac.at;xxxxxxxxxxiiasa.ac.at;xxxxxxxxxxxxxxr@tuwien.ac.at;xxxxxxxxxxxxxs.tuwien.ac.at;xxxxxxxxxxxx.tuwien.ac.at,Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models,Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models,10,Navid Rekabsaz,,,,on,Only include my submission if it is accepted.,No,None,None
463,463X-A6H2E8F6J7,Learning Better Name Translation for Cross-Lingual Wikification,Chexxxxx Tsxx and Dxx Roxx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"A notable challenge in cross-lingual wikification is the problem of retrieving
English Wikipedia title candidates given a non-English mention, a step that
requires translating names written in a foreign language into English. In order
to cover as many languages as possible, we propose a probabilistic model that
learns name translation from title pairs obtained from the inter-language links
in Wikipedia. The model jointly considers word alignment and word
transliteration. Comparing to other approaches on languages, we show that the
proposed model outperforms others not only on the transliteration metric, but
also on the ability to generate target English titles for a cross-lingual
wikifier. Consequently, as we show, it improves the end-to-end performance of a
cross-lingual wikifier on the TAC 2016 EDL dataset.",7 Feb 2017 03:57:16 GMT,Empirical/Data-Driven,Multilinguality,generative models;  cross-lingual approaches;  information extraction;  cross-language information extraction;  transliteration;  multilingual applications;  named entity disambiguation;  entity disambiguation;  alignment,Chexxxxx,Tsxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana-Champaign,No,Dxx,Roxx,xxxxxxxxinois.edu,University of Illinois,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chexxxxx,Tsxx,Bloomberg LP,,,,,,xxxxxxxxxxoomberg.net,,,,,United States,Research Scientist at Bloomberg,Chexxxxx Tsxx;Dxx Roxx,xxxxxxxxxxlinois.edu;xxxxxxxxxinois.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
464,464X-J9A2D3E6C7,Dict2Vec : Learning Word Embeddings using Lexical Dictionaries,Juxxxx Tisxxxx;Chrixxxxxxx Graxxxx and Amxxxx Haxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Learning word embeddings on large unlabeled
corpus has been shown to be successful
in improving many natural language
tasks. The most efficient and popular
approaches learn or retrofit such representations
using additional external data.
Resulting embeddings are generally better
than their corpus-only counterparts, although
such resources cover a fraction of
words in the vocabulary. In this paper,
we propose a new approach, Dict2Vec,
based on one of the largest yet refined
datasource for describing words – natural
language dictionaries. Dict2Vec
builds new word pairs from dictionaries
entries so that semantically-related words
are moved closer, and negative sampling
filters out pairs whose words are unrelated
in dictionaries. We evaluate the word representation
obtained using Dict2Vec on
eleven datasets on the word similarity task,
and on the word analogy task.",7 Feb 2017 01:26:03 GMT,Empirical/Data-Driven,Machine learning,unsupervised and semi-supervised learning;  distributional similarity;  relation discovery;  semantic relations,Juxxxx,Tisxxxx,xxxxxxxxxxxxxxxxniv-st-etienne.fr,"Laboratoire Hubert Curien UMR CNRS 5516, Saint-Etienne",No,Chrixxxxxxx,Graxxxx,xxxxxxxxxxxxxxxxxx@univ-st-etienne.fr,Université Jean Monnet,No,Amxxxx,Habxxxx,xxxxxxxxxxxxxxxxniv-st-etienne.fr,University of Saint-Etienne,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Juxxxx,Tisxxxx,"Laboratoire Hubert Curien UMR CNRS 5516, Saint-Etienne",,,,,,xxxxxxxxxxxxxxxxniv-st-etienne.fr,,,,,France,,Juxxxx Tisxxxx;Chrixxxxxxx Graxxxx;Amxxxx Habxxxx,xxxxxxxxxxxxxxxxniv-st-etienne.fr;xxxxxxxxxxxxxxxxxxx@univ-st-etienne.fr;xxxxxxxxxxxxxxxxxniv-st-etienne.fr,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
466,466X-A3F9A3H5F3,NewsQA: A Machine Comprehension Dataset,Adxx Trixxxxxx;Toxx Waxx;Xixxxx Yuxx;Juxxxx Haxxxx;Alexxxxxxx Sorxxxx;Phxxxx Bacxxxx and Kaxxxx Suxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present NewsQA, a challenging machine comprehension dataset of over 100,000
human-generated question-answer pairs. Crowdworkers supply questions and
answers based on a set of over 10,000 news articles from CNN, with answers
consisting in spans of text from the corresponding articles. We collect this
dataset through a four-stage process designed to solicit exploratory questions
that require reasoning. A thorough analysis confirms that NewsQA demands
abilities beyond simple word matching and recognizing textual entailment. We
measure human performance on the dataset and compare it to several strong
neural models. The performance gap between humans and machines (0.198)
indicates that significant progress can be made on NewsQA through future
research. The dataset is freely available at hiddensite.com.",7 Feb 2017 02:47:05 GMT,Resources/Evaluation,"Information extraction, text mining, and question answering",answer extraction;  graphical models;  open-domain question answering,Adxx,Trixxxxxx,xxxxxxxxxxxxxr@maluuba.com,"Maluuba, Inc.",No,Toxx,Waxx,xxxxxxxxxxmaluuba.com,Maluuba Research,No,Xixxxx,Yuxx,xxxxxxxxxxmaluuba.com,Maluuba Research,No,Juxxxx,Haxxxx,xxxxxxxxxxxxs@maluuba.com,Maluuba Research,No,Alexxxxxxx,Sorxxxx,xxxxxxxxxxxxxxdoni@gmail.com,Maluuba Inc.,No,Phxxxx,Bacxxxx,xxxxxxxxxxxx@maluuba.com,Maluuba Research,No,Kaxxxx,Sulxxxx,xxxxxxxxxxmaluuba.com,Maluuba Research,No,,,,,,,,,,,,,,,,,Toxx,Waxx,"Microsoft Research Maluuba, Montreal",,,,,,xxxxxxxxxxxicrosoft.com,,Montreal,QC,,Canada,,Adxx Trixxxxxx;Toxx Waxx;Xixxxx Yuxx;Juxxxx Haxxxx;Alexxxxxxx Sorxxxx;Phxxxx Bacxxxx;Kaxxxx Sulxxxx,xxxxxxxxxxxxxr@maluuba.com;xxxxxxxxxxxmaluuba.com;xxxxxxxxxxxmaluuba.com;xxxxxxxxxxxxxs@maluuba.com;xxxxxxxxxxxxxxrdoni@gmail.com;xxxxxxxxxxxxn@maluuba.com;xxxxxxxxxxxmaluuba.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
467,467X-H9B9J7A3E6,Learning bilingual word embeddings with (almost) no bilingual data,Mixxx Artxxxx;Goxxx Laxxxx and Enxxx Agxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Monday,,Undecided (Semantics),"Most methods to learn bilingual word embeddings rely on large parallel corpora,
which is difficult to obtain for most language pairs. This has motivated an
active research line to relax this requirement, with methods that use
document-aligned corpora or bilingual dictionaries of a few thousand words
instead. In this work, we further reduce the need of bilingual resources using
a very simple self-learning approach that can be combined with any
dictionary-based mapping technique. Our method exploits the structural
similarity of embedding spaces, and works with as little bilingual evidence as
a 25 word dictionary or even an automatically generated list of numerals,
obtaining results comparable to those of systems that use richer resources.",23 Apr 2017 00:23:30 GMT,Empirical/Data-Driven,Semantics,,Mixxx,Artxxxx,xxxxxxxxxxtxe@ehu.eus,University of the Basque Country (UPV/EHU),No,Goxxx,Laxxxx,xxxxxxxxxxka@ehu.eus,University of the Basque Country (UPV/EHU),No,Enxxx,Agxxxx,xxxxxxxx@ehu.eus,University of the Basque Country (UPV/EHU),No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mixxx,Artxxxx,University of the Basque Country (UPV/EHU),,,,,,xxxxxxxxxxtxe@ehu.eus,,,,,Spain,,Mixxx Artxxxx;Goxxx Laxxxx;Enxxx Agxxxx,xxxxxxxxxxtxe@ehu.eus;xxxxxxxxxxaka@ehu.eus;xxxxxxxxe@ehu.eus,Learning bilingual word embeddings with (almost) no bilingual data,Learning bilingual word embeddings with (almost) no bilingual data,12,Mikel Artetxe,,"IXA NLP group, University of the Basque Country (UPV/EHU)
Manuel Lardizabal Pasealekua, 1 - 20018 Donostia-San Sebastián, Spain",on,on,Only include my submission if it is accepted.,No,None,None
468,468X-P7H4D7E5F9,Deep Q-Learning for Greedy Parsing,Yixxx Lxx;Wanxxxxx Cxx;Tixx Lxx;Bixx Qxx and Noxx Ax,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Transition-based greedy parsers are typically estimated using
supervised learning from oracle (gold-standard) transition sequences, leaving
them unprepared for recovery from early-stage mistakes.  We consider a
different kind of supervision, Q-learning, in which future reward is
directly estimated.  We explore the strengths and weaknesses of the
approach relative to supervised learning, controlling for many factors
by using the same stack
LSTM architecture.  Experimentally, we find that an ensemble of the
supervised and Q-learned parsers outperforms both on all eight
languages considered. Q-learning can be more widely applied
than existing methods for encouraging exploration (e.g., dynamic
oracles), and exploits much of the implementation already required for a
greedy parser.",7 Feb 2017 07:28:32 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",parsing;  reinforcement learning,Yixxx,Lxx,xxxxxxxxxxu@gmail.com,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology",No,Wanxxxxx,Cxx,xxxxxxxxxgmail.com,Harbin Institute of Technology,No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,Bixx,Qxx,xxxxxxxxxit.edu.cn,Harbin Institute of Technology,No,Noaxxxx,Smxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yixxx,Lxx,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology",,,1510xxxxxxx,,,xxxxxxxxxxu@gmail.com,,,,,China,,Yixxx Lxx;Wanxxxxx Cxx;Tixx Lxx;Bixx Qxx;Noxx Ax,xxxxxxxxxxu@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxxip.126.com;xxxxxxxxxhit.edu.cn;xxxxxxxxxxxxxashington.edu,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
469,469X-H3A6A3H8F8,Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems,Laxxx Ex;Haxxxx Scxxxx;Shixxxx Shxxxx;Jerxxxx Zuxxx;Juxxxx Haxxxx;Emxxx Fixx;Raxxx Mehxxxxx and Kaxxxx Suxxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"This paper presents the Frames dataset, a corpus of 1369 human-human dialogues
with an average of 15 turns per dialogue. We developed this dataset to study
the role of memory in goal-oriented dialogue systems. Based on Frames, we
introduce a task called frame tracking, which extends state tracking to a
setting where several states are tracked simultaneously. We show that a simple
rule-based system only marginally improves over a random baseline. We show that
Frames can also be used to study memory in dialogue management and information
presentation through natural language generation.",6 Feb 2017 21:58:02 GMT,Resources/Evaluation,Dialog and interactive systems,corpus development;  dialogue;  contex modeling for dialogues;  corpus annotation methods,Laxxx,El xxxx,xxxxxxxxxxxx@maluuba.com,Maluuba,No,Haxxxx,Scxxxx,xxxxxxxxxxxxz@maluuba.com,Maluuba Inc.,No,Shixxxx,Shxxxx,xxxxxxxxxxxxxa@maluuba.com,Maluuba,No,Jerxxxx,Zuxxx,xxxxxxxxxxxxr@maluuba.com,Maluuba,No,Juxxxx,Haxxxx,xxxxxxxxxxxxs@maluuba.com,Maluuba,No,Emxxx,Fixx,xxxxxxxxxxxmaluuba.com,Maluuba,No,Raxxx,Mehxxxxx,xxxxxxxxxxxxxa@maluuba.com,Maluuba,No,Kaxxxx,Sulxxxx,xxxxxxxxxxxxxn@maluuba.com,Maluuba,No,,,,,,,,,,,,Laxxx,El xxxx,Microsoft Research Montreal,,,+1 (51xxxxxxxxxxx,,,xxxxxxxxxxxxxmicrosoft.com,,Montreal,,,Canada,,Laxxx Ex;Haxxxx Scxxxx;Shixxxx Shxxxx;Jerxxxx Zuxxx;Juxxxx Haxxxx;Emxxx Fixx;Raxxx Mehxxxxx;Kaxxxx Sulxxxx,xxxxxxxxxxxx@maluuba.com;xxxxxxxxxxxxxz@maluuba.com;xxxxxxxxxxxxxma@maluuba.com;xxxxxxxxxxxxxr@maluuba.com;xxxxxxxxxxxxxs@maluuba.com;xxxxxxxxxxx@maluuba.com;xxxxxxxxxxxxxra@maluuba.com;xxxxxxxxxxxxxan@maluuba.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
470,470X-C2H7E3D5P6,Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback,Avixxxx Pxx and Chrxxxxxx Mx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Poster Monday,,Undecided (Generation Summarization),"In this paper, we propose an extractive multi-document summarization (MDS)
system using joint optimization and active learning for content selection
grounded in user feedback. Our method interactively obtains user feedback to
gradually improve the results of a state-of-the-art integer linear programming
(ILP) framework for MDS. Our methods complement fully automatic methods in
producing high-quality summaries with a minimum number of iterations and
feedbacks.
We conduct multiple simulation-based experiments and analyze the effect of
feedback-based concept selection in the ILP setup in order to maximize the
user-desired content in the summary.",21 Apr 2017 18:54:03 GMT,Empirical/Data-Driven,Summarization,,Avixxxx,Pxx,xxxxxxxxxxxxxxxtu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,Chrixxxxxxxx,Mexxx,xxxxxxxxxxxxxxxxxxik.tu-darmstadt.de,"UKP Lab, Technische Universität Darmstadt",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Avixxxx,Pxx,"UKP Lab, Technische Universität Darmstadt",,,,,,xxxxxxxxxxxxxxxtu-darmstadt.de,,Darmstadt,Hesse,,Germany,,Avixxxx Pxx;Chrxxxxxx Mx,xxxxxxxxxxxxxxxtu-darmstadt.de;xxxxxxxxxxxxxxxxxxtik.tu-darmstadt.de,Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback,Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback,11,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
471,471X-J3C7A8A2H3,Modeling intra-textual variation with entropy and surprisal,Stexxxxx Degaetxxxxxxxxxxx and Elxx Texxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present a data-driven approach to investigate
intra-textual variation by combining
entropy and surprisal. With this
approach we detect linguistic variation
across sections of research articles focusing
on phrasal lexico-grammatical structures.
Entropy is used to detect patterns
typical of specific sections. Surprisal
is used to differentiate between more
and less informationally-loaded patterns
as well as types of information. While
we here focus on research articles in biology/
genetics, the methodology can be
applied to any text type or domain and
combined with additional variables such
as time or social group.",6 Feb 2017 20:42:34 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",information extraction;  text mining;  NLP for expert domains;  biomedical text mining;  document mining,Stexxxxx,Degaetxxxxxxxxxxx,xxxxxxxxxxxxxxxuni-saarland.de,Saarland University,No,Elxx,Texxx,xxxxxxxxxxxxxi-saarland.de,Saarland University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Stexxxxx,Degaetxxxxxxxxxxx,Saarland University,,,,,,xxxxxxxxxxxxxxxuni-saarland.de,,,,,Germany,,Stexxxxx Degaetxxxxxxxxxxx;Elxx Texxx and  Unixxxxxxxx,xxxxxxxxxxxxxxxuni-saarland.de;xxxxxxxxxxxxxni-saarland.de,,,,,,,,,Only include my submission if it is accepted.,No,None,None
472,472X-J6E2F7J3A6,Text Classification with Sparse Composite Document Vector,Dhexxxx Mexxxx;Vixxx Guxxx and Haxxxx Kaxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present a feature formation technique - sparse document vector (SDV) -
inspired by the graded-weighted Bag of Word Vectors (BoWV) (Vivek Gupta et.
al.) for text document representation. The experiments on multi-class and
multi-label text classification tasks show that our representation outperforms
state-of-art methods in performance and feature formation time.",6 Feb 2017 20:46:31 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",multiword semantics/compositionality;  information extraction;  scalability/efficiency of ML methods;  information retrieval;  discriminative learning methods;  word sense disambiguation;  distributional similarity;  text mining;  text classification;  document mining,Dhexxxx,Mexxxx,xxxxxxxxxiitk.ac.in,"Indian Institute of Technology, Kanpur",No,Vixxx,Guxxx,xxxxxxxxmail.com,"Indian Institute of Technology, Kanpur",No,Haxxxx,Karxxxx,xxxxxxxxitk.ac.in,"Indian Institute of Technology, Kanpur",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vixxx,Guxxx,Microsoft Research,,,,,,xxxxxxxxmail.com,,,,,India,,Dhexxxx Mexxxx;Vixxx Guxxx;Haxxxx Karxxxx,xxxxxxxxxiitk.ac.in;xxxxxxxxgmail.com;xxxxxxxxxitk.ac.in,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
473,473X-F7B2H7G2A5,Two perspectives on inter-rater agreement on reproducibility in natural language processing,Kx Brexxxxxx;Jixxxx Xxx;Aurxxxx Nexxxx;Prxxxx Yaxxx;Nexxxx Haxxx;Bxx Carxxxxxx;Tifxxxx Calxxxxx and Pixxxx Zwexxxxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"Recent work in a number of fields of scientific inquiry, including natural
language processing, has led to growing concern about reproducibility of
research results. However, it is difficult to know the state of the situation
in natural language processing, because it is not known to what extent two
raters would judge a finding to have been reproduced, or even to be
reproducible, and there is reason to think that this would be a difficult task.
In a first experiment, we compare ratings of replicability of a stratified
sample of language processing papers across five judges. Although the mean
Cohen's Kappa was only moderate at 0.45, 4/10 pairs of annotators reached
substantial agreement, achieving Kappas of 0.61 to 0.76. Similarly, although
the mean percent agreement was only moderate at 0.74, 4/10 pairs of annotators
achieved percent agreements of 0.88 or higher. In a second experiment, we
compare ratings between a domain expert and a non-expert on all publications
from the BioNLP 2016 workshop,
finding that they achieved Cohen's Kappa of 0.63 for annotating availability of
code and 0.57 for availability of data. These results suggest that research on
reproducibility in natural language processing can proceed on safer
methodological ground than might have been thought.

All materials used in the paper, including data, code, and the output of the
analysis, are available at the ACL reviewing site and in a public GitHub
repository.",10 Feb 2017 18:42:34 GMT,Resources/Evaluation,Resources and evaluation,corpus development;  corpus annotation methods;  NLP in software development and testing;  biomedical text mining,K. Bxxxxxxxx,Coxxx,xxxxxxxxxxn@gmail.com,"U. Colorado School of Medicine, U. Paris-Saclay/LIMSI-CNRS",No,Jixxxx,Xxx,xxxxxxxxxxxxth@gmail.com,Huazhong Agricultural University,No,Aurxxxx,Nexxxx,xxxxxxxlimsi.fr,LIMSI-CNRS,No,Prxxxx,Yaxxx,xxxxxxxxxxxx@ucdenver.edu,U. Colorado School of Medicine,No,Nexxxx,Haxxx,xxxxxxxxxxxx@ucdenver.edu,University of Colorado School of Medicine,No,Bxx,Carxxxxxx,xxxxxxxxas-i.com,"Columbia University, LingPipe",No,Tifxxxx,Calxxxxx,xxxxxxxxxxxxxxan@ucdenver.edu,U. Colorado School of Medicine,No,Pixxxx,Zweixxxxxxx,xxxxxmsi.fr,LIMSI-CNRS,No,,,,,,,,,,,,Kexxx,Coxxx,"Computational Bioscience Program, U. Colorado School of Medicine",,,,,,xxxxxxxxxxn@gmail.com,,,,,United States,,Kx Brexxxxxx;Jixxxx Xxx;Aurxxxx Nexxxx;Prxxxx Yaxxx;Nexxxx Haxxx;Bxx Carxxxxxx;Tifxxxx Calxxxxx;Pixxxx Zweixxxxxxx,xxxxxxxxxxn@gmail.com;xxxxxxxxxxxxath@gmail.com;xxxxxxxxlimsi.fr;xxxxxxxxxxxxx@ucdenver.edu;xxxxxxxxxxxxx@ucdenver.edu;xxxxxxxxias-i.com;xxxxxxxxxxxxxxxan@ucdenver.edu;xxxxxxmsi.fr,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
475,475X-G7J2E7J9A8,Synset2Vec: Learning Sense and Word Embedding Jointly in WordNet,Ganxxxx Zxx;Osxxx Arxxxx and Caxxxx Ax,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Word embeddings have recently become popular in a number of Natural Language
Processing (NLP) tasks, including sense disambiguation. Sense embedding
techniques learn a distributed vector for each sense of a word. Existing
research has used WordNet mainly as a sense inventory to initialize the vector
representation of synsets that are later augmented with annotated corpus. In
this paper we propose a different approach that transforms word embeddings to
the synset level and leverages the knowledge of WordNet and sense-annotated
datasets by creating en-riched synset profiles based on these semantic
networks, including their semantic relationships and examples. The approach has
been validated with Word Sense Disambiguation (WSD) datasets reporting its
effectiveness in representing synsets and words in the shared semantic vector
space.",6 Feb 2017 21:01:05 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  word sense disambiguation;  distributional similarity;  semantic relations,Ganxxxx,Zxx,xxxxxxxt.upm.es,Universidad Politécnica de Madrid,No,Osxxx,Arxxxx,xxxxxxxe@upm.es,Universidad Politecnica de Madrid,No,Carxxxxxx,Iglxxxxx,xxxxxxxxgmail.com,Universidad Politécnica de Madrid,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ganxxxx,Zxx,Universidad Politécnica de Madrid,,,,,,xxxxxxxt.upm.es,,,,,Spain,,Ganxxxx Zxx;Osxxx Arxxxx;Caxxxx Ax,xxxxxxxt.upm.es;xxxxxxxxe@upm.es;xxxxxxxxxgmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
476,476X-D4B6E6H6J4,Learning to Detect Contested Propositions in Web Discourse Using Distant Supervision,Nixx Retxxxxxx;Fexxx Xx and Haxx Uszxxxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Identifying prominent propositions to a discussion into an overview of only its
key points has previously been investigated using Semantic Textual Similarity
(STS) methods to group recurring propositions. Other research has looked at how
to rank propositions by how important they are in a discussion.
To combine these ideas we investigate the task of detecting the most contested
propositions in an online discussion in order to select those propositions most
important to the contributors.
We cast this problem into a binary sequence classification task, where users'
up and down votes are used to automatically label posts as contested in a
distantly supervised fashion. The corpus contains one years worth of news
articles and their respective discussion posts resulting in a collection of 67k
news articles and 813k user posts, that cover diverse domains from politics to
science. From this collection we created a nearly balanced training dataset of
9.1k contested and 12.9k non-contested propositions. We present first results
using models such as FastText and a CNN variant. Our best model achieves an
66.5% accuracy, 65.1% F1 and a ROC AUC of 71.3% in classifying contested posts.",7 Feb 2017 10:07:05 GMT,Empirical/Data-Driven,Discourse and pragmatics,corpus development;  discourse;  Web mining;  information extraction;  domain adaptation;  corpus annotation methods;  NLP on noisy unstructured text;  information retrieval;  dialogue control;  opinion mining and extraction;  opinion representation;  experimental evaluation/comparison of ML methods;  filtering and recommendation;  text mining;  text classification;  NLP in social networking media;  document summarization;  document mining;  semantic knowledge induction,Nixx,Retxxxxxx,xxxxxxxxxxxier@dfki.de,DFKI,No,Fexxx,Xx,xxxxxxdfki.de,DFKI LT Lab,No,Haxx,Uszxxxxxx,xxxxxxxxt@dfki.de,DFKI and Saarland University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nixx,Retxxxxxx,DFKI,,,,,,xxxxxxxxxxxier@dfki.de,,,,,Germany,,Nixx Retxxxxxx;Fexxx Xx;Haxx Uszxxxxxx and  Sxxxx,xxxxxxxxxxxier@dfki.de;xxxxxxxdfki.de;xxxxxxxxxt@dfki.de,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
477,477X-B9F6A2B6B8,From Characters to Words to in Between: Do We Capture Morphology?,Clxxx Vaxxx and Adxx Loxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Accept - Poster Tuesday,,,"Words can be represented by composing the representations of subword units such
as word segments, characters, and/or character n-grams. While such
representations are effective and may capture the morphological regularities of
words, they have not been systematically compared, and it is not understood how
they interact with different morphological typologies. On a language modeling
task, we present experiments that systematically vary (1) the basic unit of
representation, (2) the composition of these representations, and (3) the
morphological typology of the language modeled. Our results extend previous
findings that character representations are effective across typologies, and we
find that a previously unstudied combination of character trigram
representations composed with bi-LSTMs outperforms most others. But we also
find room for improvement: none of the character-level models match the
predictive accuracy of a model with access to true morphological analyses, even
when learned from an order of magnitude more data.",21 Apr 2017 18:26:08 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",,Clxxx,Vaxxx,xxxxxxxxxxs.ed.ac.uk,University of Edinburgh,No,Adxx,Loxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Clxxx,Vaxxx,University of Edinburgh,,,,,,xxxxxxxxed.ac.uk,,,,,United Kingdom,,Clxxx Vaxxx;Adxx Loxxx,xxxxxxxxxxs.ed.ac.uk;xxxxxxxxxxf.ed.ac.uk,From Characters to Words to in Between: Do We Capture Morphology?,From Characters to Words to in Between: Do We Capture Morphology?,12,Clara Vania,,"School of Informatics, University of Edinburgh
Informatics Forum, 10 Crichton St, Edinburgh EH8 9AB",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
478,478X-C7B2A3P9C6,Assessing the Use of Other Target Languages for Detecting Source Translation Difficulties,Juxxx Ixx;Aurxxxxxx Mxx and Fraxxxxxx Yxx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"The difficulties of translating a text vary with a number of factors, which
include the source language, the translation unit considered, the target
language, and the automatic system used. The capacity to accurately detect
difficult-to-translate segments can be efficiently leveraged by a variety of
pre-translation strategies. In this work, we provide an in-depth study of how
well  translation difficulty can be predicted for English. We tackle the task
as a sequence annotation problem, and report empirical results for three types
of translation units, two domains, and four target languages. The efficiency of
our approach is confirmed by an extrinsic evaluation in which translation
systems exploit the correct translation of predicted difficult-to-translate
units. We finally consider whether source translation difficulty can be better
predicted by using features resulting from translation into other languages,
and find that target languages can efficiently complement each others.",7 Feb 2017 12:03:42 GMT,Empirical/Data-Driven,Multilinguality,multilingual applications,Juxxx,Ixx,xxxxxxxxx@limsi.fr,LIMSI-CNRS,No,Aurxxxxxx,Mxx,xxxxxxxxxxax@limsi.fr,LIMSI-CNRS & Univ. Paris Sud,No,Fraxxxxxx,Yvxx,xxxxxximsi.fr,LIMSI/CNRS,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Juxxx,Ixx,University of Sheffield,,,,,,xxxxxxxxxx4@gmail.com,,,,,United Kingdom,,Juxxx Ixx;Aurxxxxxx Mxx;Fraxxxxxx Yvxx,xxxxxxxxx@limsi.fr;xxxxxxxxxxxax@limsi.fr;xxxxxxximsi.fr,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
479,479X-D6F4E5E6H4,Scientific Information Extraction with Semi-supervised Neural Tagging,Yx Luxx;Maxx Ostxxxxxx and Hanxxxxx Hajxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper addresses the problem of extracting                          keyphrases
from
scientific
articles and categorizing them as corresponding to a task, process, or
material. We cast the problem as sequence tagging and introduce a
semi-supervised neural tagging model, building on recent advances in named
entity recognition tagging. Since annotated training data is scarce in this
domain, we introduce a semi-supervised learning strategy for leveraging
unannotated articles, and find that good performance is obtained using
multi-domain training initially, followed by domain-dependent semi-supervised
learning. Our model achieves state-of-the-art information extraction
performance on a 2017 SemEval task, giving substantial gains over a
bootstrapping, template-based baseline.",7 Feb 2017 11:29:25 GMT,Applications/Tools,"Information extraction, text mining, and question answering",unsupervised and semi-supervised learning;  NLP applications;  information extraction,Yx,Luxx,xxxxxx@uw.edu,University of Washington,No,Maxx,Ostxxxxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,Hanxxxxx,Hajxxxxxxx,xxxxxxxxxxxshington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yx,Luxx,University of Washington,,,,,,xxxxxx@uw.edu,,,WA,,United States,,Yx Luxx;Maxx Ostxxxxxx;Hanxxxxx Hajxxxxxxx,xxxxxx@uw.edu;xxxxxxxxxxxxxashington.edu;xxxxxxxxxxxxshington.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
480,480X-P6H9H6F8H5,Predictability of Textual Entailment Labels without Premise Sentences,Masxxxxxx Tsuxxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Quality of training data is one of the crucial problems when a
learning-centered approach is employed.
This paper proposes a new approach to investigate quality of a large corpus
designed for recognizing textual entailment (RTE) task.
The proposed approach, which is inspired by a statistical hypothesis test,
consists of two phases: the first phase is to indroduce the
predictability of textual entailment labels as a null hypothesis which is
extremely unacceptable if a target corpus has no hidden bias, and the second
phase is to test the null hypothesis using a simple neural network model.
The experimental result of the Stanford Natural Language Inference (SNLI)
corpus does not reject the null hypothesis.
Therefore, it indicates that the SNLI corpus has a hidden bias which allows
prediction of textual entailment labels from hypothesis sentences even if no
context information given by a premise sentence.
This paper also presents a performance impact of modern RTE models
caused by this hidden bias.",7 Feb 2017 03:38:37 GMT,Resources/Evaluation,Machine learning,textual entailment and paraphrasing;  evaluation metrics,Masxxxxxx,Tsuxxxxx,xxxxxxxxxxxc.tut.ac.jp,Toyohashi University of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Masxxxxxx,Tsuxxxxx,Toyohashi University of Technology,,,,,,xxxxxxxxxxxc.tut.ac.jp,,Toyohashi,Aichi,,Japan,,Masxxxxxx Tsuxxxxx,xxxxxxxxxxxc.tut.ac.jp,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
481,481X-D3B6H3C5P3,FOIL it! Find One mismatch between Image and Language caption,Raxx Shexxxx;Saxxxx Pezxxxxx;Yaxxxx Klixxxxxx;Aurxxxxx Herxxxxx;Moxx Naxx;Enxxx Sanxxxxxx and Rafxxxxxx Berxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Accept - Oral Monday,,Undecided (Vision Robots Grounding),"In this paper, we aim to understand whether current language and vision (LaVi)
models truly grasp the interaction between the two modalities. To this end, we
propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images
with both correct and `foil' captions, that is, descriptions of the image that
are highly similar to the original ones, but contain one single mistake (`foil
word'). We show that current LaVi models fall into the traps of this data and
perform badly on three tasks: a) caption  classification (correct vs. foil); b)
foil word detection; c) foil word correction. Humans, in contrast, have
near-perfect performance on those tasks. We demonstrate that merely utilising
language cues is not enough to model FOIL-COCO and that it challenges the
state-of-the-art by requiring a fine-grained understanding of the relation
between text and image.",21 Apr 2017 20:15:58 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",,Raxx,Shexxxx,xxxxxxxxxxar@unitn.it,University of Trento,No,Saxxxx,Pezxxxxx,xxxxxxxxxxxxlle@unitn.it,University of Trento,No,Yaxxxx,Klixxxxxx,xxxxxxxxxxxxich@gmail.com,University of Trento,No,Aurxxxxx,Herxxxxx,xxxxxxxxxxxxxlot@cantab.net,University of Trento,No,Moxx,Naxx,xxxxxxxxx@unitn.it,University of Trento,No,Enxxx,Sanxxxxxx,xxxxxxxxxxxxeto@unitn.it,University of Trento,No,Rafxxxxxx,Berxxxxx,xxxxxxxxxxxsi.unitn.it,University of Trento,No,,,,,,,,,,,,,,,,,Rafxxxxxx,Berxxxxx,University of Trento,,,,,,xxxxxxxxxxxsi.unitn.it,,,,,Italy,,Raxx Shexxxx;Saxxxx Pezxxxxx;Yaxxxx Klixxxxxx;Aurxxxxx Herxxxxx;Moxx Naxx;Enxxx Sanxxxxxx;Rafxxxxxx Berxxxxx,xxxxxxxxxxar@unitn.it;xxxxxxxxxxxxelle@unitn.it;xxxxxxxxxxxxxich@gmail.com;xxxxxxxxxxxxxxlot@cantab.net;xxxxxxxxxi@unitn.it;xxxxxxxxxxxxneto@unitn.it;xxxxxxxxxxxisi.unitn.it,FOIL it! Find One mismatch between Image and Language caption,FOIL it! Find One mismatch between Image and Language caption,11,Aurelie Herbelot,,University of Trento,on,on,Only include my submission if it is accepted.,No,None,None
482,482X-B5P8D7C7H7,Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors,Raxx Sorxxxx and Nxx Dixx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present a dual contribution to the task of machine reading-comprehension: a
technique for creating large-sized machine-comprehension (MC) datasets using
paragraph-vector models; and a novel, hybrid neural-network architecture that
combines the representation power of recurrent neural networks with the
discriminative power of fully-connected multi-layered networks.
We use the MC-dataset generation technique to build a dataset of around 2
million examples, for which we empirically determine the high-ceiling of human
performance (around 91% accuracy), as well as the performance of a variety of
computer models. Among all the models we have experimented with, our hybrid
neural-network architecture achieves the highest performance (83.2% accuracy).
The remaining gap to the human-performance ceiling provides enough room for
future model improvements.",6 Feb 2017 21:29:35 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",generative models;  discriminative learning methods;  context-aware question answering;  answer extraction,Raxx,Sorxxxx,xxxxxxxxxxxt@gmail.com,Google Inc,No,Nxx,Dixx,xxxxxxxxxoogle.com,Google,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Raxx,Sorxxxx,Google Inc,,,,,,xxxxxxxxxxxt@gmail.com,,Mountain View,CA,,United States,,Raxx Sorxxxx;Nxx Dixx,xxxxxxxxxxxt@gmail.com;xxxxxxxxxgoogle.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
483,483X-F3J6G6F6D5,Here's My Point: Argumentation Mining with Pointer Networks,Pexxx Poxxxx;Alxxxx Romxxxx and Anxx Rumxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Argumentation mining seeks to uncover the argument structure present in
argumentative text. In order to determine this structure, one must understand
how different individual components of the overall argument are linked. General
consensus in this field dictates that the argument components form a hierarchy
of persuasion, which manifests itself in a tree structure. This work provides
the first neural network-based approach to argumentation mining, focusing on
the dual tasks of extracting links between argument components, and classifying
types of argument components. We propose to use a joint model based on a
Pointer Network architecture to simultaneously solve these tasks. In doing so,
we construct a joint model that simultaneously attempts to learn the type of
argument component, as well as continuing to predict links between argument
components. The proposed joint model achieves state-of-the-art results on two
separate evaluation corpora, achieving far superior performance than a regular
Pointer Network model. Our results show that optimizing for both tasks is
crucial for high performance.",6 Feb 2017 21:38:20 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",discourse;  parsing;  text classification;  relation/event extraction,Pexxx,Poxxxx,xxxxxxxxxs.uml.edu,University of Massachusetts Lowell,No,Alxxxx,Romxxxx,xxxxxxxxxtlook.com,UMass Lowell,No,Anxx,Rumxxxxxx,xxxxxxx.uml.edu,University of Massachusetts Lowell,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Pexxx,Poxxxx,Microsoft Research Montreal,,,415xxxxxxx,,,xxxxxxxxxxxxxmicrosoft.com,,,,,Canada,,Pexxx Poxxxx;Alxxxx Romxxxx;Anxx Rumxxxxxx,xxxxxxxxxs.uml.edu;xxxxxxxxxutlook.com;xxxxxxxx.uml.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
484,484X-E8C3H2J6A4,Joint CTC/attention decoding for end-to-end speech recognition,Takxxxx Hoxx;Shxxxx Watxxxxx and Joxx Hexxxx,Speech,Chxxxx Hoxx;Chixxxxxx Lxx,Accept - Oral Monday,,Accept - Poster - Monday (Speech),"End-to-end automatic speech recognition (ASR) has become a popular alternative
to conventional DNN/HMM systems because it avoids the need for linguistic
resources such as pronunciation dictionary, tokenization, and
context-dependency trees, leading to a greatly simplified model-building
process. There are two major types of end-to-end architectures for ASR: 
attention-based methods use an attention mechanism to perform alignment between
acoustic frames and recognized symbols, and connectionist temporal
classification (CTC), uses Markov assumptions to efficiently solve sequential
problems by dynamic programming. This paper proposes joint decoding algorithm
for end-to-end ASR with a hybrid CTC/attention architecture, which effectively
utilizes both advantages in decoding. We have applied the proposed method to
two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the
comparable performance to conventional state-of-the-art DNN/HMM ASR systems
without linguistic resources.",23 Apr 2017 06:14:02 GMT,Empirical/Data-Driven,Speech,,Takxxxx,Hoxx,xxxxxxxerl.com,MERL,No,Shxxxx,Watxxxxx,xxxxxxxxieee.org,MERL,No,Joxx,Herxxxx,xxxxxxxxxxxxey@gmail.com,MERL,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Watxxxxx,Johns Hopkins University,,,,,,xxxxxxxxieee.org,,,MA,,United States,,Takxxxx Hoxx;Shxxxx Watxxxxx;Joxx Herxxxx,xxxxxxxerl.com;xxxxxxxx@ieee.org;xxxxxxxxxxxxhey@gmail.com,Joint CTC/attention decoding for end-to-end speech recognition,Joint CTC/attention decoding for end-to-end speech recognition,12,Takaaki Hori,,"Mitsubishi Electric Research Laboratories
201 Broadway, Cambridge MA, 02139 USA",,,Only include my submission if it is accepted.,No,None,None
485,485X-J5H5D5A7J5,A Hybrid Model for Sentiment Analysis using Multi-faceted Corpus and Lexicon with a Case Study on Arabic Reviews for Governmental Services,Axx Haxxx;Khxxxx Shxxxx and Anaxxxx Zaxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Sentiment analysis in Arabic text suffers from low accuracy due to
Arabic-specific challenges such as limited resources, morphological complexity
and dialects, in addition to general linguistic issues such as fuzziness,
implicit, sarcasm and spam. Furthermore, Arabic is a limited-resource language,
hence efforts are required to build new or to improve existing corpora and
lexica. In this paper, we present a new multi-faceted Arabic corpus and
lexicon, which are annotated with domains, dialects, linguistic issues and
polarity strengths. As a case study of these resources, we collected 15,000
reviews from various sources pertaining to governmental services in an Arab
country. We also introduce a hybrid model combining a corpus-based and a
lexicon-based model using the developed resources. The corpus-based model has
two interrelated stages; training full-corpus classification models for all
facets, then training class-specific models on subsets of the corpus that are
filtered based on the performances of the full-corpus models. As such,
class-specific models are trained by removing irrelevant instances from the
corpus. To further improve performance, the lexicon is used to calculate
polarity strengths. The lexicon-based model also filters the annotated segments
based on the specific classes of the domain and dialect. Experimental results
indicate that the accuracy of this hybrid model ranges between 82 and 95%, and
between 79 and 86% after oversampling the dataset.",6 Feb 2017 21:47:58 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  lexicon development;  opinion mining and extraction;  text mining;  text classification;  NLP in social networking media,Axx,Haxxx,xxxxxxxxx@gmail.com,"Faculty of Computing, Universiti Teknologi Malaysia, Johor, Malaysia",No,Khxxxx,Shxxxx,xxxxxxxxxxxan@qu.edu.qa,"Department of Computer Science and Engineering, College of Engineering, Qatar University, Doha, Qatar",No,Anaxxxx,Zaxxxx,xxxxxxxxgmail.com,"Faculty of Computing, Universiti Teknologi Malaysia, Johor, Malaysia",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Axx,Haxxx,"Faculty of Computing, Universiti Teknologi Malaysia, Johor, Malaysia",,,9747xxxxxxx,,,xxxxxxxxx@gmail.com,,Johor Bahru,,,Malaysia,,Axx Haxxx;Khxxxx Shxxxx;Anaxxxx Zaxxxx,xxxxxxxxx@gmail.com;xxxxxxxxxxxxan@qu.edu.qa;xxxxxxxxxgmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
486,486X-G9D3H3F9P5,Massive Exploration of Neural Machine Translation Architectures,Dexxx Brxxx;Anxx Goxxxx;Minxxxxxxx Luxxx and Quxx L,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural Machine Translation (NMT) has shown remarkable progress over the past
few years, with production systems now being deployed to end-users. One major
drawback of current architectures is that they are expensive to train,
typically requiring days to weeks of GPU time to converge. This makes
exhaustive hyperparameter search, as is commonly done with other neural network
architectures, prohibitively expensive. In this work, we present the first
large-scale analysis of NMT architecture hyperparameters. We report empirical
results and variance numbers for several hundred experimental runs,
corresponding to over 250,000 GPU hours on the standard WMT English to German
translation task. Our experiments lead to novel insights and practical advice
for building and extending NMT architectures. As part of this contribution, we
release an open-source NMT framework that enables researchers to easily
experiment with novel techniques and reproduce state of the art results.",7 Feb 2017 08:24:25 GMT,Empirical/Data-Driven,Machine translation,MT evaluations;  MT applications,Dexxx,Brxxx,xxxxxxxxxx@google.com,Google,No,Anxx,Goxxxx,xxxxxxxxxoogle.com,Google Brain,No,Minxxxxxxx,Luxxx,xxxxxxxxxxxng@gmail.com,Google Brain,No,Quxx,Lx,xxxxxxxgle.com,Google Brain,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dexxx,Brxxx,Google,,,310-xxxxxxxx,,,xxxxxxxxxx@gmail.com,,,CA,,United States,,Dexxx Brxxx;Anxx Goxxxx;Minxxxxxxx Luxxx;Quxx Lx,xxxxxxxxxx@google.com;xxxxxxxxxgoogle.com;xxxxxxxxxxxxng@gmail.com;xxxxxxxogle.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
487,487X-A2F3H6B6B4,Deep Recurrent Generative Decoder for Abstractive Text Summarization,Pixx Lx;Wxx Lxx;Lixxxx Bixx and Zixxx Wxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"We propose a new framework for abstractive text summarization based on a
sequence-to-sequence oriented encoder-decoder model equipped with a deep
recurrent generative decoder (DRGN).
Latent structure information implied in the target summaries is learned based
on a recurrent latent random model for improving the summarization quality.
Neural variational inference is employed to address the intractable posterior
inference for the recurrent latent variables.
Abstractive summaries are generated based on both the generative latent
variables and the discriminative deterministic states.
Extensive experiments on some benchmark datasets in different languages show
that DRGN achieves improvements over the state-of-the-art methods.",7 Feb 2017 11:49:41 GMT,Applications/Tools,Summarization,language generation;  text mining;  document summarization,Pixx,Lx,xxxxxxxxx@gmail.com,The Chinese University of Hong Kong,No,Wxx,Lxx,xxxxxxxxxuhk.edu.hk,The Chinese University of Hong Kong,No,Lixxxx,Bixx,xxxxxxxxxx@gmail.com,"AI Platform Department, Tencent Inc.",No,Zixxx,Waxx,xxxxxxxxxxxpt@gmail.com,The Chinese University of Hong Kong,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Pixx,Lx,The Chinese University of Hong Kong,,,,,,xxxxxxxxx@gmail.com,,,,,China,,Pixx Lx;Wxx Lxx;Lixxxx Bixx;Zixxx Waxx,xxxxxxxxx@gmail.com;xxxxxxxxxxuhk.edu.hk;xxxxxxxxxxg@gmail.com;xxxxxxxxxxxxpt@gmail.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
488,488X-B4B3G3J4A6,Frame-based Semantic Patterns for Relation Extraction,Angxxxx Maxxxx;Danxxxxx Bolxxxxxx;Frxxx Coxxxx and Kaxxx Atkxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper presents novel frame-based semantic patterns, exploiting frame
element and frame annotations, provided by FrameNet. The proposed frame-based
patterns are evaluated against state-of-the-art dependency based syntactic
patterns and lexico-syntactic patterns, on three independent datasets that
differ in size and  construction. The results show that the proposed
frame-based patterns significantly improve performance, both in terms of
scoring higher precision and higher recall for relation extraction, in
comparison to dependency and lexico-syntactic patterns on all three datasets.",7 Feb 2017 10:43:06 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  information retrieval;  relation/event extraction,Angxxxx,Maxxxx,xxxxxxxxxxxxxxxliverpool.ac.uk,University of Liverpool,No,Danxxxxx,Bolxxxxxx,xxxxxxxxxxxxxxxxxa@liverpool.ac.uk,University of Liverpool,No,Frxxx,Coxxxx,xxxxxxxxxxxrpool.ac.uk,University of Liverpool,No,Kaxxx,Atkxxxxx,xxxxxxxxxxrpool.ac.uk,University of Liverpool,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Angxxxx,Maxxxx,University of Liverpool,,,,,,xxxxxxxxxxxxxxxliverpool.ac.uk,,,,,United Kingdom,,Angxxxx Maxxxx;Danxxxxx Bolxxxxxx;Frxxx Coxxxx;Kaxxx Atkxxxxx,xxxxxxxxxxxxxxxliverpool.ac.uk;xxxxxxxxxxxxxxxxxla@liverpool.ac.uk;xxxxxxxxxxxerpool.ac.uk;xxxxxxxxxxxrpool.ac.uk,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
489,489X-B4C5G9H5A4,Obtaining referential word meanings from visual and distributional information: Experiments on object naming,Sixx Zarxxxxx and Daxxx Schxxxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Accept - Oral Monday,,Undecided (Vision Robots Grounding),"We investigate object naming, which is an important sub-task of referring
expression generation on real-world images. As opposed to mutually exclusive
labels used in object recognition, object names are more flexible, subject to
communicative preferences and semantically related to each other. Therefore, we
investigate models of referential word meaning that link visual to lexical
information which we assume to be given through distributional word embeddings.

We present a model that learns individual predictors for object names that link
visual and distributional aspects of word meaning during training. We show that
this is particularly beneficial for zero-shot learning, as compared to
projecting visual objects directly into the distributional space. In a standard
object naming task, we find that different ways of combining lexical and visual
information achieve very similar performance, though experiments on model
combination suggest that they capture complementary aspects of referential
meaning.",21 Apr 2017 10:25:19 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",,Sixx,Zarxxxxx,xxxxxxxxxxxxxxxni-bielefeld.de,University of Bielefeld,No,Daxxx,Schxxxxxx,xxxxxxxxxxxxxxxxuni-bielefeld.de,Bielefeld University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sixx,Zarxxxxx,University of Bielefeld,,,,,,xxxxxxxxxxxss@gmail.com,,,,,Germany,,Sixx Zarxxxxx;Daxxx Schxxxxxx,xxxxxxxxxxxxxxxni-bielefeld.de;xxxxxxxxxxxxxxxx@uni-bielefeld.de,Obtaining referential word meanings from visual and distributional information: Experiments on object naming,Obtaining referential word meanings from visual and distributional information,12,Sina Zarrieß,,"Sina Zarrieß
Bielefeld University, Germany
Universitätsstraße 25
D-33615, Bielefeld",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
490,490X-A6H3H3F7A3,Vocalic Classification through Spectral Decomposition,Patxxxxx Thxxxx and Gexxxx Pexx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"We consider two related problems in this paper. Given an undeciphered
alphabetic writing system or mono-alphabetic cipher, determine: (1) whether the
underlying writing system is a (vocalic) alphabet or an abjad; and (2) if an
alphabet, which of its letters are vowels and which are consonants.  We are
able to show that a very simple spectral decomposition based on character
co-occurrences provides nearly perfect performance with respect to answering
both question types.  We also describe the justification for and release of a
new corpus for evaluating such methods.",7 Feb 2017 00:43:16 GMT,Resources/Evaluation,"Phonology, morphology, and word segmentation",cross-lingual approaches;  phonology;  transliteration,Patxxxxx,Thxxxx,xxxxxxxxxxxtoronto.edu,University of Toronto,No,Gexxxx,Pexx,xxxxxxxxxxoronto.edu,University of Toronto,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gexxxx,Pexx,University of Toronto,,,1416xxxxxxx,,,xxxxxxxxxxoronto.edu,,,,,Canada,,Patxxxxx Thxxxx;Gexxxx Pexx,xxxxxxxxxxxtoronto.edu;xxxxxxxxxxtoronto.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
491,491X-A9J8E3D6D6,Understanding and Predicting Empathic Behavior in Counseling Therapy,Verxxxxxx Pérxxxxxxxx;Raxx Mihxxxxx;Kenxxxx Resxxxxx;Satxxxxx Sixxx and Lawxxxxx A,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"Counselor empathy is associated with better outcomes in psychology and
behavioral counseling. In this paper, we explore several aspects pertaining to
counseling interaction dynamics and their relation to counselor empathy during
motivational interviewing encounters. Particularly, we analyze aspects such as
participants' engagement, participants' verbal and nonverbal accommodation, as
well as topics being discussed during the conversation, with the final goal of
identifying linguistic and acoustic markers of counselor empathy. We also show
how we can use these findings alongside other raw linguistic and acoustic
features to  build accurate counselor empathy classifiers with accuracies of up
to 80%.",22 Apr 2017 05:10:06 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Verxxxxxx,Pérxxxxxxxx,xxxxxxxxumich.edu,University of Michigan,No,Raxx,Mihxxxxx,xxxxxxxxxumich.edu,University of Michigan,No,Kenxxxx,Resxxxxx,xxxxxxxxumich.edu,University of Michigan,No,Satxxxxx,Sixxx,xxxxxxxxmich.edu,University of Michigan,No,Lawxxxxx,Ax,xxxxxxxich.edu,University of Michigan,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Verxxxxxx,Pérxxxxxxxx,University of Michigan,,,940xxxxxxx,,,xxxxxxxxumich.edu,,Ann Arbor,MI,,United States,,Verxxxxxx Pérxxxxxxxx;Raxx Mihxxxxx;Kenxxxx Resxxxxx;Satxxxxx Sixxx;Lawxxxxx Ax,xxxxxxxxumich.edu;xxxxxxxxx@umich.edu;xxxxxxxxxumich.edu;xxxxxxxxumich.edu;xxxxxxxmich.edu,Understanding and Predicting Empathic Behavior in Counseling Therapy,Understanding and Predicting Empathic Behavior in Counseling Therapy,10,Veronica Perez-Rosas,,"University of Michigan. 500 S State St, Ann Arbor, MI 48109",,,No. Do not include my submission in this dataset.,No,None,None
492,492X-F3J8F8J3B4,Canonical Correlation Inference for Mapping Abstract Scenes to Text,Hexxx Jixxx;Nixxx Papasaxxxxxxxxxxx and Shxx Bx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an ``abstract scene.''",6 Feb 2017 22:01:45 GMT,Empirical/Data-Driven,Machine learning,multimodal representations and processing,Hexxx,Jixxx,xxxxxxxxxxanford.edu,Stanford University,No,Nixxx,Papasaxxxxxxxxxxx,xxxxxxxxxxxa@gmail.com,University of Edinburgh,No,Shaxxxx,Coxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shaxxxx,Coxxx,University of Edinburgh,,,,,,xxxxxxxxxf.ed.ac.uk,,,,,United Kingdom,,Hexxx Jixxx;Nixxx Papasaxxxxxxxxxxx;Shxx Bx,xxxxxxxxxxanford.edu;xxxxxxxxxxxsa@gmail.com;xxxxxxxxxxf.ed.ac.uk,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
493,493X-F5F3C2E2D8,Representations of language in a model of visually grounded speech signal,Grzxxxxx Chrxxxxxx;Lixxx Gelxxxxxxx and Afxx Alixxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Accept - Oral Tuesday,,Undecided (Cognitive Modelling and Psycholinguistics),"We present a visually grounded model of speech perception which projects spoken
utterances and images to a joint semantic space. We use a multi-layer recurrent
highway network to model the temporal nature of spoken speech, and show that it
learns to extract both form and meaning-based linguistic knowledge from the
input signal. We carry out an in-depth analysis of the representations used by
different components of the trained model and show that encoding of semantic
aspects tends to become richer as we go up the hierarchy of layers, whereas
encoding of form-related aspects of the language input tends to initially
increase and then plateau or decrease.",15 Apr 2017 11:36:29 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,,Grzxxxxx,Chrxxxxxx,xxxxxxxxla@uvt.nl,Tilburg University,No,Lixxx,Gelxxxxxxx,xxxxxxxxxxxxoos@gmail.com,Tilburg University,No,Afxx,Alixxxxx,xxxxxxxxhi@uvt.nl,Tilburg University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Grzxxxxx,Chrxxxxxx,Tilburg University,,,,,,xxxxxxxxla@uvt.nl,,,,,Netherlands,,Grzxxxxx Chrxxxxxx;Lixxx Gelxxxxxxx;Afxx Alixxxxx,xxxxxxxxla@uvt.nl;xxxxxxxxxxxxxoos@gmail.com;xxxxxxxxxhi@uvt.nl,Representations of language in a model of visually grounded speech signal,Model of visually grounded speech,10,Grzegorz Chrupała,,"Communication and Information Sciences
Tilburg University
PO Box 90153
5000 LE Tilburg
The Netherlands",on,on,No. Do not include my submission in this dataset.,No,None,None
494,494X-D8F5P9E6C3,Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,Ivxx Vuxxxx;Nixxxx Mrkxxxxx;Rxx Reixxxxx;Diaxxxxx �x;Stxxx Yoxxx and Anxx Korxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Monday,,Undecided (Semantics),"Morphologically rich languages accentuate two properties of distributional
vector space models: 1) the difficulty of inducing accurate representations for
low-frequency word forms; and 2) insensitivity to distinct lexical relations
that have similar distributional signatures. These effects are detrimental for
language understanding systems, which may infer that  'inexpensive' is a
rephrasing for 'expensive' or may not associate 'acquire' with 'acquires'. In
this work, we propose a novel morph-fitting procedure which moves past the use
of curated semantic lexicons for improving distributional vector spaces.
Instead, our method injects morphological constraints generated using simple
language-specific rules, pulling inflectional forms of the same word close
together and pushing derivational antonyms far apart. In intrinsic evaluation
over four languages, we show that our approach: 1) improves low-frequency word
estimates; and 2) boosts the semantic quality of the entire word vector
collection. Finally, we show that morph-fitted vectors yield large gains in the
downstream task of dialogue state tracking, highlighting the importance of
morphology for tackling long-tail phenomena in language understanding tasks.",21 Apr 2017 10:16:20 GMT,Empirical/Data-Driven,Semantics,,Ivxx,Vuxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Nixxxx,Mrkxxxxx,xxxxxxxxxxxic@gmail.com,University of Cambridge,No,Rxx,Reixxxxx,xxxxxxxxxxt@gmail.com,Technion - Israel Institute of Technology,No,Diaxxxxx,Ó Sxxxxxxxx,xxxxxxxam.ac.uk,Apple,No,Stxxx,Yoxxx,xxxxxxxxcam.ac.uk,Cambridge University,No,Anxx,Korxxxxx,xxxxxxxxxxxxx@cl.cam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,Ivxx,Vuxxxx,University of Cambridge,,,4474xxxxxxxx,,,xxxxxxxam.ac.uk,,Cambridge,,,United Kingdom,,Ivxx Vuxxxx;Nixxxx Mrkxxxxx;Rxx Reixxxxx;Diaxxxxx �x;Stxxx Yoxxx;Anxx Korxxxxx,xxxxxxxam.ac.uk;xxxxxxxxxxxxic@gmail.com;xxxxxxxxxxxt@gmail.com;xxxxxxxxam.ac.uk;xxxxxxxxxcam.ac.uk;xxxxxxxxxxxxxn@cl.cam.ac.uk,Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,13,Ivan Vulić,Research Associate,"Language Technology Lab, DTAL, University of Cambridge, 9 West Road, CB3 9DP, Cambridge, United Kingdom",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
496,496X-E9C6F5P8C9,What do Neural Machine Translation Models Learn about Morphology?,Yonxxxx Belxxxxx;Naxxx Durxxxx;Faxxx Daxxx;Haxxxx Saxxxx and Jaxxx Glxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Oral Tuesday,,Undecided (Machine Translation),"Neural machine translation (MT) models obtain state-of-the-art performance
while maintaining a simple, end-to-end architecture. However, little is known
about what these models learn about source and target languages during the
training process. In this work, we analyze the representations learned by
neural MT models at various levels of granularity and empirically evaluate the
quality of the representations for learning morphology through extrinsic
part-of-speech and morphological tagging tasks. We conduct a thorough
investigation along several parameters: word-based vs. character-based
representations, depth of the encoding layer, the identity of the target
language, and encoder vs. decoder representations. Our data-driven,
quantitative evaluation sheds light on important aspects in the neural MT
system and its ability to capture word structure.",21 Apr 2017 19:34:31 GMT,Empirical/Data-Driven,Machine translation,,Yonxxxx,Belxxxxx,xxxxxxxx@mit.edu,MIT CSAIL,No,Naxxx,Durxxxx,xxxxxxxxxqf.org.qa,QCRI,No,Faxxx,Daxxx,xxxxxxxxxxn@qf.org.qa,Qatar Computing Research Institute,No,Haxxxx,Saxxxx,xxxxxxxxqf.org.qa,Qatar Computing Research Institute,No,Jaxxx,Glxxx,xxxxxxmit.edu,Massachusetts Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yonxxxx,Belxxxxx,MIT CSAIL,,,,,,xxxxxxxx@mit.edu,,Cambridge,MA,,United States,,Yonxxxx Belxxxxx;Naxxx Durxxxx;Faxxx Daxxx;Haxxxx Saxxxx;Jaxxx Glxxx,xxxxxxxx@mit.edu;xxxxxxxxx@qf.org.qa;xxxxxxxxxxxn@qf.org.qa;xxxxxxxxxqf.org.qa;xxxxxxxmit.edu,What do Neural Machine Translation Models Learn about Morphology?,What do Neural Machine Translation Models Learn about Morphology?,12,Yonatan Belinkov,,"MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street
Cambridge, MA 02139
USA",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
498,498X-P3P6F4F3H3,Fast Adaptation for Multi-Domain Neural Machine Translation,Mx Amxx;Maxxx Tuxxxx;Maxxxx Nexxx and Marxxxxx Fedxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"We address one main bottleneck of Neural Machine Translation technology that
currently hinders its applicability when the task data is  heterogeneous - i.e.
 it covers many different domains. We approach this issue via unsupervised
online adaption of a generic Neural MT system by making use of a few training
instances retrieved at translation time which are relevant to the input
sentence.
Besides implementing a close to real-time adaptation protocol, we investigate
different instance selection techniques and analyse the impacts of similarity
of the retrieved sentences on the performance of the system.
We empirically show that the instance-based adaptation approach yields
consistent improvements against the baseline in all the target domains, even in
the domains where the similarity of the retrieved samples is relatively low.",7 Feb 2017 11:20:32 GMT,Empirical/Data-Driven,Machine translation,MT applications;  domain adaptation;  on-line translation,M. xxxx,Farxxxxx,xxxxxxxn@fbk.eu,"University of Trento, FBK",No,Maxxx,Tuxxxx,xxxxxxxxxxxi@gmail.com,Fondazione Bruno Kessler,No,Maxxxx,Nexxx,xxxxxxfbk.eu,Fondazione Bruno Kessler,No,Marxxxxx,Fedxxxxx,xxxxxxxo@fbk.eu,FBK,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,M. xxxx,Farxxxxx,"University of Trento, FBK",,,,,,xxxxxxxn@fbk.eu,,,,,Italy,,Mx Amxx;Maxxx Tuxxxx;Maxxxx Nexxx;Marxxxxx Fedxxxxx,xxxxxxxn@fbk.eu;xxxxxxxxxxxhi@gmail.com;xxxxxx@fbk.eu;xxxxxxxxo@fbk.eu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
499,499X-D9H6G3G7D9,Cross-lingual Abstract Meaning Representation Parsing,Maxxx Damxxxx and Shxx Bx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Abstract Meaning Representation (AMR) annotation efforts have mostly focused on
English. In order to train parsers on other languages, we propose a method
based on annotation projection, which involves exploiting annotations in a
source language and a parallel corpus between the source language and a target
language. We show promising results for Italian, Spanish and German. Poor
parsing results for Chinese are attributed to errors in the word alignments.
Beside evaluating the target parsers on non-gold datasets, we further propose
an evaluation method that does not require access to gold annotations for the
target languages. This is achieved by inverting the projection process: a new
English parser is learned from the target language parser and evaluated on the
existing English gold standard. To our knowledge, we are the first to publish
results on AMR parsing for these languages.",7 Feb 2017 09:32:41 GMT,Empirical/Data-Driven,Semantics,cross-lingual approaches;  semantic relations;  parsing,Maxxx,Damxxxx,xxxxxxxxxxxms.ed.ac.uk,The University of Edinburgh,No,Shaxxxx,Coxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxx,Damxxxx,The University of Edinburgh,,,,,,xxxxxxxxxxxms.ed.ac.uk,,,,,United Kingdom,,Maxxx Damxxxx;Shxx Bx,xxxxxxxxxxxms.ed.ac.uk;xxxxxxxxxxf.ed.ac.uk,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
500,500X-A8A6J8A8F8,Word Embedding Models for Metaphor Identification,Kexxx Stxxx and Maxxxx Paxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Identification of metaphoric use in text is critical for accurate semantic
representation. Automatic metaphor identification has largely relied on
heuristic based models or feature-based machine learning, using hand-crafted
lexical resources. While recent work has started to leverage deep learning
through word embeddings as well as other neural models, word embeddings have
yet to be fully leveraged as a metaphor classification tool. This work shows
word embeddings can be used along with random forest classification to achieve
state of the art results that are robust across domain, effective for all parts
of speech, and do not rely on any hand-crafted lexical resources.",7 Feb 2017 01:41:32 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  selectional preferences;  figurative language,Kexxx,Stxxx,xxxxxxxxxxolorado.edu,"University of Colorado, Boulder",No,Maxxxx,Paxxxx,xxxxxxxxxxxxx@colorado.edu,University of Colorado,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kexxx,Stxxx,"University of Colorado, Boulder",,,,,,xxxxxxxxxxolorado.edu,,,,,United States,"I am PhD student at the University of Colorado. I study lexical semantics, natural language processing for social media, crisis informatics, and metaphor detection. I'm also interested in construction grammar and semantic parsing.",Kexxx Stxxx;Maxxxx Paxxxx,xxxxxxxxxxolorado.edu;xxxxxxxxxxxxxr@colorado.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
501,501X-D4D6B2E3F2,Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task,Nxx Dixx;Sebxxxxxx Gooxxxx;Fxx Sxx and Raxx Soxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"We introduce a new multi-modal task for computer systems, posed as a combined
vision-language comprehension challenge: identifying the most suitable text
describing a scene, given several similar options. Accomplishing the task
entails demonstrating comprehension beyond just recognizing ``keywords'' (or
key-phrases) and their corresponding visual concepts. Instead, it requires an
alignment between the representations of the two modalities that achieves a
visually-grounded ``understanding'' of various linguistic elements and their
dependencies. This new task also admits an easy-to-compute and well-studied
metric: the accuracy in detecting the true target among the decoys.

The paper makes several contributions: an effective and extensible mechanism
for generating decoys from (human-created) image captions; an instance of
applying this mechanism, yielding a large-scale machine comprehension dataset
(based on the COCO images and captions) that we make publicly available; human
evaluation results on this dataset, informing a performance upper-bound; and
several baseline and competitive learning approaches that illustrate the
utility of the proposed task and dataset in advancing both image and language
comprehension. We also show that, in a                    multi-task learning
setting, the
performance on the proposed task is positively correlated with the end-to-end
task of image captioning.",7 Feb 2017 02:00:38 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",language generation;  discriminative learning methods;  structured input/output;  multimodal representations and processing,Nxx,Dixx,xxxxxxxxxoogle.com,Google,No,Sebxxxxxx,Gooxxxx,xxxxxxxxxoogle.com,Google,No,Fxx,Sxx,xxxxxxxusc.edu,USC,No,Raxx,Sorxxxx,xxxxxxxxxxxt@gmail.com,Google Inc,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nxx,Dixx,Google,,,,,,xxxxxxxxxoogle.com,,,,,United States,,Nxx Dixx;Sebxxxxxx Gooxxxx;Fxx Sxx;Raxx Sorxxxx,xxxxxxxxxoogle.com;xxxxxxxxxgoogle.com;xxxxxxx@usc.edu;xxxxxxxxxxxut@gmail.com,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
502,502X-E6C9H6P6H3,Guiding Neural Machine Translation Decoding with External Knowledge,Raxxx Chaxxxxxxx;Marxxxxx Fedxxxxx;Maxxxx Nexxx;Maxxx Tuxxxx;Luxxx Spxxxx and Fr�xxxxxxx Blxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Differently from the phrase-based framework, neural machine translation (NMT)
operates on word/sentence representations in a continuous space. This makes the
decoding process more difficult not only to interpret, but also to influence
with external knowledge. For this latter problem, effective solutions like the
XML-markup used by phrase-based models to inject fixed translation options as
constraints at decoding time are not available. We propose a ``guide''
mechanism that enhances a state-of-the-art NMT decoder with the capability to
prioritize and properly handle translation options presented in the form of XML
annotations of                                source                    words. Positive
results
in two
different
translation
tasks indicate the effectiveness of the proposed approach.",7 Feb 2017 10:20:22 GMT,Empirical/Data-Driven,Machine translation,MT applications;  MT deployment,Raxxx,Chaxxxxxxx,xxxxxxxxee@fbk.eu,Fondazione Bruno Kessler,No,Marxxxxx,Fedxxxxx,xxxxxxxo@fbk.eu,FBK,No,Maxxxx,Nexxx,xxxxxxfbk.eu,Fondazione Bruno Kessler,No,Maxxx,Tuxxxx,xxxxxxxxxxxi@gmail.com,Fondazione Bruno Kessler,No,Luxxx,Spxxxx,xxxxxxxxxxxxffield.ac.uk,University of Sheffield,No,Fr�xxxxxxx,Blxxx,xxxxxxxxxxxffield.ac.uk,University of Sheffield,No,,,,,,,,,,,,,,,,,,,,,,Raxxx,Chaxxxxxxx,Fondazione Bruno Kessler,,,,,,xxxxxxxxee@fbk.eu,,Trento,Trentino,,Italy,,Raxxx Chaxxxxxxx;Marxxxxx Fedxxxxx;Maxxxx Nexxx;Maxxx Tuxxxx;Luxxx Spxxxx;Fr�xxxxxxx Blxxx,xxxxxxxxee@fbk.eu;xxxxxxxxo@fbk.eu;xxxxxx@fbk.eu;xxxxxxxxxxxhi@gmail.com;xxxxxxxxxxxxeffield.ac.uk;xxxxxxxxxxxxffield.ac.uk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
503,503X-J6B5P6G9E9,Probabilistic Regular Graph Languages,Soxxxx Gixxxx;Adxx Loxxx and Sebxxxxxx Maxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Distributions over strings and trees can be represented by probabilistic
regular languages, which characterize many models in natural language
processing. Recently, several datasets have become available which represent
natural language phenomena as graphs, so it is natural to ask whether there is
an equivalent of probabilistic regular languages for graphs. To answer this
question, we review three families of graph languages: Hyperedge Replacement
Languages (HRL), which can be made probabilistic; Monadic Second Order
Languages (MSOL), which support the crucial property of closure under
intersection; and Regular Graph Languages (RGL; \citealt{Courcelle:V}), a
subfamily of both HRL and MSOL which inherits these properties, and has not
been widely studied or applied to NLP. We prove that RGLs are closed under
intersection and provide an efficient parsing algorithm, with runtime linear in
the size of the input graph.",7 Feb 2017 10:19:43 GMT,Theoretical,"Tagging, chunking, syntax, and parsing",grammatical formalisms;  graph-based algorithms;  parsing,Soxxxx,Gixxxx,xxxxxxxxxxms.ed.ac.uk,University of Edinburgh,No,Adxx,Loxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,Sebxxxxxx,Maxxxx,xxxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Soxxxx,Gixxxx,University of Edinburgh,,,,,,xxxxxxxxxxms.ed.ac.uk,,,,,United Kingdom,,Soxxxx Gixxxx;Adxx Loxxx;Sebxxxxxx Maxxxx,xxxxxxxxxxms.ed.ac.uk;xxxxxxxxxxf.ed.ac.uk;xxxxxxxxxxnf.ed.ac.uk,,,,,,,,,"Yes, include my submission even if the paper is rejected.",No,None,None
504,504X-A9F5F7D3F6,Abstract Meaning Representation for Paraphrase Detection,Fuxx Isxx;Maxxx Damxxxx and Shxx Bx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Abstract Meaning Representation (AMR) parsing aims at abstracting away from the
syntactic realization of a sentence, and denote only its meaning in a canonical
form. As such, it is ideal for paraphrase detection, a problem in which one is
required to specify whether two sentences have the same meaning. We show that
na\""{i}ve use of AMR in paraphrase detection is not necessarily useful, and
turn to describe a technique based on latent semantic analysis in combination
with AMR parsing that significantly advances state-of-the-art results in
paraphrase detection for the Microsoft Research Paraphrase Corpus. Our best
results in the transductive setting are 86.6\% for accuracy and 90.0\% for
F$_1$ measure.",7 Feb 2017 09:29:11 GMT,Empirical/Data-Driven,Semantics,textual entailment and paraphrasing;  distributional similarity,Fuxx,Isxx,xxxxxxxxxxms.ed.ac.uk,The University of Edinburgh,No,Maxxx,Damxxxx,xxxxxxxxxxxms.ed.ac.uk,The University of Edinburgh,No,Shaxxxx,Coxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxx,Damxxxx,The University of Edinburgh,,,,,,xxxxxxxxxxxms.ed.ac.uk,,,,,United Kingdom,,Fuxx Isxx;Maxxx Damxxxx;Shxx Bx,xxxxxxxxxxms.ed.ac.uk;xxxxxxxxxxxsms.ed.ac.uk;xxxxxxxxxxf.ed.ac.uk,,,,,,,,,Only include my submission if it is accepted.,No,None,None
506,506X-H5H3H7P7J5,Cross-lingual Distillation for Text Classification,Ruoxxxx Xx and Yixxxx Yaxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"Cross-lingual text classification(CLTC) is the task of classifying documents
written in different languages into the same taxonomy of categories. 
This paper presents a novel approach to CLTC that builds on model distillation,
which adapts and extends a framework originally proposed for model compression.
Using soft probabilistic predictions for the documents in a label-rich language
as the (induced) supervisory labels in a parallel corpus of documents, we train
classifiers successfully for new languages in which labeled training data are
not available. An adversarial feature adaptation technique is also applied
during the model training to reduce distribution mismatch. We conducted
experiments on two benchmark CLTC datasets, treating English as the source
language and German, French, Japan and Chinese as the unlabeled target
languages. The proposed approach had the advantageous or comparable performance
of the other state-of-art methods.",23 Apr 2017 11:40:43 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",,Ruoxxxx,Xx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Yixxxx,Yaxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ruoxxxx,Xx,Carnegie Mellon University,,,412xxxxxxx,,,xxxxxxxxxcs.cmu.edu,,Pittsburgh,PA,,United States,"I am a 5th-year Ph.D. student working with Professor Yiming Yang in the Language Technologies Institute, School of Computer Science at Carnegie Mellon University. I am interested in Machine Learning and the applications in Human Language Technologies. My current research mainly focuses on Deep Learning, Transfer Learning, and Cross-lingual NLP.",Ruoxxxx Xx;Yixxxx Yaxx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu,Cross-lingual Distillation for Text Classification,Cross-lingual Distillation for Text Classification,11,Ruochen Xu,,"Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213",on,on,No. Do not include my submission in this dataset.,No,None,None
507,507X-J6D3A5P4C3,Riemannian Optimization for Skip-Gram Negative Sampling,Alexxxxxx Fonxxxx;Olexxxx Grixxxxx;Glxx Guxxx;Paxxx Serxxxxxx and Ivxx Osexxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its
implementation in ``word2vec'' software, is usually optimized by stochastic
gradient descent. However, the optimization of SGNS objective can be viewed as
a problem of searching for a good matrix with the low-rank constraint. The most
standard way to solve this type of problems is to apply Riemannian optimization
framework to optimize the SGNS objective over the manifold of required low-rank
matrices. In this paper, we propose an algorithm that optimizes SGNS objective
using Riemannian optimization and demonstrates its superiority over popular
competitors, such as the original method to train SGNS and SVD over SPPMI
matrix.",15 May 2017 07:48:35 GMT,Empirical/Data-Driven,Semantics,,Alexxxxxx,Fonxxxx,xxxxxxewo.su,"Skolkovo Institute of Science and Technology, Yandex LLC, SBDA Group",No,Olexxxx,Grixxxxx,xxxxxxxxxxxxxxxx@skolkovotech.ru,"Skolkovo Institute of Science and Technology, Yandex LLC",No,Glxx,Guxxx,xxxxxxxxxxdex-team.ru,"Yandex LLC, Moscow Institute of Physics and Technology",No,Paxxx,Serxxxxxx,xxxxxxxxxxdex-team.ru,Yandex LLC,No,Ivxx,Osexxxxxx,xxxxxxxxxxxskoltech.ru,"Skolkovo Institute of Science and Technology, Institute of Numerical Mathematics of Russian Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Alexxxxxx,Fonxxxx,"Skolkovo Institute of Science and Technology, Yandex LLC, SBDA Group",,,,,,xxxxxxewo.su,,,,,Russian Federation,,Alexxxxxx Fonxxxx;Olexxxx Grixxxxx;Glxx Guxxx;Paxxx Serxxxxxx;Ivxx Osexxxxxx and Tecxxxxxxx Insxxxxxx,xxxxxxewo.su;xxxxxxxxxxxxxxxxk@skolkovotech.ru;xxxxxxxxxxxdex-team.ru;xxxxxxxxxxxdex-team.ru;xxxxxxxxxxx@skoltech.ru,Riemannian Optimization for Skip-Gram Negative Sampling,Riemannian Optimization for Skip-Gram Negative Sampling,9,Alexander Fonarev,,,on,on,No. Do not include my submission in this dataset.,No,None,None
508,508X-J2A2B4J6E7,Learning Knowledge Embeddings by Combing Limit-based Scoring Loss,Xiaxxxx Zhxx and Qiaxxxx Zxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"In knowledge embedding learning models,
the margin-based ranking loss function
usually is used to encourage discrimination
between golden triplets and incorrect
triplets, which has been proved effectiveness
in many translation-based models for
knowledge embedding learning. However,
we find that the loss function cannot
ensure the fact that the scoring of correct
triplets must be lower enough to fulfill
the translating. In this paper, we present
to combine a limit-based scoring loss to
provide lower scoring of a golden triplet
for translation-based models, and discuss
two novel models denoted as TransE-RS
and TransH-RS, deriving from two basic
models TransE and TransH. The proposed
models both have low complexities
of parameters benefiting for application on
large scale graphs. In Experiments, we evaluate
our models on two typical tasks including
triplet classification and link prediction.
Experimental results show that the
introduced limit-based scoring losses are
effective to improve the learning capacities
of knowledge embeddings.",6 Feb 2017 23:06:54 GMT,Theoretical,Machine learning,relational Learning;  text mining;  theoretical aspects of machine learning,Xiaxxxx,Zhxx,xxxxxxxxxxi@iie.ac.cn,,No,Qiaxxxx,Zxx,xxxxxxxxxx@iie.ac.cn,"Institute of Information Engineering, CAS, Chia",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Zhxx,"Institute Information of Engineering, Chinese Academy of Science, China",,,,,,xxxxxxxxxxi@iie.ac.cn,,,,,China,,Xiaxxxx Zhxx;Qiaxxxx Zxx,xxxxxxxxxxi@iie.ac.cn;xxxxxxxxxxn@iie.ac.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
509,509X-F9E5H3A3A5,Linking Event Arguments,Alxx Juxxx and Micxxxx Stxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper we investigate the performance of event argument linking. We show
that the performance is tied to syntactic complexity. Based on this finding we
propose a novel and effective system for event argument linking. Recurrent
Neural Networks learn to produce meaningful representations of long and short
dependency paths. Convolutional Neural Networks learn to decompose the lexical
context of argument candidates. They are combined into a simple system which
beats a feature-based, state-of-the-art event argument linker without any
manual feature engineering.",7 Feb 2017 11:38:21 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",relation/event extraction,Alxx,Juxxx,xxxxxxxxxx@h-its.org,"HITS Heidelberg, Germany",No,Micxxxx,Stxxxx,xxxxxxxxxxxxbe@h-its.org,Heidelberg Institute for Theoretical Studies,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alxx,Juxxx,"HITS Heidelberg, Germany",,,4917xxxxxxxxx,,,xxxxxxxxxx@h-its.org,,,,,Germany,,Alxx Juxxx;Micxxxx Stxxxx,xxxxxxxxxx@h-its.org;xxxxxxxxxxxxube@h-its.org,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
510,510X-G4E3H6A8A8,"""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter",Sanxxxx Swxxx;Alxx Rixxxx and Mariexxxxxxxxxx dx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Social media users often make explicit predictions about upcoming events. Such
statements vary in the degree of certainty the author expresses toward the
outcome, for example: ""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo
DiCaprio may win"" or ""No way Leonardo wins!"". Can popular beliefs on social
media predict who will win?  To answer this question, we build a corpus of
tweets annotated for such veridicality on which we train a log-linear
classifier, TwiVer, which detects positive veridicality with high precision. We
then forecast uncertain outcomes using the wisdom of crowds, by aggregating
users' explicit predictions. Our method for forecasting winners requires no
training data of past outcomes and achieves an F-score of 0.76 when applied to
Academy Awards, a 16% increase over a sentiment baseline.  We further
demonstrate how our approach can be used to measure the reliability of
individual accounts' predictions and retrospectively identify surprise
outcomes.",7 Feb 2017 01:57:23 GMT,Empirical/Data-Driven,Social media,NLP on noisy unstructured text;  opinion mining and extraction;  NLP in social networking media,Sanxxxx,Swxxx,xxxxxxxx@osu.edu,The Ohio State University,No,Alxx,Rixxxx,xxxxxxxxx92@osu.edu,The Ohio State University,No,Mariexxxxxxxxxx,de Mxxxxxxx,xxxxxxxxxxxxio-state.edu,The Ohio State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sanxxxx,Swxxx,The Ohio State University,,,,,,xxxxxxxx@osu.edu,,,,,United States,,Sanxxxx Swxxx;Alxx Rixxxx;Mariexxxxxxxxxx dx,xxxxxxxx@osu.edu;xxxxxxxxxx92@osu.edu;xxxxxxxxxxxxhio-state.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
511,511X-E8P9H5G6D2,Statistical Learning for OCR Text Correction,Jxx Mxx;Amxxxx Isxxx;Yaxxxx Wx;Abidxxxxxxxx Moxxx and Evaxxxxxx Mixxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We present an OCR post-processing model that integrates a set of OCR-specific
features from different aspects and learns to suggest correction candidates.
The evaluation results show that our model can correct 61.5% of the errors with
the top suggestion and 71.5% of the OCR-errors within the top 3 suggestions,
where the theoretical correction upper-bound is 78% in three edit distance. We
also make available the first evaluation dataset of this task, which can be
used directly for benchmark testing and model comparison.",6 Feb 2017 23:12:00 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",text mining;  adaptation to noisy data,Jxx,Mxx,xxxxxxx.dal.ca,Dalhousie University,No,Amxxxx,Isxxx,xxxxxxxxxxisiana.edu,University of Louisiana at Lafayette,No,Yaxxxx,Wx,xxxxxxxxs.dal.ca,Dalhousie University,No,Abidxxxxxxxx,Moxxx,xxxxxxxs.dal.ca,Dalhousie University,No,Evaxxxxxx,Mixxxx,xxxxxx.dal.ca,Dalhousie University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Jxx,Mxx,Dalhousie University,,,902xxxxxxx,,,xxxxxxx.dal.ca,,Halifax,NS,,Canada,,Jxx Mxx;Amxxxx Isxxx;Yaxxxx Wx;Abidxxxxxxxx Moxxx;Evaxxxxxx Mixxxx,xxxxxxx.dal.ca;xxxxxxxxxxuisiana.edu;xxxxxxxxcs.dal.ca;xxxxxxxxs.dal.ca;xxxxxxx.dal.ca,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
513,513X-C9F5A2G6H4,"Elastic-substitution decoding for Hierarchical SMT: efficiency, richer search and double labels",Gixxxx Maixxxxxx;Khxxxx Simxxxx and Anxx Wx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Elastic-substitution decoding (ESD), first introduced by Chiang (2010), can be
important for getting good results when applying labels to enrich hierarchical
Statistical. However, an efficient implementation is essential for scalable
application. We describe how to achieve this, contributing essential details
that were missing in the original exposition. We compare ESD to strict matching
and show its superiority for both reordering and syntactic labels. To overcome
the sub-optimal performance due to the late evaluation of features marking
label substitution types, we diversify the rules explored during cube pruning
initialization with respect to labelings. This approach gives significant
improvements over basic ESD and performs favorably compared to extending the
search by increasing the cube pruning pop-limit. Finally, we look at combining
multiple labels.The combination of reordering labels and target-side
boundary-tags yields a significant improvement in terms of the word-order
sensitive metrics Kendall reordering score and METEOR.This confirms our
intuition that the combination of reordering labels and syntactic labels can
yield improvements over either label by itself, despite increased sparsity.",6 Feb 2017 23:50:21 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation;  Hierarchical SMT,Gixxxx,Maillettxxxxxxxxxxxxxxxxx,xxxxxxxxmail.com,Institute for Logic Language and Computation - University of Amsterdam,No,Khxxxx,Simxxxx,xxxxxxxn@uva.nl,"ILLC, University of Amsterdam",No,Anxx,Wxx,xxxxxxxxxxxaptcentre.ie,DCU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gixxxx,Maillettxxxxxxxxxxxxxxxxx,ADAPT Centre - Dublin City University,,,,,,xxxxxxxxmail.com,,,,,Ireland,,Gixxxx Maixxxxxx;Khxxxx Simxxxx;Anxx Wxx,xxxxxxxxmail.com;xxxxxxxxn@uva.nl;xxxxxxxxxxxxaptcentre.ie,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
514,514X-B7A4J8B6H6,An Analysis of Machine Learning Intelligence,Joxx Laxxx;Hxx Wx;Tsexxxxxxx Munxxxxxxx and hoxx y,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"Deep neural networks (DNNs) have set state of the art results in many machine
learning and NLP tasks. However, we do not have a strong understanding of what
DNN models learn. In this paper, we examine learning in DNNs through analysis
of their outputs. We compare DNN performance directly to a human population,
and use characteristics of individual data points such as difficulty to see how
well models perform on easy and hard examples. We investigate how training size
and the incorporation of noise affect a DNN's ability to generalize and learn.
Our experiments show that unlike traditional machine learning models (e.g.,
Naive Bayes, Decision Trees), DNNs exhibit human-like learning properties. As
they are trained with more data, they are more able to distinguish between easy
and difficult items, and performance on easy items improves at a higher rate
than difficult items. We find that different DNN models exhibit different
strengths in learning and are robust to noise in training data.",7 Feb 2017 04:45:49 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,textual entailment and paraphrasing;  experimental evaluation/comparison of ML methods;  evaluation metrics,Joxx,Laxxx,xxxxxxxxxumass.edu,University of Massachusetts Amherst,No,Hxx,Wx,xxxxxxx5@bc.edu,Boston College,No,Tsexxxxxxx,Munxxxxxxx,xxxxxxxxxxxxxxxxxalai@umassmed.edu,University of Massachusetts,No,hoxx,yx,xxxxxxxxxxassmed.edu,University of Massachusetts Medical School,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Joxx,Laxxx,University of Massachusetts Amherst,,,,,,xxxxxxxxxumass.edu,,,,,United States,,Joxx Laxxx;Hxx Wx;Tsexxxxxxx Munxxxxxxx;hoxx yx,xxxxxxxxxumass.edu;xxxxxxxx5@bc.edu;xxxxxxxxxxxxxxxxxdalai@umassmed.edu;xxxxxxxxxxmassmed.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
515,515X-E5G8F7D3G9,E-VIEW-alation – a large-scale evaluation study of association measures for collocation identification,Stxxxx Evxxx;Saxxxx Barxxxx;Thxxxx Prxxxx and Pexxx Uhxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"Statistical association measures (AM) play an important role in the automatic
extraction of collocations and multiword expressions from corpora, but many
parameters governing their performance are still poorly understood. Systematic
evaluation studies have produced conflicting recommendations for an optimal AM,
and little attention has been paid to other parameters such as the underlying
corpus, the size of the co-occurrence context, or the application of a
frequency threshold.

Our paper presents the results of a large-scale evaluation study covering 13
corpora, 8 context sizes, 4 frequency thresholds, and 20 AMs against two
different gold standards of lexical collocations. While the optimal choice of
an AM depends strongly on the particular gold standard used, other parameters
prove much more robust: small co-occurrence contexts and large, but relatively
clean corpora yield the best results; frequency thresholds seem to be
unnecessary in most situations. Reassuringly, there is little interaction
between the choice of AM and the other parameters.

In order to provide complete evidence for our observations to readers, we
created an interactive Web-based application that allows users to manipulation
all evaluation parameters and dynamically updates evaluation graphs and
summaries.",6 Feb 2017 23:24:10 GMT,Resources/Evaluation,Resources and evaluation,multiword semantics/compositionality;  lexical semantics;  experimental evaluation/comparison of ML methods,Stxxxx,Evxxx,xxxxxxxxxert@fau.de,FAU Erlangen-Nürnberg,No,Saxxxx,Barxxxx,xxxxxxxxxxxxxxx.tu-darmstadt.de,Technische Universität Darmstadt,No,Thxxxx,Prxxxx,xxxxxxxxxxisl@fau.de,FAU Erlangen-Nürnberg,No,Pexxx,Uhxxx,xxxxxxxxxig@fau.de,FAU Erlangen-Nürnberg,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Stxxxx,Evxxx,FAU Erlangen-Nürnberg,,,,,,xxxxxxxxxert@fau.de,,,,,Germany,,Stxxxx Evxxx;Saxxxx Barxxxx;Thxxxx Prxxxx;Pexxx Uhxxx,xxxxxxxxxert@fau.de;xxxxxxxxxxxxxxxx.tu-darmstadt.de;xxxxxxxxxxoisl@fau.de;xxxxxxxxxrig@fau.de,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
516,516X-G7C3A5J2C5,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,Jefxxxx Luxx;Coxxxx Coxx;Kexxx Sexxx and Joxxxx Boyxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"Interactive topic models are powerful tools
for those seeking to understand large
collections of text. However, existing
sampling-based interactive topic modeling
approaches scale poorly to large data sets.
Anchor methods, which use a single word
to uniquely identify a topic, offer the speed
needed for interactive work but lack both
a mechanism to inject prior knowledge
and lack the intuitive semantics needed
for user-facing applications. We propose
combinations of words as anchors, go-
ing beyond existing single word anchor
algorithms—an approach we call “Tan-
dem Anchors”. We begin with a synthetic
investigation of this approach then apply
the approach to interactive topic modeling
in a user study and compare it to interac-
tive and non-interactive approaches. Tan-
dem anchors are faster and more intuitive
than existing interactive approaches.",23 Apr 2017 00:07:58 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Jefxxxx,Luxx,xxxxxxxxxgmail.com,Brigham Young University,No,Coxxxx,Coxx,xxxxxxxxmail.com,Brigham Young University,No,Kexxx,Sexxx,xxxxxxxbyu.edu,Brigham Young University,No,Joxxxx,Boydxxxxxxx,xxxxxxxxxxxxxxxber@colorado.edu,University of Colorado,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jefxxxx,Luxx,Brigham Young University,,,801-xxxxxxxx,,,xxxxxxxxxgmail.com,,,,,United States,,Jefxxxx Luxx;Coxxxx Coxx;Kexxx Sexxx;Joxxxx Boydxxxxxxx,xxxxxxxxxgmail.com;xxxxxxxxgmail.com;xxxxxxx@byu.edu;xxxxxxxxxxxxxxxxber@colorado.edu,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,10,Jeffrey Lund,,Brigham Young University,on,on,Only include my submission if it is accepted.,No,None,None
517,517X-D3G6H9A2P8,Supervised Open Information Extraction,Gabxxxx Staxxxxxx and Ixx Daxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Open Information Extraction (Open IE) has gained popularity as an underlying
representation in a wide array of semantic applications.
However, the lack of a large gold standard corpus for Open IE has
limited the development of automatic extraction techniques to largely
rule-based algorithms over some core NLP pipeline.
In a very recent advancement, a first large Open IE corpus for verbal
predicates was presented, which ``opens the door'' for applying modern machine
learning techniques for this task.
In this paper, we utilize this corpus to develop a first supervised Open IE
extractor. To that end, we use a deep bi-LSTM transducer while addressing
several challenges in adapting for the task.
Our evaluation shows that the new extractor significantly outperforms
the currently most prominent Open IE systems.",7 Feb 2017 11:31:32 GMT,Applications/Tools,"Information extraction, text mining, and question answering",information extraction;  structured input/output;  relation/event extraction,Gabxxxx,Staxxxxxx,xxxxxxxxxxxxxxvsky@gmail.com,Bar Ilan University,No,Ixx,Daxxx,xxxxxxxxxbiu.ac.il,Bar-Ilan University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gabxxxx,Staxxxxxx,"University of Washington, Allen Institute for Artificial Intelligence",,,,,,xxxxxxxxxxxxxxvsky@gmail.com,,,,,United States,,Gabxxxx Staxxxxxx;Ixx Daxxx,xxxxxxxxxxxxxxvsky@gmail.com;xxxxxxxxx.biu.ac.il,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
518,518X-G7C5D3C6D5,Content Consumption Communities: A new framework for characterizing on-line sociolinguistic communities like #blacklivesmatter and #bluelivesmatter,Racxxxx Taxxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Current sociolinguistic frameworks, including social networks, dialect regions,
communities of practice and information communities, fail to characterize
diffuse communities like #bluelivesmatter and #blacklivesmatter. This paper
presents a new framework for these communities: Content Consumption
Communities. An analysis of the #bluelivesmatter and #blacklivesmatter
communities is presented within this framework. Validation on a number of text
and discourse features show that membership in these Content Consumption
Communities is reflected in adaptation of community linguistic norms.",6 Feb 2017 23:35:41 GMT,Empirical/Data-Driven,Social media,corpus development;  NLP in social networking media;  social network,Racxxxx,Taxxxx,xxxxxxxxxxxxan@gmail.com,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Racxxxx,Taxxxx,University of Washington,,,,,,xxxxxxxxxxxxan@gmail.com,,,,,United States,,Racxxxx Taxxxx,xxxxxxxxxxxxan@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
519,519X-C3B9D3P4D8,Neural Text Simplification using a RNN Encoder-Decoder,Mxx Schxxxxxx;Mixx Barxxxx and Daxxx Kaxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"In this paper we introduce a neural text simplification model for the
sentence-level text simplification task.  We modify the RNN encoder-decoder
neural translation model of Cho et al. (2014) with additional post-processing
steps that handle out-of-vocabulary (OOV) words, replace poor simplifications,
and perform model selection.  Our approach achieves the highest reported BLEU
score to date on the Wikipedia simplification dataset and strong human
evaluation results across all evaluation metrics.  The model is particularly
good at producing fluent output achieving fluency scores higher than even the
human baselines.  We find that OOV-handling and avoiding long sentences to be
the most beneficial, though all post-processing steps provide some benefit to
the output.",6 Feb 2017 23:43:37 GMT,Applications/Tools,Generation,sentence simplification,Mxx,Schxxxxxx,xxxxxxxxxxxxxil.pomona.edu,Pomona College,No,Mixx,Barxxxx,xxxxxxxxxxxxel@gmail.com,Pomona College,No,Daxxx,Kauxxxx,xxxxxxxxxxxxk@pomona.edu,Pomona College,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxx,Kauxxxx,Pomona College,,,(909)xxxxxxxxx,,,xxxxxxxxxxxxk@pomona.edu,,Claremont,CA,,United States,,Mxx Schxxxxxx;Mixx Barxxxx;Daxxx Kauxxxx,xxxxxxxxxxxxxil.pomona.edu;xxxxxxxxxxxxael@gmail.com;xxxxxxxxxxxxak@pomona.edu,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
520,520X-P3G4A8J7D5,ShapeWorld: A new test methodology for multimodal language understanding,Alxx and ex Kuxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"We introduce a novel framework for evaluating multimodal deep learning models
with respect to their language understanding and generalization abilities. In
this approach, artificial data is automatically generated according to the
experimenter's specifications. The content of the data, both during training
and evaluation, can be controlled in detail, which enables tasks to be created
which require generalization abilities, in particular the combination of
previously introduced concepts in novel ways. We demonstrate the potential of
our methodology by evaluating a multimodal architecture on four different
tasks, and show that our framework gives us insights into the model's
capabilities and limitations.",7 Feb 2017 10:55:56 GMT,Resources/Evaluation,Machine learning,language generation;  formal semantics and logic;  multimodal representations and processing;  question answering in restricted domains,Alexxxxxx,Kuxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Axx,Copxxxxxx,xxxxxxxxxxxxx@cl.cam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alexxxxxx,Kuxxxx,University of Cambridge,,,,,,xxxxxxxam.ac.uk,,,,,United Kingdom,,Alexxxxxx Kuxxxx;Axx Copxxxxxx,xxxxxxxam.ac.uk;xxxxxxxxxxxxxe@cl.cam.ac.uk,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
522,522X-E8H3F5H6H8,Neural Extractive Summarization with Side Information,Shxxxx Narxxxx;Nixxx Papasaxxxxxxxxxxx;Mirxxxx Laxxxx and Shxx Bx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Existing extractive summarization methods focus on the main body of
the document from which sentences need to be extracted. To excel at
this task one needs wide-coverage text understanding and it could be
very challenging. In this paper we argue that the gist of the
document might lie in the side information, e.g. title and image
captions. These types of side information are often available at
large for newswire articles. We propose to use this side information
along with the document to identify salient sentences in the
document.  We develop a framework for single-document summarization
composed of a hierarchical document encoder and an attention-based
extractor with attention over side information. We evaluate our
models on two large scale CNN and DailyMail corpora. We show that
extractive summarization with side information consistently improves
over its counterpart without any side information, in terms on both
informativeness and fluency. Human evaluation complements our
results as well.",7 Feb 2017 11:01:57 GMT,Empirical/Data-Driven,Summarization,document summarization,Shxxxx,Narxxxx,xxxxxxxxxxxyan@ed.ac.uk,University of Edinburgh,No,Nixxx,Papasaxxxxxxxxxxx,xxxxxxxxxxxa@gmail.com,University of Edinburgh,No,Mirxxxx,Laxxxx,xxxxxxxx.ed.ac.uk,"School of Informatics, University of Edinburgh",No,Shaxxxx,Coxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Narxxxx,University of Edinburgh,,,,,,xxxxxxxxxxxyan@ed.ac.uk,,,,,United Kingdom,,Shxxxx Narxxxx;Nixxx Papasaxxxxxxxxxxx;Mirxxxx Laxxxx;Shxx Bx,xxxxxxxxxxxyan@ed.ac.uk;xxxxxxxxxxxsa@gmail.com;xxxxxxxxx.ed.ac.uk;xxxxxxxxxxf.ed.ac.uk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
524,524X-E8C6E3G8D5,A Comparison of Robust Parsing Methods for HPSG,Wooxxxx Pacxxxx and Dxx Flixxxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"This paper explores several techniques for enhancing coverage when parsing with
HPSG grammars, determines appropriate evaluation methods, and uses them to
compare performance.  Depending on the dataset, baseline coverage gaps can be
reduced by between 75% and 100%, while simultaneously improving EDM F1 scores.",6 Feb 2017 23:59:50 GMT,Resources/Evaluation,"Tagging, chunking, syntax, and parsing",grammatical formalisms;  experimental evaluation/comparison of ML methods;  parsing;  evaluation metrics,Wooxxxx,Pacxxxx,xxxxxxxxxxxweaglesw.org,University of Washington,No,Dxx,Flixxxxxxx,xxxxxxxxnford.edu,Stanford,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wooxxxx,Pacxxxx,University of Washington,,,1650xxxxxxx,,,xxxxxxxxxxxweaglesw.org,,,,,United States,,Wooxxxx Pacxxxx;Dxx Flixxxxxxx,xxxxxxxxxxxweaglesw.org;xxxxxxxxxnford.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
525,525X-J6F8B6G6A4,EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,Muhxxxxx Abduxxxxxxxx and Lyxx Unxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Accept - Oral Tuesday,,Undecided (Social Media),"Accurate detection of emotion from natural language has applications ranging
from building emotional chatbots to better understanding individuals and their
lives. However, progress on emotion detection has been hampered by the absence
of large labeled datasets.  In this work, we build a very large dataset for
fine-grained emotions and develop deep learning models on it. We achieve a new
state-of-the-art on 24 fine-grained types of emotions (with an average accuracy
of 87.58%). We also extend the task beyond emotion types to model Robert
Plutick's 8 primary emotion dimensions, acquiring a superior accuracy of
95.68%.",24 Apr 2017 04:26:36 GMT,Empirical/Data-Driven,Social media,,Muhxxxxx,Abduxxxxxxxx,xxxxxxxxxxxgeed@ubc.ca,The University of British Columbia,No,Lyxx,Unxxx,xxxxxxxxx.upenn.edu,University of Pennsylvania,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Muhxxxxx,Abduxxxxxxxx,The University of British Columbia,,,,,,xxxxxxxxxxxgeed@ubc.ca,,Vancouver,BC,,Canada,,Muhxxxxx Abduxxxxxxxx;Lyxx Unxxx,xxxxxxxxxxxgeed@ubc.ca;xxxxxxxxxx.upenn.edu,EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,11,Muhammad Abdul-Mageed,,,,on,No. Do not include my submission in this dataset.,No,None,None
526,526X-E7F5J7F6A9,Chinese Zero Pronoun Resolution with Deep Memory Network,Qixxxx Yxx;Yx Zhxxx;Weixxxx Zhxxx and Tixx Lx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Most existing approaches for Chinese zero pronoun resolution overlook semantic
information. This is because zero pronouns have no descriptive information,
which results in difficulty in explicitly capturing their semantic similarities
with antecedents. Meanwhile, representing zero pronouns is challenging since
they are merely gaps that convey no actual content. In this paper, we address
this issue by developing a deep memory network that is capable of encoding zero
pronouns into vector representations with information obtained from their
contexts and potential antecedents. Consequently, our resolver takes advantage
of semantic information by using these continuous distributed representations.
Experiments on the Chinese portion of OntoNotes 5.0 corpus show that our method
substantially outperforms the state-of-the-art systems in various experimental
settings.",7 Feb 2017 10:41:30 GMT,Empirical/Data-Driven,Discourse and pragmatics,anaphora resolution;  coreference resolution,Qixxxx,Yxx,xxxxxxqq.com,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China",No,Yx,Zhxxx,xxxxxxxxxx.hit.edu.cn,Harbin Institute of Technology,No,Weixxxx,Zhxxx,xxxxxxxxxx.hit.edu.cn,Harbin Institute of Technology,No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixxxx,Yxx,"Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China",,,,,,xxxxxxqq.com,,南岗区,哈尔滨市,,China,,Qixxxx Yxx;Yx Zhxxx;Weixxxx Zhxxx;Tixx Lxx,xxxxxxqq.com;xxxxxxxxxxx.hit.edu.cn;xxxxxxxxxxx.hit.edu.cn;xxxxxxxxxip.126.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
527,527X-H2J3P6P6C5,Joint Modeling for Bidirectional Neural Machine Translation with Contrastive Learning,Yoxx Chxxx;Yaxx Lxx and Wxx X,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Standard neural machine translation (NMT) only captures unidirectional
dependencies to model the translation process from source sentences to target
sentences. Nevertheless, the inverse information is explicitly available to
reinforce the confidence of translation process. We propose a bidirectional NMT
model to connect the source-to-target and target-to-source translation models,
which opens up the interaction of parameters between two directional models. We
also adopt a contrastive learning approach to train the bidirectional NMT model
to enhance the information sharing. Experiments on Chinese-English and
German-English translation tasks demonstrate that our approach significantly
outperforms the state-of-the-art NMT and SMT systems.",7 Feb 2017 12:10:50 GMT,Empirical/Data-Driven,Machine translation,statistical machine translation,Yoxx,Chxxx,xxxxxxxxxxx01@gmail.com,Tsinghua University,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Wxx,Xx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yoxx,Chxxx,Tencent,,,,,,xxxxxxxxxxx01@gmail.com,,,,,China,,Yoxx Chxxx;Yaxx Lxx;Wxx Xx,xxxxxxxxxxx01@gmail.com;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxxghua.edu.cn,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
528,528X-E3C3G3C9P3,Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning,Jaxxx D;Kaxxxx Asxxx and Geoxxxxx Zwxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Tuesday,,Undecided (Dialog Interactive Systems),"End-to-end learning of recurrent neural networks (RNNs) is an attractive
solution for dialog systems; however, current techniques are data-intensive and
require thousands of dialogs to learn simple behaviors.  We introduce Hybrid
Code Networks (HCNs), which combine an RNN with domain-specific knowledge
encoded as software and system action templates. Compared to existing
end-to-end approaches, HCNs considerably reduce the amount of training data
required, while retaining the key benefit of inferring a latent representation
of dialog state. In addition, HCNs can be optimized with supervised learning,
reinforcement learning, or a mixture of both. HCNs attain state-of-the-art
performance on the bAbI dialog dataset (Bordes and Weston, 2016), and
outperform two commercially deployed customer-facing dialog systems at our
company.",18 Apr 2017 23:30:58 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Jasxxxx,Wilxxxxx,xxxxxxxxxxxxxx@microsoft.com,Microsoft Research,No,Kaxxxx,Asxxx,xxxxxxxxrown.edu,Brown University,No,Geoxxxxx,Zwxxx,xxxxxxxxgmail.com,JPMorgan Chase,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jasxxxx,Wilxxxxx,Microsoft Research,,,,,,xxxxxxxxxxxxrinceton.edu,,,,,United States,,Jaxxx D;Kaxxxx Asxxx;Geoxxxxx Zwxxx,xxxxxxxxxxxxxx@microsoft.com;xxxxxxxxbrown.edu;xxxxxxxxxgmail.com,Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning,Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning,13,Jason D Williams,,"Microsoft Corp
One Microsoft Way
Redmond, WA  98052",,,No. Do not include my submission in this dataset.,No,None,None
529,529X-F2P6D9F3F9,Out of the Woods: Universal Dependencies for Bosque,Fabxxxxx Chxxxx;Eckxxxx Bixx;Valxxxx dePxxxx;Lixx Rexx;Claxxxx Frexxxx;Guixxxxxx Pauxxxx and  Axxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"This paper describes the creation of a Portuguese corpus, annotated using the
guidelines of the Universal Dependencies (UD) project, intended to produce
suitable material for participation on CoNLL-style tasks. We describe the
process of taking an existing corpus, the Bosque, produced  by a careful,
context-sensitive conversion process of its original linguistic analysis, and
managing to make it into sound Universal Dependencies, validated according to
the UD guidelines v2. This process shows some of the essential difficulties of
dealing with a Romance language, Portuguese. The project should provide us with
a corpus suitable for the applications that  UDs are envisaged for.",7 Feb 2017 00:03:08 GMT,Resources/Evaluation,Resources and evaluation,corpus development;  multilingual resources;  corpus annotation methods,Fabxxxxx,Chxxxx,xxxxxxxxxr.ibm.com,IBM Research,No,Eckxxxx,Bixx,xxxxxxxxxxck@mail.dk,University of Southern Denmark,No,Valxxxx,dePxxxx,xxxxxxxxxxxxiva@gmail.com,Nuance Communications,No,Lixx,Rexx,xxxxxxxxxgmail.com,IBM Research,No,Claxxxx,Frexxxx,xxxxxxxxxxxxxitas@gmail.com,PUC-Rio,No,Guilhexxxxxxxxxxx,Paxxxx,xxxxxxxxxbr.ibm.com,IBM Research,No,Alexxxxxx,Radxxxxxx,xxxxxxxxxx@gmail.com,IBM Research and EMAp/FGV,No,,,,,,,,,,,,,,,,,Alexxxxxx,Radxxxxxx,IBM Research and EMAp/FGV,,,,,,xxxxxxxxxx@gmail.com,,Rio de Janeiro,RJ,,Brazil,"Alexandre is a Research Staff Member in the Natural Resources group at IBM Research – Brazil. He joined IBM in December 2012.

Prior that, Alexandre was adjunct professor at the Applied Mathematics School of Getulio Vargas Foundation (EMAp/FGV) and he was also lecturer at PUC-Rio. Alexandre has taught many graduate and undergraduate courses: logics, data structures, programming, formal languages and automata theory. During his Ph.D., Alexandre was international fellow at Microsoft Research and SRI International. At MSR, in 2008, he worked with the Z3 SMT Solver team under supervision of Leonardo Moura and Nikolaj Bjørner developing a distributed environment for testing and optimizations of Z3. At SRI International, in 2009, he worked under supervision of Natarajan Shankar in different research projects.",Fabxxxxx Chxxxx;Eckxxxx Bixx;Valxxxx dePxxxx;Lixx Rexx;Claxxxx Frexxxx;Guixxxxxx Pauxxxx; Axxx and rx Radxxxxxx,xxxxxxxxxr.ibm.com;xxxxxxxxxxick@mail.dk;xxxxxxxxxxxxxiva@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxxxxxxxitas@gmail.com;xxxxxxxxxxbr.ibm.com;xxxxxxxxxxr@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
530,530X-D2B3D9A2D6,Supporting Live Journalism in Political Debates: Automatic Identification of Worth-Checking Claims,Pexx Genxxxxx;Prexxxx Naxxx;Llxxxx Màxxxxx and Albxxxx Barr�xxxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Lately, increasing attention has been paid to fact-checking of politician's
statements, which has given rise to several specialized websites run by
investigative journalists and volunteers. While this is a great tool for
democracy, a critical aspect here is timeliness. 
Thus, efforts have emerged trying to automate the process. 
While most research has focused on checking a claim, the very first step is
actually selecting the check-worthy claims, and this is a severely
under-studied problem. 

With this consideration in mind, here we create a new corpus of claims that
have
been fact-checked by nine sources, and we build a system that predicts which
claims should be prioritized for checking. Unlike previous work, which has
looked primarily at sentences in isolation, here we focus on the context of the
debate: interaction between the opponents, and reaction by the moderator and by
the public. Our experiments show state-of-the-art results, outperforming a
strong pre-existing system.",7 Feb 2017 13:09:41 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",sentiment analysis;  discourse;  NLP applications;  dialogue;  text classification;  contex modeling for dialogues,Pexx,Genxxxxx,xxxxxxxxxxxxeva@gmail.com,Sofia University,No,Prexxxx,Naxxx,xxxxxxxxxxxov@gmail.com,"Qatar Computing Research Institute, HBKU",No,Llxxxx,Màxxxxx,xxxxxxxxxi.upc.edu,Qatar Computing Research Institute,No,Albxxxx,Barr�xxxxxxxxxx,xxxxxxxxxgmail.com,Qatar Computing Research Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Pexx,Genxxxxx,Sofia University,,,,,,xxxxxxxxxxxxeva@gmail.com,,,,,Bulgaria,,Pexx Genxxxxx;Prexxxx Naxxx;Llxxxx Màxxxxx;Albxxxx Barr�xxxxxxxxxx,xxxxxxxxxxxxeva@gmail.com;xxxxxxxxxxxxov@gmail.com;xxxxxxxxxsi.upc.edu;xxxxxxxxx@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
531,531X-F5J6A6C9E7,Demographic-Aware Word Associations,Apxxxx Garxxxxxx;Caxxxx Baxxx and Raxx Mihxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Variations of word associations across different groups of people can provide
insights into people’s psychologies and their world views. To capture these
variations, we introduce the task of demographic-aware word associations. We
build a new gold standard datasets consisting of word association responses for
approximately 300 stimulus words, collected from groups of different gender
(male/female) and from different locations (India/United States), and show that
there are significant variations in the word associations made by these groups.
We also introduce a new demographic-aware word association measure based on a
skip-gram architecture and show how computational methods for measuring word
associations that specifically account for writer demographics can outperform
generic methods that are agnostic to such information.",7 Feb 2017 05:34:22 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  NLP on noisy unstructured text;  semantic relations;  NLP in the blogosphere,Apxxxx,Garxxxxxx,xxxxxxxxumich.edu,University of Michigan,No,Caxxxx,Baxxx,xxxxxxxxxxxa@gmail.com,University of Michigan,No,Raxx,Mihxxxxx,xxxxxxxxxumich.edu,University of Michigan,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Caxxxx,Baxxx,University of Michigan,,,940xxxxxxx,,,xxxxxxxxxxxa@gmail.com,,,,,United States,,Apxxxx Garxxxxxx;Caxxxx Baxxx;Raxx Mihxxxxx,xxxxxxxxumich.edu;xxxxxxxxxxxea@gmail.com;xxxxxxxxx@umich.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
532,532X-H9H3H5D7F6,Morphological analysis as a classification problem for a morphologically-rich language,Claxxxx Boxx and Alxxxx Gaxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Maltese is a morphologically rich language for which morphological analysis
tools are as yet unavailable. Rather than classifying a word on the basis of
its full morphological label, we adopt a sequential approach, training separate
classifiers for each feature. Each classifier is placed in a cascade such that
new information is passed onto classifiers downstream. This paper describes the
experiments carried out to find the optimal cascade sequence, as well as
results on the morphological labelling task. Moreover, we describe insights
gained with respect to ambiguity and co-dependence of morphological features.",7 Feb 2017 11:41:47 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",morphology,Claxxxx,Boxx,xxxxxxxxxxxg@um.edu.mt,University of Malta,No,Alxxxx,Gaxx,xxxxxxxxxxt@um.edu.mt,University of Malta,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Claxxxx,Boxx,University of Malta,,,,,,xxxxxxxxxxxg@um.edu.mt,,Msida,,,Malta,,Claxxxx Boxx;Alxxxx Gaxx,xxxxxxxxxxxg@um.edu.mt;xxxxxxxxxxxt@um.edu.mt,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
533,533X-J2B5P6H9G2,Extracting Relational Facts fromWikipedia via Semantic Modeling of the Natural Language of Annotations,Jixxx Mx;Suxx Bhxx and Prxxxx Visxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Knowledge bases (KB) store relational facts and constitute a significant
resource for a variety of NLP tasks. Improving their coverage and refining the
relations is a basic and pressing research effort. Prior works have addressed
this by harvesting relational facts via crowdsourcing, and collating
information across existing KBs. We introduce a novel approach which extracts
relational facts directly from unstructured resources in a semi-supervised
fashion: specifically, we treat the annotations (concepts) in Wikipedia as
entities, extract low dimensional representations for them, show that the
representations can strongly capture semantic relatedness and thus demonstrate
that they can capture relational facts in KB by treating them as generalized
analogies. Further, we observe that relationships defined in current KB can be
ambiguous and show how our approach to unsupervised entity representations
suggests an innovative unsupervised solution to disambiguate relations. The
result is an immediate unsupervised and automatic refinement of the KB.",7 Feb 2017 00:25:38 GMT,Applications/Tools,"Information extraction, text mining, and question answering",information extraction;  information retrieval;  mathematical linguistics;  relation discovery;  relation/event extraction,Jixxx,Mx,xxxxxxxxxxllinois.edu,University of Illinois at Urbana Champaign,No,Suxx,Bhxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana Champaign,No,Prxxxx,Visxxxxxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana Champaign,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixxx,Mx,University of Illinois at Urbana Champaign,,,217xxxxxxx,,,xxxxxxxxxxllinois.edu,,URBANA,IL,,United States,,Jixxx Mx;Suxx Bhxx;Prxxxx Visxxxxxx,xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
534,534X-F6C2D6D2J3,Stacking with Auxiliary Features for Entity Linking in the Medical Domain,Nazxxxx Faxxxx;Mihxxxx Boxxxx and Kxx Baxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Linking spans of natural language text to concepts in a structured source is an
important task for many problems. It allows intelligent systems to leverage
rich knowledge available in those sources (such as concept properties and
relations) to enhance the semantics of the mentions of these concepts in text.
In the medical domain, it is common to link text spans to medical concepts in
large, curated knowledge repositories such as the Unified Medical Language
System.
Different approaches have different strengths: some are precision-oriented,
some recall-oriented; some better at considering context but more prone to
hallucination.
The variety of techniques suggests that ensembling could outperform component
technologies at this task.
In this paper, we describe our process for building a Stacking ensemble using
additional, auxiliary features for Entity Linking in the medical domain. We
report experiments that show that naive ensembling does not always outperform
component Entity Linking systems, that stacking usually outperforms naive
ensembling, and that auxiliary features added to the stacker further improve
its performance on three distinct datasets. Our best model produces
state-of-the-art results on several medical datasets.",7 Feb 2017 04:57:18 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  named entity recognition;  named entity disambiguation,Naznexxxxxxxxx,Raxxxx,xxxxxxxxxx.utexas.edu,The University of Texas at Austin,No,Mihxxxx,Boxxxx,xxxxxxxxxus.ibm.com,IBM Research,No,Kxx,Baxxxx,xxxxxxxxxus.ibm.com,IBM Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Naznexxxxxxxxx,Raxxxx,The University of Texas at Austin,,,512xxxxxxx,,,xxxxxxxxxx.utexas.edu,,Austin,TX,,United States,,Nazxxxx Faxxxx;Mihxxxx Boxxxx;Kxx Baxxxx,xxxxxxxxxx.utexas.edu;xxxxxxxxxxus.ibm.com;xxxxxxxxxxus.ibm.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
535,535X-G5D4G4G6G8,Neural Lattice-to-Sequence Models for Uncertain Inputs,Matxxxxx Spexxxx;Grxxxx Nexxxx;Jxx Niexxxx and Alxx Waxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"The input to a neural sequence-to-sequence model is often determined by an
up-stream system, e.g. a word segmenter, part of speech tagger, or speech
recognizer. These up-stream models are potentially error-prone. Representing
inputs through word lattices allows making this uncertainty explicit by
capturing alternative sequences and their posterior probabilities in a compact
form.
In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that
is able to consume word lattices, and can be used as encoder in an attentional
encoder-decoder model. We integrate lattice posterior scores into this
architecture by extending the TreeLSTM's child-sum and forget gates and
introducing a bias term into the attention mechanism. We experiment with speech
translation lattices and report consistent improvements over baselines that
translate either the 1-best hypothesis or the lattice without posterior scores.",7 Feb 2017 11:42:26 GMT,Empirical/Data-Driven,Machine translation,speech translation;  statistical machine translation,Matxxxxx,Spexxxx,xxxxxxxxxxxxrber@kit.edu,Karlsruhe Institute of Technology,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Jxx,Niexxxx,xxxxxxxxxes@kit.edu,Karlsruhe Institute of Technology,No,Alxx,Waxxxx,xxxxxxxxxel@kit.edu,Karlsruhe Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Matxxxxx,Spexxxx,Karlsruhe Institute of Technology,,,,,,xxxxxxxxxxxxrber@kit.edu,,,,,Germany,,Matxxxxx Spexxxx;Grxxxx Nexxxx;Jxx Niexxxx;Alxx Waxxxx,xxxxxxxxxxxxrber@kit.edu;xxxxxxxxxcs.cmu.edu;xxxxxxxxxxes@kit.edu;xxxxxxxxxxel@kit.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
536,536X-C3A5P9G9J7,"Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal",Anixxxxxx Ghxxx and Toxx Vexxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Sarcasm is a pervasive phenomenon in social media, permitting the concise
communication of meaning, affect and attitude. Concision requires wit to
produce and wit to understand, which demands from each party knowledge of
norms, context and a speaker's mindset. Insight into a speaker's psychological
profile at the time of production can be as useful for sarcasm detection as
knowledge of the context in which an utterance is produced. Using a neural
architecture, we demonstrate significant gains in detection accuracy when
knowledge of the speaker's mood at the time of production is available (or can
be inferred). We focus here on sarcasm detection on Twitter, and show that the
mood exhibited by a speaker over tweets leading up to a new post is as useful a
cue for sarcasm as the topical context of the post itself. The work opens the
door to an empirical exploration not just of sarcasm in text but of the
sarcastic state of mind.",7 Feb 2017 10:47:35 GMT,Theoretical,Sentiment analysis and opinion mining,opinion mining and extraction;  subjectivity analysis;  text classification;  NLP in social networking media;  social network,Anixxxxxx,Ghxxx,xxxxxxxxxxxe@gmail.com,University College Dublin,No,Toxx,Vexxx,xxxxxxxxxx@gmail.com,University College Dublin,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anixxxxxx,Ghxxx,University College Dublin,,,,,,xxxxxxxxxxxe@gmail.com,,,,,Ireland,,Anixxxxxx Ghxxx;Toxx Vexxx,xxxxxxxxxxxe@gmail.com;xxxxxxxxxxe@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
537,537X-D6A2E5H9J2,Using Morphological Knowledge in Open-Vocabulary Neural Language Models,Auxxxx Matxxxxx;Grxxxx Nexxxx and Chxxx Dxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Languages with productive morphology pose problems for language models that
generate words from a fixed vocabulary.
Although character-based models allow any possible word type to be generated,
they are linguistically naïve: they must discover that words exist and are
delimited by spaces---basic linguistic facts that are built in to the structure
of word-based models. We introduce an open-vocabulary language model that
incorporates more sophisticated linguistic knowledge by predicting words using
a mixture of three generative processes: (1) by generating words as a sequence
of characters, (2) by directly generating full word forms, and (3) by
generating words as a sequence of morphemes that are combined using a
hand-written morphological analyzer.
Experiments on Finnish, Turkish, and Russian show that our model outperforms
character sequence models and other strong baselines on intrinsic and extrinsic
measures.
Furthermore, we show that our model learns to exploit morphological knowledge
encoded in the analyzer, and, as a byproduct, it can perform effective
unsupervised morphological disambiguation.",7 Feb 2017 00:35:18 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",generative models;  morphology,Auxxxx,Matxxxxx,xxxxxxxx@cmu.edu,Carnegie Mellon University,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Chxxx,Dyxx,xxxxxxxxogle.com,Google DeepMind,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Auxxxx,Matxxxxx,Carnegie Mellon University,,,,,,xxxxxxxx@cmu.edu,,,,,United States,,Auxxxx Matxxxxx;Grxxxx Nexxxx;Chxxx Dyxx,xxxxxxxx@cmu.edu;xxxxxxxxxcs.cmu.edu;xxxxxxxxoogle.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
538,538X-D5G3B3A9C4,Support Vector Machine Active Learning Algorithms with Query-by-Committee versus Closest-to-Hyperplane Selection,Micxxxx Bloxxxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"This paper compares and contrasts the performance of closest-to-hyperplane
selection with query-by-committee selection when support vector machines are
used as the base learners in both cases. Evaluations are conducted on
imbalanced datasets, which commonly arise in practice for natural language
processing applications such as information extraction applications. Algorithms
based on closest-to-hyperplane selection and query-by-committee selection are
combined with methods for addressing imbalance such as positive amplification
based on prevalence statistics from initial random samples. Three algorithms
(ClosestPA, QBagPA, and QBoostPA) are presented and carefully evaluated on
datasets for text classification and relation extraction. The ClosestPA
algorithm is shown to consistently outperform the other two in a variety of
ways and insights are provided as to why this is the case.",7 Feb 2017 01:29:26 GMT,Empirical/Data-Driven,Machine learning,experimental evaluation/comparison of ML methods,Micxxxx,Bloxxxxxx,xxxxxxxxxd@tcnj.edu,The College of New Jersey,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Micxxxx,Bloxxxxxx,The College of New Jersey,,,,,,xxxxxxxxxd@tcnj.edu,,,NJ,,United States,,Micxxxx Bloxxxxxx,xxxxxxxxxd@tcnj.edu,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
539,539X-F3A5F3A3E5,Sentiment Centroid based Hierarchical Attention Network,Lx Zhxx and waxx maoxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Deep neural networks based sentiment classification approaches have achieved
significant improvement for the task of sentiment classification. Most exiting
algorithms typically model text representations by various deep neural networks
but ignore the global sentiment features of corpora. Other approaches learn
text representations and sentiments separately, and use a tuning parameter to
combine them. In this study, we propose a novel sentiment centroid based
hierarchical attention network to learn text representations as well as the
global sentiment features of a corpus. The characteristics of our approach are
(1) we model global sentiment features of text collections using sentiment
centroids; (2) we train text representations which are most close to the
sentiment centroids of their sentiment categories; and (3) we design a novel
joint objective function which employ softmax as well as sentiment centroid
based classifier. We conduct experiments on three real large-scale review
datasets. Evaluations show that our approach not only achieves competitive
results with state-of- the-art methods but also get more powerful feature
representations.",7 Feb 2017 00:40:24 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction;  opinion representation;  text classification,Lx,Zhxx,xxxxxxxxx.stc.sh.cn,East China Normal University,No,waxx,maoxxxx,xxxxxxxxxxxxica.stc.sh.cn,East China normal University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,waxx,maoxxxx,East China normal University,,,,,,xxxxxxxxxxxxica.stc.sh.cn,,,,,China,,Lx Zhxx;waxx maoxxxx,xxxxxxxxx.stc.sh.cn;xxxxxxxxxxxxxica.stc.sh.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
540,540X-P9F5B3C9D8,Multi-instance Multi-label Learning for Aspect Identification from Reviews,Tiaxxxx Nixx;XIxxxx Dxx;Jixxxx Huxxx;Shuxxxx Huxxx and Jixxxx Cxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Aspect-Based Sentiment Analysis(ABSA) is becoming increasingly important in
opinion mining using customer reviews. In particular, aspect identification is
a fundamental task in ABSA. Since users usually express opinions on multiple
aspects within a review, we propose to formulate the aspect identification task
as a multi-instance multi-label learning problem. We begin by dividing a review
into several instances based on sentence structure. We then apply co-training
to transfer labels from reviews to instances, so that we can build an
instance-level labeled dataset for better prediction of aspect identification.
Another contribution of this paper is extracting alignment features, which can
be used to further improve aspect identification accuracy. The experimental
results show the effectiveness of both our proposed methodology and alignment
features. On two SemEval2015 benchmark datasets, our best results outperformed
the winning solution in classification F1 scores by 5.5% and 14.3%,
respectively.",7 Feb 2017 08:34:05 GMT,Applications/Tools,Sentiment analysis and opinion mining,sentiment analysis,Tiaxxxx,Nixx,xxxxxxxxxx.nju.edu.cn,"National Key Laboratory for Novel Software Technology, Nanjing University",No,XIxxxx,Dxx,xxxxxxxxxnju.edu.cn,"National Key Laboratory for Novel Software Technology, Nanjing University",No,Jixxxx,Huxxx,xxxxxxxxxxx.nju.edu.cn,"National Key Laboratory for Novel Software Technology, Nanjing University",No,Shuxxxx,Huxxx,xxxxxxxxxxxn@gmail.com,Nanjing University,No,Jixxxx,CHxx,xxxxxxxxju.edu.cn,Nanjing University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,XIxxxx,Dxx,"National Key Laboratory for Novel Software Technology, Nanjing University",,,1891xxxxxxx,,,xxxxxxxxxnju.edu.cn,,,,,China,,Tiaxxxx Nixx;XIxxxx Dxx;Jixxxx Huxxx;Shuxxxx Huxxx;Jixxxx CHxx,xxxxxxxxxx.nju.edu.cn;xxxxxxxxxxnju.edu.cn;xxxxxxxxxxxp.nju.edu.cn;xxxxxxxxxxxan@gmail.com;xxxxxxxxxju.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
541,541X-C9H8H3E7H2,Toward learning morphology of natural language as a finite-state grammar,Jaxxx Noxxx and Roxxx Yanxxxxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"We present a set of algorithms that learn to segment words in morphologically
rich languages, in an unsupervised fashion. Morphology of many languages can be
modeled by finite state machines (FSMs). We start with a MDL-based learning
algorithm as a baseline. We then formulate well-motivated and general
linguistic principles about morphological systems, and incorporate them into
the algorithm as heuristics, to constrain the search space. We evaluate the
algorithm on three highly-inflecting languages. Evaluation of segmentation
shows gains in performance compared to the state of the art. We conclude with a
discussion about how the learned model relates to a morphological FSM, which is
the ultimate goal.",7 Feb 2017 11:24:15 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",unsupervised and semi-supervised learning;  morphology,Jaxxx,Noxxx,xxxxxxxxxxxxxs.helsinki.fi,University of Helsinki,No,Roxxx,Yanxxxxxx,xxxxxxxxxxxxxxx@cs.helsinki.fi,University of Helsinki,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Roxxx,Yanxxxxxx,University of Helsinki,,,+358 5xxxxxxxxxxxx,,,xxxxxxxxxxxxxxx@cs.helsinki.fi,,,,,Finland,,Jaxxx Noxxx;Roxxx Yanxxxxxx,xxxxxxxxxxxxxs.helsinki.fi;xxxxxxxxxxxxxxxr@cs.helsinki.fi,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
542,542X-E3A3P8P3J4,Exploring Neural Fusion Operations for Multi-Modal Classification,Doxxx Kixxx;Edoxxxx Grxxx; Axx and  Joxxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"While the incipient internet was largely text-based, the modern world is
becoming increasingly multi-modal. In this work, we systematically explore
various neural fusion operations for multi-modal classification where one
modality is discrete, e.g. text, and the other is continuous, e.g. visual
representations transferred from a convolutional neural network. Furthermore,
we examine the trade-off between the availability of high-quality uni-modal
data and the inclusion of data from another modality. Lastly, we experiment
with discretizing the continuous features in order to speed up and simplify the
fusion process. We hope that our work will serve as a guide and useful baseline
for future work on the topic of multi-modal classification.",7 Feb 2017 05:53:34 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",discriminative learning methods;  experimental evaluation/comparison of ML methods;  multimodal representations and processing;  text classification,Doxxx,Kixxx,xxxxxxxxxxxxcl.cam.ac.uk,Facebook,No,Edoxxxx,Grxxx,xxxxxxxxxxxve@gmail.com,Facebook,No,Arxxxx,Joxxxx,xxxxxxxxxxxin@gmail.com,Facebook AI Research,No,Toxxx,Mikxxxx,xxxxxxxxxgmail.com,Facebook,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Doxxx,Kixxx,Facebook,,,,,,xxxxxxxxxxxxcl.cam.ac.uk,,,NY,,United States,,Doxxx Kixxx;Edoxxxx Grxxx;Arxxxx Joxxxx;Toxxx Mikxxxx,xxxxxxxxxxxxcl.cam.ac.uk;xxxxxxxxxxxxve@gmail.com;xxxxxxxxxxxxin@gmail.com;xxxxxxxxx@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
543,543X-F7C9E6G2B5,Learning Character-level Compositionality with Visual Features,Frexxxxxx Lxx;Hxx Lx;Chxxx Lx and Grxxxx Nexxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"Previous work has modeled the compositionality of words by creating
character-level models of meaning, reducing problems of sparsity for rare
words. However, in many writing systems compositionality has an effect even on
the character-level: the meaning of a character is derived by the sum of its
parts. In this paper, we model this effect by creating embeddings for
characters based on their visual characteristics, creating an image for the
character and running it through a convolutional neural network to produce a
visual character embedding. Experiments on a text classification task
demonstrate that such model allows for better processing of instances with rare
characters in languages such as Chinese, Japanese, and Korean. Additionally,
qualitative analyses demonstrate that our proposed model learns to focus on the
parts of characters that carry topical content which resulting in embeddings
that are coherent in visual space.",21 Apr 2017 18:28:48 GMT,Empirical/Data-Driven,Semantics,,Frexxxxxx,Lxx,xxxxxxxxxxew.cmu.edu,Carnegie Mellon University,No,Hxx,Lx,xxxxxxx.cmu.edu,Carnegie Mellon University,No,Chxxx,Lx,xxxxxxxxxxrew.cmu.edu,Carnegie Mellon University,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Frexxxxxx,Lxx,Carnegie Mellon University,,,+1-41xxxxxxxxxx,,,xxxxxxxxxxx29@gmail.com,,Pittsburgh,PA,,United States,,Frexxxxxx Lxx;Hxx Lx;Chxxx Lx;Grxxxx Nexxxx,xxxxxxxxxxew.cmu.edu;xxxxxxxx.cmu.edu;xxxxxxxxxxxrew.cmu.edu;xxxxxxxxxcs.cmu.edu,Learning Character-level Compositionality with Visual Features,Learning Character-level Compositionality with Visual Features,10,Frederick Liu,,"Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213",on,on,Only include my submission if it is accepted.,No,None,None
544,544X-E2J4E7B3D8,Semantic Similarity based Clustering of License Excerpts,Naxxxx Mouxxxx;Sixxx Scxxxx and S�xxxx Axx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"With the omnipresent availability and use of cloud services, software tools,
Web portals or services, legal contracts in the form of license agreements or
terms and conditions regulating their use are of paramount importance. Often
the textual documents describing these regulations comprise many pages and can
not be reasonably assumed to be read and understood by humans. In this work, we
describe a method for extracting and clustering relevant parts of such
documents, including permissions, obligations, and prohibitions. The clustering
is based on semantic similarity employing a distributional semantics approach
on large word embeddings database. An evaluation shows that it can
significantly improve human comprehension and that improved feature-based
clustering has a potential to further reduce the time required for EULA
digestion. Our implementation is available as a Web service, which can directly
be used to process and prepare legal usage contracts.",7 Feb 2017 11:27:29 GMT,Applications/Tools,Semantics,multiword semantics/compositionality;  NLP applications;  information extraction;  lexical semantics;  rule-based/symbolic learning methods;  distributional similarity;  text mining;  text classification;  ontology development;  term extraction;  ontological semantics,Naxxxx,Mousxxxxxxxxx,xxxxxxxxxxni-bonn.de,University of Bonn,No,Sixxx,Scxxxx,xxxxxxxxxxxuni-bonn.de,University of Bonn,No,S�xxxx,Auxx,xxxxxxxxxni-bonn.de,University of Bonn,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Naxxxx,Mousxxxxxxxxx,Fraunhofer IAIS & University of Bonn,,,,,,xxxxxxxxxxni-bonn.de,,,,,Germany,,Naxxxx Mouxxxx;Sixxx Scxxxx;S�xxxx Auxx,xxxxxxxxxxni-bonn.de;xxxxxxxxxxx.uni-bonn.de;xxxxxxxxxxni-bonn.de,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
545,545X-C7E6J9A3A9,Challenges for Information Extraction in the Oil and Gas Domain,Fabxxxxx Chxxxx;Shxxx Trxxxx;Roxxxx Farxxxx;Lixx Rexx and  Axxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Increasingly, governments, corporations, and scientific organizations need to
extract complex information from highly technical documents expressed in
natural languages. While linguistic resources exist in some technical domains,
they are largely unavailable for the oil and gas domain. We applied natural
language processing tools trained on the news domain to extract information
from 155 annotated text passages from geological reports. In recognizing oil
field entity names, we achieved a precision of $.94$ and recall of $.43$
(F1=$.295$) without supervised learning. We describe the impact of errors
including incorrect part-of-speech tags, multiword expressions, numeric
quantities, and other issues leading to incorrect entity classifications. These
could be reduced with a domain-specific dictionary that includes part-of-speech
tags. We conclude that the high quality NLP tools trained on news corpora can
be adapted for entity extraction in technical domains without requiring
domain-specific supervision.",7 Feb 2017 00:55:50 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP applications;  information extraction;  named entity recognition;  domain adaptation;  NLP for expert domains,Fabxxxxx,Chxxxx,xxxxxxxxxr.ibm.com,IBM Research,No,Shxxx,Trxxxx,xxxxxxxxs.ibm.com,IBM Research,No,Roxxxx,Farxxxx,xxxxxxxxxs.ibm.com,IBM Research,No,Lixx,Rexx,xxxxxxxxxgmail.com,IBM Research,No,Alexxxxxx,Radxxxxxx,xxxxxxxxxx@gmail.com,IBM Research and EMAp/FGV,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Alexxxxxx,Radxxxxxx,IBM Research and EMAp/FGV,,,,,,xxxxxxxxxx@gmail.com,,Rio de Janeiro,RJ,,Brazil,"Alexandre is a Research Staff Member in the Natural Resources group at IBM Research – Brazil. He joined IBM in December 2012.

Prior that, Alexandre was adjunct professor at the Applied Mathematics School of Getulio Vargas Foundation (EMAp/FGV) and he was also lecturer at PUC-Rio. Alexandre has taught many graduate and undergraduate courses: logics, data structures, programming, formal languages and automata theory. During his Ph.D., Alexandre was international fellow at Microsoft Research and SRI International. At MSR, in 2008, he worked with the Z3 SMT Solver team under supervision of Leonardo Moura and Nikolaj Bjørner developing a distributed environment for testing and optimizations of Z3. At SRI International, in 2009, he worked under supervision of Natarajan Shankar in different research projects.",Fabxxxxx Chxxxx;Shxxx Trxxxx;Roxxxx Farxxxx;Lixx Rexx; Axxx and rx Radxxxxxx,xxxxxxxxxr.ibm.com;xxxxxxxxxs.ibm.com;xxxxxxxxxus.ibm.com;xxxxxxxxx@gmail.com;xxxxxxxxxxr@gmail.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
546,546X-H3P4C3F8D4,Modeling Vision and Language Memories in Video Description Generation,Raxxxx Faxxxx;Abdexxxxxxx Mohxxxx;Marxxxxx Mitxxxxx;Sixx Bixx and Pusxxxxx Koxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"We present a method to improve video description generation by modeling
higher-order interactions between video frames and described concepts. By
storing past visual attention in the video associated to previously generated
words, the system is able to decide what to look at and describe in light of
what it has already looked at and described. This enables not only more
effective local attention, but tractable consideration of the video sequence
while generating each word. Evaluation on the challenging and popular MSVD and
Charades datasets demonstrates that the proposed architecture outperforms
previous video description approaches without requiring external temporal video
features.",7 Feb 2017 09:07:15 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",multimodal representations and processing,Raxxxx,Faxxxx,xxxxxxxxxxxxx@mavs.uta.edu,University of Texas Arlington,No,Abdexxxxxxx,Mohxxxx,xxxxxxxxxxrosoft.com,Microsoft,No,Marxxxxx,Mitxxxxx,xxxxxxxxxxxxll@gmail.com,Microsoft Research,No,Sinxxxxxx,Kaxx,xxxxxxxxxxxxx@microsoft.com,Microsoft,No,Pusxxxxx,Koxxx,xxxxxxxxxxrosoft.com,Microsoft,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Raxxxx,Faxxxx,University of Texas Arlington,,,,,,xxxxxxxxxxxxx@mavs.uta.edu,,,,,United States,,Raxxxx Faxxxx;Abdexxxxxxx Mohxxxx;Marxxxxx Mitxxxxx;Sixx Bixx;Pusxxxxx Koxxx,xxxxxxxxxxxxx@mavs.uta.edu;xxxxxxxxxxcrosoft.com;xxxxxxxxxxxxell@gmail.com;xxxxxxxxxxxxxx@microsoft.com;xxxxxxxxxxcrosoft.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
547,547X-F9H9A6J6F2,Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling,Kaxxxx Kawxxxxx;Chxxx Dyxx and Phxx Blxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Poster Monday,,Undecided (Machine Learning),"Fixed-vocabulary language models fail to account for one of the most
characteristic statistical facts of natural language: the frequent creation and
reuse of new word types. Although character-level language models offer a
partial solution in that they can create word types not attested in the
training corpus, they do not capture the ``bursty'' distribution of such words.
In this paper, we augment a hierarchical LSTM language model that generates
sequences of word tokens character by character with a caching mechanism that
learns to reuse previously generated words. 
To validate our model we construct a new open-vocabulary language modeling
corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia
articles in 7 typologically diverse languages and demonstrate the effectiveness
of our model across this range of languages.",23 Apr 2017 00:08:10 GMT,Empirical/Data-Driven,Machine learning,,Kaxxxx,Kawxxxxx,xxxxxxxxxxxxxxakami@gmail.com,University of Oxford,No,Chxxx,Dyxx,xxxxxxxxogle.com,Google DeepMind,No,Phxx,Bluxxxx,xxxxxxxxxxxx@cs.ox.ac.uk,University of Oxford,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxxxx,Kawxxxxx,University of Oxford,,,412xxxxxxx,,,xxxxxxxxxxxxxxakami@gmail.com,,Oxford,,,United Kingdom,,Kaxxxx Kawxxxxx;Chxxx Dyxx;Phxx Bluxxxx,xxxxxxxxxxxxxxakami@gmail.com;xxxxxxxxoogle.com;xxxxxxxxxxxxm@cs.ox.ac.uk,Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling,Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling,11,Kazuya Kawakami,,,,on,No. Do not include my submission in this dataset.,No,None,None
548,548X-A8B3A4H5P5,The Rich Event Ontology,Suxxx Brxxx;Clxxxx Boxxxx and Maxxxx Paxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"The Rich Event Ontology is a freely available resource that can serve as a
valuable tool for deep semantic analysis of events, a key area in NLP tasks
like question answering, information extraction, and knowledge representation.
It provides an independent conceptual backbone that unifies valuable lexical
resources and adds critical relational information in the form of
event-to-event causal and temporal relations. The initial three resources
linked to the ontology are FrameNet, VerbNet and the Rich Entities Relations
and Events resources.  Each of these represents a rich repository of lexical
event semantics, in the form of both event classifications and semantic role
labeling schema.  We have shown how the ontology could be used to move from the
ontology to query text for event types and scenarios of interest, and for
moving from text to the ontology to access in-terpretations of events using the
combined semantic information housed there. We believe REO is unique among
existing ontologies in combining in-depth representation of events with the
ability to link valuable but disparate lexical resources and annotation
schemes.",7 Feb 2017 01:05:11 GMT,Resources/Evaluation,Resources and evaluation,lexical semantics;  lexicon development;  formal semantics and logic;  semantic relations;  relation/event extraction;  ontology development;  semantic role labelling,Suxxx,Brxxx,xxxxxxxxxxxxcolorado.edu,University of Colorado at Boulder,No,Clxxxx,Boxxxx,xxxxxxxxxxxxxxl.civ@mail.mil,Army Research Labs,No,Maxxxx,Paxxxx,xxxxxxxxxxxxx@colorado.edu,University of Colorado,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Susanxxxxxxxxx,Brxxx,University of Colorado at Boulder,,,720xxxxxxx,,,xxxxxxxxxxxxcolorado.edu,,Boulder,CO,,United States,,Suxxx Brxxx;Clxxxx Boxxxx;Maxxxx Paxxxx,xxxxxxxxxxxxcolorado.edu;xxxxxxxxxxxxxxal.civ@mail.mil;xxxxxxxxxxxxxr@colorado.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
549,549X-D8P4H6H4F7,Data Selection Strategies for Multi-Domain Sentiment Analysis,Sebxxxxxx Ruxxx;Paxxx Ghaxxxxx and Joxx Gx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Domain adaptation is important in sentiment analysis due to the shift in
domain. Recently, multi-domain adaptation has become more pervasive, but
existing approaches train on all available source domains including dissimilar
ones. However, the selection of appropriate training data is as important as
the choice of algorithm. We undertake -- to our knowledge for the first time --
an extensive study of domain similarity metrics in the context of sentiment
analysis and propose novel representations, metrics, and a new scope for data
selection. We evaluate the proposed methods on two large-scale multi-domain
adaptation settings on tweets and reviews and demonstrate that they
consistently outperform strong random and balanced baselines, while our
proposed selection strategy outperforms instance-level selection and yields the
best score on a large reviews corpus. All experiments are available at
url_redacted.",7 Feb 2017 01:03:57 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  domain adaptation;  opinion mining and extraction;  adaptation to noisy data,Sebxxxxxx,Ruxxx,xxxxxxxxxxxxian@gmail.com,"National University of Ireland, Galway",No,Paxxx,Ghaxxxxx,xxxxxxxxlien.com,Aylien Ltd.,No,Johxxxx,Brexxxx,xxxxxxxxxxxxxxxsight-centre.org,Insight Centre for Data Analytics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sebxxxxxx,Ruxxx,"National University of Ireland, Galway",,,,,,xxxxxxxxxxxxian@gmail.com,,,,,Ireland,,Sebxxxxxx Ruxxx;Paxxx Ghaxxxxx;Joxx Gx,xxxxxxxxxxxxian@gmail.com;xxxxxxxxylien.com;xxxxxxxxxxxxxxxxsight-centre.org,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
550,550X-B2C8C9F7G2,Multi-task Coupled Attentions for Category-specific Aspect and Opinion Terms Co-extraction,Wexxx Waxx;Sixxx Jixxxx and Daxxxx Dahxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In aspect-based sentiment analysis, most existing methods either focus on
aspect/opinion terms extraction or aspect terms categorization. However, each
task by itself only provides partial information to end users. To generate more
detailed and structured opinion analysis, we propose a finer-grained problem,
which we call category-specific aspect and opinion terms extraction. This
problem involves the identification of aspect and opinion terms within each
sentence, as well as the categorization of the identified terms. To this end,
we propose an end-to-end multi-task attention model, where each task
corresponds to aspect/opinion terms extraction for a specific category. Our
model benefits from exploring the commonalities and relationships among
different tasks to address the data sparsity issue. We demonstrate its
state-of-the-art performance on three benchmark datasets.",7 Feb 2017 08:15:00 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction,Wexxx,Waxx,xxxxxxxxxx.ntu.edu.sg,Nanyang Technological University,No,Sinnxxxxxxxx,Pxx,xxxxxxxxxgmail.com,Nanyang Technological University,No,Daxxxx,Dahxxxxxx,xxxxxxxxxer@sap.com,SAP,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wexxx,Waxx,Nanyang Technological University,,,,,,xxxxxxxxxx.ntu.edu.sg,,,,,Singapore,,Wexxx Waxx;Sixxx Jixxxx;Daxxxx Dahxxxxxx,xxxxxxxxxx.ntu.edu.sg;xxxxxxxxx@gmail.com;xxxxxxxxxxer@sap.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
551,551X-E6B3F6H2G8,Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation without Manual Training Data,tomxxxx paxxxx and Robxxxx Navxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"Annotating large numbers of sentences with senses is the heaviest requirement
of current Word Sense Disambiguation. We present Train-O-Matic, a method for
generating millions of sense-annotated training instances for virtually all
meanings of words in a language's vocabulary. The approach is fully automatic:
no human intervention is required and the only type of human knowledge used is
a WordNet-like resource. Train-O-Matic achieves consistently state-of-the-art
performance across gold standard datasets, while at the same time relieving the
burden of manual annotation.",7 Feb 2017 11:56:25 GMT,Resources/Evaluation,Resources and evaluation,corpus development;  lexical semantics;  corpus annotation methods;  word sense disambiguation;  NLP on Wikipedia and other collaboratively constructed resources,tomxxxx,paxxxx,xxxxxxxxxxuniroma1.it,Sapienza University of Rome,No,Robxxxx,Navxxxx,xxxxxxxxxxxuniroma1.it,Sapienza University of Rome,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tomxxxx,Paxxxx,Sapienza University of Rome,,,,,,xxxxxxxxxxuniroma1.it,,,,,Italy,,tomxxxx paxxxx;Robxxxx Navxxxx,xxxxxxxxxxuniroma1.it;xxxxxxxxxxx.uniroma1.it,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
552,552X-G5C3H6D9D9,Approximations of Predictive Entropy Correlate with Reading Times,Maxxxx vxx and Wilxxxx Schxxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"It has been shown that the lexical frequency of an upcoming word can affect
reading times even when the upcoming word is masked from readers (Angele et
al., 2015). One explanation for this observation is that reading times are
affected by the amount of (un)certainty a reader has about upcoming material.
That is, readers may slow down in anticipation of regions of greater
uncertainty. In line with this hypothesis, this study finds a positive
correlation between predictive entropy (as distinct from entropy reduction) and
self-paced reading times. This study also demonstrates that such predictive
entropy can be effectively approximated by the surprisal of upcoming
observations. A further experiment shows improved fit of this future surprisal
estimate to reading times when the grammar is made more granular, which would
be prohibitively expensive for predictive entropy. These results suggest
readers engage in fine-grained predictive estimations of certainty about
upcoming material, that such predictions can influence reading times, and that
estimating the uncertainty can be achieved through the much cheaper but more
robust means of information-theoretic surprisal.",7 Feb 2017 02:08:55 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,syntax,Maxxxx,van xxxxxxxxx,xxxxxxxxxxxel.1@osu.edu,The Ohio State University,No,Wilxxxx,Schxxxx,xxxxxxxxxxng.osu.edu,The Ohio State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,van xxxxxxxxx,Johns Hopkins University,,,,,,xxxxxxxxxxxxxxjndel@gmail.com,,,OH,,United States,,Maxxxx vxx;Wilxxxx Schxxxx,xxxxxxxxxxxel.1@osu.edu;xxxxxxxxxxing.osu.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
553,553X-E8E8E7A8F3,Cross-Context Lexical Analysis,Sexx Masxxxx;Chxxx Gexxxx and Chexxxxxxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We propose a general framework for performing cross-context lexical             
    analysis; that is, analyzing similarities and differences in term meaning  
 
    and representation with respect to different, potentially overlapping      
 
    partitions of a text collection.                                               
 

    We apply our framework to three different tasks: semantic change detection 
 
    (discovering words whose meanings changed over time), comparative lexical  
 
    analysis over context (finding context-sensitive and                       
 
    context-\emph{in}sensitive terms), and word representation comparison      
 
    (investigating randomness inherent in word embeddings).",7 Feb 2017 06:31:35 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",sentiment analysis;  opinion mining and extraction;  text mining;  multi-document summarization;  term extraction;  evaluation metrics;  document mining;  pragmatics,Sexx,Masxxxx,xxxxxxxxxxllinois.edu,University of Illinois at Urbana-Champaign,No,Chxxx,Gexxxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana-Champaign,No,Chexxxxxxx,Zhxx,xxxxxxxxxinois.edu,University of Illinois at Urbana-Champaign,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sexx,Masxxxx,University of Illinois at Urbana-Champaign,,,513xxxxxxx,,,xxxxxxxxxxllinois.edu,,,,,United States,,Sexx Masxxxx;Chxxx Gexxxx;Chexxxxxxx Zhxx,xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu;xxxxxxxxxlinois.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
554,554X-A4H5D7A3G2,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,Zxx Gxx;Chuxxxxx Lx;Chaxxxxx Chxx;Yunxxxx Px;Qinxxxxx Sx and Lawxxxxx Caxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Monday,,Undecided (Machine Learning),"Recurrent neural networks (RNNs) have shown promising performance for language
modeling. However, traditional training of RNNs using back-propagation through
time often suffers from overfitting. One reason for this is that stochastic
optimization (used for large training sets) does not provide good estimates of
model uncertainty. This paper leverages recent advances in stochastic gradient
Markov Chain Monte Carlo (also appropriate for large training sets) to learn
weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm,
adding gradient noise during training (enhancing exploration of the
model-parameter space) and model averaging when testing. Extensive experiments
on various RNN models and across a broad range of applications demonstrate the
superiority of the proposed approach relative to stochastic optimization.",22 Apr 2017 23:37:55 GMT,Applications/Tools,Machine learning,,Zxx,Gxx,xxxxxxuke.edu,Duke University,No,Chuxxxxx,Lx,xxxxxxxxxxi@duke.edu,Duke University,No,Chaxxxxx,Chxx,xxxxxxxuke.edu,Duke University,No,Yunxxxx,Px,xxxxxxuke.edu,Duke University,No,Qinxxxxx,Sx,xxxxxxuke.edu,Duke University,No,Lawxxxxx,Caxxx,xxxxxxxduke.edu,Duke University,No,,,,,,,,,,,,,,,,,,,,,,Zxx,Gxx,Microsoft,,,919xxxxxxx,,,xxxxxxxxxxcrosoft.com,,,,,United States,,Zxx Gxx;Chuxxxxx Lx;Chaxxxxx Chxx;Yunxxxx Px;Qinxxxxx Sx;Lawxxxxx Caxxx,xxxxxxuke.edu;xxxxxxxxxxli@duke.edu;xxxxxxxduke.edu;xxxxxxxuke.edu;xxxxxxxuke.edu;xxxxxxxxduke.edu,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,11,Zhe Gan,,Duke University,on,,Only include my submission if it is accepted.,No,None,None
555,555X-P7A4C6P5P3,Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,Waxx Lixx;Daxx Yogxxxxx;Chxxx Dyxx and Phxx Blxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Monday,,Undecided (Generation Summarization),"Solving algebraic word problems requires executing a series of arithmetic
operations---a program---to obtain a final answer. However, since programs can
be arbitrarily complicated, inducing them directly from question-answer pairs
is a formidable challenge. To make this task more feasible, we solve these
problems by generating answer rationales, sequences of natural language and
human-readable mathematical expressions that derive the final answer through a
series of small steps. Although rationales do not explicitly specify programs,
they provide a scaffolding for their structure via intermediate milestones. To
evaluate our approach, we have created a new 100,000-sample dataset of
questions, answers and rationales. Experimental results show that indirect
supervision of program learning via answer rationales is a promising strategy
for inducing arithmetic programs.",24 Apr 2017 04:18:17 GMT,Empirical/Data-Driven,Generation,,Waxx,Lixx,xxxxxxxxxx2@gmail.com,Google DeepMind,No,Daxx,Yogxxxxx,xxxxxxxxxxgoogle.com,DeepMind,No,Chxxx,Dyxx,xxxxxxxxogle.com,Google DeepMind,No,Phxx,Bluxxxx,xxxxxxxxxgoogle.com,"Google DeepMind, Oxford University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Waxx,Lixx,Google DeepMind,,,3519xxxxxxxx,,,xxxxxxxxxx2@gmail.com,,,AL,,United States,,Waxx Lixx;Daxx Yogxxxxx;Chxxx Dyxx;Phxx Bluxxxx,xxxxxxxxxx2@gmail.com;xxxxxxxxxx@google.com;xxxxxxxxoogle.com;xxxxxxxxxxgoogle.com,Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,10,NA,,,,on,No. Do not include my submission in this dataset.,No,None,None
556,556X-J5P9F3B2D3,Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication,Laxxx Sxx and Joxxx Chxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"To enable human-robot communication and collaboration, previous works represent
grounded verb semantics as the potential change of state to the physical world
caused by these verbs. Grounded verb semantics are acquired mainly based on the
parallel data of the use of a verb phrase and its corresponding sequences of
primitive actions demonstrated by humans. The
rich interaction between teachers and students that is considered important in
learning new skills has not yet been explored. To address this limitation, this
paper presents a new interactive learning approach that allows robots to
proactively engage in interaction with human partners by asking good questions
to learn models for grounded verb semantics. The proposed approach uses
reinforcement learning to allow the robot to acquire an optimal policy for its
question-asking behaviors by maximizing the long-term reward. Our empirical
results have shown that the interactive learning approach leads to more
reliable models for grounded verb semantics, especially in the noisy
environment which is full of uncertainties. Compared to previous work, the
models acquired from interactive learning result in a 48% to 145% performance
gain when applied in new situations.",22 Apr 2017 03:36:56 GMT,Empirical/Data-Driven,Semantics,,Laxxx,Sxx,xxxxxxxxgmail.com,Michigan State University,No,Joxxx,Chxx,xxxxxxxxe.msu.edu,Michigan State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Laxxx,Sxx,Microsoft Cloud + AI,,,,,,xxxxxxxxgmail.com,,,,,United States,,Laxxx Sxx;Joxxx Chxx,xxxxxxxxgmail.com;xxxxxxxxxe.msu.edu,Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication,Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication,11,Lanbo She,,"Michigan State University
220 Trowbridge Rd, East Lansing, MI 48824",,,No. Do not include my submission in this dataset.,No,None,None
557,557X-C3F6A9H2H3,End-to-End Neural Relation Extraction with Global Optimization,Meixxxx Zhxxx;Yxx Zhxxx and Guoxxxx F,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Neural networks have recently shown promising results for relation extraction.
State-of-the-art models cast the task as an end-to-end problem, solved
incrementally using a local classifier.
Yet previous work using statistical models have demonstrated that global
optimization can achieve better performances compared to local classification.
We build a globally optimized neural model for end-to-end relation extraction,
proposing novel LSTM features in order to better learn representations.
Experiments show that our model is highly effective, achieving the best
performances on two standard benchmarks.",7 Feb 2017 07:05:43 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",relation/event extraction,Meixxxx,Zhxxx,xxxxxxxxxx.hit.edu.cn,Heilongjiang University,No,Yxx,Zhxxx,xxxxxxxxxxsutd.edu.sg,Singapore University of Technology and Design,No,Guoxxxx,Fx,xxxxxxxxmail.com,Heilongjiang University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Meixxxx,Zhxxx,Heilongjiang University,,,+86-4xxxxxxxxxxx,,,xxxxxxxxxx.hit.edu.cn,,,,,China,,Meixxxx Zhxxx;Yxx Zhxxx;Guoxxxx Fx,xxxxxxxxxx.hit.edu.cn;xxxxxxxxxxxsutd.edu.sg;xxxxxxxxtmail.com,,,,,,,on,,"Yes, include my submission even if the paper is rejected.",No,None,None
558,558X-E8F5D7C6P6,Abstractive Document Summarization with a Graph-Based Attentional Neural Model,Jixxx Txx;Xiaxxxx Wxx and Jiaxxxx Xxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Wednesday,,Undecided (Generation Summarization),"Abstractive summarization is the ultimate goal of document summarization
research, but previously it is less investigated due to the immaturity of text
generation techniques. Recently impressive progress has been made to
abstractive sentence summarization using neural models. Unfortunately, attempts
on abstractive document summarization are still in a primitive stage, and the
evaluation results are worse than extractive methods on benchmark datasets. In
this paper, we review the difficulties of neural abstractive document
summarization, and propose a novel graph-based attention mechanism in the
sequence-to-sequence framework. The intuition is to address the saliency factor
of summarization, which has been overlooked by prior works. Experimental
results demonstrate our model is able to achieve considerable improvement over
previous neural abstractive models. The data-driven neural abstractive method
is also competitive with state-of-the-art extractive methods.",23 Apr 2017 06:36:39 GMT,Empirical/Data-Driven,Summarization,,Jixxx,Txx,xxxxxxxxxpku.edu.cn,Peking University,No,Xiaxxxx,Wxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,Jiaxxxx,Xixx,xxxxxxxxxxx@pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Wxx,Peking University,,,,,,xxxxxxxxxx@pku.edu.cn,,,,,China,,Jixxx Txx;Xiaxxxx Wxx;Jiaxxxx Xixx,xxxxxxxxxpku.edu.cn;xxxxxxxxxxx@pku.edu.cn;xxxxxxxxxxxo@pku.edu.cn,Abstractive Document Summarization with a Graph-Based Attentional Neural Model,Abstractive Document Summarization with a Graph-Based Attentional Neural Model,11,Jiwei Tan,,"Jiwei Tan, Peking University, No. 128 Zhongguancun North Street, Haidian District, Beijing, China.",on,,No. Do not include my submission in this dataset.,No,None,None
559,559X-P4E7P7B7B4,Problem Description Summarizer Using Neural Networks,Rxx Iixx;Kenxxxx Torxxxxx;Jonxxxxxx Ox;Canxxxx Kruxxxxxxx;Yosxxxxxx Asxx;Norxxxxx Axx;Juxxx Mixxxx and Juxxxx Kloxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"This paper proposes a neural network-based method that detects the descriptions
of a wide variety problems and generates compact summaries of those problem
descriptions. We created a data set that includes about 40,000 problem
descriptions sampled from Japanese newspaper articles and web documents, and
manually created their summaries. Our experimental results show that our
method, which was trained using the data set, achieved higher average precision
than an existing method on problem detection and also obtained better ROUGE
scores than the baselines including a neural network-based method for general
summarization.",7 Feb 2017 12:05:54 GMT,Empirical/Data-Driven,Summarization,document summarization,Rxx,Iixx,xxxxxxxxxnict.go.jp,National Institute of Information and Communications Technology,No,Kenxxxx,Torxxxxx,xxxxxxxxxnict.go.jp,NICT,No,Jonxxxxxx,Ox,xxxxxxxxxnict.go.jp,NICT,No,Canxxxx,Kruxxxxxxx,xxxxxxxxxict.go.jp,NICT,No,Yosxxxxxx,Asxx,xxxxxxxct.go.jp,NICT,No,Norxxxxx,Axx,xxxxxxxt.go.jp,NICT,No,Juxxx,Mixxxx,xxxxxxxxxict.go.jp,NICT,No,Juxxxx,Kloxxxxx,xxxxxxxxict.go.jp,National Institute of Information and Communications Technology,No,,,,,,,,,,,,Rxx,Iixx,National Institute of Information and Communications Technology,,,,,,xxxxxxxxxnict.go.jp,,,,,Japan,,Rxx Iixx;Kenxxxx Torxxxxx;Jonxxxxxx Ox;Canxxxx Kruxxxxxxx;Yosxxxxxx Asxx;Norxxxxx Axx;Juxxx Mixxxx;Juxxxx Kloxxxxx and Commuxxxxxxxxx Techxxxxxxx,xxxxxxxxxnict.go.jp;xxxxxxxxxxnict.go.jp;xxxxxxxxxxnict.go.jp;xxxxxxxxxnict.go.jp;xxxxxxxxct.go.jp;xxxxxxxct.go.jp;xxxxxxxxxnict.go.jp;xxxxxxxxxict.go.jp,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
561,561X-A4F9E9G9J6,Semi-supervised sequence tagging with bidirectional language models,Matxxxx Pexxxx;Waxxxx Amxxx; xx and rx Bhagxxxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Poster Monday,,Undecided (Tagging Chunking Syntax Parsing),"Pre-trained word embeddings learned from unlabeled text have become a stan-
dard component of neural network archi- tectures for NLP tasks. However, in
most cases, the recurrent network that oper- ates on word-level representations
to pro- duce context sensitive representations is trained on relatively little
labeled data. In this paper, we demonstrate a general semi-supervised approach
for adding pre- trained context embeddings from bidi- rectional language models
to NLP sys- tems and apply it to sequence labeling tasks. We evaluate our model
on two stan- dard datasets for named entity recognition (NER) and chunking, and
in both cases achieve state of the art results, surpassing previous systems
that use other forms of transfer or joint learning with additional labeled data
and task specific gazetteers.",18 Apr 2017 20:27:38 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Matxxxx,Pexxxx,xxxxxxxxxxllenai.org,Allen Institute for Artificial Intelligence,No,Waxxxx,Amxxx,xxxxxxxxxxxr@gmail.com,Allen Institute for Artificial Intelligence,No,Chaxxxx,Bhagxxxxxxx,xxxxxxxxxxllenai.org,Allen Institute for Artificial Intelligence,No,Rusxxxx,Poxxx,xxxxxxxxxxllenai.org,Allen Institute for Artificial Intelligence,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Matxxxx,Pexxxx,Allen Institute for Artificial Intelligence,,,,,,xxxxxxxxxxllenai.org,,,,,United States,,Matxxxx Pexxxx;Waxxxx Amxxx;Chaxxxx Bhagxxxxxxx;Rusxxxx Poxxx,xxxxxxxxxxllenai.org;xxxxxxxxxxxar@gmail.com;xxxxxxxxxxallenai.org;xxxxxxxxxxallenai.org,Semi-supervised sequence tagging with bidirectional language models,Semi-supervised sequence tagging with bidirectional language models,10,Matthew Peters,,"Allen Institute for Artificial Intelligence
2157 N Northlake Way Suite 110, Seattle, WA 98103",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
562,562X-D3D3E9E4P9,Zero-Shot Relation Extraction via Reading Comprehension,Omxx Lexx;Mxx Joxx;Euxxxx Chxx and Luxx Zetxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We show that relation extraction can be reduced to answering simple reading
comprehension questions, by associating one or more natural-language questions
with each relation slot. This reduction has several advantages: we can (1)
learn high-quality relation-extraction models by extending recent neural
reading-comprehension techniques, (2) build very large training sets for those
models by combining relation-specific crowd-sourced questions with distant
supervision, and even (3) do zero-shot learning by extracting new relations
that are only specified at test-time, for which we have no labeled training
examples. Experiments on a Wikipedia slot-filling task demonstrate that the
approach can generalize to new questions for known relations with high
accuracy, and that zero-shot generalization to unseen relations is possible, at
lower accuracy levels, setting the bar for future work on this task.",7 Feb 2017 03:18:06 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction,Omxx,Lexx,xxxxxxxxxxxxxashington.edu,University of Washington,No,Minxxxxx,Sxx,xxxxxxxxxx@gmail.com,University of Washington,No,Euxxxx,Chxx,xxxxxxxxxxxxshington.edu,University of Washington,No,Luxx,Zettxxxxxxx,xxxxxxxxxxhington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Omxx,Lexx,University of Washington,,,+1-42xxxxxxxxx,,,xxxxxxxxxxxxxashington.edu,,Seattle,WA,,United States,,Omxx Lexx;Mxx Joxx;Euxxxx Chxx;Luxx Zettxxxxxxx,xxxxxxxxxxxxxashington.edu;xxxxxxxxxxn@gmail.com;xxxxxxxxxxxxashington.edu;xxxxxxxxxxxhington.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
563,563X-C9F5C7D3J7,Exploring Vector Spaces for Semantic Relations,Kaxx G�xxxx;Daxxxx Busxxxxx;Thixxxx Chaxxxxx;Isaxxxxx Telxxxx and Haxxx Zarxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Word embeddings are used with success for a variety of tasks involving lexical
semantic similarities between individual words. Using unsupervised methods and
just cosine similarity, encouraging results were obtained for analogical
similarities. In this paper, we explore the potential of pre-trained word
embeddings to identify generic types of semantic relations in an unsupervised
experiment. We propose a new relational similarity measure based on the
combination of word2vec's CBOW input and output vectors which outperforms
concurrent vector representations, when used for unsupervised clustering on
SemEval 2010 Relation Classification data.",7 Feb 2017 10:53:40 GMT,Empirical/Data-Driven,Semantics,unsupervised and semi-supervised learning;  information extraction;  distributional similarity;  semantic relations;  relation/event extraction,Kaxx,G�xxxx,xxxxxxxxxgmail.com,LIPN Université Paris13,No,Daxxxx,Busxxxxx,xxxxxxxxxxxxxxxxxxpn.univ-paris13.fr,"LIPN, Université Paris 13",No,Thixxxx,Chaxxxxx,xxxxxxxxxxxxxxxxxxipn.univ-paris13.fr,LIPN - CNRS University Paris 13,No,Isaxxxxx,Telxxxx,xxxxxxxxxxxxxxxr@univ-paris3.fr,"university Paris 3 - Sorbonne Nouvelle, LaTTiCe",No,Haxxx,Zarxxxxxxx,xxxxxxxxxxxxxxxxxxipn.univ-paris13.fr,"LIPN, UMR CNRS 7030,  Paris 13 University, Sorbonne Paris Cité",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxx,G�xxxx,LIPN Université Paris13,,,,,,xxxxxxxxxgmail.com,,Paris,,,France,,Kaxx G�xxxx;Daxxxx Busxxxxx;Thixxxx Chaxxxxx;Isaxxxxx Telxxxx;Haxxx Zarxxxxxxx,xxxxxxxxxgmail.com;xxxxxxxxxxxxxxxxxxipn.univ-paris13.fr;xxxxxxxxxxxxxxxxxxxipn.univ-paris13.fr;xxxxxxxxxxxxxxxxr@univ-paris3.fr;xxxxxxxxxxxxxxxxxxxipn.univ-paris13.fr,,,,,,,,on,"Yes, include my submission even if the paper is rejected.",No,None,None
564,564X-B8G3F9B5C5,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Chxxx Hoxxxx and Qxx Lxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Monday,,Undecided (Machine Translation),"We present Grid Beam Search (GBS), an algorithm which extends beam search
to allow the inclusion of pre-specified lexical constraints. The algorithm can
be used with any model which generates sequences token by token. Lexical
constraints take the form of phrases or words that must be present in the
output sequence. This is a very general way to incorporate auxillary knowledge
into a model's output without requiring any modification of the parameters or
training data. We demonstrate the feasibility and flexibility of
Lexically Constrained Decoding by conducting experiments on Neural
Interactive-Predictive Translation, as well as Domain Adaptation for Neural
Machine Translation. Experiments show that GBS can provide large improvements
in translation quality in interactive scenarios, and that, even without any
user input, GBS can be used to achieve significant gains in performance in
domain adaptation scenarios.",23 Apr 2017 02:09:15 GMT,Theoretical,Machine translation,,Chxxx,Hoxxxx,xxxxxxxxxxxp@gmail.com,Dublin City University - CNGL,No,Qxx,Lxx,xxxxxxx@dcu.ie,Dublin City University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chxxx,Hoxxxx,Aylien Ltd.,,,+353 xxxxxxxxxxx,,,xxxxxxxxxxxp@gmail.com,,,,,Ireland,"I am a research scientist at Aylien Ltd. in Dublin, Ireland, specializing in machine translation.",Chxxx Hoxxxx;Qxx Lxx,xxxxxxxxxxxp@gmail.com;xxxxxxxu@dcu.ie,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,12,Chris Hokamp,,"ADAPT Centre
School of Computing
Dublin City University
Glasnevin, Dublin 9
Ireland",on,on,Only include my submission if it is accepted.,No,None,None
565,565X-G4H5E7J5G3,Semantic Word Clusters Using Signed Spectral Clustering,Joxx Sexxx;Jexx Galxxxx;Dexx Foxxxx and Lyxx Unxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Tuesday,,Undecided (Semantics),"Vector space representations of words capture many aspects of word similarity,
but such methods tend to produce vector spaces in which antonyms (as well as
synonyms) are close to each other. For spectral clustering using such word
embeddings, words are points in a vector space where synonyms are linked with
positive weights, while antonyms are linked with negative weights. We present a
new signed spectral normalized graph cut algorithm, {\em signed clustering},
that overlays existing thesauri upon distributionally derived vector
representations of words, so that antonym relationships between word pairs are
represented by negative weights. Our signed clustering algorithm produces
clusters of words that simultaneously capture distributional and synonym
relations.  
By using randomized spectral decomposition (Halko et al., 2011) and sparse
matrices, our method is both fast and scalable. We validate our clusters using
datasets containing human judgments of word pair similarities and show the
benefit of using our word clusters for sentiment prediction.",30 Apr 2017 22:47:35 GMT,Theoretical,Semantics,,Joxx,Sexxx,xxxxxxxxxupenn.edu,University of Pennsylviania,No,Jexx,Galxxxx,xxxxxxxxxupenn.edu,University of Pennsylvania,No,Dexx,Foxxxx,xxxxxxxster.net,University of Pennsylvania,No,Lyxx,Unxxx,xxxxxxxxx.upenn.edu,University of Pennsylvania,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Joxx,Sexxx,University of Pennsylviania,,,,,,xxxxxxxxxupenn.edu,,Philadelphia,PA,,United States,,Joxx Sexxx;Jexx Galxxxx;Dexx Foxxxx;Lyxx Unxxx,xxxxxxxxxupenn.edu;xxxxxxxxx.upenn.edu;xxxxxxxxster.net;xxxxxxxxxx.upenn.edu,Semantic Word Clusters Using Signed Spectral Clustering,Semantic Word Clusters Using Signed Spectral Clustering,11,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
567,567X-P3H6G3D7E4,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,Jixxx Mx;Suxx Bhxx and Prxxxx Visxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Real-valued word representations have transformed NLP applications; popular
examples are word2vec and GloVe, recognized for their ability to capture
linguistic regularities. In this paper, we demonstrate a very simple, and yet
counterintuitive, postprocessing technique – eliminate the common mean vector
and a few top dominating directions from the word vectors – that renders
off-the-shelf representations even stronger. The postprocessing is empirically
validated on a variety of lexical-level intrinsic tasks (word similarity,
concept categorization, word analogy) and sentence-level extrinsic tasks
(semantic textual similarity) on multiple datasets and with a variety of
representation methods and hyperparameter choices in multiple languages; in
each case, the processed representations are consistently better than the
original ones. Furthermore, we demonstrate quantitatively in downstream
applications that neural network architectures ``automatically learn” the
postprocessing operation.",7 Feb 2017 01:50:07 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  multimodal representations and processing;  theoretical aspects of machine learning,Jixxx,Mx,xxxxxxxxxxllinois.edu,University of Illinois at Urbana Champaign,No,Suxx,Bhxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana Champaign,No,Prxxxx,Visxxxxxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana Champaign,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixxx,Mx,University of Illinois at Urbana Champaign,,,217xxxxxxx,,,xxxxxxxxxxllinois.edu,,URBANA,IL,,United States,,Jixxx Mx;Suxx Bhxx;Prxxxx Visxxxxxx,xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
568,568X-P3D5A3G3A9,Detecting annotation noise in automatically labelled data,Inxx Rehxxxx and Joxxx Ruppxxxxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Accept - Oral Wednesday,,Undecided (Resources Evaluation),"We introduce a method for error detection in automatically annotated text,
aimed at supporting the creation of high-quality language resources at
affordable cost. Our method combines an unsupervised generative model with
human supervision from active learning. We test our approach on in-domain and
out-of-domain data in two languages, in AL simulations and in a real world
setting. For all settings, the results show that our method is able to detect
annotation errors with high precision and high recall.",21 Apr 2017 22:12:07 GMT,Resources/Evaluation,Resources and evaluation,,Inxx,Rehxxxx,xxxxxxxxxxxxxx-heidelberg.de,Leibniz ScienceCampus,No,Joxxx,Ruppxxxxxxx,xxxxxxxxxxxxxds-mannheim.de,Institute for German Language,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Inxx,Rehxxxx,Leibniz ScienceCampus,,,,,,xxxxxxxxxxxxxx-heidelberg.de,,,,,Germany,,Inxx Rehxxxx;Joxxx Ruppxxxxxxx,xxxxxxxxxxxxxx-heidelberg.de;xxxxxxxxxxxxxxds-mannheim.de,Detecting annotation noise in automatically labelled data,Detecting annotation noise in automatically labelled data,11,Ines Rehbein,,"Leibniz ScienceCampus, IDS Mannheim/University of Heidelberg, Germany",on,,No. Do not include my submission in this dataset.,No,None,None
569,569X-P2A6J9P5E2,An Evolutionary Approach for Automatic Summarization,Aurxxxxxx Bosxxxx and Chrxxxxxxx Rodxxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"This paper proposes a novel method for automatic summarization based on a
genetic algorithm. The algorithm explores candidate summaries space following
an objective function computed over ngrams probability distributions of the
candidate summary and the source documents. This method does not consider a
summary as a stack of independent sentences but as a whole text, and makes use
of advances in unsupervised summarization evaluation. We compare this method to
one of the best existing methods which is based on integer linear programming,
and show its efficiency on three different acknowledged corpora.",7 Feb 2017 02:02:25 GMT,Empirical/Data-Driven,Summarization,distributional similarity;  document summarization;  multi-document summarization,Aurxxxxxx,Bosxxxx,xxxxxxxxxxxxxard@gmail.com,Laboratoire d'Informatique Avancée de Saint-Denis - Université Paris 8,No,Chrxxxxxxx,Rodxxxxxx,xxxxxxxxxxxxxxxxxxes.bento@gmail.com,ESILV,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Aurxxxxxx,Bosxxxx,Laboratoire d'Informatique Avancée de Saint-Denis - Université Paris 8,,,,,,xxxxxxxxxxxxxard@gmail.com,,,,,France,,Aurxxxxxx Bosxxxx;Chrxxxxxxx Rodxxxxxx,xxxxxxxxxxxxxard@gmail.com;xxxxxxxxxxxxxxxxxxues.bento@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
570,570X-C3J6C5G2D5,Fact Checking in Community Forums,Tsvxxxxxxx Mihxxxxxx;Prexxxx Naxxx;Llxxxx Màxxxxx;Albxxxx Barr�xxxxxxxxxx;Mixxx Mohxxxxxx;Gexxxx Karxxxxx and Jaxxx Glxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Community question answering forums allow users to fulfil their information
needs by asking questions to other forum users. Nevertheless, the received
answers are commonly doubtful or lack any support. In this paper, we propose a
simple 
but feature-rich classification model to assess the veracity of the answers in
a Web forum. On top of a recent shared task, we annotate a corpus that assesses
the veracity of direct answers to a given question and propose a model to
automatically estimate their veracity, on the basis of both internal (e.g.,
user profiles, language bias, and writing style) and external (e.g.,
authoritative websites) information sources. Our experiments show promising
results, with a clear impact of features looking for support information in
external and high-quality sources, and features modeling the linguistic
characteristics of the answers.",7 Feb 2017 10:40:27 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",question answering in restricted domains,Tsvxxxxxxx,Mihxxxxxx,xxxxxxxxxxxxxxxylova@gmail.com,Sofia University,No,Prexxxx,Naxxx,xxxxxxxxxxxov@gmail.com,"Qatar Computing Research Institute, HBKU",No,Llxxxx,Màxxxxx,xxxxxxxxxi.upc.edu,Qatar Computing Research Institute,No,Albxxxx,Barr�xxxxxxxxxx,xxxxxxxxxgmail.com,Qatar Computing Research Institute,No,Mixxx,Mohxxxxxx,xxxxxxxxxil.mit.edu,MIT Computer Science and Artificial Intelligence Lab,No,Gexxxx,Karxxxxx,xxxxxxxxxxxv@gmail.com,Sofia University,No,Jaxxx,Glxxx,xxxxxxmit.edu,Massachusetts Institute of Technology,No,,,,,,,,,,,,,,,,,Prexxxx,Naxxx,"Qatar Computing Research Institute, HBKU",,,+974xxxxxxxxx,,,xxxxxxxxxxxov@gmail.com,,Doha,,,Qatar,"Dr. Preslav Nakov is a Senior Scientist at the Qatar Computing Research Institute, HBKU. His research interests include computational linguistics and natural language processing (for English, Arabic and other languages), question answering, fact-checking, machine translation, sentiment analysis, lexical semantics, Web as a corpus, and biomedical text processing.

Preslav Nakov received a PhD degree in Computer Science from the University of California at Berkeley (supported by a Fulbright grant and a UC Berkeley fellowship), and an MSc degree from the Sofia University. He was a Research Fellow at the National University of Singapore (2008-2011), an honorary lecturer in the Sofia University (2008), research staff at the Bulgarian Academy of Sciences (2008), and a visiting researcher at the University of Southern California, Information Sciences Institute (2005).

Preslav Nakov co-authored a Morgan & Claypool book on Semantic Relations between Nominals, two books on computer algorithms, and many research papers in top-tier conferences and journals. He received the Young Researcher Award at RANLP'2011. He was also the first to receive the Bulgarian President's John Atanasoff award, named after the inventor of the first automatic electronic digital computer.

Preslav Nakov is Secretary of ACL SIGLEX, the Special Interest Group (SIG) on the Lexicon of the Association for Computational Linguistics (ACL). He is also Secretary of SIGSLAV, the ACL SIG on Slavic Natural Language Processing. He is an Action Editor of the Transactions of the Association for Computational Linguistics (TACL) journal, a Member of the Editorial Board of the Journal of Natural Language Engineering, an Associate Editor of the AI Communications journal, and Editorial Board member of the Language Science Press Book Series on Phraseology and Multiword Expressions. He served on the program committees of the major conferences and workshops in Computational Linguistics, including as a co-organizer and as an area/publication/tutorial/shared task chair, Senior PC member, student faculty advisor, etc.; he co-chaired SemEval 2014-2016 and was an area co-chair of ACL, EMNLP, NAACL-HLT, and *SEM, a Senior PC member of IJCAI, and a shared task co-chair of IJCNLP.",Tsvxxxxxxx Mihxxxxxx;Prexxxx Naxxx;Llxxxx Màxxxxx;Albxxxx Barr�xxxxxxxxxx;Mixxx Mohxxxxxx;Gexxxx Karxxxxx;Jaxxx Glxxx,xxxxxxxxxxxxxxxylova@gmail.com;xxxxxxxxxxxxov@gmail.com;xxxxxxxxxsi.upc.edu;xxxxxxxxx@gmail.com;xxxxxxxxxxil.mit.edu;xxxxxxxxxxxov@gmail.com;xxxxxxxmit.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
571,571X-B6B8P4H4C3,Hierarchical Rare Words Clustering for Neural Machine Translation,Honxxxxx Zhxxx;Shxxxx Lxx;Mx Lx;Mixx Zhxx and Muxxx Yxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Rare words are usually replaced with a <unk> in the current encoder-decoder
network of neural machine translation (NMT), and a post process is used for
translation replacement after decoding.  This method may confuse NMT in
choosing wrong target words and the context independent post process may
generate poor translations.
In this paper, we propose to build a semantic representation for rare words
through a hierarchical clustering method to group rare words together, and
integrate it into the encoder-decoder framework. Not only the hierarchical
structure can provide well trained cluster information in the source side, but
also the context information can be used for translation disambiguation. The
introduced clustering embedding can also alleviate the data sparseness, which
is especially important for rare words. Our experiments on Chinese-to-English
translation tasks confirm a significant improvement in the translation quality
brought by the proposed method.",7 Feb 2017 11:59:55 GMT,Empirical/Data-Driven,Machine translation,phrase-based SMT,Honxxxxx,Zhxxx,xxxxxxxxx@gmail.com,Harbin Institute of Technology,No,Shxxxx,Lxx,xxxxxxxxxxcrosoft.com,"Microsoft Research Asia, Beijing, China",No,Mx,Lx,xxxxxxxxxosoft.com,Microsoft Research Asia,No,Mixx,Zhxx,xxxxxxxxxxxcrosoft.com,microsoft research asia,No,Muxxx,Yaxx,xxxxxxxxx@gmail.com,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Honxxxxx,Zhxxx,Harbin Institute of Technology,,,,,,xxxxxxxxx@gmail.com,,,,,China,,Honxxxxx Zhxxx;Shxxxx Lxx;Mx Lx;Mixx Zhxx;Muxxx Yaxx,xxxxxxxxx@gmail.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxx@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
573,573X-J3H2E9H8H4,Learning Word-Like Units from Joint Audio-Visual Analysis,Daxxx Harxxxx and Jaxxx Glxxx,Speech,Chxxxx Hoxx;Chixxxxxx Lxx,Accept - Oral Monday,,Accept - Oral (Speech),"Given a collection of images and spoken audio captions, we present a method for
discovering word-like acoustic units in the continuous speech signal and
grounding them to semantically relevant image regions. For example, our model
is able to detect spoken instances of the word 'lighthouse' within an utterance
and associate them with image regions containing lighthouses. We do not use any
form of conventional automatic speech recognition, nor do we use any text
transcriptions or conventional linguistic annotations. Our model effectively
implements a form of spoken language acquisition, in which the computer learns
not only to recognize word categories by sound, but also to enrich the words it
learns with semantics by grounding them in images.",30 May 2017 11:57:01 GMT,Empirical/Data-Driven,Speech,,Daxxx,Harxxxx,xxxxxxxxxxxail.mit.edu,Massachusetts Institute of Technology,No,Jaxxx,Glxxx,xxxxxxmit.edu,Massachusetts Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxx,Harxxxx,Massachusetts Institute of Technology,,,,,,xxxxxxxxxxxail.mit.edu,,,,,United States,,Daxxx Harxxxx;Jaxxx Glxxx,xxxxxxxxxxxail.mit.edu;xxxxxxxmit.edu,Learning Word-Like Units from Joint Audio-Visual Analysis,Learning Word-Like Units from Joint Audio-Visual Analysis,12,David Harwath,Graduate Student,"Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, MA 02139",,,No. Do not include my submission in this dataset.,No,None,None
574,574X-J4H4E2J9B6,Exploring Implicit Feedback for Conversation Generation,Weixxxx Zhxxx;Tixx Lxx;Donxxxx Cxx and Linxxxx L,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"User feedback is an effective indicator to the success of the human-computer
conversation. In the process of online realtime conversations, users' responses
usually contain their implicit feedback, such as opinion, sentiment, emotion,
etc., towards the conversation content or the interlocutors. Therefore,
exploring the implicit feedback to optimize the conversation generation model
are quite crucial. In this paper, we propose a novel reward function which
explores the implicit feedback to optimize the future reward in conversation
generation. A simulation strategy is applied to explore the state-action space
in training and test. Experimental results show that the proposed approach
outperforms the Seq2Seq model and the state-of-the-art reinforcement learning
model for conversation generation on automatic and human evaluations over two
datasets.",7 Feb 2017 10:38:06 GMT,Empirical/Data-Driven,Dialog and interactive systems,language generation;  dialogue;  reinforcement learning,Weixxxx,Zhxxx,xxxxxxxxxx.hit.edu.cn,Harbin Institute of Technology,No,Tixx,Lxx,xxxxxxxxxit.edu.cn,Harbin Institute of Technology,No,Donxxxx,Cxx,xxxxxxxxxhit.edu.cn,Harbin Institute of Technology,No,Linxxxx,Lx,xxxxxxxxxit.edu.cn,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Weixxxx,Zhxxx,Harbin Institute of Technology,,,,,,xxxxxxxxxx.hit.edu.cn,,,,,China,,Weixxxx Zhxxx;Tixx Lxx;Donxxxx Cxx;Linxxxx Lx,xxxxxxxxxx.hit.edu.cn;xxxxxxxxxhit.edu.cn;xxxxxxxxxxhit.edu.cn;xxxxxxxxxhit.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
576,576X-H2H8D9P2J9,Predicting Success in Goal-Driven Human-Human Dialogues,Micxxxx Nosxxxxxxx;Jaxxxx Cxx and Joxxxx Pixxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"In goal-driven dialogue systems, success is typically defined based on a
structured definition of the goal. However, in human-human dialogues, such as
those found on online help forums, the goal and its attainment can be
infeasible to define. In this paper, we consider the task of automatically
predicting success in goal-driven human-human dialogues. Success is determined
by whether one participant is able to complete some task or gain some
information from the exchange. We build a dataset from \emph{stackoverflow.com}
which consists of exchanges between two users in which one is trying to solve a
technical problem. This dataset can be used to work in a large, open-ended
domain with success labels available. We further propose a turn-based
hierarchical neural network model that can be used to predict success without
requiring a structured goal definition. We show this model outperforms
rule-based heuristics and other baselines as it is able to detect patterns over
the course of a dialogue and capture notions such as gratitude.",7 Feb 2017 02:30:48 GMT,Resources/Evaluation,Dialog and interactive systems,corpus development;  NLP on noisy unstructured text;  dialogue;  evaluation methods for dialogues,Micxxxx,Nosxxxxxxx,xxxxxxxxxxxxxxxxhy@mail.mcgill.ca,McGill University,No,Jackixxxxxxxxx,Chxxxx,xxxxxxxxxx.mcgill.ca,McGill University,No,Joxxxx,Pixxxx,xxxxxxxxxx.mcgill.ca,McGill University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Micxxxx,Nosxxxxxxx,McGill University,,,,,,xxxxxxxxxxxxxxxxhy@mail.mcgill.ca,,Montreal,QC,,Canada,,Micxxxx Nosxxxxxxx;Jaxxxx Cxx;Joxxxx Pixxxx,xxxxxxxxxxxxxxxxhy@mail.mcgill.ca;xxxxxxxxxxs.mcgill.ca;xxxxxxxxxxs.mcgill.ca,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
577,577X-G3B6C4B8F8,Cross-domain Semantic Parsing via Paraphrasing,Yx Sx and Xixxxx Yxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"We propose the cross-domain semantic parsing problem: Leveraging training data
from a source domain to train semantic parsers for a target domain.  Due to the
diversity of logical forms in different domains, this problem presents unique
and intriguing challenges. We propose to decompose cross-domain semantic
parsing into two steps: (1) mapping natural language to canonical natural
language, which is essentially paraphrasing; and (2) mapping canonical natural
language and logic form, which can be done deterministically. As the first step
is shared by different domains, it becomes feasible to carry over semantic
parsing knowledge across domains. We introduce pre-trained word embeddings to
combat the vocabulary variety across domains, and show the advantage of
standardization before using them in neural networks. With cross-domain
training and standardized pre-trained word embeddings, a vanilla
sequence-to-sequence model is able to achieve a new state-of-the-art accuracy
on the Overnight dataset, outperforming a rather advanced model.",7 Feb 2017 11:14:18 GMT,Empirical/Data-Driven,Semantics,natural language interfaces to databases;  domain adaptation;  scalability and portability of question answering systems;  semantic knowledge induction;  question answering in restricted domains,Yx,Sx,xxxxxxxucsb.edu,University of California Santa Barbara,No,Xixxxx,Yxx,xxxxxxxxucsb.edu,University of California Santa Barbara,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yx,Sx,The Ohio State University,,,,,,xxxxxxxosu.edu,,,,,United States,Assistant Professor of Computer Science & Engineering at The Ohio State University. Got PhD from UC Santa Barbara and bachelor degree from Tsinghua University.,Yx Sx;Xixxxx Yxx,xxxxxxxucsb.edu;xxxxxxxx.ucsb.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
578,578X-D7G3G5C3C3,Robust Incremental Neural Semantic Graph Parsing,Jxx Buxx and Phxx Bluxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Wednesday,,Undecided (Tagging Chunking Syntax Parsing),"Parsing sentences to linguistically-expressive semantic representations is a
key goal of Natural Language Processing. Yet statistical parsing has focussed
almost exclusively on bilexical dependencies or domain-specific logical forms.
We propose a neural encoder-decoder transition-based parser which is the first
full-coverage semantic graph parser for Minimal Recursion Semantics (MRS).
The model architecture uses stack-based embedding features, predicting graphs
jointly with unlexicalized predicates and their token alignments. Our parser
is more accurate than attention-based baselines on MRS, and on an additional
Abstract Meaning Representation (AMR) benchmark, and GPU batch processing
makes it an order of magnitude faster than a high-precision grammar-based
parser. Further, the 86.69% Smatch score of our MRS parser is higher than the
upper-bound on AMR parsing, making MRS an attractive choice as a semantic
representation.",24 Apr 2017 14:40:18 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Jxx,Buxx,xxxxxxxxxxs.ox.ac.uk,"Department of Computer Science, University of Oxford",No,Phxx,Bluxxxx,xxxxxxxxxxxx@cs.ox.ac.uk,University of Oxford,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jxx,Buxx,University of Washington,,,,,,xxxxxxxxxxxshington.edu,,,,,United States,Postdoctoral researcher at the University of Washington. Completed PhD at the University of Oxford.,Jxx Buxx;Phxx Bluxxxx,xxxxxxxxxxs.ox.ac.uk;xxxxxxxxxxxxm@cs.ox.ac.uk,Robust Incremental Neural Semantic Graph Parsing,Robust Incremental Neural Semantic Graph Parsing,12,Jan Buys,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
579,579X-D6A6P6A4C6,MinIE: Minimizing Facts in Open Information Extraction,Kixxx Gashxxxxxxx and Raxxxx Gemxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"The goal of open information extraction (OIE) is to extract surface relations
and their arguments from natural language text in an unsupervised,
domain-independent manner. In this paper, we explore how overly-specific
extractions can be reduced in OIE systems without producing uninformative or
inaccurate results. We propose MinIE, an OIE system that produces minimized,
annotated extractions. At its heart, MinIE rewrites OIE extractions by (1)
identifying and removing parts that are considered overly specific; (2)
representing information about polarity, modality, attribution, and quantities
with suitable annotations instead of in the actual extraction. We conducted an
experimental study with several real-world datasets and found that MinIE
achieves competitive or higher precision and recall than most prior systems,
while at the same time producing much shorter extractions.",7 Feb 2017 03:26:22 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  information extraction;  NLP on noisy unstructured text;  relation discovery;  relation/event extraction,Kixxx,Gashxxxxxxx,xxxxxxxxxxxxxxuni-mannheim.de,University of Mannheim,No,Raxxxx,Gemxxxx,xxxxxxxxxxxx-mannheim.de,University of Mannheim,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kixxx,Gashxxxxxxx,University of Mannheim,,,,,,xxxxxxxxxxxxxxuni-mannheim.de,,,,,Germany,,Kixxx Gashxxxxxxx;Raxxxx Gemxxxx,xxxxxxxxxxxxxxuni-mannheim.de;xxxxxxxxxxxxi-mannheim.de,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
580,580X-D2P5D5H9B8,"Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain",Luxxx Buxxx;Stexxxx Clxxx and Ekaxxxxxx Shxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Research in computational semantics is increasingly guided by our understanding
of human semantic processing. However, semantic models are typically studied in
the context of natural language processing system performance. In this paper,
we present a systematic evaluation and comparison of a range of widely-used,
state-of-the-art semantic models in their ability to predict patterns of
conceptual representation in the human brain. Our results provide new insights
both for the design of computational semantic models and for further research
in cognitive neuroscience.",7 Feb 2017 03:21:34 GMT,Empirical/Data-Driven,Semantics,lexical semantics,Luxxx,Buxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Stexxxx,Clxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Ekaxxxxxx,Shuxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Luxxx,Buxxx,University of Cambridge,,,-,,,xxxxxxxam.ac.uk,,,,,United Kingdom,,Luxxx Buxxx;Stexxxx Clxxx;Ekaxxxxxx Shuxxxx,xxxxxxxam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxam.ac.uk,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
581,581X-A3B6D9D4A2,Semi-Supervised Affective Meaning Lexicon Expansion Using Semantic and Distributed Word Representations,Arxxx Alhxxxxxx and Jexxx Hoxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"In this paper, we propose an extension to graph-based sentiment lexicon
induction methods by incorporating distributed and semantic word
representations in building the similarity graph to expand a three-dimensional
sentiment lexicon. We also implemented and evaluated the label propagation
using four different word representations and similarity metrics. Our
comprehensive evaluation of the four approaches was performed on a single data
set, demonstrating that all four methods can generate a significant number of
new sentiment assignments with high accuracy. The highest correlations
(tau=0.51) and the lowest error (mean absolute error < 1.1$), obtained by
combining both the semantic and the distributional features, outperformed the
distributional-based and semantic-based label-propagation models and approached
a supervised algorithm.",7 Feb 2017 02:48:52 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  lexicon development;  graph-based algorithms;  multimodal representations and processing,Arxxx,Alhxxxxxx,xxxxxxxxxxwaterloo.ca,University of waterloo,No,Jexxx,Hoxx,xxxxxxxxxxwaterloo.ca,University of Waterloo,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Arxxx,Alhxxxxxx,University of waterloo,,,226xxxxxxx,,,xxxxxxxxxxwaterloo.ca,,,,,Canada,,Arxxx Alhxxxxxx;Jexxx Hoxx,xxxxxxxxxxwaterloo.ca;xxxxxxxxxxxwaterloo.ca,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
582,582X-P6D5P6G2H3,Fusing Knowledge Graphs with Text for Knowledge Representation Learning,Jixxxx Wx;Ruoxxxx Xxx;Zhixxxx Lxx and Maoxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Textual information is considered as significant supplement to knowledge
representation learning (KRL). There are two main challenges for previous
fusion methods: (1) How to take full advantages of sequential context of
entities in plain text. (2) How to dynamically select informative sentences of
the corresponding entities. In this paper, we propose the Sequential
Text-embodied Knowledge Representation Learning to build knowledge
representations with the help of multiple sentences. Compared with other fusion
methods, our framework can utilize more sequential information of plain text
towards KRL and has less constraints on the types of text. We evaluate our
model on two tasks, including triple classification and link prediction.
Experimental results demonstrate that our framework outperforms other baselines
on both tasks, which indicates that our framework is capable of selecting
informative sentences and encoding the sequential textual information well into
knowledge representations.",7 Feb 2017 09:27:49 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  text mining;  NLP on Wikipedia and other collaboratively constructed resources,Jixxxx,Wx,xxxxxxxxxxs@gmail.com,Tsinghua University,No,Ruoxxxx,Xxx,xxxxxxxxxg@163.com,Tsinghua University,No,Zhixxxx,Lxx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixxxx,Wx,UC Santa Barbara,,,,,,xxxxxxxxxxcs.ucsb.edu,,Beijing,,,United States,,Jixxxx Wx;Ruoxxxx Xxx;Zhixxxx Lxx;Maoxxxx Sxx,xxxxxxxxxxs@gmail.com;xxxxxxxxxng@163.com;xxxxxxxxxxxghua.edu.cn;xxxxxxxxxxhua.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
583,583X-H3J6H2D2J8,A Multidimensional Lexicon for Interpersonal Stancetaking,Umaxxxxxxx Pavaxxxxxxxx;Jxx Fitzxxxxxxx;Scxxx Kiexxxxx and Jaxxx Eisxxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Oral Tuesday,,Undecided (Sentiment Analysis Opinion Mining),"The sociolinguistic construct of stancetaking describes the activities through
which discourse participants create and signal relationships to their
interlocutors, to the topic of discussion, and to the talk itself. Stancetaking
underlies a wide range of interactional phenomena, relating to formality,
politeness, affect, and subjectivity. We present a computational approach to
stancetaking, in which we build a theoretically-motivated lexicon of stance
markers, and then use multidimensional analysis to identify a set of underlying
stance dimensions. We validate these dimensions intrinscially and
extrinsically, showing that they are internally coherent, match pre-registered
hypotheses, and correlate with social phenomena.",24 Apr 2017 07:32:42 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Umaxxxxxxx,Pavaxxxxxxxx,xxxxxxxxxx@gatech.edu,Georgia Institute of Technology,No,Jxx,Fitzxxxxxxx,xxxxxxxxxxxxick@gmail.com,University of Pittsburgh,No,Scxxx,Kiexxxxx,xxxxxxxx@pitt.edu,University of Pittsburgh,No,Jaxxx,Eisxxxxxxx,xxxxxxxxmail.com,Georgia Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Umaxxxxxxx,Pavaxxxxxxxx,Georgia Institute of Technology,,,,,,xxxxxxxxxx@gatech.edu,,,,,United States,,Umaxxxxxxx Pavaxxxxxxxx;Jxx Fitzxxxxxxx;Scxxx Kiexxxxx;Jaxxx Eisxxxxxxx,xxxxxxxxxx@gatech.edu;xxxxxxxxxxxxxick@gmail.com;xxxxxxxxx@pitt.edu;xxxxxxxxgmail.com,A Multidimensional Lexicon for Interpersonal Stancetaking,A Multidimensional Lexicon for Interpersonal Stancetaking,12,,,,,on,No. Do not include my submission in this dataset.,No,None,None
584,584X-H2B6B5C3E9,Unsupervised Learning of Sentence Representations Using Convolutional Neural Networks,Zxx Gxx;Yunxxxx Px;Ricxxxx Hexxx;Chuxxxxx Lx;Xiaxxxxx Hx and Lawxxxxx Caxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"We propose a new encoder-decoder approach to learn distributed sentence
representations from unlabeled sentences. The word-to-vector representation is
used, and convolutional neural networks are employed as sentence encoders,
mapping an input sentence into a fixed-length vector. This representation is
decoded using long short-term memory recurrent neural networks, considering
several tasks, such as reconstructing the input sentence, or predicting the
future sentence. We further describe a hierarchical encoder-decoder model to
encode a sentence to predict multiple future sentences. By training our models
on a large collection of novels, we obtain a highly generic convolutional
sentence encoder that performs well in practice. Experimental results on
several benchmark datasets, and across a broad range of applications,
demonstrate the superiority of the proposed model over competing methods.",7 Feb 2017 05:06:54 GMT,Applications/Tools,Machine learning,unsupervised and semi-supervised learning,Zxx,Gxx,xxxxxxuke.edu,Duke University,No,Yunxxxx,Px,xxxxxxuke.edu,Duke University,No,Ricxxxx,Hexxx,xxxxxxxxduke.edu,Duke University,No,Chuxxxxx,Lx,xxxxxxxxxxi@duke.edu,Duke University,No,Xiaxxxxx,Hx,xxxxxxxxxxrosoft.com,Microsoft Research,No,Lawxxxxx,Caxxx,xxxxxxxduke.edu,Duke University,No,,,,,,,,,,,,,,,,,,,,,,Zxx,Gxx,Microsoft,,,919xxxxxxx,,,xxxxxxxxxxcrosoft.com,,,,,United States,,Zxx Gxx;Yunxxxx Px;Ricxxxx Hexxx;Chuxxxxx Lx;Xiaxxxxx Hx;Lawxxxxx Caxxx,xxxxxxuke.edu;xxxxxxxuke.edu;xxxxxxxx@duke.edu;xxxxxxxxxxli@duke.edu;xxxxxxxxxxcrosoft.com;xxxxxxxxduke.edu,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
585,585X-E6C6C3C2J3,Semi-Supervised QA with Generative Domain-Adaptive Nets,Zhxxxx Yaxx;Juxxxx Hx;Ruxxxx Salaxxxxxxxxx and Wilxxxx Coxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Tuesday,,Undecided (Machine Learning),"We study the problem of semi-supervised question answering----utilizing
unlabeled text to boost the performance of question answering models. We
propose a novel training framework, the \textit{Generative Domain-Adaptive
Nets}. In this framework, we train a generative model to generate questions
based on the unlabeled text, and combine model-generated questions with
human-generated questions for training question answering models. We develop
novel domain adaptation algorithms, based on reinforcement learning, to
alleviate the discrepancy between the model-generated data distribution and the
human-generated data distribution. Experiments show that our proposed framework
obtains substantial improvement from unlabeled text.",22 Apr 2017 20:26:25 GMT,Empirical/Data-Driven,Machine learning,,Zhxxxx,Yaxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Juxxxx,Hx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Ruxxxx,Salaxxxxxxxxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Wilxxxx,Coxxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zhxxxx,Yaxx,Carnegie Mellon University,,,,,,xxxxxxxxxs.cmu.edu,,,,,China,,Zhxxxx Yaxx;Juxxxx Hx;Ruxxxx Salaxxxxxxxxx;Wilxxxx Coxxx,xxxxxxxxxs.cmu.edu;xxxxxxxxxcs.cmu.edu;xxxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu,Semi-Supervised QA with Generative Domain-Adaptive Nets,Semi-Supervised QA with Generative Domain-Adaptive Nets,11,Zhilin Yang,,"Carnegie Mellon University
5000 Forbe Ave, 15213, Pittsburgh, PA",on,on,No. Do not include my submission in this dataset.,No,None,None
586,586X-E7H2C7D6G3,TSP: Learning Task-Specific Pivots for Unsupervised Domain Adaptation,Xxx Cxx;Danxxxxx Bolxxxxxx and Frxxx Coxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Unsupervised Domain Adaptation (UDA) considers the problem of adapting a
classifier trained using labelled training instances
 from a source domain to a different target domain, without having access to
any labelled training instances from the target domain.
 Projection-based methods where the source and target domain instances are
first projected onto a common feature space in which
 a classifier can be trained and applied have reported the state-of-the-art
results for UDA.
 However, a critical pre-processing step required in these methods is selecting
a set of common features (aka. \emph{pivots}),
 which has been done so far using heuristic approaches, ignoring the target
task.
In contrast to these prior heuristics, we propose a method for learning
Task-Specific Pivots (TSPs) in a systematic manner 
by considering both labelled and unlabelled data available from both domains.
We evaluate TSPs against pivots selected using prior proposals in two
state-of-the-art cross-domain sentiment classification methods.
Our experimental results show that the proposed TSPs significantly outperform
prior pivot selection strategies in both tasks.
Moreover, when applied in a cross-domain sentiment classification task, TSP
captures many sentiment-bearing pivots.",7 Feb 2017 10:57:34 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  domain adaptation;  text classification,Xxx,Cxx,xxxxxxxxxxxerpool.ac.uk,University of Liverpool,No,Danxxxxx,Bolxxxxxx,xxxxxxxxxxxxxxxxxa@liverpool.ac.uk,University of Liverpool,No,Frxxx,Coxxxx,xxxxxxxxxxxrpool.ac.uk,University of Liverpool,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Danxxxxx,Bolxxxxxx,University of Liverpool,,,+44-1xxxxxxxxx,,,xxxxxxxxxxxxerpool.ac.uk,,,,,United Kingdom,,Xxx Cxx;Danxxxxx Bolxxxxxx;Frxxx Coxxxx,xxxxxxxxxxxerpool.ac.uk;xxxxxxxxxxxxxxxxxla@liverpool.ac.uk;xxxxxxxxxxxerpool.ac.uk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
588,588X-G5H5F3C6H2,Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs,Texx Loxx;Emmxxxxx Bexxxx;Ryxx Loxx;Jaxxxx Chxxxx and Doxxx Prxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Reading comprehension in NLP refers to the ability of models to answer any
question about a passage accurately. An important open problem is how to
effectively use external knowledge to answer such questions. In this paper, we
introduce a new task and derive new models to drive progress towards this goal.
In particular, we propose the task of rare entity prediction: given a web
document with several entities removed, models are tasked with predicting the
correct missing entities conditioned on the document context and the lexical
resources. This task is challenging due to the diversity of language styles and
the extremely large number of rare entities. Our experiments show that models
that make use of external knowledge in the form of lexical resources,
particularly our model using hierarchical LSTMs, perform significantly better
at rare entity prediction than those that do not.",7 Feb 2017 07:55:43 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  NLP on Wikipedia and other collaboratively constructed resources;  semantic knowledge induction,Texx,Loxx,xxxxxxxxxxxxil.mcgill.ca,McGill University,No,Emmxxxxx,Bexxxx,xxxxxxxxxxxxxxx@mail.mcgill.ca,McGill University,No,Ryxx,Loxx,xxxxxxxxxxxxil.mcgill.ca,McGill University,No,Jaxxxx,Chxxxx,xxxxxxxxxx.mcgill.ca,McGill University,No,Doxxx,Prxxxx,xxxxxxxxxx.mcgill.ca,McGill University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Texx,Loxx,McGill University,,,(778)xxxxxxxxxxx,,,xxxxxxxxxxxxil.mcgill.ca,,Montreal,QC,,Canada,,Texx Loxx;Emmxxxxx Bexxxx;Ryxx Loxx;Jaxxxx Chxxxx;Doxxx Prxxxx,xxxxxxxxxxxxil.mcgill.ca;xxxxxxxxxxxxxxxo@mail.mcgill.ca;xxxxxxxxxxxxail.mcgill.ca;xxxxxxxxxxs.mcgill.ca;xxxxxxxxxxs.mcgill.ca,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
590,590X-C9E4G5J2A4,Multi-Task Video Captioning with Video and Entailment Generation,Ramxxxxxx Pasxxxxx and Moxxx Baxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Accept - Oral Wednesday,,Undecided (Vision Robots Grounding),"Video captioning, the task of describing the content of a video, has seen some
promising improvements in recent years with sequence-to-sequence models, but
accurately learning the temporal and logical dynamics involved in the task
still remains a challenge, especially given the lack of sufficient annotated
data. We improve video captioning by sharing knowledge with two related
directed-generation tasks: a temporally-directed unsupervised video prediction
task to learn richer context-aware video encoder representations, and a
logically-directed language entailment generation task to learn better
video-entailing caption decoder representations. For this, we present a
many-to-many multi-task learning model that shares parameters across the
encoders and decoders of the three tasks. We achieve significant improvements
and the new state-of-the-art on several standard video captioning datasets
using diverse automatic and human evaluations. We also show mutual multi-task
improvements on the entailment generation task.",25 Apr 2017 07:40:07 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",,Ramxxxxxx,Pasxxxxx,xxxxxxxunc.edu,UNC Chapel Hill,No,Moxxx,Baxxxx,xxxxxxxxxs.unc.edu,University of North Carolina at Chapel Hill,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Moxxx,Baxxxx,University of North Carolina at Chapel Hill,,,,,,xxxxxxxxxs.unc.edu,,,NC,,United States,Homepage: http://www.cs.unc.edu/~mbansal/,Ramxxxxxx Pasxxxxx;Moxxx Baxxxx,xxxxxxxunc.edu;xxxxxxxxxcs.unc.edu,Multi-Task Video Captioning with Video and Entailment Generation,Multi-Task Video Captioning with Video and Entailment Generation,11,Ramakath Pasunuru,,,on,on,No. Do not include my submission in this dataset.,No,None,None
591,591X-G9D3A8C7C5,Learning Word Representations with Regularization from Prior Knowledge,Yxx Soxx and Chixxxxxx Lxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Conventional word embeddings are trained with specific criteria (e.g., based on
language modeling or co-occurrence) inside a single information source,
disregarding the opportunity for further calibration using external knowledge.
This paper presents a unified framework that leverages pre-learned or external
priors, in the form of a regularizer, for enhancing conventional language
model-based embedding learning. We consider two types of regularizers. The
first type is derived from topic distribution by running LDA on unlabeled data.
The second type is based on dictionaries that are created with human annotation
efforts. To effectively learn with the regularizers, we propose a novel data
structure, trajectory softmax, in this paper. We evaluate the resulting
embeddings using measures that assess word similarity and sentiment
classification. Experimental results show that our learning framework with
regularization from prior knowledge improves embedding quality across multiple
datasets, compared to a diverse collection of baseline methods.",7 Feb 2017 09:14:50 GMT,Empirical/Data-Driven,Semantics,unsupervised and semi-supervised learning;  lexical semantics,Yxx,Soxx,xxxxxxxxgmail.com,Microsoft,No,Chixxxxxx,Lxx,xxxxxxxxxrosoft.com,Microsoft,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yxx,Soxx,Tencent AI Lab,,,,,,xxxxxxxxxgmail.com,,,WA,,United States,,Yxx Soxx;Chixxxxxx Lxx,xxxxxxxxgmail.com;xxxxxxxxxxrosoft.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
592,592X-B6H4P3E5F6,Adversarial Adaptation of Synthetic or Stale Data,Youxxxxxx Kxx;Kaxx Strxxxx and Donxxxxx Kx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Poster Monday,,Undecided (Dialog Interactive Systems),"Two types of data shift common in practice are 1. transferring from synthetic
data to live user data (a deployment shift), and
2. transferring from stale data to current data (a temporal shift). Both cause
a distribution mismatch between training and evaluation, leading to a model
that overfits the flawed training data and performs poorly on the test data. We
propose a solution to this mismatch problem by framing it as domain adaptation,
treating the flawed training dataset as a source domain and
the evaluation dataset as a target domain. To this end, we use and build on
several recent advances in neural domain adaptation such as adversarial
training (Ganinet al., 2016) and domain separation network (Bousmalis et al.,
2016), proposing a new effective adversarial training scheme. In both
supervised and unsupervised adaptation scenarios, our approach yields clear
improvement over strong baselines.",26 Apr 2017 01:35:28 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Youxxxxxx,Kxx,xxxxxxxxx@gmail.com,Microsoft,No,Kaxx,Strxxxx,xxxxxxxxxratos.com,Toyota Technological Institute at Chicago,No,Donxxxxx,Kxx,xxxxxxxxxxxxxmicrosoft.com,Microsoft AI and Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Youxxxxxx,Kxx,Amazon Alexa Brain,,,608xxxxxxx,,,xxxxxxxxx@gmail.com,,Redmond,WA,,United States,I currently lead the Scientist team @ Amazon Alexa Brain .,Youxxxxxx Kxx;Kaxx Strxxxx;Donxxxxx Kxx and  Rexxxxxxx,xxxxxxxxx@gmail.com;xxxxxxxxxtratos.com;xxxxxxxxxxxxx@microsoft.com,Adversarial Adaptation of Synthetic or Stale Data,Adversarial Adaptation of Synthetic or Stale Data,11,Young-Bum Kim,,,,,No. Do not include my submission in this dataset.,No,None,None
594,594X-H5B8E6P6P6,Recognizing Emotions in Tweets: An Iterative Framework of Multi-view Classifiers,Ashxxxx Qaxxx;Kaxxx Lxx;Vixxx Daxxx;Saxxx Ax;Aadxxxx Praxxxx;Yuxx Lixx;Joxx Lxx and Olaxxxxxx Faxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Social media such as Twitter offers a rich
source of data for analyzing the emotions
expressed in publicly available comments,
posts and other narratives. Traditional
supervised classification methods require
emotion-labeled training data but do not
exploit unlabeled data which are more accessible.
In this work, we present a novel
semi-supervised classification framework
of multi-view classifiers. Our method targets
the weakest of the classifiers with any
individual view and aims to iteratively improve
the classifier with guidance from
a classifier with complementary views of
the feature space. The experimental results
demonstrate that our classifier with
the combined feature set outperforms several
baseline text and emotion classification
methods from related work, while the
semi-supervised classification framework
further improves emotion classification results
by up to +3% F1-score.",7 Feb 2017 07:19:26 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  NLP in social networking media;  text classification;  social network,Ashxxxx,Qaxxx,xxxxxxxx.utah.edu,Philips Research North America,No,Kaxxx,Lxx,xxxxxxxxxxx@philips.com,Philips Research North America,No,Vixxx,Daxxx,xxxxxxxxxxx@philips.com,Philips Research North America,No,Sadxxxxx,Haxxx,xxxxxxxxxxx@philips.com,Philips Research North America,No,Aadxxxx,Praxxxx,xxxxxxxxxxxxxsh@philips.com,Philips Research North America,No,Yuxx,Lixx,xxxxxxxxxxphilips.com,Philips Research North America,No,Joxx,Lxx,xxxxxxxxxxhilips.com,Philips Research North America,No,Olaxxxxxx,Faxxx,xxxxxxxxxxxx@philips.com,Philips Research NA,No,,,,,,,,,,,,Ashxxxx,Qaxxx,Philips Research North America,,,+1-80xxxxxxxxxx,,,xxxxxxxx.utah.edu,,,MA,,United States,,Ashxxxx Qaxxx;Kaxxx Lxx;Vixxx Daxxx;Saxxx Ax;Aadxxxx Praxxxx;Yuxx Lixx;Joxx Lxx;Olaxxxxxx Faxxx,xxxxxxxx.utah.edu;xxxxxxxxxxxx@philips.com;xxxxxxxxxxxx@philips.com;xxxxxxxxxxxx@philips.com;xxxxxxxxxxxxxxsh@philips.com;xxxxxxxxxxxphilips.com;xxxxxxxxxxphilips.com;xxxxxxxxxxxxi@philips.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
595,595X-D8B3F6H3P2,Aggregating and Predicting Sequence Labels from Crowd Annotations,Ax Thxxx;Byxxx Walxxxx;Juxxx Jexxx;Axx Nenxxxx and Matxxxx Lexx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Monday,,Undecided (Tagging Chunking Syntax Parsing),"Despite sequences being core to NLP, scant work has considered how to handle
noisy sequence labels from multiple annotators for the same text. Given such
annotations, we consider two complementary tasks:  (1) aggregating sequential
crowd labels to infer a best single set of consensus annotations; and (2) using
crowd annotations as training data for a model that can predict sequences in
unannotated text. For aggregation, we propose a novel Hidden Markov Model
variant. To predict sequences in unannotated text, we propose a neural approach
using Long Short Term Memory. We evaluate a suite of methods across two
different applications and text genres: Named-Entity Recognition in news
articles and Information Extraction from biomedical abstracts. Results show
improvement over strong baselines. Our source code and data are available
online.",23 Apr 2017 04:01:04 GMT,Applications/Tools,"Tagging, chunking, syntax, and parsing",,An xxxxx,Ngxxxx,xxxxxxxxtexas.edu,University of Texas at Austin,No,Byxxx,Walxxxx,xxxxxxxxs.neu.edu,Northeastern University,No,Junyxxxxxxx,Lx,xxxxxxxxxxs.upenn.edu,University of Pennsylvania,No,Axx,Nenxxxx,xxxxxxxxxxxs.upenn.edu,University of Pennsylvania,No,Matxxxx,Lexxx,xxxxxxxas.edu,University of Texas at Austin,No,,,,,,,,,,,,,,,,,,,,,,,,,,,An xxxxx,Ngxxxx,University of Texas at Austin,,,,,,xxxxxxxxtexas.edu,,,,,United States,,Ax Thxxx;Byxxx Walxxxx;Juxxx Jexxx;Axx Nenxxxx;Matxxxx Lexxx,xxxxxxxxtexas.edu;xxxxxxxxxs.neu.edu;xxxxxxxxxxxs.upenn.edu;xxxxxxxxxxxas.upenn.edu;xxxxxxxxas.edu,Aggregating and Predicting Sequence Labels from Crowd Annotations,Aggregating and Predicting Sequence Labels from Crowd Annotations,11,An Thanh Nguyen,,"Department of Computer Science
University of Texas at Austin
2317 Speedway, Stop D9500
Austin, TX 78712
USA",on,,No. Do not include my submission in this dataset.,No,None,None
596,596X-E7F7A4D9J4,Deep Multitask Learning for Semantic Dependency Parsing,Hxx Pexx;Sxx Thoxxxx and Noxx Ax,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"We present a deep neural architecture that parses sentences into three semantic
dependency graph formalisms. By using efficient, nearly arc-factored inference
and a bidirectional-LSTM composed with a multi-layer perceptron,  our base
system is able to significantly improve the state of the art for semantic
dependency parsing, without using hand-engineered features or syntax. We then
explore two multitask learning approaches---one that shares parameters across
formalisms, and one that uses higher-order structures to predict the graphs
jointly. We find that both approaches improve performance across formalisms on
average, achieving a new state of the art. Our code is open-source and
available at https://github.com/Noahs-ARK/NeurboParser.",30 Apr 2017 22:56:13 GMT,Empirical/Data-Driven,Semantics,,Hxx,Pexx,xxxxxxxxxxxxshington.edu,University of Washington,No,Sxx,Thoxxxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Noaxxxx,Smxxx,xxxxxxxxxxxxashington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hxx,Pexx,University of Washington,,,+1 20xxxxxxxxxx,,,xxxxxxxxxxxxshington.edu,,Seattle,WA,,United States,,Hxx Pexx;Sxx Thoxxxx;Noxx Ax,xxxxxxxxxxxxshington.edu;xxxxxxxxxxcs.cmu.edu;xxxxxxxxxxxxxashington.edu,Deep Multitask Learning for Semantic Dependency Parsing,Deep Multitask Learning for Semantic Dependency Parsing,12,Hao Peng,,"Name: Paul G. Allen School of Computer Science & Engineering, University of Washington.
Address: Box 352350, 185 Stevens Way, Seattle WA 98195, USA",on,on,No. Do not include my submission in this dataset.,No,None,None
597,597X-D4B3C7H9F2,Domain Adaptation for MT: A Study with Unknown and Out-of-Domain Tasks,Hoxxx Cuxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Domain adaptation is a powerful technique for MT. However, translation quality
could degrade nongracefully outside the desired domain. This raises the
question whether having an ""ecosystem"" of pre-trained adapted MT systems is
useful, given that translation requests are often unknown and out-of-domain.
This paper claims that having a reasonably large-scale ecosystem is very
effective: a translation task can be out-of-scope of most pre-trained MT
systems in the ecosystem, but a few others can be capable of handling the task.
Another question we ask is how to obtain the best translation from an ecosystem
for such translation requests? We contribute two frameworks to address the
problem. The intuition behind our frameworks is that good translations tend to
be similar to the others, whereas bad translations tend to be different from
the others. Experiments show that our frameworks give the performance in the
middle between top rank MT systems in the ecosystem, without any knowledge
about translation task.",7 Feb 2017 04:31:02 GMT,Empirical/Data-Driven,Machine translation,MT applications;  domain adaptation;  phrase-based SMT;  statistical machine translation;  MT deployment,Hoxxx,Cuxxx,xxxxxxxxxxxx11@gmail.com,"ILLC, University of Amsterdam",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hoxxx,Cuxxx,City University of New York,,,,,,xxxxxxxxxxxx11@gmail.com,,,,,United States,,Hoxxx Cuxxx,xxxxxxxxxxxx11@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
598,598X-A6J4P7A7P6,Bandit Structured Prediction for Neural Sequence-to-Sequence Learning,Juxxx Krexxxxx;Arxxx Sokxxxx and Stxxxx Rixxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Monday,,Undecided (Machine Translation),"Bandit structured prediction describes a stochastic optimization framework
where learning is performed from partial feedback. This feedback is received in
the form of a task loss evaluation to a predicted output structure, without
having access to gold standard structures. We advance this framework by lifting
linear bandit learning to neural sequence-to-sequence learning problems using
attention-based recurrent neural networks. Furthermore, we show how to
incorporate control variates into our learning algorithms for variance
reduction and improved generalization. We present an evaluation on a neural
machine translation task that shows improvements of up to 5.89 BLEU points for
domain adaptation from simulated bandit feedback.",21 Apr 2017 11:14:46 GMT,Empirical/Data-Driven,Machine translation,,Juxxx,Krexxxxx,xxxxxxxxxxxxxxi-heidelberg.de,"Department of Computational Linguistics, Heidelberg University",No,Arxxx,Sokxxxx,xxxxxxxxgmail.com,Heidelberg University,No,Stxxxx,Riexxxx,xxxxxxxxxxxxxx-heidelberg.de,Heidelberg University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Juxxx,Krexxxxx,"Department of Computational Linguistics, Heidelberg University",,,,,,xxxxxxxxxxxxxxi-heidelberg.de,,Heidelberg,,,Germany,,Juxxx Krexxxxx;Arxxx Sokxxxx;Stxxxx Riexxxx,xxxxxxxxxxxxxxi-heidelberg.de;xxxxxxxxxgmail.com;xxxxxxxxxxxxxxi-heidelberg.de,Bandit Structured Prediction for Neural Sequence-to-Sequence Learning,Bandit Structured Prediction for Neural Sequence-to-Sequence Learning,11,Julia Kreutzer,,,on,on,No. Do not include my submission in this dataset.,No,None,None
599,599X-F3B8B6E2F6,Cross-lingual Named Entity Recognition using Deep Learning via Parameter Sharing,Ruxxx Muxxxx;Mixxxx Mx and Pusxxxx Bhatxxxxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Most state of the art approaches for Named Entity Recognition rely on hand
crafted features and annotated corpora. However, sufficient labeled corpora
required to train such models may not be available for many languages. Several
traditional approaches tackle the problem by inducing cross-lingual features
via parallel annotated/unlabeled corpus. Recently, Neural network based models
have been proposed which do not require handcrafted features. In this paper, we
demonstrate that by sharing specific layers of the neural network between
languages, language $L_1$ can benefit from another language $L_2$ without the
need for parallel annotated corpus. By sharing we mean the parameters/weights
of the layers are tied between the languages. Specifically, we focus on the
case when limited annotated corpora is available in one language ($L_1$) and
abundant annotated corpora is available in another language ($L_2$). Sharing
the network architecture and parameters between $L_1$ and $L_2$ leads to
improved performance in $L_1$. The improvements for $L_1$ is large when there
is higher named entity overlap with $L_2$ and when introducing $L_2$ data does
not contribute to further ambiguity.",7 Feb 2017 12:06:50 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",cross-lingual approaches;  named entity recognition;  cross-language information extraction,Ruxxx,Muxxxx,xxxxxxxxxxiitb.ac.in,Indian Institute Of Technology Bombay,No,Mitxxxxxx,Khxxxx,xxxxxxxxxxx.iitm.ac.in,Indian Institute of Technology Madras,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,"CSE Department, IIT Bombay",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ruxxx,Muxxxx,Indian Institute Of Technology Bombay,,,,,,xxxxxxxxxxiitb.ac.in,,,,,India,,Ruxxx Muxxxx;Mixxxx Mx;Pusxxxx Bhatxxxxxxxxx,xxxxxxxxxxiitb.ac.in;xxxxxxxxxxxe.iitm.ac.in;xxxxxxxxxx@gmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
600,600X-G4P6B2D9B6,Typologically Motivated Parts-of-Speech Induction for Low-resource Languages,Hirxxxx Hayxxxx;Texxxx Mitxxxxx and Edxxxx Hxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Parts-of-speech (POS) induction for low-resource languages has been
investigated in many different approaches, including semi-supervised and
unsupervised learning. The former requires resources to bridge from known
languages, and the latter can only produce uninterpretable labels. Considering
the real low-resource setting, such multi-lingual data assumption is limited,
yet the labeled outputs are still critical.

In this work, we present a novel approach that overcomes the two challenges by
requiring only typological knowledge to learn a POS tagger for low-resource
languages. It can learn interpretable labels without any other supervisions at
all. Our experiment achieved 53% accuracy on Uyghur which is even better than
common baseline semi-supervised approaches. Furthermore, we experimented on 16
major languages in the same setting and achieved similar competitive
performances.",7 Feb 2017 04:23:55 GMT,Applications/Tools,"Tagging, chunking, syntax, and parsing",unsupervised and semi-supervised learning;  part-of-speech tagging;  cross-lingual approaches;  multilingual applications;  learning with small datasets;  multilingual resources,Hirxxxx,Hayxxxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Texxxx,Mitxxxxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Edxxxx,Hoxx,xxxxxxmu.edu,CMU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hirxxxx,Hayxxxx,Carnegie Mellon University,,,,,,xxxxxxxxxcs.cmu.edu,,,PA,,United States,,Hirxxxx Hayxxxx;Texxxx Mitxxxxx;Edxxxx Hoxx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu;xxxxxxcmu.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
601,601X-H9A3F6A9P5,Inducing Event Types and Roles in Reverse: Using Function to Discover Theme,Natxxxx Axx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Rejected - Withdrawn,,Rejected - Withdrawn (IE QA Text Mining Applications),"There is growing interest in expanding the use of automated event extraction,
and in overcoming the high start-up costs associated with manually-constructed
event templates, entity taxonomies, and annotated corpora. In the last few
years, more inductive approaches have emerged, which aim to discover unknown
event types from raw text, as well as their participant roles. The main efforts
so far have applied probabilistic generative models, commonly used in topic
modeling, which are formally concise but do not always yield stable or easily
interpretable results. We argue that event schema induction can be improved by
distinguishing thematic from functional similarities in event words, and by
using more information from event role structures to inform the discovery of
thematically related text. We reverse the traditional process of inducing event
schemas and introduce a new algorithm for clustering event terms, to build an
intuitively structured process for event induction with no pre-specified
patterns or component parts.",7 Feb 2017 05:31:14 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",unsupervised and semi-supervised learning;  information extraction;  relation/event extraction;  semantic role labelling,Natxxxx,Axx,xxxxxxxxxxxberkeley.edu,University of California - Berkeley,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Natxxxx,Axx,University of California - Berkeley,,,,,,xxxxxxxxxx@gmail.com,,,,,United States,,Natxxxx Axx,xxxxxxxxxxxberkeley.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
602,602X-C3A2B7F6C3,Unsupervised Pretraining for Sequence to Sequence Learning,Prxxxx Ramaxxxxxxxx;Pexxx Lxx and Quxx L,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"This work presents a general unsupervised learning method to improve
the accuracy of sequence to sequence (seq2seq) models. In our method, the
weights of the encoder and decoder of a seq2seq model are initialized
with the pretrained weights of two language models and then 
fine-tuned with labeled data. We apply this method to
challenging benchmarks in machine translation and abstractive
summarization and find that it significantly improves the subsequent
supervised models.  Our main result is that the pretraining
accelerates training and improves generalization of seq2seq models,
achieving state-of-the-art results on the WMT
English->German task and surpassing a range of methods using
both phrase-based machine translation and neural machine
translation. Our method achieves an improvement of 1.3 BLEU from the
previous best models on both WMT'14 and WMT'15
English->German. On summarization, our method beats
the supervised learning baseline.",7 Feb 2017 03:58:15 GMT,Empirical/Data-Driven,Machine translation,unsupervised and semi-supervised learning;  domain adaptation;  learning with small datasets;  structured input/output;  example-based MT;  document summarization;  transfer-based MT;  adaptation to noisy data,Prxxxx,Ramaxxxxxxxx,xxxxxxxxxxllinois.edu,University of Illinois at Urbana-Champaign,No,Pexxx,Lxx,xxxxxxxxxxgoogle.com,Google Brain,No,Quxx,Lx,xxxxxxxgle.com,Google Brain,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Prxxxx,Ramaxxxxxxxx,University of Illinois at Urbana-Champaign,,,,,,xxxxxxxxxxllinois.edu,,,,,United States,,Prxxxx Ramaxxxxxxxx;Pexxx Lxx;Quxx Lx,xxxxxxxxxxllinois.edu;xxxxxxxxxx@google.com;xxxxxxxogle.com,,,,,,,,on,"Yes, include my submission even if the paper is rejected.",No,None,None
603,603X-F8C5E5D3F7,Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder,Huaxxxx Chxx;Shuxxxx Huxxx;Daxxx Chxxxx and Jixxxx Cxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Tuesday,,Undecided (Machine Translation),"Most neural machine translation (NMT) models are based on the sequential
encoder-decoder framework, which makes no use of syntactic information. In this
paper, we improve this model by explicitly incorporating source-side syntactic
trees. More specifically, we propose (1) a bidirectional tree
encoder which learns both sequential and tree structured representations; (2) a
tree-coverage model that lets the attention depend on the source-side syntax.
Experiments on Chinese-English translation demonstrate that our proposed models
outperform the sequential attentional model as well as a stronger baseline with
a bottom-up tree encoder and word coverage.",23 Apr 2017 04:56:59 GMT,Empirical/Data-Driven,Machine translation,,Huaxxxx,Chxx,xxxxxxxxxx.nju.edu.cn,Nanjing University,No,Shuxxxx,Huxxx,xxxxxxxxxxxn@gmail.com,Nanjing University,No,Daxxx,Chxxxx,xxxxxxx@nd.edu,University of Notre Dame,No,Jixxxx,Chxx,xxxxxxxxju.edu.cn,Nanjing University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Huaxxxx,Chxx,Nanjing University,,,1515xxxxxxx,,,xxxxxxxxxx.nju.edu.cn,,Nanjing,Jiangsu,,China,,Huaxxxx Chxx;Shuxxxx Huxxx;Daxxx Chxxxx;Jixxxx Chxx,xxxxxxxxxx.nju.edu.cn;xxxxxxxxxxxan@gmail.com;xxxxxxxg@nd.edu;xxxxxxxxxju.edu.cn,Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder,Improved NMT with a Syntax-Aware Encoder and Decoder,10,Huadong Chen,,"State Key Laboratory for Novel Software Technology, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing 210023, China",on,,No. Do not include my submission in this dataset.,No,None,None
605,605X-E3C2B6J7G3,"Joint Acquisition of Words’ Meaning, Word Order and Referential Intentions in an Incremental Bayesian Model of Cross-Situational Word Learning with Limited Access to Past Observations",Sepxxxx Sadxxxx and Matxxxxx Schxxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"Word learning in ambiguous contexts is a challenging task which is intertwined
with the process of understanding the referential intentions of the speaker. It
has been suggested that infant word learning is guided by cross-situational
word learning and bootstrapped by syntactic regularities such as word order.
Cross-situational word learning can benefit from full access to all of the
situations and their statistical regularities to arrive at the right
hypothesis. However, it is cognitively implausible for children to remember all
word learning situations they encounter. We study the interplay of the joint
acquisition of word order, words’ meanings and referential intentions of
speaker in an incremental Bayesian model of cross-situational word learning
with limited
memory of past situations. Our results show that learning the language word
order incrementally and from individual examples of language sentences in a
model with no prior knowledge of lexical categories, six possible word orders,
or meaning of words is feasible. Furthermore, the results suggest that word
learning benefits from the parallel acquisition of word order and that the
benefit is more pronounced in ambiguous contexts.",7 Feb 2017 04:30:46 GMT,Theoretical,Cognitive modeling and psycholinguistics,language acquisition;  generative models;  Bayesian learning;  adaptation to noisy data,Sepxxxx,Sadxxxx,xxxxxxxxxx@gmail.com,Tufts University,No,Matxxxxx,Schxxxx,xxxxxxxxxxxxxutz@tufts.edu,Tufts University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sepxxxx,Sadxxxx,Tufts University,,,,,,xxxxxxxxxxxxghi@tufts.edu,,,,,United States,,Sepxxxx Sadxxxx;Matxxxxx Schxxxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxxxxeutz@tufts.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
606,606X-D3F4D6H7D3,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,Chxx Lixxx;Jonxxxxx Bexxxx;Quxx Lx;Kenxxxx Dx and Nx Lx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"Harnessing the statistical power of neural networks to perform language
understanding and symbolic reasoning is difficult, when it requires executing
efficient discrete operations against a large knowledge-base. In this work, we
introduce a Neural Symbolic Machine, which contains (a) a neural ""programmer"",
i.e., a sequence-to-sequence model that maps language utterances to programs
and utilizes a key-variable memory to handle compositionality (b) a symbolic
""computer"", i.e., a Lisp interpreter that performs program execution, and helps
find good programs by pruning the search space. We apply REINFORCE to directly
optimize the task reward of this structured prediction problem. To train with
weak supervision and improve the stability of REINFORCE, we augment it with an
iterative maximum-likelihood training process. NSM outperforms the
state-of-the-art on the WebQuestionsSP dataset when trained from
question-answer pairs only, without requiring any feature engineering or
domain-specific knowledge.",23 Apr 2017 06:32:15 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Chxx,Lixxx,xxxxxxxxxxxxxxxxnorthwestern.edu,Northwestern University,No,Jonxxxxx,Bexxxx,xxxxxxxxxxs.tau.ac.il,Tel-Aviv University,No,Quxx,Lx,xxxxxxxgle.com,Google Inc.,No,Kenxxxxxxx,Foxxxx,xxxxxxxxxxxxthwestern.edu,Northwestern University,No,Nx,Lxx,xxxxxxxogle.com,Google,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Nx,Lxx,SayMosaic,,,412xxxxxxx,,,xxxxxxxxmail.com,,,CA,,United States,,Chxx Lixxx;Jonxxxxx Bexxxx;Quxx Lx;Kenxxxx Dx;Nx Lxx,xxxxxxxxxxxxxxxxnorthwestern.edu;xxxxxxxxxxxs.tau.ac.il;xxxxxxxogle.com;xxxxxxxxxxxxxthwestern.edu;xxxxxxxxogle.com,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,11,Chen Liang,,"Northwestern University, 633 Clark St, Evanston, IL 60208",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
608,608X-H7G3E9E2A6,Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory,Yexxxx Shxx;Poxxxx Huxxx;Minxxxxx Chxxx and Jiaxxxxx Gx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Recent studies on knowledge base completion, the task of recovering missing
relationships based on recorded relations, demonstrate the importance of
learning embeddings from multi-step relations. However, due to the size of
knowledge bases, learning multi-step relations directly on top of observed
instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs),
which is designed to perform large-scale inference implicitly through a search
controller and shared memory. IRNs use training data to learn to perform
multi-step inference through the shared memory, which is also jointly updated
during training. While the inference procedure is not operating on top of
observed instances for IRNs, our proposed model outperforms all previous
approaches on the popular FB15k benchmark by more than 5.7%.",7 Feb 2017 09:52:05 GMT,Empirical/Data-Driven,Machine learning,semantic relations;  semantic knowledge induction,Yexxxx,Shxx,xxxxxxxxxxrosoft.com,Microsoft Research,No,Poxxxx,Huxxx,xxxxxxxxxxllinois.edu,Microsoft Research,No,Minxxxxx,Chxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,"Microsoft Research, Redmond",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Poxxxx,Huxxx,Microsoft Research,,,,,,xxxxxxxxxxllinois.edu,,,WA,,United States,,Yexxxx Shxx;Poxxxx Huxxx;Minxxxxx Chxxx;Jiaxxxxx Gxx,xxxxxxxxxxrosoft.com;xxxxxxxxxxxllinois.edu;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxrosoft.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
609,609X-B7P8A2B5E5,Multi-task Multi-domain Representation Learning,Naxxxx Pexx and Maxx Drxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Representation learning with deep models have demonstrated success in a range
of Natural Language Processing (NLP) problems, such as sequence tagging,
sentiment analysis and relation extraction. We consider its use in a multi-task
multi-domain setting. We propose a unified framework for learning robust
representations across tasks and domains, and partially share parameters among
different tasks and domains. We apply the proposed framework to sequence
tagging and text classification problems. 
The sequence tagging setting considers two tasks: Chinese word segmentation and
named entity recognition, and two domains: news and social media, and achieve
the state-of-the-art results for both social media tasks. The text
classification setting focuses on the multi-tasks variation of our framework,
and achieve the state-of-the-art performance on business review text
classification problems.",7 Feb 2017 04:18:17 GMT,Empirical/Data-Driven,Machine learning,information extraction;  named entity recognition;  domain adaptation;  text classification;  adaptation to noisy data,Naxxxx,Pexx,xxxxxxxxxx@gmail.com,Johns Hopkins University,No,Maxx,Drxxxx,xxxxxxxxxs.jhu.edu,Johns Hopkins University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Naxxxx,Pexx,University of Southern California,,,410xxxxxxx,,,xxxxxxisi.edu,,,,,United States,,Naxxxx Pexx;Maxx Drxxxx,xxxxxxxxxx@gmail.com;xxxxxxxxxcs.jhu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
610,610X-A8J3H6J8F9,Document-level event localization in news articles,Jinxxxx Chxxx;Jinxxxx Yxx and Joxx Pxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"It is important to identify spatial information associated with events for
better understanding of such events and event relations. Previous related work
is limited to classifying intra-sentential relations between events and their
spatial arguments. In this paper, we introduce a new task, document-level event
localization where we are to recognize the cross-sentence relationship between
events and their locations, aiming specifically at identifying an expression in
news articles that best indicates where a given event occurs. Based on the
observation that coherent narratives such as news articles usually mention a
series of events that occur together in a similar location, we explore a
multi-pass approach where locally captured, more precise information is
propagated to neighboring events through particular context. Experimental
results show that coreference relations among event agents and the
distributional similarities between events are key to such propagation.",7 Feb 2017 05:23:52 GMT,Resources/Evaluation,Discourse and pragmatics,relation/event extraction;  temporal/spatial information extraction,Jinxxxx,Chxxx,xxxxxxxxxxx.kaist.ac.kr,KAIST,No,Jinxxxx,Yxx,xxxxxxxxxxkaist.ac.kr,KAIST,No,Joxx,Paxx,xxxxxxxxxxaist.ac.kr,KAIST,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jinxxxx,Chxxx,KAIST,,,0,,,xxxxxxxxxxx.kaist.ac.kr,,,,,Republic of Korea,,Jinxxxx Chxxx;Jinxxxx Yxx;Joxx Paxx,xxxxxxxxxxx.kaist.ac.kr;xxxxxxxxxxxkaist.ac.kr;xxxxxxxxxxkaist.ac.kr,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
611,611X-H3P9J3C2B9,Using Global Constraints and Reranking to Improve Cognates Detection,Micxxxx Bloxxxxxx and Benxxxxx Strxxxx,Multilingual,Omxx Abxxx;Moxx Dixx,Accept - Poster Tuesday,,Undecided (Multilingual),"Global constraints and reranking have not been used in cognates detection
research to date. We propose methods for using global constraints by performing
rescoring of the score matrices produced by state of the art cognates detection
systems. Using global constraints to perform rescoring is complementary to
state of the art methods for performing cognates detection and results in
significant performance improvements beyond current state of the art
performance on publicly available datasets with different language pairs and
various conditions such as different levels of baseline state of the art
performance and different data size conditions, including with more realistic
large data size conditions than have been evaluated with in the past.",24 Apr 2017 07:54:31 GMT,Empirical/Data-Driven,Multilinguality,,Micxxxx,Bloxxxxxx,xxxxxxxxxd@tcnj.edu,The College of New Jersey,No,Benxxxxx,Strxxxx,xxxxxxxxx05@osu.edu,The Ohio State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Micxxxx,Bloxxxxxx,The College of New Jersey,,,,,,xxxxxxxxxd@tcnj.edu,,Ewing,NJ,,United States,,Micxxxx Bloxxxxxx;Benxxxxx Strxxxx,xxxxxxxxxd@tcnj.edu;xxxxxxxxxx05@osu.edu,Using Global Constraints and Reranking to Improve Cognates Detection,Using Global Constraints and Reranking to Improve Cognates Detection,10,Michael Bloodgood,,,,on,No. Do not include my submission in this dataset.,No,None,None
612,612X-B5E3H5E9H4,A Challenge Set Approach to Evaluating Machine Translation,Pixxxx Isaxxxxx;Coxxx Chxxxx and Gexxxx Foxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation represents an exciting leap forward in
translation quality.  But what longstanding weaknesses does it
resolve, and which remain?  We address these questions with a
challenge set approach to translation evaluation and error analysis.
A challenge set consists of a small set of sentences, each
hand-designed to probe a system's capacity to bridge a particular
structural divergence between languages.  To exemplify this
approach, we present an English-French challenge set, and use it to
analyze phrase-based and neural systems.  The resulting analysis
provides not only a more fine-grained picture of the strengths of
neural systems, but also insight into which linguistic phenomena
remain out of reach.",7 Feb 2017 07:26:01 GMT,Resources/Evaluation,Machine translation,MT evaluations;  human judgments of MT;  statistical machine translation,Pixxxx,Isaxxxxx,xxxxxxxxxxxxxxx@cnrc-nrc.gc.ca,National Research Council,No,Coxxx,Chxxxx,xxxxxxxxxxxxry@gmail.com,NRC,No,Gexxxx,Foxxxx,xxxxxxxxxxxxxxcnrc-nrc.gc.ca,NRC,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Coxxx,Chxxxx,Google,,,(514)xxxxxxxxx,,,xxxxxxxxxxxxry@gmail.com,,"Ottawa, ON",,,Canada,,Pixxxx Isaxxxxx;Coxxx Chxxxx;Gexxxx Foxxxx,xxxxxxxxxxxxxxx@cnrc-nrc.gc.ca;xxxxxxxxxxxxrry@gmail.com;xxxxxxxxxxxxxx@cnrc-nrc.gc.ca,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
613,613X-D3B7F8D7C3,Decoding as Continuous Optimization in Neural Machine Translation,Coxx Dxx;Ghoxxxxxxx Hafxxxx and Trxxxx Cxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"We propose a novel decoding approach for neural machine translation (NMT) based
on continuous optimisation. The resulting optimisation problem is then tackled
using constrained gradient optimisation. Our powerful decoding framework,
enables decoding intractable models such as the intersection of left-to-right
and right-to-left (bidirectional) as well as source-to-target and
target-to-source (bilingual) NMT models. Our empirical results show that our
decoding framework is effective, and leads to substantial improvements in
translations generated from the intersected models where the typical greedy or
beam search is
infeasible.",7 Feb 2017 11:25:42 GMT,Empirical/Data-Driven,Machine translation,structured input/output;  statistical machine translation;  theoretical aspects of machine learning,Congxxxxxxx,Hoxxx,xxxxxxxxxgmail.com,The University of Melbourne,No,Ghoxxxxxxx,Hafxxxx,xxxxxxxxxxxi@gmail.com,Monash University,No,Trxxxx,Coxx,xxxxxxxxxxelb.edu.au,University of Melbourne,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vu Cxxxxxxx,Hoxxx,The University of Melbourne,,,6145xxxxxxx,,,xxxxxxxxxgmail.com,,Melbourne,VIC,,Australia,"Currently PhD Candidate at The University of Melbourne, VIC, Australia",Coxx Dxx;Ghoxxxxxxx Hafxxxx;Trxxxx Coxx,xxxxxxxxxgmail.com;xxxxxxxxxxxri@gmail.com;xxxxxxxxxxmelb.edu.au,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
614,614X-D6F4A3H8F3,Clustering Paraphrases for Substitutability,Anxx Coxxx;Marxxxxx Apixxxxxxx;Dexxx Wixxxx and Chxxx Callxxxxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Current state-of-the-art models for lexical substitution -- the task of
nominating substitutes for a word in context -- ignore word sense, instead
relying on powerful vector and embedded word representations to find good
substitutes. We present a simple method for improving the lexical substitution
rankings of existing models by integrating word sense inventories, filtering
substitutes from the correct sense to the top of the rankings. To enable
maximum coverage of our method, we also propose a novel method for clustering
paraphrases by word sense with substitutability in mind. Our method results in
sense clusters that are more substitutable and have wider coverage than
existing sense inventories. They can be applied as a filter over lexical
substitution rankings generated by existing vector- and embedding-based ranking
models to significantly improve their performance.",7 Feb 2017 05:13:21 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  lexical paraphrasing;  textual entailment and paraphrasing;  word sense disambiguation;  word sense induction,Anxx,Coxxx,xxxxxxxxxxs.upenn.edu,University of Pennsylvania,No,Marxxxxx,Apixxxxxxx,xxxxxxxxxxxxxxanaki@limsi.fr,"LIMSI-CNRS, University Paris-Saclay",No,Dexxx,Wixxxx,xxxxxxxxxx.upenn.edu,University of Pennsylvania,No,Chxxx,Callixxxxxxxxx,xxxxxxenn.edu,University of Pennsylvania,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxx,Coxxx,University of Pennsylvania,,,+1 48xxxxxxxxxx,,,xxxxxxxxxxs.upenn.edu,,Philadelphia,PA,,United States,"Anne Cocos is a second-year PhD student at the University of Pennsylvania studying with Chris Callison-Burch. She is also conducting research with the Department of Biomedical and Health Informatics at The Children's Hospital of Philadelphia under the supervision of Aaron Masino. Research interests include paraphrasing, crowdsourcing, and the application of natural language techniques to clinical and health data.",Anxx Coxxx;Marxxxxx Apixxxxxxx;Dexxx Wixxxx;Chxxx Callixxxxxxxxx,xxxxxxxxxxs.upenn.edu;xxxxxxxxxxxxxxianaki@limsi.fr;xxxxxxxxxxs.upenn.edu;xxxxxxxenn.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
615,615X-H6J9H3E4A3,GROUGE: A Graph-based ROUGE for Evaluating Abstractive Summaries,Elxxxx Shafxxxxxxxxx;Mohxxxxx Ebrxxxxx;Rayxxxx Woxx and Faxx Cxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"This paper proposes an effective approach to enhance Rouge for assessing
abstractive summaries. Applying a distributional lexico-semantic model, we
adopt a graph-based algorithm into Rouge to capture the semantic similarities
between model and system-generated summaries. The semantic similarity is then
combined into the lexical similarity score. Experiment results over TAC AESOP
datasets indicate that evaluating summaries using both lexical and semantic
similarities better correlates with human judgments.",7 Feb 2017 04:32:13 GMT,Resources/Evaluation,Summarization,document summarization;  evaluation metrics,Elxxxx,Shafxxxxxxxxx,xxxxxxxxxxx.unsw.edu.au,UNSW,No,Mohxxxxx,Ebrxxxxx,xxxxxxxxxxxxe.unsw.edu.au,UNSW,No,Rayxxxx,Woxx,xxxxxxxxxxnsw.edu.au,University of New South Wales,No,Faxx,Chxx,xxxxxxxxxxnsw.edu.au,UNSW,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rayxxxx,Woxx,University of New South Wales,,,6129xxxxxxx,,,xxxxxxxxxxnsw.edu.au,,Kensington,NSW,,Australia,,Elxxxx Shafxxxxxxxxx;Mohxxxxx Ebrxxxxx;Rayxxxx Woxx;Faxx Chxx,xxxxxxxxxxx.unsw.edu.au;xxxxxxxxxxxxxe.unsw.edu.au;xxxxxxxxxxunsw.edu.au;xxxxxxxxxxunsw.edu.au,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
616,616X-C2H6P2J9J6,Beyond Binary Labels: Political Ideology Prediction of Twitter Users,Daxxxx Preo�xxxxxxxxxxx;Yx Lxx;Daxxxx Hopxxxx and Lyxx Unxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Accept - Oral Tuesday,,Undecided (Social Media),"Automatic political orientation prediction from social media posts has to date
proven successful only in distinguishing between publicly declared liberals and
conservatives in the US. This study examines users’ political ideology using
a seven-point scale which enables us to identify politically moderate and
neutral users – groups which are of particular interest to political
scientists and pollsters. Using a novel data set with political ideology labels
self-reported through surveys, our goal is two-fold: a) to characterize the
groups of politically engaged users through language use on Twitter; b) to
build a fine-grained model that predicts political ideology of unseen users.
Our results identify differences in both political leaning and engagement and
the extent to which each group tweets using political keywords. Finally, we
demonstrate how to improve ideology prediction accuracy by exploiting the
relationships between the user groups.",25 Apr 2017 07:27:13 GMT,Empirical/Data-Driven,Social media,,Daxxxx,Preo�xxxxxxxxxxx,xxxxxxxxxgmail.com,University of Pennsylvania,No,Yx,Lxx,xxxxxxxxx@gmail.com,National University of Singapore,No,Daxxxx,Hopxxxx,xxxxxxxxxx.upenn.edu,University of Pennsylvania,No,Lyxx,Unxxx,xxxxxxxxx.upenn.edu,University of Pennsylvania,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxxx,Preo�xxxxxxxxxxx,Bloomberg,,,1267xxxxxxx,,,xxxxxxxxxgmail.com,,,,,United States,"His research aims to leverage large-scale social media footprints to learn about people and to aid with psychology and health problems, receiving extensive media coverage.

Previously, Daniel completed his PhD studies on temporal models for social media as part of the Natural Language Processing Research Group at the University of Sheffield. During his time in Sheffield, he was also a part-time research associate for the TrendMiner EU FP7 project where he worked on predicting real world outcomes (e.g. political voting intention) and uncovering spatio-temporal patterns in large user-generated content.",Daxxxx Preo�xxxxxxxxxxx;Yx Lxx;Daxxxx Hopxxxx;Lyxx Unxxx,xxxxxxxxxgmail.com;xxxxxxxxxx@gmail.com;xxxxxxxxxxs.upenn.edu;xxxxxxxxxx.upenn.edu,Beyond Binary Labels: Political Ideology Prediction of Twitter Users,Beyond Binary Labels: Political Ideology Prediction of Twitter Users,12,Daniel Preotiuc-Pietro,,,on,on,Only include my submission if it is accepted.,No,None,None
617,617X-B6C5J8A9G3,MalwareTextDB: A Database for Annotated Malware Articles,Swxx Kixx;Aldxxxx Obxxx;Wxx Lx and Chxx Hxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Accept - Poster Monday,,Undecided (Resources Evaluation),"Cybersecurity risks and malware threats are becoming increasingly dangerous and
common. Despite the severity of the problem, there has been few NLP efforts
focused on tackling cybersecurity.

In this paper, we discuss the construction of a new database for annotated
malware texts. An annotation framework is introduced based on the MAEC
vocabulary for defining malware characteristics, along with a database
consisting of 39 annotated APT reports with a total of 6,819 sentences. We also
use the database to construct models that can potentially help cybersecurity
researchers in their data collection and analytics efforts.",28 Jun 2017 11:03:02 GMT,Resources/Evaluation,Resources and evaluation,,Swexxxxxx,Lxx,xxxxxxxxxxt@gmail.com,Singapore University of Technology and Design,No,Aldrxxxxxxxxx,Muxx,xxxxxxxxxxxx@sutd.edu.sg,Singapore University of Technology and Design,No,Wxx,Lx,xxxxxxxxtd.edu.sg,Singapore University of Technology and Design,No,Chexxxxx,Oxx,xxxxxxxxxdso.org.sg,DSO National Laboratories,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Swexxxxxx,Lxx,Singapore University of Technology and Design,,,,,,xxxxxxxxxxt@gmail.com,,,,,Singapore,,Swxx Kixx;Aldxxxx Obxxx;Wxx Lx;Chxx Hxx,xxxxxxxxxxt@gmail.com;xxxxxxxxxxxxs@sutd.edu.sg;xxxxxxxxxtd.edu.sg;xxxxxxxxxxdso.org.sg,MalwareTextDB: A Database for Annotated Malware Articles,MalwareTextDB: A Database for Annotated Malware Articles,11,Lim Swee Kiat,,"Singapore University of Technology and Design, 8 Somapah Road, Singapore, 487372",,on,No. Do not include my submission in this dataset.,No,None,None
618,618X-C7H4G7A3F6,Detecting Code Answers to Natural Language Questions,Zixx Yxx;Yx Sx;Daxxxx Wexx and Huxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Natural language question and code answer pairs are critical to model
development on tasks like code snippet retrieval and annotation. In most cases,
such datasets were collected in heuristic ways, e.g., by simply pairing a
question title with the first code snippet or with every code snippet in an
answer post. These heuristics have the following weaknesses respectively: (1)
Low recall: There can be multiple code solutions in an answer post, most of
which are unemployed. (2) Low precision: The questions match poorly with their
paired code snippets. In this paper, we study the new problem of systematically
mining question-code pairs from Stack Overflow. The challenge lies in how to
utilize the context and content of a code snippet to predict whether it is a
solution. We develop an effective framework based on Conditional Random Fields
with a comprehensive set of features and propose a novel Hierarchical Neural
Network for modeling the contextual information around a code snippet.
Experimental results  show that our framework collects question-code pairs with
roughly 10% higher F1 and  accuracy  than heuristic methods. We will release
totally 182,251 such pairs, which can significantly help  tasks like code
annotation and retrieval.",7 Feb 2017 11:21:58 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP in software development and testing;  answer extraction;  text mining;  question answering in restricted domains,Zixx,Yxx,xxxxxxx@osu.edu,The Ohio State University,No,Yx,Sx,xxxxxxxucsb.edu,University of California Santa Barbara,No,Daxxxx,Wexx,xxxxxxxxxxxhington.edu,University of Washington,No,Huxx,Sxx,xxxxxxx@osu.edu,The Ohio State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Zixx,Yxx,The Ohio State University,,,,,,xxxxxxx@osu.edu,,,,,United States,,Zixx Yxx;Yx Sx;Daxxxx Wexx;Huxx Sxx,xxxxxxx@osu.edu;xxxxxxxxucsb.edu;xxxxxxxxxxxshington.edu;xxxxxxxx@osu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
619,619X-A2H3G6H6J3,A Corpus of Annotated Revisions for Studying Argumentative Writing,Fxx Zhxxx;Hoxx Bx;Rebxxxx Hxx and Dixxx Lixxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Accept - Poster Monday,,Undecided (Resources Evaluation),"This paper presents ArgRewrite, a corpus of between-draft revisions of
argumentative essays. Drafts are manually aligned at the sentence level, and
the writer’s purpose for each revision is annotated with categories analogous
to those used in argument mining and discourse analysis. The corpus should
enable advanced research in writing comparison and revision analysis, as
demonstrated via our own studies of student revision behavior and of automatic
revision purpose prediction.",21 Apr 2017 01:14:11 GMT,Resources/Evaluation,Resources and evaluation,,Fxx,Zhxxx,xxxxxxxitt.edu,University of Pittsburgh,No,Hoxx,B. xxxxxxx,xxxxxxxxxs.pitt.edu,"Intelligent Systems Program, University of Pittsburgh",No,Rebxxxx,Hxx,xxxxxxxpitt.edu,University of Pittsburgh,No,Dixxx,Lixxxx,xxxxxxxxx.pitt.edu,University of Pittsburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fxx,Zhxxx,University of Pittsburgh,,,412xxxxxxx,,,xxxxxxxitt.edu,,,,,United States,,Fxx Zhxxx;Hoxx Bx;Rebxxxx Hxx;Dixxx Lixxxx,xxxxxxxitt.edu;xxxxxxxxxxs.pitt.edu;xxxxxxxxpitt.edu;xxxxxxxxxs.pitt.edu,A Corpus of Annotated Revisions for Studying Argumentative Writing,A Corpus of Annotated Revisions for Studying Argumentative Writing,11,Fan Zhang,,"Department of Computer Science, University of Pittsburgh
210 S Bouquet St. Pittsburgh, PA",,on,"Yes, include my submission even if the paper is rejected.",No,None,None
620,620X-J3G4A3P9F5,Unsupervised Morphology Learning with Statistical Paradigms,Xx Xx;Mixxx Maxxxx;Lyxx Unxxx and Chaxxxx Yxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"This paper describes an unsupervised model of morphological segmentation that
 exploits the notion of paradigms, which are sets of morphological categories
(e.g.,
 suffixes) that can be applied to a homogeneous set of words (e.g., nouns or
 verbs). Our algorithm identifies reliable paradigms from the morphological
segmentation result of a probabilistic model, and chooses reliable suffixes
from them.  Iteratively, the new suffixes can be fed
 back to improve the accuracy of the prob abilistic model. Finally, the
unreliable
 paradigms are subjected to pruning to              eliminate unreliable
morphological
relations. The resulting paradigm-based algorithm significantly improves
segmentation accuracy. Our method achieves start-of- the-art results on
experiments using the Morpho-Challenge English and Turkish  data.",7 Feb 2017 04:56:30 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",unsupervised and semi-supervised learning;  cross-lingual approaches;  MT quality control;  morphology,Xx,Xx,xxxxxxxxxgmail.com,University of Pennsylvania,No,Mixxx,Maxxxx,xxxxxxxxx.upenn.edu,University of Pennsylvania,No,Lyxx,Unxxx,xxxxxxxxx.upenn.edu,University of Pennsylvania,No,Chaxxxx,Yaxx,xxxxxxxxxxxxxling.upenn.edu,University of Pennsylvania,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chaxxxx,Yaxx,University of Pennsylvania,,,215-xxxxxxx,,,xxxxxxxxxxxxxling.upenn.edu,,,,,United States,,Xx Xx;Mixxx Maxxxx;Lyxx Unxxx;Chaxxxx Yaxx,xxxxxxxxxgmail.com;xxxxxxxxxx.upenn.edu;xxxxxxxxxx.upenn.edu;xxxxxxxxxxxxxxling.upenn.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
621,621X-G3G4D3A9J9,A Novel Group-Level Objective for Deep Answer Triggering,Jxx Zhxx;Yx Sx;Zixx Guxx and Huxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Answer triggering is critical to building practical question answering (QA)
systems. Given a question and a set of candidate answers, it aims at
determining whether the candidate set contains any correct answer, and if so,
outputs a correct one. Existing QA systems always output an answer from a
candidate set, which can greatly hurt user  experience. Despite its importance,
research on answer triggering has just started and existing methods yield
unsatisfactory results. One main drawback of existing methods is that they
consider individual candidate answers separately. In contrast, we propose
group-level prediction for answer triggering, which takes a holistic view of
the entire candidate answer set. However, the richer information at the group
level poses a more challenging prediction problem. We design a general
framework based on deep neural networks, optimized by a novel objective
function which jointly considers multiple important factors at the group level.
Experimental results on the WIKIQA bench-mark demonstrate that our framework
out-performs the state of the arts by a 6.6% absolute gain under the F1
measure.",7 Feb 2017 10:08:19 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",scalability and portability of question answering systems;  answer extraction;  open-domain question answering,Jxx,Zhxx,xxxxxxxx9@osu.edu,The Ohio State University,No,Yx,Sx,xxxxxxxucsb.edu,University of California Santa Barbara,No,Zixx,Guxx,xxxxxxxxxnwu.edu.cn,"Northwest University, China",No,Huxx,Sxx,xxxxxxx@osu.edu,The Ohio State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jxx,Zhxx,The Ohio State University,,,614xxxxxxx,,,xxxxxxxx9@osu.edu,,COLUMBUS,OH,,United States,,Jxx Zhxx;Yx Sx;Zixx Guxx;Huxx Sxx,xxxxxxxx9@osu.edu;xxxxxxxxucsb.edu;xxxxxxxxxxnwu.edu.cn;xxxxxxxx@osu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
622,622X-H3A2G4G5A7,On the Evaluation of Summaries: Assessing Quality with Pairwise Preferences,Maxxxx Zoxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"ROUGE, the current evaluation method of choice in text summarization, depends
on the availability of reference summaries, which are expensive to obtain and
often require the availability of domain experts. For these reasons, gold
standard summaries are only available for a few benchmark datasets, which
restricts the scope of current research in this area. In this paper, we propose
an alternative evaluation approach based on pairwise comparisons of individual
sentences, which can be obtained in a simple and inexpensive way. Furthermore,
we revise the previously used validation methodology and propose a more
suitable procedure instead. In our experiments, we show that humans are able to
provide useful feedback in the form of pairwise preferences and achieve
state-of-the-art results with little human input. We also show that our model
can simulate the annotation process based on already available data and achieve
even better results.",7 Feb 2017 13:06:29 GMT,Empirical/Data-Driven,Summarization,corpus development;  corpus annotation methods;  document summarization;  multi-document summarization;  evaluation metrics,Maxxxx,Zoxx,xxxxxxxxxxxxxu-darmstadt.de,Technische Universität Darmstadt,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,Zoxx,Technische Universität Darmstadt,,,,,,xxxxxxxxxxxxxu-darmstadt.de,,,,,Germany,,Maxxxx Zoxx,xxxxxxxxxxxxxu-darmstadt.de,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
623,623X-H6B3A8C9A4,Detection of Computer-Generated Essays with Density Estimators,Kaxxx Tagxxxxxx and Hwxx Txx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Current automated essay scoring systems perform well on essays written by
students and it has been shown that these systems are able to achieve high
agreement with human scorers. However, such systems can assign high scores to
certain types of automatically generated nonsensical essays, thus allowing the
systems to be gamed. We address this problem by proposing a novel approach for
detecting computer-generated fake essays, using density estimation methods. Our
method only relies on essays written by humans and does not make any prior
assumptions about the computer-generated fake essays. We have evaluated our
method on essays automatically generated by sampling language models and
context-free grammars. The results show that current state-of-the-art automated
essay scoring systems fail to detect these two types of computer-generated fake
essays. However, after integrating our method, these systems detect and
penalize computer-generated essays effectively and as a result, they continue
to perform well on essay scoring, on both human-written essays and
computer-generated essays.",7 Feb 2017 05:45:23 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",educational applications,Kaxxx,Tagxxxxxx,xxxxxxxxxxxxour@gmail.com,National University of Singapore,No,Hwexxxxx,Nx,xxxxxxxxxxnus.edu.sg,National University of Singapore,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxxx,Tagxxxxxx,National University of Singapore,,,659xxxxxxx,,,xxxxxxxxxxxxour@gmail.com,,,,,Singapore,,Kaxxx Tagxxxxxx;Hwxx Txx,xxxxxxxxxxxxour@gmail.com;xxxxxxxxxx.nus.edu.sg,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
624,624X-F9J9C4G4H8,Crowdsourcing Multiple Choice Questions to Improve Model Performance on Science Exams,Johxxxxx Wexxx;Nexxxx Lxx and Maxx Gaxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"The construction of large datasets for en-
tailment and reading comprehension has
led to the rapid improvement of neural
architectures for performing these tasks.
However, for more specialized domains,
such as science exam question answering,
the relatively small available datasets pre-
vent the successful application of neural
networks. Crowdsourcing large datasets
in these specialized domains is more chal-
lenging than previous dataset collection
efforts. We present a method for obtain-
ing high-quality, domain-targeted multi-
ple choice questions from crowd work-
ers. With this method we have assembled
SciQ, a novel dataset of 13.7K multiple
choice science exam questions. We show
through experiments that this dataset is an
interesting target for future research, and
that it is useful as additional training data
for answering real 4th and 8th grade sci-
ence exam questions.",7 Feb 2017 04:40:28 GMT,Resources/Evaluation,"Information extraction, text mining, and question answering",corpus development;  NLP applications;  interactive question answering;  information extraction;  domain adaptation;  scalability and portability of question answering systems;  NLP on noisy unstructured text;  information retrieval;  textual entailment and paraphrasing;  context-aware question answering;  answer extraction;  experimental evaluation/comparison of ML methods;  text classification;  collaborative methods for question answering;  open-domain question answering;  question answering in restricted domains,Johxxxxx,Wexxx,xxxxxxxxxxxxxl.14@ucl.ac.uk,University College London,No,Nexxxx,Lxx,xxxxxxxxxxington.edu,University of Washington,No,Maxx,Garxxxx,xxxxxxxxlenai.org,Allen Institute for Artificial Intelligence,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Johxxxxx,Wexxx,University College London,,,,,,xxxxxxxxxxxxxl.14@ucl.ac.uk,,,,,United Kingdom,,Johxxxxx Wexxx;Nexxxx Lxx;Maxx Garxxxx,xxxxxxxxxxxxxl.14@ucl.ac.uk;xxxxxxxxxxhington.edu;xxxxxxxxxlenai.org,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
626,626X-D6E8A6B8G7,Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations,Phxxxx Blxxx;Yuxxx Mexxxx and Joxx Baxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"We propose a language-agnostic way of automatically generating sets of
semantically similar clusters of entities along with sets of ""outlier""
elements, which may then be used to perform an intrinsic evaluation of word
embeddings in the outlier detection task. We used our methodology to create a
gold-standard dataset, which we call WikiSem500, and evaluated multiple
state-of-the-art embeddings. The results show a correlation between performance
on this dataset and performance on sentiment analysis.",7 Feb 2017 04:45:28 GMT,Resources/Evaluation,Resources and evaluation,unsupervised and semi-supervised learning;  cross-lingual approaches;  multilingual applications;  graph-based algorithms;  multilingual resources;  multimodal representations and processing;  evaluation metrics,Phxxxx,Blxxx,xxxxxxxxxxistech.com,pblair,No,Yuxxx,Mexxxx,xxxxxxxxx@gmail.com,Basis Technology,No,Joxx,Baxxx,xxxxxxxxxistech.com,Basis Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Phxxxx,Blxxx,Basis Technology,,,,,,xxxxxxxxxxistech.com,,,,,United States,,Phxxxx Blxxx;Yuxxx Mexxxx;Joxx Baxxx,xxxxxxxxxxistech.com;xxxxxxxxxx@gmail.com;xxxxxxxxxxistech.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
627,627X-P5G9J4P3D8,Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access,Bhxxxx Dhixxxx;Lixxxx Lx;Xixxxx Lx;Jiaxxxxx Gxx;Yunxxxxx Chxx;Faxxxx Ahxxx and Lx Dxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Monday,,Undecided (Dialog Interactive Systems),"This paper proposes KB-InfoBot - a multi-turn dialogue agent which helps users
search Knowledge Bases (KBs) without composing complicated queries. Such
goal-oriented dialogue agents typically need to interact with an external
database to access real-world knowledge. Previous systems achieved this by
issuing a symbolic query to the KB to retrieve entries based on their
attributes. However, such symbolic operations break the differentiability of
the system and prevent end-to-end training of neural dialogue agents. In this
paper, we address this limitation by replacing symbolic queries with an induced
``soft'' posterior distribution over the KB that indicates which entities the
user is interested in. Integrating the soft retrieval process with a
reinforcement learner leads to higher task success rate and reward in both
simulations and against real users. We also present a fully neural end-to-end
agent, trained entirely from user feedback, and discuss its application towards
personalized dialogue agents.",20 Apr 2017 17:12:45 GMT,Applications/Tools,Dialog and interactive systems,,Bhxxxx,Dhixxxx,xxxxxxxxxxxdrew.cmu.edu,Carnegie Mellon University,No,Lixxxx,Lx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Xixxxx,Lx,xxxxxxxxxosoft.com,Microsoft Research Redmond,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,"Microsoft Research, Redmond",No,Yunxxxxx,Chxx,xxxxxxxx@ieee.org,National Taiwan University,No,Faxxxx,Ahxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Lx,Dexx,xxxxxxxxxosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,Bhxxxx,Dhixxxx,Carnegie Mellon University,,,,,,xxxxxxxxxxxdrew.cmu.edu,,,,,United States,,Bhxxxx Dhixxxx;Lixxxx Lx;Xixxxx Lx;Jiaxxxxx Gxx;Yunxxxxx Chxx;Faxxxx Ahxxx;Lx Dexx,xxxxxxxxxxxdrew.cmu.edu;xxxxxxxxxxxicrosoft.com;xxxxxxxxxrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxx@ieee.org;xxxxxxxxxxxcrosoft.com;xxxxxxxxxrosoft.com,Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access,Towards End-to-End RL of Dialogue Agents for Information Access,12,Bhuwan Dhingra,,"Carnegie Mellon University,
5000 Forbes Ave, Pittsburgh, PA-15213, USA",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
628,628X-P2G3D9A4F9,"Read, Decide and Explain! Recollective (Explanatory) Question Answering",Trxxxx Gooxxxx and  x,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Current Question/Answering (QA) systems cannot provide automatically generated
explanations when knowledge bases (KBs) are not available. Many recent QA
systems consider descriptions from which questions are answered.
Explanations for the answers to these questions are not yet available and
cannot be produced from KBs alone. In this paper we introduce a novel
Recollective QA (RQA) framework that automatically generates explanations for
the answers inferred from the descriptions. Using the ""recollective"" properties
of its Reader, Decider and Explainer, the RQA learns to automatically generate
explanations for the answers without relying on any KBs. The promising
experimental results on a vast dataset of expert-generated explanations
indicate that the RQA provides a stepping stone for the future generation of
expert-quality answer explanations.",7 Feb 2017 06:52:09 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",context-aware question answering;  question answering in restricted domains,Trxxxx,Gooxxxx,xxxxxxxxxxxutdallas.edu,University of Texas at Dallas,No,Saxxx,Harxxxxxx,xxxxxxxxxxxtdallas.edu,University of Texas at Dallas,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Trxxxx,Gooxxxx,University of Texas at Dallas,,,,,,xxxxxxxxxxxutdallas.edu,,,,,United States,,Trxxxx Gooxxxx; x and a Harxxxxxx,xxxxxxxxxxxutdallas.edu;xxxxxxxxxxxutdallas.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
629,629X-C6H3D5D4A3,Resolving Event Coreference with Supervised Representation Learning and Clustering-Oriented Regularization,Kixx Kenyxxxxxxx;Jaxxxx Cxx and Doxxx Prxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"We present a novel approach to event coreference resolution by developing a
general framework for performing clustering tasks with supervised
representation learning. We use an hourglass-shaped neural network with novel
regularization terms in the objective function to create representations of
event mentions, which are then used to build event coreference chains via
agglomerative clustering. On the ECB+ corpus, our model obtains results better
than models that rely on significantly more annotated information, and
considerably improves upon strong baseline algorithms. We provide insight into
a new general approach for using supervised representation learning to solve
non-parametric clustering problems, and our results motivate continued
development of this framework.",7 Feb 2017 04:50:58 GMT,Empirical/Data-Driven,Semantics,unsupervised and semi-supervised learning;  discriminative learning methods;  coreference resolution,Kixx,Kenyxxxxxxx,xxxxxxxxxxxxxxxn@mail.mcgill.ca,McGill University,No,Jackixxxxxxxxx,Chxxxx,xxxxxxxxxx.mcgill.ca,McGill University,No,Doxxx,Prxxxx,xxxxxxxxxx.mcgill.ca,McGill University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kixx,Kenyxxxxxxx,McGill University,,,,,,xxxxxxxxxxxxxxxn@mail.mcgill.ca,,,,,Canada,,Kixx Kenyxxxxxxx;Jaxxxx Cxx;Doxxx Prxxxx,xxxxxxxxxxxxxxxn@mail.mcgill.ca;xxxxxxxxxxs.mcgill.ca;xxxxxxxxxxs.mcgill.ca,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
630,630X-J3A6D2P5C8,Generation of French Sign Language using a non-derivational approach,mohxxxx nasxxxx and micxxxx fixxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"This article presents an approach for the generation of French sign language
(LSF). One of the particularities of Sign Languages (SL) is the presence of
simultaneous movements of manual and non-manual articulators. Also, use of the
space (signing space) is a linguistic phenomenon that is not present in other
modalities. It is consequently not incorporated in existing formal grammar
models developed for spoken languages. In order to account for this and other
characteristics that are unique to Sign Language, we propose a model that
abstracts away from existing linguistics categorizations (distinction between
lexicon, syntax, etc.). We present a methodology to define production rules of
a new kind of grammar, and apply it to an example case of juxtaposed signed
units. Then, we present a comparison between our approach and existing models.",7 Feb 2017 06:05:28 GMT,Resources/Evaluation,Semantics,grammatical formalisms;  corpus annotation methods;  multimodal communication;  multimodal representations and processing,mohamxxxxxxxxxx,hadxxxx,xxxxxxxxxxxxime@gmail.com,LIMSI,No,micxxxx,fixxxx,xxxxxxive.fr,LIMSI,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,mohamxxxxxxxxxx,hadxxxx,LIMSI,,,,,,xxxxxxxxxxxxime@gmail.com,,,,,France,,mohxxxx nasxxxx;micxxxx fixxxx,xxxxxxxxxxxxime@gmail.com;xxxxxxlive.fr,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
631,631X-G9P8C6C4B8,Multimodal Cartesian Neural Fusion for Multimodal Sentiment Analysis,Amxx Zaxxx;Minxxxx Chxx and Louisxxxxxxxxx Moxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"With the advent of online video sharing websites such as YouTube and Facebook,
multimodal sentiment analysis has seen increasing attention from the scientific
community. Central to this research problem is the challenge of fusion between
verbal (spoken language), acoustic and visual behaviors. In this paper,  we
introduce a new fusion method for sentiment analysis, called Multimodal
Cartesian Neural Fusion (MCNF), which explicitly represent unimodal, bimodal
and trimodal interactions between behaviors. These multi-faceted interactions
are internally represented using a 3-D tensor which integrates verbal, acoustic
and visual intermediate representations. Our MCNF is learned end-to-end. Our
experiments on the publicly available MOSI multimodal sentiment analysis
datasets show state-of-the-art performance when compared with both
language-only and multimodal approaches. Our detailed ablation study also show
that all three types of interactions (unimodal, bimodal and trimodal) are
needed for optimal performance.",7 Feb 2017 08:57:08 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  opinion mining and extraction;  opinion representation;  multimodal communication;  multimodal representations and processing,Amxx,Zaxxx,xxxxxxxxxxxdrew.cmu.edu,CMU,No,Minxxxx,Chxx,xxxxxxxxxxxdrew.cmu.edu,CMU LTI,No,Louisxxxxxxxxx,Morxxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Amxx,Zaxxx,CMU,,,,,,xxxxxxxxxxxdrew.cmu.edu,,,,,United States,,Amxx Zaxxx;Minxxxx Chxx;Louisxxxxxxxxx Morxxxx,xxxxxxxxxxxdrew.cmu.edu;xxxxxxxxxxxxdrew.cmu.edu;xxxxxxxxxcs.cmu.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
632,632X-E6G3D8F6F7,Subsequence Neural Machine Translation with Attentional Switch,Tsuxxxxx Okxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Although both of Neural Machine Translation (NMT) and Statistical
  Machine Translation (SMT) obtain phrase correspondences in the end
  how to obtain these are in the opposite directions: NMT goes from
  the sentence-wide mappings (single alignment between sentence to
  sentence) to the smaller units while SMT goes from the word-based
  mappings (single alignment between word and word) to the bigger
  units.  This difference characterizes the performance: NMT does not
  suffer from the n-to-m mapping problems which are problems in SMT,
  while NMT suffers more from the small-sized training data.  We
  propose a subsequence NMT (sNMT) in order to boost the performance
  of NMT particularly for the small-sized parallel corpora. This sNMT
  takes advantage in the possible alignment information in
  optimization.  Since sNMT is orthogonal to various attention models,
  we can deploy sNMT with any attention models.  sNMT with global
  attention model (Luong et al., 2015) worked very well especially for three
  language pairs of post-editing (EN-CS, EN-JA, EN-PL) obtained more than 30%
  relative gains in BLEU. Furthermore, using Bayesian optimization on
  sNMT, we obtain comparable performance with SMT in small
  corpora.",10 Feb 2017 02:28:08 GMT,Empirical/Data-Driven,Machine translation,scalability/efficiency of ML methods;  learning with small datasets,Tsuxxxxx,Okxxx,xxxxxxxxxxxxta@gmail.com,Kyushuu institute of technology university,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tsuxxxxx,Okxxx,Kyushuu institute of technology university,,,,,,xxxxxxxxxxxxta@gmail.com,,Munich,,,Japan,,Tsuxxxxx Okxxx,xxxxxxxxxxxxta@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
633,633X-D2H7A6D9J4,Geolocation Inference using Local Celebrities and Local Interactions,Mohxxxxx Ebrxxxxx;Elxxxx Shafxxxxxxxxx;Rayxxxx Woxx and Faxx Cxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Determining the geographical location of social media users and where a message
is posted from is important in different applications, such as regional
sentiment analysis, local event detection, and crisis management. However, this
location information is either missing, incomplete or not accessible in many
social platforms. This limitation has encouraged the recent research on
inferring the locations of social media posts or users. 
We propose a new approach that unifies both social networks and textual content
for geolocation inference. We incorporate two key contributions to improve
geolocation inference: (1) a novel approach is proposed to categorize
highly-mentioned users (celebrities) into Local and Global types, and
consequently utilize Local celebrities as powerful location indicators. (2) We
leverage linguistic similarity between users' posts to predict which
individuals in the ego network may be most proximate. Empirical experiments
over three standard Twitter benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods.",7 Feb 2017 05:46:13 GMT,Empirical/Data-Driven,Social media,temporal/spatial information extraction;  social network,Mohxxxxx,Ebrxxxxx,xxxxxxxxxxxxe.unsw.edu.au,UNSW,No,Elxxxx,Shafxxxxxxxxx,xxxxxxxxxxx.unsw.edu.au,UNSW,No,Rayxxxx,Woxx,xxxxxxxxxxnsw.edu.au,University of New South Wales,No,Faxx,Chxx,xxxxxxxxxxnsw.edu.au,UNSW,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rayxxxx,Woxx,University of New South Wales,,,6129xxxxxxx,,,xxxxxxxxxxnsw.edu.au,,Kensington,NSW,,Australia,,Mohxxxxx Ebrxxxxx;Elxxxx Shafxxxxxxxxx;Rayxxxx Woxx;Faxx Chxx,xxxxxxxxxxxxe.unsw.edu.au;xxxxxxxxxxxx.unsw.edu.au;xxxxxxxxxxunsw.edu.au;xxxxxxxxxxunsw.edu.au,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
634,634X-J4G3A5E2D4,Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling,Xiaxxxxx Zxx;Dixxx Klaxxxx and Patxxxx Blxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we present hierarchical relation-based latent Dirichlet
allocation (hrLDA), a data-driven hierarchical topic model for extracting
terminological ontologies from a large number of heterogeneous documents. In
contrast to traditional topic models, hrLDA relies on noun phrases instead of
unigrams, it takes syntax and document structures into consideration, and it
enriches topic hierarchies with topic relations. Through a series of
experiments, we demonstrate the superiority of hrLDA over existing topic
models, especially for building hierarchies. Furthermore, we illustrate the
robustness of hrLDA in the settings of noisy data sets, which are likely to
occur in many practical scenarios. Our ontology evaluation results show that
ontologies extracted from hrLDA are very competitive with the ontologies
created by domain experts.",7 Feb 2017 08:39:07 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  NLP applications;  NLP on noisy unstructured text;  text mining;  ontology development;  NLP on Wikipedia and other collaboratively constructed resources;  document mining;  graphical models;  semantic knowledge induction;  document clustering,Xiaxxxxx,Zxx,xxxxxxxxxxxxww@gmail.com,Northwestern University,No,Dixxx,Klaxxxx,xxxxxxxxxxxxxthwestern.edu,Northwestern University,No,Patxxxx,Blxxx,xxxxxxxxxxxxess@intel.com,Intel Corporation,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxxx,Zxx,Northwestern University,,,773xxxxxxx,,,xxxxxxxxxxxxww@gmail.com,,,,,United States,,Xiaxxxxx Zxx;Dixxx Klaxxxx;Patxxxx Blxxx,xxxxxxxxxxxxww@gmail.com;xxxxxxxxxxxxxrthwestern.edu;xxxxxxxxxxxxxess@intel.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
635,635X-C6B3H6F8G6,Fully Differentiable Neural Easy-First Decoders,Anxxxx Fx and Juxxx Krexxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"We introduce a novel neural easy-first decoder that learns to solve sequence
tagging tasks in a flexible order. In contrast to previous easy-first decoders,
our model is end-to-end differentiable. The decoder iteratively updates a
""sketch"" of the predictions over the sequence. At its core is an attention
mechanism that controls which parts of the input are considered the
“easiest” to process next. We present a new constrained softmax
transformation that ensures the same cumulative attention to every word, and
show how to efficiently evaluate and backpropagate over it. Our model compares
favourably to a BILSTM tagger on three sequence tagging tasks.",7 Feb 2017 11:51:58 GMT,Empirical/Data-Driven,Machine learning,part-of-speech tagging;  named entity recognition;  theoretical aspects of machine learning,Andrxxxxxxxx,Marxxxx,xxxxxxxcmu.edu,"Priberam, Instituto de Telecomunicacoes",No,Juxxx,Krexxxxx,xxxxxxxxxxxxxxi-heidelberg.de,"Department of Computational Linguistics, Heidelberg University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Andrxxxxxxxx,Marxxxx,"Unbabel, Instituto de Telecomunicacoes",,,,,,xxxxxxxcmu.edu,,,,,Portugal,,Anxxxx Fx;Juxxx Krexxxxx,xxxxxxxcmu.edu;xxxxxxxxxxxxxxxi-heidelberg.de,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
636,636X-A6H3C8H6B7,Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions,Emxx Strxxxxx;Patxxxx Vexxx;Daxxx Belxxxxx and Anxxxx McCxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Bi-directional LSTMs have emerged as a standard method for obtaining per-token
vector representations serving as input to various token labeling tasks
(whether followed by Viterbi prediction or independent classification).  This
paper proposes an alternative to Bi-LSTMs for this purpose: iterated dilated
convolutional neural networks (ID-CNNs), which have better capacity than
traditional CNNs for large context and structured prediction.  We describe a
distinct combination of network structure, parameter sharing and training
procedures that is not only more accurate than Bi-LSTM-CRFs, but also 8x faster
at test time on long sequences.  Moreover, ID-CNNs with independent
classification enable a dramatic 14x test-time speedup, while still attaining
accuracy comparable to the Bi-LSTM-CRF.  We further demonstrate the ability of
ID-CNNs to combine evidence over long sequences by demonstrating their improved
accuracy on whole-document (rather than per-sentence) inference.  Unlike LSTMs
whose sequential processing on sentences of length N requires O(N) time even in
the face of parallelism, IDCNNs permit fixed-depth convolutions to run in
parallel across entire documents.  Today when many companies run basic NLP on
the entire web and large-volume traffic, faster methods are paramount to saving
time and energy costs.",7 Feb 2017 05:09:25 GMT,Empirical/Data-Driven,Machine learning,named entity recognition;  scalability/efficiency of ML methods;  structured input/output,Emxx,Strxxxxx,xxxxxxxxxxs.umass.edu,"University of Massachusetts, Amherst",No,Patxxxx,Vexxx,xxxxxxxxmass.edu,UMass Amherst,No,Daxxx,Belxxxxx,xxxxxxxxxxxxxger@gmail.com,University of Massachusetts Amherst,No,Anxxxx,McCxxxxx,xxxxxxxxxxs.umass.edu,UMass Amherst,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Emxx,Strxxxxx,"University of Massachusetts, Amherst",,,207xxxxxxx,,,xxxxxxxxxxs.umass.edu,,,,,United States,,Emxx Strxxxxx;Patxxxx Vexxx;Daxxx Belxxxxx;Anxxxx McCxxxxx,xxxxxxxxxxs.umass.edu;xxxxxxxxumass.edu;xxxxxxxxxxxxxnger@gmail.com;xxxxxxxxxxxs.umass.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
637,637X-H5F3C9C9G3,Idiomatic Continuum and Automatic Idiom Classification,Maxxxx Praxxxx;Jixx Pexx and Anxx Fexxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Idiomatic expressions, such as ‘hit the
wall’ or ‘blow the whistle’, can also
have literal interpretations. This paper
describes novel algorithms for automatic
classification of idiomatic and literal expressions
based on the idea that literal and
idiomatic expressions appear in different
contexts. For comparison, we implement
Fazly et al. (2009)’s, (Sporleder and Li,
2009)’s, and Li and Sporleder (2010)’s
methods and apply them to our data.

We also report the results of an experiment
in which human annotators rank idiomatic
expressions in context on a scale
from 1 (literal) to 4 (highly idiomatic).
Our experiment supports the hypothesis
that idioms fall on a continuum and that
one might differentiate between highly idiomatic,
mildly idiomatic and weakly idiomatic
expressions. In addition, we measure
the relative idiomaticity of 11 idiomatic
types and compute the correlation
between the relative idiomaticity of an expression
and the performance of various
automatic models for idiom detection. We
show that our model, based on the distributional
semantics ideas, not only outperforms
the previous models, but also positively
correlates with the human judgements,
which suggests that we’re moving
in the right direction toward automatic idiom
detection.",7 Feb 2017 05:07:46 GMT,Empirical/Data-Driven,Semantics,figurative language,Maxxxx,Praxxxx,xxxxxxxxxxxxxx.montclair.edu,Montclair State University,No,Jixx,Pexx,xxxxxxxxxtclair.edu,Montclair State University,No,Anxx,Felxxxx,xxxxxxxxxxxxx.montclair.edu,Montclair State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxx,Felxxxx,Montclair State University,,,,,,xxxxxxxxxxxxx.montclair.edu,,,,,United States,,Maxxxx Praxxxx;Jixx Pexx;Anxx Felxxxx,xxxxxxxxxxxxxx.montclair.edu;xxxxxxxxxxtclair.edu;xxxxxxxxxxxxxx.montclair.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
638,638X-F3A4F3B9D2,"Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction",Yasxxxxx Mixxx;Moxxxx Tanxxxxxx;Toxxxx Tanxxxxxx and Toxxxx Ohxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Accept - Oral Wednesday,,Undecided (Social Media),"We propose a novel geolocation prediction model using a complex neural network.
Geolocation prediction in social media has attracted many researchers to use
information of various types. Our model unifies text, metadata, and user
network representations with an attention mechanism to overcome previous
ensemble approaches. In an evaluation using two open datasets, the proposed
model exhibited a maximum 3.8% increase in accuracy and a maximum of 6.6%
increase in accuracy@161 against previous models. We further analyzed several
intermediate layers of our model, which revealed that their states capture some
statistical characteristics of the datasets.",22 Apr 2017 14:55:36 GMT,Empirical/Data-Driven,Social media,,Yasxxxxx,Mixxx,xxxxxxxxxxxxxxxfujixerox.co.jp,"Research & Technology Group, Fuji Xerox Co., Ltd.",No,Moxxxx,Tanxxxxxx,xxxxxxxxxxxxxxxx@fujixerox.co.jp,"Research & Technology Group, Fuji Xerox Co., Ltd.",No,Toxxxx,Tanxxxxxx,xxxxxxxxxxxxxxxx@fujixerox.co.jp,"Research & Technology Group, Fuji Xerox Co., Ltd.",No,Toxxxx,Ohxxxx,xxxxxxxxxxxxxxfujixerox.co.jp,"Research & Technology Group, Fuji Xerox Co., Ltd.",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yasxxxxx,Mixxx,Fuji Xerox/Tokyo Institute of Technology,,,,,,xxxxxxxxxxxxxxxfujixerox.co.jp,,,,,Japan,,Yasxxxxx Mixxx;Moxxxx Tanxxxxxx;Toxxxx Tanxxxxxx;Toxxxx Ohxxxx,xxxxxxxxxxxxxxxfujixerox.co.jp;xxxxxxxxxxxxxxxxi@fujixerox.co.jp;xxxxxxxxxxxxxxxxi@fujixerox.co.jp;xxxxxxxxxxxxxxxfujixerox.co.jp,"Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction","Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction",13,Yasuhide Miura,,"Fuji Xerox Co., Ltd.
6-1 Minatomirai Nishi-ku Yokohama-shi
Kanagawa 220-8668 Japan",,on,No. Do not include my submission in this dataset.,No,None,None
639,639X-F5F7B6B2H9,Semantic Document Distance Measures and Unsupervised Document Revision Detection,Xiaxxxxx Zxx;Dixxx Klaxxxx and Patxxxx Blxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we model the document revision detection problem as a minimum
cost branching problem that relies on computing document distances.
Furthermore, we propose two new document distance measures, word vector-based
Dynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED).
Our revision detection system is designed for a large scale corpus and
implemented in Apache Spark. We demonstrate that our system can more precisely
detect revisions than state-of-the-art methods on the Wikipedia revision dumps
and on a simulated data set.",7 Feb 2017 07:26:32 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  NLP applications;  scalability/efficiency of ML methods;  NLP on noisy unstructured text;  distributional similarity;  text mining;  NLP on Wikipedia and other collaboratively constructed resources,Xiaxxxxx,Zxx,xxxxxxxxxxxxww@gmail.com,Northwestern University,No,Dixxx,Klaxxxx,xxxxxxxxxxxxxthwestern.edu,Northwestern University,No,Patxxxx,Blxxx,xxxxxxxxxxxxess@intel.com,Intel Corporation,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxxx,Zxx,Northwestern University,,,773xxxxxxx,,,xxxxxxxxxxxxww@gmail.com,,,,,United States,,Xiaxxxxx Zxx;Dixxx Klaxxxx;Patxxxx Blxxx,xxxxxxxxxxxxww@gmail.com;xxxxxxxxxxxxxrthwestern.edu;xxxxxxxxxxxxxess@intel.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
640,640X-B3J3A7G6C2,Learning to Rank for Coordination Detection,Xxx Waxx;Ruxxxx Lx;Hirxxxxx Shxxxx;Katxxxxxx Suxxx and Masxxxx Naxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"This paper studies the problem of coordination structure detection. Existing
work normally regards the detection of coordination boundaries as a
classification problem, and suffers from the imbalance of training data. We
formulate the detection of coordination structure as a ranking problem instead
of classification.
The proposed method fully exploits the differences between training data. A
novel recurrent residual network is developed for this problem. Experiments on
Penn Treebank and Genia show that the proposed method has achieved satisfying
results compared with previous work.",7 Feb 2017 11:25:34 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",semantic relations,Xxx,Waxx,xxxxxxxxxxxb.ntt.co.jp,NTT Communication Science Laboratories,No,Ruxxxx,Lx,xxxxxxxxxxx@foxmail.com,Umass/NAIST,No,Hirxxxxx,Shxxxx,xxxxxxxxxutlook.com,Nara Institute of Science and Technology,No,Katxxxxxx,Suxxx,xxxxxxxxx@sudoh.nl,NTT Communication Science Laboratories,No,Masxxxx,Naxxxx,xxxxxxxxxxxxxx@lab.ntt.co.jp,+81-774-93-5235,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Xxx,Waxx,NTT Communication Science Laboratories,,,,,,xxxxxxxxxxxb.ntt.co.jp,,,,,Japan,,Xxx Waxx;Ruxxxx Lx;Hirxxxxx Shxxxx;Katxxxxxx Suxxx;Masxxxx Naxxxx,xxxxxxxxxxxb.ntt.co.jp;xxxxxxxxxxxx@foxmail.com;xxxxxxxxxxutlook.com;xxxxxxxxxo@sudoh.nl;xxxxxxxxxxxxxxi@lab.ntt.co.jp,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
641,641X-G7E3C8E8C6,Learning Concepts through Natural Language Instruction,Shaxxxxx Srixxxxxxx;Igxx Labxxxx and Txx Mitxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"We introduce the problem of concept learning (learning a binary classifier to
identify instances of a concept) using natural language statements and a small
number of labeled examples. The key technical challenge in learning from
natural language instructions is the semantic parsing of a statement into a
logical representation, which can then operationalize a concept by acting as a
feature function. We present a joint model for language understanding (semantic
parsing) and concept learning (classification) that does not require labeling
statements with logical forms. On a dataset of email-related concepts, we
demonstrate that this approach yields across-the-board improvements in
classification performance in the low data regime, with an average 30% relative
improvement in F1 score over state-of-the-art methods.",7 Feb 2017 07:13:36 GMT,Empirical/Data-Driven,Semantics,unsupervised and semi-supervised learning;  NLP applications;  learning with small datasets;  discriminative learning methods;  structured input/output;  parsing;  text classification,Shaxxxxx,Srixxxxxxx,xxxxxxxxgmail.com,Carnegie Mellon University,No,Igxx,Labxxxx,xxxxxxxxxxxv@gmail.com,Cornell University,No,Txx,Mitxxxxx,xxxxxxxxxxxl@cs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shaxxxxx,Srixxxxxxx,Carnegie Mellon,,,001-3xxxxxxxxxxx,,,xxxxxxxxgmail.com,,,,,United States,,Shaxxxxx Srixxxxxxx;Igxx Labxxxx;Txx Mitxxxxx,xxxxxxxxgmail.com;xxxxxxxxxxxov@gmail.com;xxxxxxxxxxxxl@cs.cmu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
642,642X-P6F8D3J3D6,Real Time Semantic Events Detection from Social Media Stream,phyxxxxx khxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Real time monitoring of twitter tweet streams for events has popularity in the
last decade. This provide effective in-formation for government, business and
other organization to know what hap-pening right now. The task comprise many
challenges including the pro-cessing of large volume of data in real time and
high levels of noise. The main objective of this work is timely detection of
semantic bursty trends which have happened recently and discovery of their
evolutionary patterns along the timeline. We present semantic burst de-tection
in adaptive time windows and then retrieve evolutionary patterns of burst over
time period. Burst is the task of finding unexpected change of some quantity in
real time tweet stream. More-over burst is highly depend on the sam-pled time
window size and threshold values. Thus we propose how to adjust time windows
sizes and threshold values for burst detection in real time.  To get accurate
burst from real time twitter stream, semantic words and phrase ex-traction from
noise polluted text stream is proposed. Our experimental results show that
these semantic burst detection in adaptive time windows is efficient and
effectiveness for processing in both real time data stream and offline data
stream.",7 Feb 2017 05:19:44 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",sentiment analysis;  information extraction;  information retrieval;  distributional similarity;  statistical machine translation;  semantic relations;  text mining;  document summarization;  multi-document summarization;  term extraction;  temporal/spatial information extraction;  document mining,phyxxxxx,khxxxx,xxxxxxxxxxxxxg07@gmail.com,"University of Computer Studies, Mandalay",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,phyxxxxx,khxxxx,"University of Computer Studies, Mandalay",,,,,,xxxxxxxxxxxxxg07@gmail.com,,,,,Myanmar,,phyxxxxx khxxxx and alxx,xxxxxxxxxxxxxg07@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
643,643X-P3G5D2C4H3,CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles,Chexxxxxx Waxx;Alxx Akxxx;Laxxx Chixxxxxxx;Yuxxxx Lx;Fxx Xxx and Anxxxx X,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Crowdsourcing has proven to be an effective method for generating labeled data
for a range of NLP tasks. However, multiple recent attempts of using
crowdsourcing to generate gold-labeled training data for semantic role labeling
(SRL) reported only modest results, indicating that SRL is perhaps too
difficult a task to be effectively crowdsourced. In this paper, we postulate
that while producing SRL annotation does require expert involvement in general,
a large subset of SRL labeling tasks is in fact appropriate for the crowd. We
present a novel workflow in which we employ a classifier to identify difficult
annotation tasks and route each task either to experts or crowd workers
according to their difficulty. Our experimental evaluation shows that our
proposed approach reduces the workload for experts by over two-thirds, and thus
significantly reduces the cost of producing SRL annotation at little loss in
quality.",7 Feb 2017 09:24:29 GMT,Empirical/Data-Driven,Semantics,NLP applications;  corpus annotation methods;  semantic role labelling,Chexxxxxx,Waxx,xxxxxxxxxxxang@ibm.com,IBM Research,No,Alxx,Akxxx,xxxxxxxxs.ibm.com,IBM Research,No,Laxxx,Chixxxxxxx,xxxxxxxx.ibm.com,IBM Research,No,Yuxxxx,Lx,xxxxxxxxxus.ibm.com,IBM Research,No,Fxx,Xxx,xxxxxuw.edu,University of Washington,No,Anxxxx,Xx,xxxxxxxxxus.ibm.com,IBM Research,No,,,,,,,,,,,,,,,,,,,,,,Chexxxxxx,Waxx,Amazon AI,,,,,,xxxxxxxxxx@gmail.com,,,CA,,United States,,Chexxxxxx Waxx;Alxx Akxxx;Laxxx Chixxxxxxx;Yuxxxx Lx;Fxx Xxx;Anxxxx Xx,xxxxxxxxxxxang@ibm.com;xxxxxxxxxs.ibm.com;xxxxxxxxs.ibm.com;xxxxxxxxxxus.ibm.com;xxxxxxuw.edu;xxxxxxxxxxus.ibm.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
644,644X-J3A5P5G7H8,Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees,Arxxx Katxxxx and Clxxxx Caxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"We present a novel attention-based recurrent neural network for joint
extraction of entity mentions and relations. We show that attention along with
long short term memory (LSTM) network can extract semantic relations between
entity mentions without having access to dependency trees. 
Experiments on Automatic Content Extraction (ACE) corpora show that our model
significantly outperforms feature-based joint model by Li and Ji (2014). We
also compare our model with an end-to-end tree-based LSTM model (SPTree) by
Miwa and Bansal (2016) and show that our model performs within 1\% on entity
mentions and 2\% on relations. Our fine-grained analysis also shows that our
model performs significantly better on Agent-Artifact relations, while SPTree
performs better on Physical and Part-Whole relations.",23 Apr 2017 05:40:54 GMT,Applications/Tools,"Information extraction, text mining, and question answering",,Arxxx,Katxxxx,xxxxxxxxrnell.edu,Cornell University,No,Clxxxx,Caxxxx,xxxxxxxxxxcornell.edu,Cornell University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Arxxx,Katxxxx,Cornell University,,,607xxxxxxx,,,xxxxxxxxrnell.edu,,Ithaca,NY,,United States,,Arxxx Katxxxx;Clxxxx Caxxxx,xxxxxxxxrnell.edu;xxxxxxxxxxxcornell.edu,Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees,Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees,12,,,,on,,No. Do not include my submission in this dataset.,No,None,None
645,645X-G5J3E6P8C8,Zero-annotation OCR Post-correction with Bidirectional Long Short-Term Memory Networks,Exx D'hxxxx;Cyxxx Grxxxx and Brixxxxx Gxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper we present a novel approach to the automatic correction of
OCR-induced orthographical errors in a given text. While current OCR error
detection and correction systems depend heavily on large training corpora or
external information, such as domain-specific lexicons or confidence scores
from the OCR process, our system only requires a small amount of (relatively)
clean training data from a representative corpus to learn a character-based
statistical language model using Bidirectional Long Short-Term Memory Networks
(biLSTMs). We demonstrate the versatility and adaptability of our system on
different text corpora with varying degrees of textual noise, including two
real-life OCR corpora in the folklore and medical domain.",7 Feb 2017 11:47:32 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  NLP on noisy unstructured text;  adaptation to noisy data,Exx,D'hxxxx,xxxxxxxxxt@limsi.fr,LIMSI-CNRS,No,Cyxxx,Grxxxx,xxxxxxxxxxin@limsi.fr,LIMSI-CNRS,No,Brixxxxx,Grxx,xxxxxmsi.fr,CNRS-LIMSI & ENSIIE,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Exx,D'hxxxx,LIMSI-CNRS,,,,,,xxxxxxxxxt@limsi.fr,,Orsay,Île-de-France,,France,,Exx D'hxxxx;Cyxxx Grxxxx;Brixxxxx Grxx,xxxxxxxxxt@limsi.fr;xxxxxxxxxxxin@limsi.fr;xxxxxxmsi.fr,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
646,646X-C6D6D4A9F7,Learning to Solve General Arithmetic Problems Using World Knowledge,Arixxxx Mixxx and Chxxxx Baxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Solving general arithmetic word problems is one of the challenges in Natural
Language Understanding. To solve such problem one needs background knowledge
which is often missing in the text. This paper describes various types of such
knowledge that are useful for solving arithmetic word problems. It then
presents a noble learning algorithm which allows the machine to use such
knowledge to create the equation. Unlike the existing algorithms which predict
the equation in a single step, our system makes a sequence of decision to find
the correct equation. The system is trained with conditional log-linear model
and gradient descent and achieves state-of-the-art performance on two standard
datasets.",7 Feb 2017 09:48:12 GMT,Applications/Tools,Semantics,information retrieval;  text mining;  parsing;  relation/event extraction;  semantic knowledge induction;  question answering in restricted domains,Arixxxx,Mixxx,xxxxxxx@asu.edu,Arizona State University,No,Chxxxx,Baxxx,xxxxxxxasu.edu,Arizona State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Arixxxx,Mixxx,Arizona State University,,,,,,xxxxxxx@asu.edu,,,,,United States,,Arixxxx Mixxx;Chxxxx Baxxx,xxxxxxx@asu.edu;xxxxxxx@asu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
648,648X-C9H6H8G3P3,Multi-Task Learning of Speaker-Role-Based Neural Conversation Models,Yx Luxx;Mixxxx Gaxxxx;Chxxx Broxxxxx;Jiaxxxxx Gxx and Bixx Doxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Building a persona-based conversation agent is challenging due to the lack of
large amounts of personal conversation data for model training. 
This paper addresses the challenge by proposing a multi-task learning approach
to training neural conversation models that leverages both conversation data
across speakers and other types of data pertaining to the individual speaker to
be modeled, such as email, blog posts, and diaries. 
Experiments show that our approach leads to significant improvements over the
baseline in model quality, generating responses that are more personal,
capturing more precisely the speaker’s traits and characters.",10 Feb 2017 05:59:21 GMT,Empirical/Data-Driven,Dialog and interactive systems,spoken language  generation,Yx,Luxx,xxxxxx@uw.edu,University of Washington,No,Mixxxx,Gaxxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Chxxx,Broxxxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,"Microsoft Research, Redmond",No,Bixx,Doxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yx,Luxx,University of Washington,,,,,,xxxxxx@uw.edu,,,WA,,United States,,Yx Luxx;Mixxxx Gaxxxx;Chxxx Broxxxxx;Jiaxxxxx Gxx;Bixx Doxxx,xxxxxx@uw.edu;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxcrosoft.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
649,649X-J6B6P3G4G9,Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses,Ryxx Loxx;Micxxxx Nosxxxxxxx;Iuxxxx Vlxx;Nicxxxx Angelxxxxxxxxxxx;Yoxxxx Bexxxx and Joxxxx Pixxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Wednesday,,Undecided (Dialog Interactive Systems),"Automatically evaluating the quality of dialogue responses for unstructured
domains is a challenging problem.  Unfortunately, existing automatic evaluation
metrics are biased and correlate very poorly with human judgements of response
quality (Liu et al., 2016). Yet having an accurate automatic evaluation
procedure is crucial for dialogue research, as it allows rapid prototyping and
testing of new models with fewer expensive human evaluations. In response to
this challenge, we formulate automatic dialogue evaluation as a learning
problem.We present an evaluation model (ADEM)that learns to predict human-like
scores to input responses, using a new dataset of human response scores.   We
show that the ADEM model’s predictions correlate significantly,  and at a
level much higher than word-overlap metrics such as BLEU, with human judgements
at both the utterance and system-level. We also show that ADEM can generalize
to evaluating dialogue mod-els unseen during training,                    an
important step
for
automatic dialogue evaluation.",22 Apr 2017 02:01:59 GMT,Resources/Evaluation,Dialog and interactive systems,,Ryxx,Loxx,xxxxxxxxxxxxil.mcgill.ca,McGill University,No,Micxxxx,Nosxxxxxxx,xxxxxxxxxxxxxxxxhy@mail.mcgill.ca,McGill University,No,Iulixxxxxxx,Sexxxx,xxxxxxxxxxxn@gmail.com,University of Montreal,No,Nicxxxx,Angelxxxxxxxxxxx,xxxxxxxxxxxxxxxxxxxntier@mail.mcgill.ca,McGill University,No,Yoxxxx,Bexxxx,xxxxxxxxxxxxx@umontreal.ca,U. Montreal,No,Joxxxx,Pixxxx,xxxxxxxxxx.mcgill.ca,McGill University,No,,,,,,,,,,,,,,,,,,,,,,Ryxx,Loxx,McGill University,,,,,,xxxxxxxxxxxxil.mcgill.ca,,,,,Canada,,Ryxx Loxx;Micxxxx Nosxxxxxxx;Iuxxxx Vlxx;Nicxxxx Angelxxxxxxxxxxx;Yoxxxx Bexxxx;Joxxxx Pixxxx,xxxxxxxxxxxxil.mcgill.ca;xxxxxxxxxxxxxxxxxhy@mail.mcgill.ca;xxxxxxxxxxxan@gmail.com;xxxxxxxxxxxxxxxxxxxxntier@mail.mcgill.ca;xxxxxxxxxxxxxo@umontreal.ca;xxxxxxxxxxs.mcgill.ca,Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses,Learning to Evaluate Dialogue Responses,11,Ryan Lowe,,"McGill University, 845 rue Sherbrooke, Montreal, QC",on,,"Yes, include my submission even if the paper is rejected.",No,None,None
650,650X-H9B9E3E8A5,Characterization of Languages on Twitter from User Data,Dixxx Pacxxxx;Prxxx Saxx and Ronxxxx Mexxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Online social networks (e.g., Twitter) offer an open platform for people to
interact and connect without restrictions of language usage or geographic
borders. Because of their pervasiveness, online social networks provide data
and become real-time sensors of society.
This work looks at Twitter to reveal the hidden relationship of languages that
stems from users' language preference for writing their tweets. The
characterization of how languages are clustered is key to the understanding of
information spread in social media.",7 Feb 2017 09:27:13 GMT,Empirical/Data-Driven,Social media,MT evaluations;  MT applications;  information extraction;  multilingual applications;  information retrieval;  multimodal representations and processing;  document mining;  adaptation to noisy data;  document clustering;  social network,Dixxx,Pacxxxx,xxxxxxxxxxxxxomplexlab.org,Florida Institute of Technology,No,Prxxx,Saxx,xxxxxxxxxxxmplexlab.org,Florida Institute of Technology,No,Ronxxxx,Menxxxx,xxxxxxxxxcs.fit.edu,Florida Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dixxx,Pacxxxx,Florida Institute of Technology,,,,,,xxxxxxxxxxxxxomplexlab.org,,,,,United States,,Dixxx Pacxxxx;Prxxx Saxx;Ronxxxx Menxxxx,xxxxxxxxxxxxxomplexlab.org;xxxxxxxxxxxxmplexlab.org;xxxxxxxxxxcs.fit.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
651,651X-H4C9A6P9G3,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,Kaxxxx Hasxxxxxx;caixxxx xixxx;Yosxxxxxx Tsuxxxxx and Ricxxxx Soxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Transfer and multi-task learning have traditionally focused on either a single
source-target pair or very few, similar tasks.
Ideally, the linguistic levels of morphology, syntax and semantics would
benefit each other by being trained in a single model.
We introduce a joint many-task model together with a strategy for successively
growing its depth to solve increasingly complex tasks.
All layers include shortcut connections to both word representations and
lower-level task predictions.
We use a simple regularization term to allow for optimizing all model weights
to improve one task's loss without exhibiting catastrophic interference of the
other tasks.
Our single end-to-end model obtains state-of-the-art results on chunking,
dependency parsing, semantic relatedness and textual entailment.
It also performs competitively on POS tagging.",10 Feb 2017 04:50:03 GMT,Empirical/Data-Driven,Machine learning,experimental evaluation/comparison of ML methods;  parsing;  text classification,Kaxxxx,Hasxxxxxx,xxxxxxxxxxxxx.u-tokyo.ac.jp,University of Tokyo,No,caixxxx,xixxx,xxxxxxxxxxng@ucla.edu,Metamind,No,Yosxxxxxx,Tsuxxxxx,xxxxxxxxxxxxxxxt.u-tokyo.ac.jp,University of Tokyo,No,Ricxxxx,Soxxxx,xxxxxxxxxxxesforce.com,Salesforce Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxxxx,Hasxxxxxx,Salesforce Research,,,,,,xxxxxxxxxxxxx.u-tokyo.ac.jp,,,,,United States,,Kaxxxx Hasxxxxxx;caixxxx xixxx;Yosxxxxxx Tsuxxxxx;Ricxxxx Soxxxx,xxxxxxxxxxxxx.u-tokyo.ac.jp;xxxxxxxxxxxng@ucla.edu;xxxxxxxxxxxxxxx.t.u-tokyo.ac.jp;xxxxxxxxxxxlesforce.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
652,652X-D8C3D3C9B6,Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance,Yaxx Xx and Daxxx Reixxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Accept - Oral Tuesday,,Undecided (Cognitive Modelling and Psycholinguistics),"We propose a perspective on dialogue that focuses on relative information
contributions of conversation partners as a key to successful communication. We
predict the success of collaborative task in English and Danish corpora of
task-oriented dialogue. Two features are extracted from the frequency domain
representations of  the lexical entropy series of each interlocutor, power
spectrum overlap (PSO) and relative phase (RP). We find that PSO is a negative
predictor of task success, while RP is a positive one. An SVM with these
features significantly improved on previous task success prediction models. Our
findings suggest that the strategic distribution of information density between
interlocutors  is relevant to task success.",23 Apr 2017 01:02:48 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,,Yaxx,Xx,xxxxxxx@psu.edu,The Pennsylvania State University,No,Daxxx,Reixxxx,xxxxxxx@psu.edu,Penn State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Xx,San Diego State University,,,,,,xxxxxxdsu.edu,,State College,PA,,United States,,Yaxx Xx;Daxxx Reixxxx,xxxxxxx@psu.edu;xxxxxxxx@psu.edu,Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance,Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance,11,Yang Xu,,"College of Information Sciences and Technology
The Pennsylvania State University 
University Park, PA 16802, USA",,,No. Do not include my submission in this dataset.,No,None,None
654,654X-F8E8E4C6F3,Deep Semantic Role Labeling: What Works and What’s Next,Luxxxx Hx;Kexxxx Lxx;Mixx Lexxx and Luxx Zetxxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Tuesday,,Undecided (Semantics),"We introduce a new deep learning model for semantic role labeling (SRL) that
significantly improves the state of the art, along with detailed analyses to
reveal its strengths and limitations. We use a deep highway BiLSTM architecture
with constrained decoding, while observing a number of recent best practices
for initialization and regularization. Our 8-layer ensemble model achieves 83.2
F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative
error reduction over the previous state of the art. Extensive empirical
analysis of these gains show that (1) deep models excel at recovering
long-distance dependencies but can still make surprisingly obvious errors, and
(2) that there is still room for syntactic parsers to improve these results.",28 Jun 2017 05:22:33 GMT,Empirical/Data-Driven,Semantics,,Luxxxx,Hx,xxxxxx@uw.edu,University of Washington,No,Kexxxx,Lxx,xxxxxxxxxxxxashington.edu,University of Washington,No,Mixx,Lexxx,xxxxxxxxxx@gmail.com,Facebook AI Research,No,Luxx,Zettxxxxxxx,xxxxxxxxxxhington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Luxxxx,Hx,University of Washington,,,650-xxxxxxxx,,,xxxxxx@uw.edu,,Seattle,WA,,United States,,Luxxxx Hx;Kexxxx Lxx;Mixx Lexxx;Luxx Zettxxxxxxx,xxxxxx@uw.edu;xxxxxxxxxxxxxashington.edu;xxxxxxxxxx0@gmail.com;xxxxxxxxxxxhington.edu,Deep Semantic Role Labeling: What Works and What’s Next,Deep Semantic Role Labeling: What Works and What’s Next,11,Luheng He,,"Computer Science & Engineering
University of Washington
AC101 Paul G. Allen Center for
     Computer Science & Engineering
Box 352350
185 Stevens Way
Seattle WA 98195-2350Computer Science & Engineering
University of Washington
AC101 Paul G. Allen Center for
     Computer Science & Engineering
Box 352350
185 Stevens Way
Seattle WA 98195-2350",on,,Only include my submission if it is accepted.,No,None,None
655,655X-H3E5G7A3B3,Use WordNet to Improve Word Embedding,Junxxxx Lx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"In natural language processing(NLP) task, unsupervised models become more and
more important for create distributed word vector. Because they only need
corpus as input without additional knowledge. Futhermore, there are some
methods that even don’t need big corpus to train, just need a specific
resource, WordNet, which is a dictionary based on synsets and lexemes. By using
WordNet, we could not only get better word vector, but also synset vector and
lexeme vector, which could help us handle more NLP task. In this paper, we base
on such a word embedding method and improve it. After that we do some
experiment on word sense disambiguation. Finally, we get better result in
semantic similarity evalution.",7 Feb 2017 05:44:42 GMT,Theoretical,Semantics,,Junxxxx,Lx,xxxxxxxxee@qq.com,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Junxxxx,Lx,Peking University,,,,,,xxxxxxxxxx@pku.edu.cn,,,,,China,,Junxxxx Lx,xxxxxxxxee@qq.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
656,656X-C2G8J4H6C8,Comparative Quality Estimation for Machine Translation. Observations on machine learning and features,Elefxxxxxxx Avrxxxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"The paper presents a deeper analysis on Comparative Quality Estimation for
Machine Traslation, extending previous work with new features and machine
learning methods. We examine the trained models and provide
linguistically-intuitive explanations for the contribution of individual
features. We compete with reference-aware metrics and we indicate that simple
source features are not useful for predicting comparisons.",7 Feb 2017 06:07:27 GMT,Empirical/Data-Driven,Machine translation,MT evaluations;  MT quality control;  phrase-based SMT;  human judgments of MT;  statistical machine translation;  parsing,Elefxxxxxxx,Avrxxxxxx,xxxxxxxxxxxxxxramidis@dfki.de,German Research Center for Artificial Intelligence (DFKI),No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Elefxxxxxxx,Avrxxxxxx,German Research Center for Artificial Intelligence (DFKI),,,,,,xxxxxxxxxxxxxxxamidis@gmail.com,,Berlin,,,Germany,"Resarcher in the German Research Center for Artificial Intelligence (DFKI)
Senior Researcher in Language Technology
PhD Candidate in Machine Translation, Computational Linguistics",Elefxxxxxxx Avrxxxxxx,xxxxxxxxxxxxxxramidis@dfki.de,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
657,657X-A6P2C3P2H3,Interpreting Neural Networks to Understand Written Justifications in Values-Affirmation Essays,Trxxxx Rixxxx;Vixxx Mohxxxxxx;Smaxxxxx Murxxxx;Jonxxxxx Coxx;Gexxx Coxxx and Valxxxx Purdxxxxxxxxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),,7 Feb 2017 06:00:32 GMT,Empirical/Data-Driven,Multidisciplinary,educational applications;  experimental evaluation/comparison of ML methods,Trxxxx,Rixxxx,xxxxxxxxxxlumbia.edu,Columbia University,No,Vixxx,Mohxxxxxx,xxxxxxxxxlumbia.edu,Columbia University,No,Smaxxxxx,Murxxxx,xxxxxxxxxxxcolumbia.edu,Columbia University,No,Jonxxxxx,Coxx,xxxxxxxpsu.edu,The Pennsylvania State University,No,Gexxx,Coxxx,xxxxxxxxford.edu,Stanford University,No,Valxxxx,Purdixxxxxxxxx,xxxxxxxxxumbia.edu,Columbia University,No,,,,,,,,,,,,,,,,,,,,,,Trxxxx,Rixxxx,Columbia University,,,,,,xxxxxxxxxxlumbia.edu,,,,,United States,,Trxxxx Rixxxx;Vixxx Mohxxxxxx;Smaxxxxx Murxxxx;Jonxxxxx Coxx;Gexxx Coxxx;Valxxxx Purdixxxxxxxxx,xxxxxxxxxxlumbia.edu;xxxxxxxxxxlumbia.edu;xxxxxxxxxxxxcolumbia.edu;xxxxxxx@psu.edu;xxxxxxxxnford.edu;xxxxxxxxxlumbia.edu,,,,,,,on,,"Yes, include my submission even if the paper is rejected.",No,None,None
658,658X-D7B2C3E4E3,Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks,Abhxxxx Chakxxxxxxx;Onxxx Arxx and ixx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Poster Monday,,Undecided (Machine Learning),"We introduce a composite deep neural network architecture for supervised and
language independent context sensitive lemmatization. The proposed method
considers the task as to identify the correct edit tree representing the
transformation between a word-lemma pair. To find the lemma of a surface word,
we exploit two successive bidirectional gated recurrent structures - the first
one is used to extract the character level dependencies and the next one
captures the contextual information of the given word. The key advantages of
our model compared to the state-of-the-art lemmatizers such as Lemming and
Morfette are - (i) it is independent of human decided features (ii) except the
gold lemma, no other expensive morphological attribute is required for joint
learning. We evaluate the lemmatizer on nine languages - Bengali, Catalan,
Dutch, Hindi, Hungarian, Italian, Latin, Romanian and Spanish. It is found that
except Bengali, the proposed method outperforms Lemming and Morfette on the
other languages. To train the model on Bengali, we develop a gold lemma
annotated dataset (having 1,702 sentences with a total of 20,257 word tokens),
which is an additional contribution of this work.",23 Apr 2017 02:46:02 GMT,Empirical/Data-Driven,Machine learning,,Abhxxxx,Chakxxxxxxx,xxxxxxxxxx2@gmail.com,"Indian Statistical Institute, Kolkata",No,Onkxxxxxxx,Paxxxx,xxxxxxxxxgmail.com,"Indian Statistical Institute, Kolkata",No,Utxxx,Gaxxxx,xxxxxxxxxxxn@gmail.com,Indian Statistical Institute,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Abhxxxx,Chakxxxxxxx,"Indian Statistical Institute, Kolkata",,,9194xxxxxxxx,,,xxxxxxxxxx2@gmail.com,,,,,India,,Abhxxxx Chakxxxxxxx;Onxxx Arxx;Utxxx Gaxxxx,xxxxxxxxxx2@gmail.com;xxxxxxxxx@gmail.com;xxxxxxxxxxxin@gmail.com,Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks,Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks,11,Abhisek Chakrabarty,,"Indian Statistical Institute, 203 B.T. Road, Kolkata 700108, India",on,on,No. Do not include my submission in this dataset.,No,None,None
659,659X-D6G5P8A4E6,Using Complex Argumentative Interactions to Reconstruct the Argumentative Structure of Large-Scale Debates,Joxx Lawxxxxx and Chxxx Rexx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper we consider the insights that can be gained by considering large
scale argument networks and the complex interactions between their constituent
propositions. We investigate metrics for analysing properties of these
networks, illustrating these using a corpus of arguments taken from the 2016 US
Presidential Debates. We present techniques for determining these features
directly from natural language text and show that there is a strong correlation
between these automatically identified features and the argumentative structure
contained within the text. Finally, we combine these metrics with argument
mining techniques and show how the identification of argumentative relations
can be improved by considering the larger context in which they occur.",7 Feb 2017 05:46:06 GMT,Applications/Tools,"Information extraction, text mining, and question answering",corpus development;  discourse;  NLP applications;  NLP on noisy unstructured text;  dialogue;  text mining;  text classification;  NLP in social networking media,Joxx,Lawxxxxx,xxxxxxxxxxxxxxxxxuting.dundee.ac.uk,University of Dundee,No,Chxxx,Rexx,xxxxxxxxxxundee.ac.uk,University of Dundee,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Joxx,Lawxxxxx,University of Dundee,,,,,,xxxxxxxxxxxxxxxxxuting.dundee.ac.uk,,,,,United Kingdom,,Joxx Lawxxxxx;Chxxx Rexx,xxxxxxxxxxxxxxxxxuting.dundee.ac.uk;xxxxxxxxxxxundee.ac.uk,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
660,660X-B7J7J7G5D4,Automatically Generating Rhythmic Verse with Neural Networks,Jaxx Hopxxxx and Doxxx Kixxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Monday,,Undecided (Generation Summarization),"We propose two novel methodologies for the automatic generation of rhythmic
poetry in a variety of forms. The first approach uses a neural language model
trained on a phonetic encoding to learn an implicit representation of both the
form and content of English poetry. This model can effectively learn common
poetic devices such as rhyme, rhythm and alliteration. The second approach
considers poetry generation as a constraint satisfaction problem where a
generative neural language model is tasked with learning a representation of
content, and a discriminative weighted finite state machine constrains it on
the basis of form. By manipulating the constraints of the latter model, we can
generate coherent poetry with arbitrary forms and themes. A large-scale
extrinsic evaluation demonstrated that participants consider machine-generated
poems to be written by humans 54% of the time. In addition, participants rated
a machine-generated poem to be the best amongst all evaluated.",21 Apr 2017 20:18:47 GMT,Empirical/Data-Driven,Generation,,Jaxx,Hopxxxx,xxxxxxxxxins@me.com,Cambridge University,No,Doxxx,Kixxx,xxxxxxxxxxxxcl.cam.ac.uk,Facebook,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Doxxx,Kixxx,Facebook,,,,,,xxxxxxxxxxxxcl.cam.ac.uk,,,NY,,United States,,Jaxx Hopxxxx;Doxxx Kixxx,xxxxxxxxxins@me.com;xxxxxxxxxxxx@cl.cam.ac.uk,Automatically Generating Rhythmic Verse with Neural Networks,Automatically Generating Rhythmic Verse with Neural Networks,11,DOUWE KIELA,,,on,on,Only include my submission if it is accepted.,No,None,None
661,661X-D6B5B9A9D6,An Effective Approach for Event Coreference Resolution by Exploiting Inter-dependencies Between Events,Praxxxxx Kuxxx and Ruixxxx Huxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We introduce a novel iterative approach to event coreference resolution that
gradually builds event clusters by exploiting inter-dependencies among event
mentions within the same chain as well as across event chains. Among event
mentions in the same chain, we distinguish within- and cross-document event
coreference links by using two distinct pairwise classifiers, trained
separately to capture differences in feature distributions of within- and
cross- document event clusters. Our  event coreference approach alternates
between WD and CD clustering and combines arguments from both event clusters
after every merge, continuing till no more merge can be made. And then it
performs further merging between event chains that are both closely related to
a set of other chains of events. Experiments on the ECB+ corpus show that our
model outperforms state-of-the-art methods for both WD and CD event coreference
resolution.",7 Feb 2017 05:49:04 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  coreference resolution,Prafuxxxxxxxxx,Choxxxx,xxxxxxxxxxxxxxey@cse.tamu.edu,"Computer Science and Engineering, Texas A&M University",No,Ruixxxx,Huxxx,xxxxxxxxxxe.tamu.edu,"Computer Science and Engineering, Texas A&M University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Prafuxxxxxxxxx,Choxxxx,"Computer Science and Engineering, Texas A&M University",,,979xxxxxxx,,,xxxxxxxxxxxxubey@tamu.edu,,College Station,TX,,United States,,Praxxxxx Kuxxx;Ruixxxx Huxxx and Engixxxxxxx Texxx,xxxxxxxxxxxxxxey@cse.tamu.edu;xxxxxxxxxxse.tamu.edu,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
662,662X-G5G3B7P5J4,A Comparison among Significance Tests and Other Feature Building Methods for Sentiment Analysis: A First Study,Raxxxx Shxxxx;Dibxxxxx Moxxxx and Pusxxxx Bhatxxxxxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Words that participate in the sentiment (positive or negative) classification
decision are known as significant words for sentiment classification.
Identification of such significant words as features from the corpus reduces
the amount of irrelevant information in the feature set under supervised
sentiment classification settings. In this paper, we conceptually study and
compare various types of feature building methods, viz., unigrams, TFIDF,
Relief, Delta-TFIDF, χ2 test and Welch’s t-test for sentiment analysis task.

We show the effectiveness of significance tests over other feature building
methods for three types of sentiment analysis tasks, viz., in-domain,
cross-domain and cross-lingual. Delta-TFIDF, χ2 test and Welch’s t-test
compute the significance of the word for classification in the corpus, unlike
unigrams, TFIDF and Relief. Furthermore, significance tests can be divided into
two categories, bag-of-words-based test and distribution-based test. In this
paper, we substantiate that the distribution-based Welch’s t-test is more
accurate than bag-of-words-based χ2 test and Delta-TFIDF in identification of
significant words from the corpus.",7 Feb 2017 05:52:36 GMT,Survey Papers,Sentiment analysis and opinion mining,sentiment analysis;  NLP applications;  lexical semantics;  cross-language information extraction;  opinion mining and extraction;  text classification,Raxxxx,Shxxxx,xxxxxxxxxx.iitb.ac.in,Indian Institute of Technology Bombay,No,Dibxxxxx,Moxxxx,xxxxxxxxxxxxxl14@gmail.com,IIT Bombay,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,"CSE Department, IIT Bombay",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Raxxxx,Shxxxx,Indian Institute of Technology Bombay,,,887xxxxxxx,,,xxxxxxxxxx.iitb.ac.in,,,,,India,,Raxxxx Shxxxx;Dibxxxxx Moxxxx;Pusxxxx Bhatxxxxxxxxx,xxxxxxxxxx.iitb.ac.in;xxxxxxxxxxxxxal14@gmail.com;xxxxxxxxxx@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
663,663X-D4A8E2B9P6,Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network,Jinxxxxx Xx;Xx Sxx and Xiaxxxx Cx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Recent works have been shown effective in using neural networks for Chinese
word segmentation. However, these models rely on large-scale data and are less
effective for low-resource datasets because of insufficient training data.
Thus, we propose a transfer learning method to improve low-resource word
segmentation by leveraging high-resource corpora. First, we train a teacher
model on high-resource corpora and then use the learned knowledge to initialize
a student model. Second, a weighted data similarity method is proposed to train
the student model on low-resource data with the help of high-resource corpora.
Finally, given that insufficient data puts forward higher requirements for
feature extraction, we propose a novel neural network which improves feature
learning. Experiment results show that our work significantly improves the
performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB
datasets. Furthermore, this paper achieves state-of-the-art results: 97.4%,
96.1%, and 96.2% F-score on MSR, PKU and CTB datasets. Besides, we explore an
asynchronous parallel method on neural word segmentation to speed up training.
The parallel method accelerates training substantially and is almost five times
faster than a serial mode.",7 Feb 2017 12:11:36 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",word segmentation,Jinxxxxx,Xx,xxxxxxxxxx@pku.edu.cn,"Department of Computer Science, School of EECS, Peking University",No,Xx,Sxx,xxxxxxxxu.edu.cn,"Department of Computer Science, School of EECS, Peking University",No,Xiaxxxx,Cxx,xxxxxxxxxxwpu.edu.cn,"School of Automation, Northwestern Polytechnical University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jinxxxxx,Xx,"Department of Computer Science, School of EECS, Peking University",,,1881xxxxxxx,,,xxxxxxxxxx@pku.edu.cn,,Beijing,Beijing,,China,,Jinxxxxx Xx;Xx Sxx;Xiaxxxx Cxx,xxxxxxxxxx@pku.edu.cn;xxxxxxxxku.edu.cn;xxxxxxxxxxnwpu.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
664,664X-D3D6G5A4G5,Discovering and Learning the Semantic Structure of Named Entities,Nixxxx Bhuxxxx;Yuxxxx Lx;Mauxxxxx Herxxxxxx;Mixxxx Vaxx and Hx Vx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Handling named entities is essential for a wide range of natural language
processing tasks. A key challenge is coping with heterogeneity in named entity
mentions: different mentions often refer to the same entity. Traditional
measures for handling such name variations largely rely on surface string
similarity that interprets each mention as a sequence of characters. However,
entity mentions often have internal semantic structures, which can be valuable
to reconcile the name variations in mentions. These semantic structures depend
on the entity type. While it is possible for an expert to specify rules to map
mentions to the semantic structures of any one specific entity type, such human
specification requires considerable effort and does not scale to the numbers of
entity types encountered in practice.

In this paper, we propose SuEntity, an active-learning based approach, to learn
both the semantic structures of any entity type of interest from its mentions,
and rules to parse the mentions to their corresponding semantic structures.
Experiments show that SuEntity can effectively learn the semantic structures
and parsing rules for types person and company in less than 15 iterations with
over 85% recall and 91% precision. Using a configurable variant generation
algorithm, we show how learning semantic structures can indeed help reconcile
name variations and improve tasks such as entity linking and relation
extraction.",7 Feb 2017 11:32:45 GMT,Empirical/Data-Driven,Semantics,multiword semantics/compositionality;  named entity disambiguation;  entity disambiguation;  parsing;  semantic knowledge induction,Nixxxx,Bhuxxxx,xxxxxxxxxumich.edu,"University of Michigan, Ann Arbor",No,Yuxxxx,Lx,xxxxxxxxxus.ibm.com,IBM Research - Almaden,No,Mauxxxxx,Herxxxxxx,xxxxxxxxxus.ibm.com,IBM Research - Almaden,No,Mixxxx,Vaxx,xxxxxxxxxxx@us.ibm.com,IBM Research - Almaden,No,H.xxx,Jagxxxxx,xxxxxxich.edu,"University of Michigan, Ann Arbor",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Nixxxx,Bhuxxxx,"University of Michigan, Ann Arbor",,,,,,xxxxxxxxxumich.edu,,,,,United States,,Nixxxx Bhuxxxx;Yuxxxx Lx;Mauxxxxx Herxxxxxx;Mixxxx Vaxx;Hx Vx,xxxxxxxxxumich.edu;xxxxxxxxxxus.ibm.com;xxxxxxxxxxus.ibm.com;xxxxxxxxxxxa@us.ibm.com;xxxxxxxich.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
665,665X-F3G4B7P6H3,Acquiring Regular Event Pairs and a Contextual Temporal Relation Classifier Simultaneously,Wexxxx Yxx;Saipxxxxxxxxx Netxxxx and Ruixxxx Huxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Capabilities of detecting temporal and causal relations between two events
benefit many applications, such as event timeline generation, script knowledge
extraction and question answering among others.
However, event temporal relations are essentially semantic relations and their
detection largely depends on understanding meanings of surrounding contexts.
Due to the limited size of temporally annotated data, supervised systems do not
sufficiently capture diverse contexts that describe temporal relations. 
We explore the observation that regular event pairs show a consistent temporal
relation despite of their various contexts and these rich contexts can be used
to train a contextual temporal relation classifier, which can further label new
contexts and identify new regular event pairs. We focus on detecting
``after/before'' temporal relations and designed a bootstrapping system that
simultaneously learn regular event pairs and a temporal relation classifier.
Evaluation shows that both the learned regular event pairs and the classifier
are accurate and they uniquely compensate with the existing temporal event pair
knowledge base of VerbOcean and a state-of-the-art supervised temporal
classifier.",7 Feb 2017 07:41:25 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",unsupervised and semi-supervised learning;  information extraction;  text mining;  text classification;  temporal/spatial information extraction;  document mining,Wexxxx,Yxx,xxxxxxxxx@tamu.edu,"Computer Science and Engineering, Texas A&M University",No,Saipxxxxxxxxx,Netxxxx,xxxxxxxx@tamu.edu,"Computer Science and Engineering, Texas A&M University",No,Ruixxxx,Huxxx,xxxxxxxxxxe.tamu.edu,"Computer Science and Engineering, Texas A&M University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wexxxx,Yxx,"Computer Science and Engineering, Texas A&M University",,,,,,xxxxxxxxx@tamu.edu,,College Station,TX,,United States,,Wexxxx Yxx;Saipxxxxxxxxx Netxxxx;Ruixxxx Huxxx and Engixxxxxxx Texxx,xxxxxxxxx@tamu.edu;xxxxxxxxx@tamu.edu;xxxxxxxxxxse.tamu.edu,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
666,666X-D9G5G3H6F2,Neural Machine Translation with Gumbel-Greedy Decoding,Jixxxx Gx;Daxxxx Jiwxxxx and Vixxxx O.xx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Previous neural machine translation models used some heuristic search
algorithms (e.g., beam search) in order to avoid solving maximum a posteriori
problem over translation sentences at test time. In this paper, we propose the
Gumbel-Greedy Decoding which trains a generative network to predict translation
under a trained  model. We solve such a problem using the Gumbel-softmax
reparameterization, which makes our generative network differentiable and
trainable through standard stochastic gradient methods. We empirically
demonstrate that our proposed model is effective for generating sequence of
discrete words.",7 Feb 2017 11:59:50 GMT,Empirical/Data-Driven,Machine learning,generative models;  MT quality control,Jixxxx,Gx,xxxxxxxxxeee.hku.hk,"Department of Electrical and Electronic Engineering, The University of Hong Kong",No,Daniexxxxxxxxx,Ix,xxxxxxxxxxxifounded.com,AIFounded Inc.,No,Victxxxxxxx,Lx,xxxxxxx.hku.hk,"Department of Electrical and Electronic Engineering, The University of Hong Kong",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixxxx,Gx,"Department of Electrical and Electronic Engineering, The University of Hong Kong",,,8525xxxxxxx,,,xxxxxxxxxeee.hku.hk,,Hong Kong,,,Hong Kong Special Administrative Region of China,"PhD Candidate in Dept. of Electrical and Electronic Engineering, HKU",Jixxxx Gx;Daxxxx Jiwxxxx;Vixxxx O.xx and Elexxxxxxx Engixxxxxxx,xxxxxxxxxeee.hku.hk;xxxxxxxxxxxxifounded.com;xxxxxxxe.hku.hk,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
667,667X-C8E8B3G4D2,Question Answering from Unstructured Text by Retrieval and Comprehension,Yuxxxx Watxxxxx;Bhxxxx Dhixxxx and Ruxxxx Salaxxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Open domain Question Answering (QA) systems must interact with external
knowledge sources, such as web pages, to find relevant information. Information
sources like Wikipedia, however, are not well structured and difficult to
utilize in comparison with Knowledge Bases (KBs). In this work we present a
two-step approach to question answering from unstructured text, consisting of a
retrieval step and a comprehension step. For comprehension, we present an RNN
based attention model with a novel mixture mechanism for selecting answers from
either retrieved articles or a fixed vocabulary. For retrieval we introduce a
hand-crafted model and a neural model for ranking relevant articles. We achieve
state-of-the-art performance on WikiMovies dataset, reducing the error by 40%.
Our experimental results further demonstrate the importance of each of the
introduced components.",7 Feb 2017 06:52:02 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",NLP on noisy unstructured text;  open-domain question answering;  question answering in restricted domains,Yuxxxx,Watxxxxx,xxxxxxxxxxxxxxbe@jp.sony.com,Sony Corporation; Carnegie Mellon University,No,Bhxxxx,Dhixxxx,xxxxxxxxxxxdrew.cmu.edu,Carnegie Mellon University,No,Ruxxxx,Salaxxxxxxxxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxxx,Watxxxxx,Amazon.com,,,,,,xxxxxxxxx@gmail.com,,,PA,,United States,,Yuxxxx Watxxxxx;Carxxxxx Mexxxx;Bhxxxx Dhixxxx;Ruxxxx Salaxxxxxxxxx,xxxxxxxxxxxxxxbe@jp.sony.com;xxxxxxxxxxxxdrew.cmu.edu;xxxxxxxxxxcs.cmu.edu,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
668,668X-G2H6F6F2C4,Towards a Map of the Syntactic Similarity of Languages,Alxxx Maxxx;Lixxx Px and Anxxxx Sgxxx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"In this paper we propose a computational method for determining the syntactic
similarity between languages. We investigate multiple approaches and metrics,
showing that the results are consistent across methods. We report results on 15
languages belonging to various language families. The analysis that we conduct
is adaptable to any languages, as far as resources are available.",7 Feb 2017 06:01:00 GMT,Empirical/Data-Driven,Multilinguality,cross-lingual approaches;  multilingual applications;  multilingual resources;  alignment,Alinxxxxxxx,Cioxxxx,xxxxxxxxxxxxxxxy.fmi.unibuc.ro,University of Bucharest,No,Livxxxxx,Dixx,xxxxxxxxxxxu@gmail.com,University of Bucharest,No,Anxxxx,Sgxxxx,xxxxxxxunits.it,University of Trieste,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alinxxxxxxx,Cioxxxx,University of Bucharest,,,,,,xxxxxxxxxxxxxxxy.fmi.unibuc.ro,,,,,Romania,,Alxxx Maxxx;Lixxx Px;Anxxxx Sgxxxx,xxxxxxxxxxxxxxxy.fmi.unibuc.ro;xxxxxxxxxxxnu@gmail.com;xxxxxxxxunits.it,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
669,669X-D5E6H8H4H3,Beyond Captions: Using Vision and Language Datasets to Detect Concreteness and Metaphor in Text,Gixxx Kexxx and Jaxxx Pustxxxxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"Vision and language datasets are becoming increasingly popular for training
algorithms to solve a wide range of problems, such as automatic image caption
generation, visual question answering, and image retrieval. In this paper we
use several major corpora, created from vision and language datasets, to solve
tasks such as metaphor classification and concreteness measurement. Our method,
involving the construction of what we call visibility embeddings, takes
advantage of the inherent visualizability properties of words in visual
corpora. Our algorithm, though simple, yields results in both tasks that are
comparable to state-of-the-art classifiers.",7 Feb 2017 05:57:43 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",figurative language,Gixxx,Kexxx,xxxxxxxxxxrandeis.edu,Brandeis University,No,Jaxxx,Pustxxxxxxx,xxxxxxxxxxxrandeis.edu,Brandeis University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gixxx,Kexxx,Brandeis University,,,,,,xxxxxxxxxxrandeis.edu,,Waltham,MA,,United States,,Gixxx Kexxx;Jaxxx Pustxxxxxxx and exx Univxxxxxxx,xxxxxxxxxxrandeis.edu;xxxxxxxxxxxbrandeis.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
670,670X-P5F3J6J3P7,A Supervised Approach To The Interpretation Of Imperative To-Do Lists,Paxx L and exx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),"To-do lists are a popular medium for personal information management. As to- do
tasks are increasingly tracked in electronic form with mobile and desktop
organizers, so does the potential for software support for the corresponding
tasks by means of software agents. While there has been work in intelligent
assistance for to-do tasks, nobody has focused on extracting specific agents
and the items to act on, as we do here ; indeed scarce data ex- ists in this
regard, so the first contribution of our work is the corpus we built. We also
show that our methods are generalizable to other tasks, such as the
classification of supersenses on the DiMSUM16 corpus1 . Our methods to classify
supersenses perform at a significantly higher accuracy than published
baselines.",7 Feb 2017 05:58:01 GMT,Applications/Tools,Multidisciplinary,NLP applications;  parsing;  text classification;  term extraction;  evaluation metrics;  semantic role labelling,Paxx,Laxxxx,xxxxxxxxxxx@fastmail.fm,University of Illinois at Chicago,No,Barxxxx,Di xxxxxxx,xxxxxxxx@uic.edu,University of Illinois at Chicago,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Paxx,Laxxxx,University of Illinois at Chicago,,,,,,xxxxxxxxxxx@fastmail.fm,,,IL,,United States,,Paxx Laxxxx;Barxxxx Dx,xxxxxxxxxxx@fastmail.fm;xxxxxxxxn@uic.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
671,671X-G3B2P6B5C7,Improving Neural Relation Extraction via a Large Crowdsourced Dataset,Yuxxx Zhxxx;Vixxxx Zhxxx;Daxxx Chxx;Gaxxx Anxxxx and Chrixxxxxxx Dx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper explores and extends state-of-the-art neural architectures for the
critical and difficult problem of relation extraction. Due to the lack of good,
large-scale, labelled data, most existing work either relies on small labelled
datasets, which hinders the development of data-driven models, such as deep
neural networks, or uses noisy distantly supervised methods. Therefore, as part
of this research, we introduce a large crowd-annotated relation extraction
dataset of 119,474 examples, building on NIST's TAC KBP evaluations. We
evaluate several neural architectures, both on this dataset and a downstream
TAC KBP slot filling task, and improve on them by proposing a novel
position-aware attention mechanism over recurrent neural networks. The
combination of better data and a better learned model, ensembled with a
pattern-based system, yields a 26.7% hop-all F1 score on the TAC KBP 2015 cold
start slot filling task, exceeding the previous state of the art by 4.5%.",7 Feb 2017 06:04:06 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction,Yuxxx,Zhxxx,xxxxxxxxxxtanford.edu,Stanford University,No,Vixxxx,Zhxxx,xxxxxxxxxxxorzhong.com,Stanford University,No,Daxxx,Chxx,xxxxxxxxxxtanford.edu,Stanford University,No,Gaxxx,Anxxxx,xxxxxxxxxanford.edu,Stanford University,No,Chrisxxxxxxxxx,Manxxxx,xxxxxxxxxxxstanford.edu,Stanford University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxx,Zhxxx,Stanford University,,,,,,xxxxxxxxxxtanford.edu,,Stanford,CA,,United States,,Yuxxx Zhxxx;Vixxxx Zhxxx;Daxxx Chxx;Gaxxx Anxxxx;Chrixxxxxxx Dx,xxxxxxxxxxtanford.edu;xxxxxxxxxxxtorzhong.com;xxxxxxxxxxxtanford.edu;xxxxxxxxxxanford.edu;xxxxxxxxxxxxstanford.edu,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
672,672X-B2C9B6H8E8,Parallel Deep Genetic Programming for Weak Sentiment Detection},Ixx Chaxxxxxxx; x and rx Cavxxxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Weak sentiments are opinions that are neither strongly positive nor strongly
negative. These are difficult to classify in product reviews due to the
presence of ‘neutral’ phrases and, hence, require a large training corpus
that is usually unavailable in resource-deficient domains and languages. In
this paper, we employ parallel deep genetic programming to classify product
reviews into different intensities of sentiments. This is achieved by grouping
training sentences based on highest activation at a particular neuron. The
proposed method outperforms the accuracy of baselines by 20% and is several
times faster.",7 Feb 2017 06:20:35 GMT,Empirical/Data-Driven,Machine learning,sentiment analysis;  domain adaptation;  cross-language information extraction,Ixx,Chaxxxxxxx,xxxxxxx.edu.sg,NTU,No,Saxxxx,Cavxxxxxx,xxxxxxxxxxx.ntu.edu.sg,NTU,No,Erxx,Camxxxx,xxxxxxxxxxdia.mit.edu,Nanyang Technological University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ixx,Chaxxxxxxx,NTU,,,,,,xxxxxxx.edu.sg,,,,,,,Ixx Chaxxxxxxx;Saxxxx Cavxxxxxx;Erxx Camxxxx,xxxxxxx.edu.sg;xxxxxxxxxxxe.ntu.edu.sg;xxxxxxxxxxxdia.mit.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
673,673X-P8C9J4P3G6,Coarse-to-Fine Question Answering for Long Documents,Euxxxx Chxx;Daxxxx Hewxxxx;Jaxxx Uszxxxxxx;Ilxxx Polxxxxxxx; Axxx and rx Lacxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"We present a framework for question answering that can efficiently scale to
longer documents while maintaining or even improving performance of
state-of-the-art models. While most successful approaches for reading
comprehension rely on recurrent neural networks (RNNs), running them over long
documents is prohibitively slow because it is difficult to parallelize over
sequences. Inspired by how people first skim the document, identify relevant
parts, and carefully read these parts to produce an answer, we combine a
coarse, fast model for selecting relevant sentences and a more expensive RNN
for producing the answer from those sentences.
We treat sentence selection as a latent variable trained jointly from the
answer only using reinforcement learning. Experiments demonstrate
state-of-the-art performance on a challenging subset of the WikiReading dataset
and on a new dataset, while speeding up the model by 3.5x-6.7x.",22 Apr 2017 18:34:06 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Euxxxx,Chxx,xxxxxxxxxxxxshington.edu,University of Washington,No,Daxxxx,Hewxxxx,xxxxxxxxxgoogle.com,Google,No,Jaxxx,Uszxxxxxx,xxxxxxxxxkoreit.net,"Google, Inc.",No,Ilxxx,Polxxxxxxx,xxxxxxxxxxx@google.com,Google,No,Alexxxxxx,Lacxxxx,xxxxxxxxogle.com,Google,No,Jonxxxxx,Bexxxx,xxxxxxxxxxs.tau.ac.il,Tel Aviv University,No,,,,,,,,,,,,,,,,,,,,,,Euxxxx,Chxx,University of Washington,,,206xxxxxxx,,,xxxxxxxxxxxxshington.edu,,,,,United States,,Euxxxx Chxx;Daxxxx Hewxxxx;Jaxxx Uszxxxxxx;Ilxxx Polxxxxxxx;Alexxxxxx Lacxxxx;Jonxxxxx Bexxxx,xxxxxxxxxxxxshington.edu;xxxxxxxxxxgoogle.com;xxxxxxxxxxkoreit.net;xxxxxxxxxxxn@google.com;xxxxxxxxoogle.com;xxxxxxxxxxxs.tau.ac.il,Coarse-to-Fine Question Answering for Long Documents,Coarse-to-Fine Question Answering for Long Documents,12,Eunsol Choi,,Eunsol Choi University of Washington,,,No. Do not include my submission in this dataset.,No,None,None
674,674X-P4D4C9C8D9,A Machine Reading Comprehension Dataset for Natural Language Understanding Research,Txx Ngxxxx;Mxx Rosxxxxxx;Xxx Soxx;Jiaxxxxx Gxx;Sauxxxx Tixxxx;Raxxxx Majxxxxx and Lx Dxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper presents our work on the design and development of a new, large
scale dataset, for MAchine Reading COmprehension (MARCO). This new dataset is
aimed to overcome a number of well-known weaknesses of previous publicly
available datasets for the same task of reading comprehension and question
answering. In the MARCO dataset, all questions are sampled from real,
anonymized user queries. The context passages, from which answers in the
dataset are derived, are extracted from real web documents using the most
advanced version of a commercial web search engine. The answers to the queries
are human generated. Finally, a subset of these queries has multiple answers.
We aim to release one million queries and the corresponding answers in the
dataset, which, to the best of our knowledge, is the most comprehensive
real-world dataset of its kind in both quantity and quality. We are currently
releasing 100,000 queries with their corresponding answers to inspire work in
reading comprehension and question answering along with gathering feedback from
the research community.",7 Feb 2017 06:11:41 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",corpus development;  multiword semantics/compositionality;  interactive question answering;  Web mining;  information extraction;  scalability and portability of question answering systems;  NLP on noisy unstructured text;  information retrieval;  context-aware question answering;  answer extraction;  experimental evaluation/comparison of ML methods;  chunking;  parsing;  question interpretation;  NLP on Wikipedia and other collaboratively constructed resources;  evaluation methods for dialogues;  unsupervised and semi-supervised learning;  NLP applications;  language generation;  sentence simplification;  corpus annotation methods;  learning with small datasets;  document summarization;  multi-document summarization;  collaborative methods for question answering;  open-domain question answering;  evaluation metrics;  document mining;  query logs,Txx,Ngxxxx,xxxxxxxxxxcrosoft.com,Microsoft AI & Research,No,Mxx,Rosxxxxxx,xxxxxxxxxxcrosoft.com,Microsoft,No,Xxx,Soxx,xxxxxxxxxrosoft.com,Microsoft AI & Research,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,Microsoft AI & Research,No,Sauxxxx,Tixxxx,xxxxxxxxxxxcrosoft.com,Microsoft AI & Research,No,Raxxxx,Majxxxxx,xxxxxxxxxxcrosoft.com,Microsoft AI & Research,No,Lx,Dexx,xxxxxxxxxosoft.com,Microsoft AI & Research,No,,,,,,,,,,,,,,,,,Mxx,Rosxxxxxx,Microsoft,,,,,,xxxxxxxxxxcrosoft.com,,,,,United States,,Txx Ngxxxx;Mxx Rosxxxxxx;Xxx Soxx;Jiaxxxxx Gxx;Sauxxxx Tixxxx;Raxxxx Majxxxxx;Lx Dexx,xxxxxxxxxxcrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxrosoft.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
675,675X-F7B9P8H7P2,Knowledge Base Embedding via Mining and Encoding Implicit Knowledge,Tinxxxxx Jixxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Knowledge base (KB) embedding aims to embed each entity and relation into a
low-dimensional space. Most existing methods only consider defined relations
between entities, i.e., explicit facts, for training, which are obviously
incomplete and inadequate. In this paper, we propose that the implicit
knowledge including missing facts and missing relations between relevant
entities should be considered. We use the automatically learned logical rules
to add missing facts and missing relations between entities for KB embedding
and unify the process in a bootstrapping paradigm. By densifying the KB and
considering more complete knowledge, we propose an extended KB embedding model
to incorporate both explicit and implicit facts.
Empirical experiments show that our proposed method is able to capture implicit
knowledge effectively and the new generated embedding space becomes
semantically smooth, outperforming the state-of-the-art methods significantly.",7 Feb 2017 06:11:56 GMT,Empirical/Data-Driven,Machine learning,relational Learning;  relation/event extraction,Tinxxxxx,Jixxx,xxxxxxxxxpku.edu.cn,"Institute of Computational Linguistics,Peking University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tinxxxxx,Jixxx,"Institute of Computational Linguistics,Peking University",,,,,,xxxxxxxxxpku.edu.cn,,,,,China,,Tinxxxxx Jixxx,xxxxxxxxxpku.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
676,676X-F2F2B6F9A7,Neural Machine Translation via Binary Code Prediction,Yuxxxx Oxx;Phxxxx Arxxxx;Grxxxx Nexxxx;Koixxxxx Yosxxxx and Satxxxx Nakxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Oral Tuesday,,Undecided (Machine Translation),"In this paper, we propose a new method for calculating the output layer in
neural machine translation systems. The method is based on predicting a
binary code for each word and can reduce computation time/memory requirements
of the output layer to be logarithmic in vocabulary size in the best case.
In addition, we also introduce two advanced approaches to improve the
robustness of the proposed model: using error-correcting codes and combining
softmax and binary codes. Experiments on two English-Japanese bidirectional
translation tasks show proposed models achieve BLEU scores that approach the
softmax, while reducing memory usage to the order of less than 1/10 and
improving decoding speed on CPUs by x5 to x10.",23 Apr 2017 11:45:50 GMT,Empirical/Data-Driven,Machine translation,,Yuxxxx,Oxx,xxxxxxxxxxxxx9@is.naist.jp,Nara Institute of Science and Technology,No,Phxxxx,Arxxxx,xxxxxxxxxxxxxxom0@is.naist.jp,Nara Institute of Science and Technology,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Koixxxxx,Yosxxxx,xxxxxxxxxxs.naist.jp,Nara Institute of Science and Technology,No,Satxxxx,Nakxxxxx,xxxxxxxxxxxis.naist.jp,Nara Institute of Science and Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxxx,Oxx,Google,,,,,,xxxxxxxxxx@gmail.com,,,,,Japan,,Yuxxxx Oxx;Phxxxx Arxxxx;Grxxxx Nexxxx;Koixxxxx Yosxxxx;Satxxxx Nakxxxxx and  Tecxxxxxxxx,xxxxxxxxxxxxx9@is.naist.jp;xxxxxxxxxxxxxxxom0@is.naist.jp;xxxxxxxxxcs.cmu.edu;xxxxxxxxxxis.naist.jp;xxxxxxxxxxx@is.naist.jp,Neural Machine Translation via Binary Code Prediction,Neural Machine Translation via Binary Code Prediction,11,Yusuke Oda,,"Nara Institute of Science and Technology, 8916-5 Takayama-cho, Ikoma, Nara 630-0192, Japan.",on,on,Only include my submission if it is accepted.,No,None,None
677,677X-P6H2C2F4P2,Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction,Chuxxxxx Zhxx and Grxxxx Nexxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Monday,,Undecided (Machine Learning),"Labeled sequence transduction is a task of transforming one sequence into
another sequence that satisfies desiderata specified by a set of labels. In
this paper we propose multi-space variational encoder-decoders, a new model for
labeled sequence transduction with semi-supervised learning. The generative
model can use neural networks to handle both discrete and continuous latent
variables to exploit various features of data. Experiments show that our model
provides not only a powerful supervised framework but also can effectively take
advantage of the unlabeled data. On the SIGMORPHON morphological inflection
benchmark, our model outperforms single-model state-of-art results by a large
margin for the majority of languages.",22 Apr 2017 20:37:59 GMT,Empirical/Data-Driven,Machine learning,,Chuxxxxx,Zhxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Grxxxx,Nexxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chuxxxxx,Zhxx,Carnegie Mellon University,,,,,,xxxxxxxxs.cmu.edu,,,PA,,United States,,Chuxxxxx Zhxx;Grxxxx Nexxxx,xxxxxxxxs.cmu.edu;xxxxxxxxxcs.cmu.edu,Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction,Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction,11,Chunting Zhou,,"Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA, USA",on,on,No. Do not include my submission in this dataset.,No,None,None
678,678X-F6H5D4D5J6,"Utilizing Lexical Similarity for pivot translation involving resource-poor, related languages",Anxxx Kuncxxxxxxxx;Maxxxx Shxx;Praxxxx Praxxxx and Pusxxxx Bhatxxxxxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"We investigate the use of pivot languages for phrase-based statistical machine
translation (PB-SMT) between related languages with limited parallel corpora.
We show that subword-level pivot translation via a related pivot language is:
(i) highly competitive with the best direct translation model and (ii) better
than a pivot model which uses an unrelated pivot language, but has at its
disposal large parallel corpora to build the source-pivot (S-P) and
pivot-target (P-T) translation models. In contrast, pivot models trained at
word and morpheme level are far inferior to their direct counterparts. We also
show that using multiple related pivot languages can outperform a direct
translation model. Thus, the use of subwords as translation units coupled with
the use of multiple related pivot languages can compensate for the lack of a
direct parallel corpus. Subword units make pivot models competitive by (i)
utilizing lexical similarity to improve the underlying S-P and P-T translation
models, and (ii) reducing loss of translation candidates during pivoting.",7 Feb 2017 12:45:14 GMT,Empirical/Data-Driven,Machine translation,phrase-based SMT;  MT for gisting purposes;  statistical machine translation,Anxxx,Kuncxxxxxxxx,xxxxxxxxxx.iitb.ac.in,IIT Bombay,No,Maxxxx,Shxx,xxxxxxxxxxxxxse.iitb.ac.in,IIT Bombay,No,Praxxxx,Praxxxx,xxxxxxxxmail.com,IIT Bombay,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,"CSE Department, IIT Bombay",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxxx,Kuncxxxxxxxx,IIT Bombay,,,986xxxxxxx,,,xxxxxxxxxx.iitb.ac.in,,,,,India,,Anxxx Kuncxxxxxxxx;Maxxxx Shxx;Praxxxx Praxxxx;Pusxxxx Bhatxxxxxxxxx,xxxxxxxxxx.iitb.ac.in;xxxxxxxxxxxxxcse.iitb.ac.in;xxxxxxxxgmail.com;xxxxxxxxxx@gmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
679,679X-C6D5C3J3J6,Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification,Raxxxx Shxxxx;Pusxxxx Bhatxxxxxxxxx; x and ipxx D,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Getting manually labeled data in each domain is always an expensive and a time
consuming task. Cross-domain sentiment analysis has emerged as a demanding
concept where a labeled source domain facilitates a sentiment classifier for an
unlabeled target domain. However, polarity orientation (positive or negative)
and the significance of a word to express an opinion often differ from one
domain to another domain. Owing to these differences, cross-domain sentiment
classification is still a challenging task. In this paper, we propose that
words that do not change their polarity and significance represent the
transferable (usable) information across domains for cross-domain sentiment
classification. We present a novel approach based on χ2 test and
cosine-similarity between context vector of words to identify polarity
preserving significant words across domains. Furthermore, we show that a
weighted ensemble of the classifiers enhances the cross-domain classification
performance.",7 Feb 2017 07:01:22 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  lexical semantics;  domain adaptation;  opinion mining and extraction;  distributional similarity,Raxxxx,Shxxxx,xxxxxxxxxx.iitb.ac.in,Indian Institute of Technology Bombay,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,"CSE Department, IIT Bombay",No,Sanxxxxx,Danxxxxx,xxxxxxxxxxxxxapat@xerox.com,Xerox Research Centre India,No,Himanxxxxxxxxxx,Bhxxx,xxxxxxxxxxxu@gmail.com,Xerox Research Center India,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Raxxxx,Shxxxx,Indian Institute of Technology Bombay,,,887xxxxxxx,,,xxxxxxxxxx.iitb.ac.in,,,,,India,,Raxxxx Shxxxx;Pusxxxx Bhatxxxxxxxxx;Sanxxxxx Danxxxxx;Himxxxxx Shxxxx,xxxxxxxxxx.iitb.ac.in;xxxxxxxxxx@gmail.com;xxxxxxxxxxxxxxapat@xerox.com;xxxxxxxxxxxhu@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
680,680X-A9C9B9C6J6,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,Bixxxx Yaxx and Txx Mitxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Monday,,Undecided (IE QA Text Mining Applications),"This paper focuses on how to take advantage of external knowledge bases (KBs)
to improve recurrent neural networks for machine reading. Traditional methods
that exploit knowledge from KBs encode knowledge as discrete indicator
features. Not only do these features generalize poorly, but they require
task-specific feature engineering to achieve good performance. We propose
KBLSTM, a novel neural model that leverages continuous representations of KBs
to enhance the learning of recurrent neural networks for machine reading. To
effectively integrate background knowledge with information from the currently
processed text, our model employs an attention mechanism with a sentinel to
adaptively decide whether to attend to background knowledge and which
information from KBs is useful. Experimental results show that our model
achieves accuracies that surpass the previous state-of-the-art results for both
entity extraction and event extraction on the widely used ACE2005 dataset.",23 Apr 2017 00:37:52 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Bixxxx,Yaxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Txx,Mitxxxxx,xxxxxxxxxxxl@cs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bixxxx,Yaxx,Carnegie Mellon University,,,607xxxxxxx,,,xxxxxxxxs.cmu.edu,,Pittsburgh,PA,,United States,,Bixxxx Yaxx;Txx Mitxxxxx,xxxxxxxxs.cmu.edu;xxxxxxxxxxxxl@cs.cmu.edu,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,11,Bishan Yang,,"Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA 15213",on,on,No. Do not include my submission in this dataset.,No,None,None
681,681X-J6D6P7D9J4,Mining Crowd Intelligence towards Clinical Treatments: Predicting the Severity from Medical Blogs,Shxxxx Yaxxx;Amxx Shxxx;Asxx Ekxxx;Srixxxxx Saxx and Pusxxxx Bhatxxxxxxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Open discussion groups such as medical forums are enriched with a variety of
user posts that can serve as the mirror to understand the blogger’s
state-of-mind. We present here a very first study to analyze the potential of
social media to categorize the posts on the basis of severity of health
conditions. We examine users on popular medical forums (Patient.info) where
they post contents on important topics such as depression, anxiety, allergy,
and asthma. At first, we provide a benchmark setup for the task by crawling
data, defining the target and annotating dataset. Thereafter, we propose an
efficient architecture that uses Convolutional Neural Network (CNN) as a
data-driven feature extractor and Support Vector Machine (SVM) as a classifier
instead of the conventional full-connection classifier utilizing Softmax
function. We further augment medical sentiment features to the extracted
features of CNN. Our evaluation shows high correlation with the actual severity
measure.",7 Feb 2017 07:16:06 GMT,Applications/Tools,Social media,corpus development;  sentiment analysis;  opinion mining and extraction;  subjectivity analysis;  text mining;  text classification;  biomedical text mining;  NLP in the blogosphere;  social network,Shxxxx,Yaxxx,xxxxxxxxxxx4@iitp.ac.in,Indian Institute of Technology Patna,No,Amxx,Shxxx,xxxxxxxxxx@gmail.com,Wright State University Ohio,No,Asxx,Ekxxx,xxxxxxxxxx@gmail.com,Indian Institute of Technology Patna,No,Srixxxxx,Saxx,xxxxxxxxxxxha@gmail.com,Indian Institute of Technology Patna,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,Indian Institute of Technology Patna,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Yaxxx,Indian Institute of Technology Patna,,,,,,xxxxxxxxxxx4@iitp.ac.in,,Patna,Bihar,,India,,Shxxxx Yaxxx;Amxx Shxxx;Asxx Ekxxx;Srixxxxx Saxx;Pusxxxx Bhatxxxxxxxxx,xxxxxxxxxxx4@iitp.ac.in;xxxxxxxxxxh@gmail.com;xxxxxxxxxxl@gmail.com;xxxxxxxxxxxxha@gmail.com;xxxxxxxxxx@gmail.com,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
682,682X-G8D6D9H6J8,Generating Visually Descriptive Language from Abstract Scene Layouts,Xuxxxx Yxx and Vicxxxx Ordxxxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"Generating visually descriptive language for images usually involves coupling
two challenging problems: visual recognition, and language generation. In this
paper, we present an approach to generate natural language descriptions of
images from abstract human-annotated scene layouts instead of image pixels,
thus bypassing the need for visual recognition, and exploring an alternative
problem that focuses on semantics. We present an end-to-end trainable model
that generates visually descriptive text given an input set of object
annotations. Experiments with our model in the task of image captioning from
scene layouts show that (1) semantically meaningful descriptions can be
inferred only from scene layouts even in the absence of object appearance
information, and is comparable to a model that uses pixel data (2) object
localization and object counting are important to generate better descriptions,
(3) choice of entry-level nouns in the language generation process greatly
affects the quality of generated descriptions.",7 Feb 2017 07:53:57 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",language generation,Xuxxxx,Yxx,xxxxxxxxxginia.edu,University of Virginia,No,Vicxxxx,Ordxxxx,xxxxxxxxxxrginia.edu,University of Virginia,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vicxxxx,Ordxxxx,University of Virginia,,,,,,xxxxxxxxxxrginia.edu,,Charlottesville,VA,,United States,,Xuxxxx Yxx;Vicxxxx Ordxxxx,xxxxxxxxxginia.edu;xxxxxxxxxxirginia.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
683,683X-D7C8D4C6F6,Inducing Premise-Conclusion Topic Models for Argument Mining,Joxx Lawxxxxx and Chxxx Rexx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper presents a method of processing natural language text, by leveraging
high-precision, low-recall techniques in order to automatically build a large
corpus of inferential statements related to the text's topic. These statements
are then used to produce a matrix representing the inferential relationship
between different aspects of the topic. From this matrix, we are able to
determine connectedness and directionality of inference between statements in
the original text.",7 Feb 2017 06:31:36 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",sentiment analysis;  discourse;  information extraction;  learning with small datasets;  opinion mining and extraction;  dialogue;  text classification;  relation/event extraction,Joxx,Lawxxxxx,xxxxxxxxxxxxxxxxxuting.dundee.ac.uk,University of Dundee,No,Chxxx,Rexx,xxxxxxxxxxundee.ac.uk,University of Dundee,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Joxx,Lawxxxxx,University of Dundee,,,,,,xxxxxxxxxxxxxxxxxuting.dundee.ac.uk,,,,,United Kingdom,,Joxx Lawxxxxx;Chxxx Rexx,xxxxxxxxxxxxxxxxxuting.dundee.ac.uk;xxxxxxxxxxxundee.ac.uk,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
684,684X-H2E5H2G8P3,Gated-Attention Readers for Text Comprehension,Bhxxxx Dhixxxx;Hanxxxx Lxx;Zhxxxx Yaxx;Wilxxxx Coxxx and Ruxxxx Salaxxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"In this paper we study the problem of answering cloze-style questions over
documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop
architecture with a novel attention mechanism, which is based on multiplicative
interactions between the query embedding and the intermediate states of a
recurrent neural network document reader. This enables the reader to build
query-specific representations of tokens in the document for accurate answer
selection. The GA Reader obtains state-of-the-art results on three benchmarks
for this task--the CNN \& Daily Mail news stories and the Who Did What dataset.
The effectiveness of multiplicative interaction is demonstrated by an ablation
study, and by comparing to alternative compositional operators for implementing
the gated-attention.",21 Apr 2017 18:43:28 GMT,Applications/Tools,"Information extraction, text mining, and question answering",,Bhxxxx,Dhixxxx,xxxxxxxxxxxdrew.cmu.edu,Carnegie Mellon University,No,Hanxxxx,Lxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Zhxxxx,Yaxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Wilxxxx,Coxxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Ruxxxx,Salaxxxxxxxxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Bhxxxx,Dhixxxx,Carnegie Mellon University,,,,,,xxxxxxxxxxxdrew.cmu.edu,,,,,United States,,Bhxxxx Dhixxxx;Hanxxxx Lxx;Zhxxxx Yaxx;Wilxxxx Coxxx;Ruxxxx Salaxxxxxxxxx,xxxxxxxxxxxdrew.cmu.edu;xxxxxxxxxxcs.cmu.edu;xxxxxxxxxcs.cmu.edu;xxxxxxxxxs.cmu.edu;xxxxxxxxxxcs.cmu.edu,Gated-Attention Readers for Text Comprehension,GA Reader for Text Comprehension,15,Bhuwan Dhingra,,"Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA-15213, USA",on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
685,685X-A7C6P5D5B4,Multimodal Dialogs (MMD): A large-scale dataset for studying multimodal domain-aware conversations,Amxxxx Saxx;Mixxxx Mx and Karxxxx Sankaxxxxxxxxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Owing to the success of deep learning techniques for tasks such as Q/A and
text-based dialog, there is an increasing demand for AI agents in several
domains such as retail, travel, entertainment, etc. that can carry on
multimodal conversations with humans employing both text and images within a
dialog
seamlessly. However, deep learning research is this area has been limited
primarily due to the lack of availability of large-scale, open conversation
datasets. To overcome this bottleneck, in this paper we introduce the task of
multimodal, domain-aware conversations, and propose the MMD benchmark dataset
towards this task. This dataset was gathered by working in close coordination
with large number of domain experts in the retail domain and consists of over
1.5 Million conversation sessions between shoppers and sales agents. 
With this dataset, we propose 5 new sub-tasks for multimodal conversations
along with their evaluation methodology. We also propose two novel multimodal
deep learning models in the encode-attend-decode paradigm and demonstrate their
performance on two of the sub-tasks, namely text response generation and best
image response selection. These experiments serve to establish baseline
performance numbers and open new directions of research for each of these
sub-tasks.",7 Feb 2017 11:45:29 GMT,Resources/Evaluation,Dialog and interactive systems,dialogue;  multimodal communication;  multimodal representations and processing;  contex modeling for dialogues,Amxxxx,Saxx,xxxxxxxxxxx87@gmail.com,IBM Research India,No,Mitxxxxxx,Khxxxx,xxxxxxxxxxx.iitm.ac.in,Indian Institute of Technology Madras,No,Karxxxx,Sankaxxxxxxxxxxx,xxxxxxxxxin.ibm.com,IBM Research India,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Amxxxx,Saxx,IBM Research,,,,,,xxxxxxxxxxx87@gmail.com,,,,,India,,Amxxxx Saxx;Mixxxx Mx;Karxxxx Sankaxxxxxxxxxxx,xxxxxxxxxxx87@gmail.com;xxxxxxxxxxxe.iitm.ac.in;xxxxxxxxxxin.ibm.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
686,686X-H3E7B4G6D3,Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique,Shxxxx Rijxxxxx;Roxxx Seqxxxxx;Monxxxx Choxxxxxx;Kaxxxx Baxx and  xx,Multilingual,Omxx Abxxx;Moxx Dixx,Accept - Poster Tuesday,,Undecided (Multilingual),"Word-level language detection is necessary for analyzing code-switched text,
where multiple languages could be mixed within a sentence. Existing models are
restricted to code-switching between two specific languages and fail in
real-world scenarios as text input rarely has a priori information on the
languages used. We present a novel unsupervised word-level language detection
technique for code-switched text for an arbitrarily large number of languages,
which does not require any manually annotated training data. Our experiments
with tweets in seven languages show a 74% relative error reduction in
word-level labeling with respect to competitive baselines. We then use this
system to conduct a large-scale quantitative analysis of code-switching
patterns on Twitter, both global as well as region-specific, with 58M tweets.",23 Apr 2017 07:53:04 GMT,Applications/Tools,Multilinguality,,Shxxxx,Rijxxxxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Roxxx,Seqxxxxx,xxxxxxxxxxwaterloo.ca,University of Waterloo,No,Monxxxx,Choxxxxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Kaxxxx,Baxx,xxxxxxxxxxcrosoft.com,Microsoft Research Labs,No,Chandxxxxxxxxxx,Madxxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxxxx,Rijxxxxx,Carnegie Mellon University,,,,,,xxxxxxxxxcs.cmu.edu,,Pittsburgh,PA,,United States,,Shxxxx Rijxxxxx;Roxxx Seqxxxxx;Monxxxx Choxxxxxx;Kaxxxx Baxx; xx and rx Shexxxx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxxxwaterloo.ca;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxxicrosoft.com,Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique,Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique,12,Shruti Rijhwani,,,on,on,No. Do not include my submission in this dataset.,No,None,None
687,687X-F4D3B3F7C6,Thank Goodness! A Measure for Sentence Fluency,S and exx Matxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"This paper describes a method that we propose to calculate the sentence fluency
of student essays. We do this by looking at other essays of a similar score.
Our approach is a way of first scoring individual words and phrases of the
essay - a property called goodness - and using those scores to predict the
overall sentence fluency score of the essay. We compare our approach to finding
the sentence fluency of the essay with other approaches, such as looking at the
errors in the essay (like badly constructed sentences, and spelling mistakes),
the complexity of the essay, etc. We compare our approach to solve the problem
of sentence fluency with language modeling  - a technique which is used to
handle fluency in automatic evaluation of machine translation output - to show
that our system performs significantly better, as well as an earlier work by
Chae and Nenkova (2009) to show an improvement in predicting the sentence
fluency of essays.",7 Feb 2017 12:02:20 GMT,Resources/Evaluation,Resources and evaluation,NLP applications;  NLP on noisy unstructured text;  educational applications;  text classification;  evaluation metrics,Sanxxxx,Matxxxx,xxxxxxxxxxxxep@gmail.com,IIT Bombay,No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,"CSE Department, IIT Bombay",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sanxxxx,Matxxxx,IIT Bombay,,,,,,xxxxxxxxxxxxep@gmail.com,,Mumbai,Maharashtra,,India,,Sanxxxx Matxxxx;Pusxxxx Bhatxxxxxxxxx,xxxxxxxxxxxxep@gmail.com;xxxxxxxxxx@gmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
688,688X-H9E9J3C4H4,Seeking General Rules for Online Deception Detection Refueled by Large-scale Data Collection,Wexxxx Yxx;Zexx Dxx;Ruixxxx Huxxx and Jaxxx Cavxxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Deceptive opinion spam detection is difficult because deception makers can
target various objects and domains (e.g., commercial products or services)
using dynamic language in order to describe distinct objects.
We seek general rules for online deception detection that perform well across
domains. However, one major obstacle to this research is the lack of large and
rich data sets. We first introduce a data collection method based on social
network analysis that can quickly identify a large set of deceptive and
truthful online reviews. 
The dataset contains more than 10,000 deceptive reviews and is diverse in
product domains as well as reviewers. Using the dataset, we reveal
generalizable features for deception detection including two novel features
modeling advertising language and syntactic complexities. We demonstrate that
with generalizable features, we can achieve further performance gains by
training with additional deceptive reviews from assorted domains. Furthermore,
our pilot experiments show that personalities of reviewers should also be
considered in general rules for deception detection.",7 Feb 2017 08:28:17 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,Web mining;  opinion mining and extraction;  text mining;  text classification;  social network,Wexxxx,Yxx,xxxxxxxxx@tamu.edu,"Computer Science and Engineering, Texas A&M University",No,Zexx,Dxx,xxxxxxxxx@tamu.edu,"Department of Computer Science and Engineering, Texas A&M University",No,Ruixxxx,Huxxx,xxxxxxxxxxe.tamu.edu,"Computer Science and Engineering, Texas A&M University",No,Jaxxx,Cavxxxxx,xxxxxxxx@tamu.edu,Texas A&M University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wexxxx,Yxx,"Computer Science and Engineering, Texas A&M University",,,,,,xxxxxxxxx@tamu.edu,,College Station,TX,,United States,,Wexxxx Yxx;Zexx Dxx;Ruixxxx Huxxx;Jaxxx Cavxxxxx,xxxxxxxxx@tamu.edu;xxxxxxxxxu@tamu.edu;xxxxxxxxxxse.tamu.edu;xxxxxxxxx@tamu.edu,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
689,689X-B3G5F8H7E4,"""Hey Dude"" or ""Dear Sir""?: Modeling Lexical Formality for Downstream Language Generation Applications",Xixx Nxx;Marxxxxx Marxxxxxxx and Maxxxx Caxxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Quantifying stylistic variations of language, such as formality, is crucial for
capturing or conveying speakers' non-literal intention computationally. We
propose a novel lexical formality model that places words on a formality scale
by projecting them along a learned formality direction in a vector space model
of word meaning. Not only does our method consistently yield high performance
on intrinsic formality assessment tasks, but we also show its effectiveness at
controlling the formality level when generating language, in a              machine
translation application.",7 Feb 2017 08:53:55 GMT,Empirical/Data-Driven,Discourse and pragmatics,language generation;  human judgments of MT;  lexical paraphrasing;  pragmatics,Xixx,Nxx,xxxxxxxxxs.umd.edu,"University of Maryland, College Park",No,Marxxxxx,Marxxxxxxx,xxxxxxxx@umd.edu,"University of Maryland, College Park",No,Maxxxx,Carxxxx,xxxxxxxxs.umd.edu,University of Maryland,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xixx,Nxx,University of Maryland,,,,,,xxxxxxxxxs.umd.edu,,,,,United States,,Xixx Nxx;Marxxxxx Marxxxxxxx;Maxxxx Carxxxx and ,xxxxxxxxxs.umd.edu;xxxxxxxxd@umd.edu;xxxxxxxxxs.umd.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
690,690X-G5B9A3H9P3,Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification,Jiaxxxxxx Waxx and Mxx Lxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"We present a novel multi-task attention-based neural network model to address
implicit discourse relationship representation and identification through two
types of representation learning, i.e., discourse relationship representation
learning between two discourse arguments and multi-task learning between
annotated and unannotated corpora. The extensive experiments have been
performed on benchmark corpora, i.e., PDTB 2.0 and CoNLL-2016 datasets.
Experimental results show that our proposed model outperforms the
state-of-the-art systems on benchmark corpora.",7 Feb 2017 11:58:08 GMT,Theoretical,Discourse and pragmatics,discourse,Jiaxxxxxx,Waxx,xxxxxxxxx62@ecnu.cn,East China Normal University,No,Mxx,Lxx,xxxxxxxxxcnu.edu.cn,East China Normal University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mxx,Lxx,East China Normal University,,,(86)1xxxxxxxxxx,,,xxxxxxxxxcnu.edu.cn,,ShangHai,,,China,,Jiaxxxxxx Waxx;Mxx Lxx,xxxxxxxxx62@ecnu.cn;xxxxxxxxxxcnu.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
691,691X-B7G3P3F3E4,Ontology-Aware Token Embeddings for Prepositional Phrase Attachment,Praxxxx Daxxxx;Waxxxx Amxxx;Chxxx Dyxx and Edxxxx Hxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"Type-level word embeddings use the same set of parameters to represent all
instances of a word regardless of its context, ignoring the inherent lexical
ambiguity in language. Instead, we embed semantic concepts (or synsets) as
defined in WordNet and represent a word token in a particular context by
estimating a distribution over relevant semantic concepts. We use the new,
context-sensitive embeddings in a model for predicting prepositional phrase
(PP) attachments and jointly learn the concept embeddings and model parameters.
We show that using context-sensitive embeddings improves the accuracy of the PP
attachment model by 5.4% absolute points, which amounts to a 34.4% relative
reduction in errors.",23 Apr 2017 02:41:02 GMT,Empirical/Data-Driven,Semantics,,Praxxxx,Daxxxx,xxxxxxxxxs.cmu.edu,"Language Technologies Institute, Carnegie Mellon University",No,Waxxxx,Amxxx,xxxxxxxxxxxr@gmail.com,Allen Institute for Artificial Intelligence,No,Chxxx,Dyxx,xxxxxxxxogle.com,Google DeepMind,No,Edxxxx,Hoxx,xxxxxxmu.edu,CMU,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Praxxxx,Daxxxx,"Language Technologies Institute, Carnegie Mellon University",,,,,,xxxxxxxxxs.cmu.edu,,,,,United States,,Praxxxx Daxxxx;Waxxxx Amxxx;Chxxx Dyxx;Edxxxx Hoxx,xxxxxxxxxs.cmu.edu;xxxxxxxxxxxar@gmail.com;xxxxxxxxoogle.com;xxxxxxcmu.edu,Ontology-Aware Token Embeddings for Prepositional Phrase Attachment,Ontology-Aware Token Embeddings for Prepositional Phrase Attachment,10,Pradeep Dasigi,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
692,692X-G5B8B3J9C8,The Timing of Lexical Memory Retrievals in Language Production,Jexxxx Coxx and Daxxx Reixxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"This paper explores the time course of lexical memory retrieval by modeling
fluent language production. The duration of retrievals is predicted using the
ACT-R cognitive architecture. In a large-scale observational study of a spoken
corpus, we find that language production at a time point preceding a word is
sped up or slowed down depending on activation of that word. This computational
analysis has consequences for the theoretical model of language production. 
The data point to interference between lexical and phonological stages as well
as a quantifiable buffer for lexical information.",7 Feb 2017 07:08:47 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,,Jexxxx,Coxx,xxxxxxxpsu.edu,Penn State,No,Daxxx,Reixxxx,xxxxxxx@psu.edu,Penn State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jexxxx,Coxx,Penn State,,,,,,xxxxxxxpsu.edu,,,,,United States,,Jexxxx Coxx;Daxxx Reixxxx,xxxxxxxpsu.edu;xxxxxxxx@psu.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
693,693X-H6D3P8J2C4,Towards Understanding and Answering Multi-Sentence Questions on Community Forums,Daxxxx Conxxxxxxx;Baxxx Paxxx;Maxxxx - and Paxxx Sixxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We introduce one of the first systems to directly answer questions on community
forums. Since many forum questions have multiple sentences, we propose the
novel task of multi-sentence question understanding (MSQU), aimed at converting
a forum question into machine representation. We define an expressive open
semantic representation that makes minimal assumptions about the domain and
supports traditional operators like subset, negation, disjunction and
conjunction. We use a semi-supervised adaptation of Constrained Conditional
Model (CCM), trained over expert and partiallylabeled crowdsourced data for
this task. We demonstrate the usefulness of MSQU by directly answering tourism
forum questions – we report double the accuracy in answering questions as
compared to competitive baselines.",7 Feb 2017 12:38:50 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  learning with small datasets;  question interpretation;  open-domain question answering;  question answering in restricted domains,Daxxxx,Conxxxxxxx,xxxxxxxxxin.ibm.com,"IBM Research & Indian Institute of Technology, New Delhi",No,Baxxx,Paxxx,xxxxxxxxxxxxe.iitd.ac.in,"Indian Institute of Technology, New Delhi",No,Maxxxx,-,xxxxxxxxxxxxshington.edu,"Indian Institute of Technology, Delhi",No,Paxxx,Sixxxx,xxxxxxxxxx.iitd.ac.in,IIT Delhi,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxxx,Conxxxxxxx,"IBM Research & Indian Institute of Technology, New Delhi",,,9111xxxxxxxx,,,xxxxxxxxxin.ibm.com,,,,,India,,Daxxxx Conxxxxxxx;Baxxx Paxxx;Maxxxx -;Paxxx Sixxxx,xxxxxxxxxin.ibm.com;xxxxxxxxxxxxse.iitd.ac.in;xxxxxxxxxxxxashington.edu;xxxxxxxxxxx.iitd.ac.in,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
694,694X-J9F4F4F9J6,Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task,Omxx Baxxxx and exx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"Understanding common entities and their attributes is a primary requirement for
any system that comprehends natural language. In order to enable learning about
common entities, we introduce a novel machine comprehension task, GuessTwo:
given a short paragraph comparing different aspects of two real-world
semantically-similar entities, a system should guess what those entities are.
Accomplishing this task requires deep language understanding which enables
inference, connecting each comparison paragraph to different levels of
knowledge about world entities and their attributes. So far we have
crowdsourced a dataset of more than 14K comparison paragraphs comparing
entities from a variety of categories such as fruits and animals. We have
designed two schemes for evaluation: open-ended, and binary-choice prediction.
For benchmarking further progress in the task, we have collected a set of
paragraphs as the test set on which human can accomplish the task with an
accuracy of 94.2\% on open-ended prediction. We have implemented various models
for tackling the task, ranging from semantic-driven to neural models. The
semantic-driven approach outperforms the neural models, however, the results
indicate that the task is very challenging across the models.",22 Apr 2017 20:57:26 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",,Omxx,Bakhxxxxxxx,xxxxxxxxxxxchester.edu,University of Rochester,No,Jaxxx,Alxxx,xxxxxxxihmc.us,University of Rochester,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Omxx,Bakhxxxxxxx,University of Rochester,,,,,,xxxxxxxxxxxchester.edu,,,AL,,United States,,Omxx Bakhxxxxxxx;Jaxxx Alxxx,xxxxxxxxxxxchester.edu;xxxxxxx@ihmc.us,Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task,Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task,11,Omid Bakhshandeh,,"University of Rochester, Rochester, NY",on,on,No. Do not include my submission in this dataset.,No,None,None
695,695X-H4J2E3H9J9,Inference on Syntactic and Semantic Structures for Machine Comprehension,Chexxxx Lx and Yuaxxxx Wx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this work, we focus on hidden variable models for the machine comprehension
task. Different the independence assumption in previous work, we capture the
dependencies among latent factors based on syntactic and semantic structures.
The answers are extracted according to structured inference on parse trees and
semantic frames. Experiments on MCTest dataset demonstrate that the proposed
models are highly competitive with state-of-the-art machine comprehension
systems.",7 Feb 2017 13:05:21 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",structured input/output;  open-domain question answering,Chexxxx,Lx,xxxxxxxx@163.com,East China Normal University,No,Yuaxxxx,Wx,xxxxxxxxxxxdu@gmail.com,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuaxxxx,Wx,,,,,,,xxxxxxxxxxxdu@gmail.com,,,,,China,,Chexxxx Lx;Yuaxxxx Wx,xxxxxxxx@163.com;xxxxxxxxxxxxdu@gmail.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
696,696X-G4D9F8H7B4,Siamese LSTM with 2D CNN for Similar Question Retrieval in cQA,Avixxxx Kamxxxxx;Haxxxx Yexxxx;Maxxxx Shrixxxxxxx and Maxxx Chixxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"There is a shift from using traditional search engines for finding answers to
getting more personalized answers from experts with the growing Community
Question Answering (cQA) forums like Yahoo! Answers, StackOverflow, Quora etc.
The word-based query matching techniques will not be useful with growing type
of more specific queries. And also, The time delay after a user post a question
and it gets answered, is high. The challenge here is current similarity metrics
used mostly take lexical or syntactical similarity into account, not
considering the semantic information. In this paper, we propose two novel
approaches of ”Siamese LSTM for cQA (SLcQA)”, ”Two-dimensional (2D)
convolution neural network for cQA (2DcQA)” to find the semantic similarity
between new question and existing question. SLcQA has twin LSTM networks with
shared parameters and contrastive loss function joining them, whereas 2DcQA has
a single network with (2D) (feature) convolution between the two questions. We
also showcase a hybrid approach of ”2D-SLcQA”, taking advantages of Siamese
LSTM network and two-dimensional feature convolution for finding similarity
between query and related query. 
Experiments on large scale real world Yahoo Answers dataset show that
SLcQA,2DcQA, 2D-SLcQA outperform the state of the art approaches based on topic
models, translation, deep learning and Siamese cQA approaches. In the two
approaches, 2DcQA improves over Siamese techniques, which shows that 2DCNN
learns significant semantic information which can be used for textual
similarity problems.",7 Feb 2017 09:41:34 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  information retrieval;  open-domain question answering,Avixxxx,Kamxxxxx,xxxxxxxxxxx4@gmail.com,"International Institute of Information Technology, Hyderabad",No,Haxxxx,Yexxxx,xxxxxxxxxxxxxxxxsearch.iiit.ac.in,Student,No,Maxxxx,Shrixxxxxxx,xxxxxxxxxxxxa@iiit.ac.in,International Institute of Information Technology Hyderabad,No,Maxxx,Chinxxxxxxx,xxxxxxxxxxxxxotla@gmail.com,Microsoft,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Avixxxx,Kamxxxxx,"International Institute of Information Technology, Hyderabad",,,,,,xxxxxxxxxxx4@gmail.com,,,,,India,,Avixxxx Kamxxxxx;Haxxxx Yexxxx;Maxxxx Shrixxxxxxx;Maxxx Chinxxxxxxx,xxxxxxxxxxx4@gmail.com;xxxxxxxxxxxxxxxxxsearch.iiit.ac.in;xxxxxxxxxxxxva@iiit.ac.in;xxxxxxxxxxxxxxotla@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
697,697X-E6B4E3P3P6,"StalemateAvoider, Looking ""Backward"" and Looking ""Forward"": Towards a Two-Step Proactive Human-Computer Conversation System",Rxx Yxx;Donxxxx Zhxx and  Wexxxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"For a long time, people work on human-computer conversation systems which are
responsible to retrieve (or synthesize) a response given the human utterance
(i.e., a query), and follow human's lead to the best of their capabilities.
This passive conversation style is not enough to mimic human behaviors: in a
human-human conversation, both counterparts are proactive. In this paper, we
propose a 2-step proactive conversation style, ranking responses with next
utterances. Given a user query, the system returns a response and anticipates
next utterances by looking backward into past conversation contents and looking
forward into future information. The system is based on a co-ranking model
given a tripartite graph, which consists of previous utterances, candidate
responses, and next utterances. Experiments show that the new conversation
style is effective for the human-computer conversation, and it is beneficial to
be proactive. The new system outperforms baselines in terms of p@1, MAP and
nDCG metrics.",7 Feb 2017 11:56:40 GMT,Applications/Tools,Dialog and interactive systems,NLP applications;  dialogue,Rxx,Yxx,xxxxxxxxxxu@gmail.com,Peking University,No,Donxxxx,Zhxx,xxxxxxedu.cn,Peking University,No,Wexxxx,E,xxxxxxedu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rxx,Yxx,Peking University,,,,,,xxxxxxxxxxu@gmail.com,,,,,China,Assistant professor in Peking University,Rxx Yxx;Donxxxx Zhxx;Wexxxx E,xxxxxxxxxxu@gmail.com;xxxxxx.edu.cn;xxxxxx.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
698,698X-E7J5E8J8B2,Enhancing Generalization: Augmenting Semantic Parsing with Vector Space Representations,Kazxxxx Kasxxxxxx;Nicxxxx Luxxxx and Chxxxx Baxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Systems which translate natural language sentences to a formal language must
often handle the challenges posed by unseen words. One popular technique used
by translation systems to address these challenges is called generalization. In
this paper, we seek to improve the existing generalization algorithm for two
translation systems, NL2KR and JNL2KR, with the use of vector space
representations of words. We compare three popular algorithms for training word
vector representations utilizing WordNet, GloVe and Word2Vec, and evaluate how
each approach improves the accuracy NL2KR and JNL2KR. Our generalization
algorithm incorporating word vector representations and applied to NL2KR and
JNL2KR results in state-of-the-art performance on the GeoQuery250, BioKR and
Jobs corpora. We find that while utilizing word vector representations improves
the overall translation accuracy, there are differences in how the vector
representations perform depending on the corpus.",7 Feb 2017 07:30:14 GMT,Applications/Tools,Semantics,NLP applications;  parsing;  semantic knowledge induction,Kazxxxx,Kasxxxxxx,xxxxxxxx@asu.edu,Arizona State University,No,Nicxxxx,Luxxxx,xxxxxxx@asu.edu,Arizona State University,No,Chxxxx,Baxxx,xxxxxxxasu.edu,Arizona State University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kazxxxx,Kasxxxxxx,Arizona State University,,,,,,xxxxxxxx@asu.edu,,Tempe,AZ,,United States,,Kazxxxx Kasxxxxxx;Nicxxxx Luxxxx;Chxxxx Baxxx,xxxxxxxx@asu.edu;xxxxxxxx@asu.edu;xxxxxxx@asu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
699,699X-C2D9F3P8P3,Deep Keyphrase Generation,Rxx Mexx;Sanxxxxx Zhxx;Shuxxxxx Hxx;Daxxxx Hx;Pexxx Brusxxxxxxx and Yx Cx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Tuesday,,Undecided (IE QA Text Mining Applications),"Keyphrase provides highly-summative information that can be effectively used
for understanding, organizing and retrieving text content. Though previous
studies have provided many workable solutions for automated keyphrase
extraction, they commonly divided the to-be-summarized content into multiple
text chunks, then ranked and selected the most meaningful ones. These
approaches could neither identify keyphrases that do not appear in the text,
nor capture the real semantic meaning behind the text. We propose a generative
model for keyphrase prediction with an encoder-decoder framework, which can
effectively overcome the above drawbacks.  We name it as \textit{deep keyphrase
generation} since it attempts to capture the deep semantic meaning of the
content with a deep learning method. Empirical analysis on six datasets
demonstrates that our proposed model not only achieves a significant
performance boost on extracting keyphrases that appear in the source text, but
also can generate absent keyphrases based on the semantic meaning of the text.
Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.",23 Apr 2017 04:28:22 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Rxx,Mexx,xxxxxxxxgmail.com,University of Pittsburgh,No,Sanxxxxx,Zhxx,xxxxxxxitt.edu,University of Pittsburgh,No,Shuxxxxx,Hxx,xxxxxxxxxxg@gmail.com,University of Pittsburgh,No,Daxxxx,Hx,xxxxxxxxxs.pitt.edu,University of Pittsburgh,No,Pexxx,Brusxxxxxxx,xxxxxxxxxs.pitt.edu,University of Pittsburgh,No,Yx,Cxx,xxxxxxxitt.edu,University of Pittsburgh,No,,,,,,,,,,,,,,,,,,,,,,Rxx,Mexx,University of Pittsburgh,,,412xxxxxxx,,,xxxxxxxitt.edu,,,,,United States,,Rxx Mexx;Sanxxxxx Zhxx;Shuxxxxx Hxx;Daxxxx Hx;Pexxx Brusxxxxxxx;Yx Cxx,xxxxxxxxgmail.com;xxxxxxxpitt.edu;xxxxxxxxxxxg@gmail.com;xxxxxxxxxxs.pitt.edu;xxxxxxxxxxs.pitt.edu;xxxxxxxpitt.edu,Deep Keyphrase Generation,Deep Keyphrase Generation,11,Rui Meng,,"School of Computing and Information, University of Pittsburgh, 135 N Bellefield Ave, Pittsburgh, PA 15260",on,on,Only include my submission if it is accepted.,No,None,None
700,700X-P5C3F7H3C5,Diversity driven attention model for query-based abstractive summarization,Prexxxx Nexx;Mixxxx Mx;Anixxxx Laxx and Balxxxxxx Ravxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Tuesday,,Undecided (Generation Summarization),"Abstractive summarization aims to generate a shorter version of the document
covering all the salient points in a compact and coherent fashion. On the other
hand, query-based summarization highlights those points that are relevant in
the context of a given query. The encode-attend-decode paradigm has achieved
notable success in machine translation, extractive summarization, dialog
systems, etc. But it suffers from the drawback of generation of repeated
phrases. In this work we propose a model for the query-based summarization task
based on the encode-attend-decode paradigm with two key additions (i) a query
attention model (in addition to document attention model) which learns to focus
on different portions of the query at different time steps (instead of using a
static representation for the query) and (ii) a new diversity based attention
model which aims to alleviate the problem of repeating phrases in the summary.
In order to enable the testing of this model we introduce a new query-based
summarization dataset building on debatepedia. Our experiments show that with
these two additions the proposed model clearly outperforms vanilla
encode-attend-decode models with a gain of 28\% (absolute) in ROUGE-L scores.",23 Apr 2017 07:05:47 GMT,Empirical/Data-Driven,Summarization,,Prexxxx,Nexx,xxxxxxxxxxxxil.iitm.ac.in,Indian Institute of Technology Madras,No,Mitxxxxxx,Khxxxx,xxxxxxxxxxx.iitm.ac.in,Indian Institute of Technology Madras,No,Anixxxx,Laxx,xxxxxxxxxin.ibm.com,IBM Research India,No,Balxxxxxx,Ravxxxxxx,xxxxxxxxxiitm.ac.in,Indian Institute of Technology Madras,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mitxxxxxx,Khxxxx,Indian Institute of Technology Madras,,,,,,xxxxxxxxxxx.iitm.ac.in,,,,,India,,Prexxxx Nexx;Mixxxx Mx;Anixxxx Laxx;Balxxxxxx Ravxxxxxx,xxxxxxxxxxxxil.iitm.ac.in;xxxxxxxxxxxe.iitm.ac.in;xxxxxxxxxxin.ibm.com;xxxxxxxxxxiitm.ac.in,Diversity driven attention model for query-based abstractive summarization,Diversity driven attention model for query-based abstractive summarization,10,Mitesh M Khapra,,,on,on,No. Do not include my submission in this dataset.,No,None,None
701,701X-P9G5B3D4J4,Including New Patterns to Improve Event Extraction Systems,Kxx Cxx;Xixxx Lx and Weixxxxx M,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Event Extraction (EE) is a challenging Information Extraction task which aims
to discover event triggers with specific types and their arguments. Most recent
research on Event Extraction relies on pattern-based or feature-based
approaches, trained on annotated corpora, to recognize combinations of event
triggers, arguments, and other contextual information. However, as the event
in- stances in the ACE corpus are not evenly distributed, some frequent
expressions involving ACE event triggers do not appear in the training data,
adversely affecting the performance. In this paper, we demonstrate the
effectiveness of systematically importing expert-level patterns from TABARI to
boost EE performance. The experimental results demonstrate that our
pattern-based system with the expanded patterns can achieve 69.8% (with 1.9%
absolute improvement) F-measure over the baseline, an advance over current
state-of-the-art systems.",7 Feb 2017 07:29:56 GMT,Survey Papers,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction,Kxx,Cxx,xxxxxxx.nyu.edu,New York University,No,Xixxx,Lx,xxxxxxxxxs.nyu.edu,New York University,No,Weixxxxx,Mx,xxxxxbu.edu,Boston University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kxx,Cxx,New York University,,,347xxxxxxx,,,xxxxxxx.nyu.edu,,New York,NY,,United States,,Kxx Cxx;Xixxx Lx;Weixxxxx Mx,xxxxxxx.nyu.edu;xxxxxxxxxcs.nyu.edu;xxxxxxbu.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
703,703X-C7B9C5H9P6,Idea density for predicting Alzheimer's disease from transcribed speech,Kaxxxx Sixxx and Maxx Johxxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"Idea Density (ID) measures the rate at which ideas or elementary predications
are expressed in an utterance or in a text. Lower ID is found to be associated
with an increased risk of developing Alzheimer's disease (AD) Snowdon et al.,
1996; Engelman et al., 2010). ID has been realised in two different versions.
Propositional idea density (PID) counts distinct ideas expressed and can be
applied to any text. Semantic idea density (SID) counts pre-defined information
content units and is naturally more applicable to normative domains, such as
picture description tasks. In this paper, we conduct the first comparison of
automatically extracted PID and SID in the diagnostic classification task on
two different AD datasets, covering both closed-topic and free-recall domains.
While SID performs better on the normative dataset, adding PID leads to a small
but significant improvement (+1.5% F-score). On the free-topic dataset, we find
that PID performs better than SID as expected (77.6 vs 72.4 in F-score), but
adding the features derived from the word embedding clustering underlying the
automatic SID increase the results on the free-topic dataset considerably,
leading to an F-score of 84.8%.",7 Feb 2017 10:07:16 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,NLP applications;  biomedical text mining,Kaxxxx,Sixxx,xxxxxxxxxrts@ut.ee,University of Tartu,No,Maxx,Johxxxx,xxxxxxxxxxxn@MQ.edu.au,Macquarie University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxxxx,Sixxx,University of Tartu,,,,,,xxxxxxxxxrts@ut.ee,,,,,Estonia,,Kaxxxx Sixxx;Maxx Johxxxx,xxxxxxxxxrts@ut.ee;xxxxxxxxxxxon@MQ.edu.au,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
704,704X-J6D7F3A3P7,One Sentence One Model for Neural Machine Translation,Xiaxxxxx Lx;Jixxxx Zhxxx and Chexxxxxx Zxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation (NMT) becomes a new state of the art and achieves
promising translation performance using a simple encoder-decoder neural
network. This neural network is trained once on the parallel corpus and the
fixed network is used to translate all the test sentences. We argue that the
general fixed network parameters cannot best fit each specific testing
sentence. In this paper, we propose the dynamic NMT which learns a general
network as usual, and then fine-tunes the network for each test sentence. The
fine-tune work is done on a small set of the bilingual training data that is
obtained through similarity search according to the test sentence. Extensive
experiments demonstrate that this method can significantly improve the
translation performance, especially when highly similar sentences are
available.",7 Feb 2017 07:34:51 GMT,Empirical/Data-Driven,Machine translation,hybrid MT;  on-line translation,Xiaxxxxx,Lx,xxxxxxxxx.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Jixxxx,Zhxxx,xxxxxxxxxxng@ia.ac.cn,Institute of Automation Chinese Academy of Sciences,No,Chexxxxxx,Zoxx,xxxxxxxxxxr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixxxx,Zhxxx,Institute of Automation Chinese Academy of Sciences,,,1342xxxxxxx,,,xxxxxxxxxxxxing@gmail.com,,beijing,beijing,,China,,Xiaxxxxx Lx;Jixxxx Zhxxx;Chexxxxxx Zoxx,xxxxxxxxx.ia.ac.cn;xxxxxxxxxxxng@ia.ac.cn;xxxxxxxxxxpr.ia.ac.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
705,705X-D6P8H2G6A3,Learning Concept Embeddings for Efficient Bag-of-Concepts Densification,Waxxx Shaxxxx and Wlxxxx Zadxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Explicit concept space models have proven efficacy for text representation in
many natural language and text mining applications. The idea is to embed
textual structures into a semantic space of concepts which captures the main
topics of these structures. That so called bag-of-concepts representation
suffers from data sparsity causing low similarity scores between similar texts
due to low concept overlap. In this paper we propose two neural embedding
models in order to learn continuous concept vectors. Once learned, we propose
an efficient vector aggregation method to generate fully dense bag-of-concepts
representations. Empirical results on a benchmark dataset for measuring entity
semantic relatedness show superior performance over other concept embedding
models. In addition, by utilizing our efficient aggregation method, we
demonstrate the effectiveness of the densified vector representation over the
typical sparse representations for dataless classification where we can achieve
at least same or better accuracy with much less dimensions.",7 Feb 2017 08:06:14 GMT,Empirical/Data-Driven,Semantics,discriminative learning methods;  distributional similarity;  text classification;  NLP on Wikipedia and other collaboratively constructed resources,Waxxx,Shaxxxx,xxxxxxxx@uncc.edu,University of North Carolina at Charlotte,No,Wlxxxx,Zadxxxxx,xxxxxxxx@uncc.edu,UNC Charlotte,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Waxxx,Shaxxxx,University of North Carolina at Charlotte,,,,,,xxxxxxxx@uncc.edu,,Charlotte,NC,,United States,,Waxxx Shaxxxx;Wlxxxx Zadxxxxx,xxxxxxxx@uncc.edu;xxxxxxxxx@uncc.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
706,706X-B4B6P4P2D3,Naturalizing a Programming Language via Interactive Learning,Sixx Ix;Saxxxx Gixx;Pexxx Lixxx and Chrixxxxxxx Dx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Tuesday,,Undecided (Semantics),"Our goal is to create a convenient natural language interface for performing
well-specified but complex actions such as analyzing data, manipulating text,
and querying databases. However, existing natural language interfaces for such
tasks are quite primitive compared to the power one wields with a programming
language. To bridge this gap, we start with a core programming language and
allow users to ``naturalize'' the core language incrementally by defining
alternative, more natural syntax and increasingly complex concepts in terms of
compositions of simpler ones. In a voxel world, we show that a community of
users can simultaneously teach a common system a diverse language and use it to
build hundreds of complex voxel structures. Over the course of three days,
these users went from using only the core language to using the naturalized
language in 85.9\% of the last 10K utterances.",24 Apr 2017 04:29:06 GMT,Empirical/Data-Driven,Semantics,,Sidxxxx,Waxx,xxxxxxxxxxtanford.edu,Stanford University,No,Saxxxx,Gixx,xxxxxxxxxxxstanford.edu,Stanford University,No,Pexxx,Lixxx,xxxxxxxxxxxtanford.edu,Stanford University,No,Chrisxxxxxxxxx,Manxxxx,xxxxxxxxxxxstanford.edu,Stanford University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sidxxxx,Waxx,Stanford University,,,425xxxxxxx,,,xxxxxxxxxxtanford.edu,,,,,United States,,Sixx Ix;Saxxxx Gixx;Pexxx Lixxx;Chrixxxxxxx Dx,xxxxxxxxxxtanford.edu;xxxxxxxxxxxxstanford.edu;xxxxxxxxxxxstanford.edu;xxxxxxxxxxxxstanford.edu,Naturalizing a Programming Language via Interactive Learning,Naturalizing a Programming Language via Interactive Learning,10,Sida I. Wang,,Stanford University,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
707,707X-B6J7G4E7J6,Abstract Syntax Networks for Code Generation and Semantic Parsing,Maxxx Rabxxxxxxx;Mitxxxxx Stxxx and Dxx Klxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Wednesday,,Undecided (Semantics),"Tasks like code generation and semantic parsing require mapping unstructured
(or partially structured) inputs to well-formed, executable outputs. We
introduce abstract syntax networks, a modeling framework for these problems.
The outputs are represented as abstract syntax trees (ASTs) and constructed by
a decoder with a dynamically-determined modular structure paralleling the
structure of the output tree. On the benchmark Hearthstone dataset for code
generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy,
compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we
perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with
no task-specific engineering.",22 Apr 2017 19:24:10 GMT,Empirical/Data-Driven,Semantics,,Maxxx,Rabxxxxxxx,xxxxxxxxxxxxxxs.berkeley.edu,UC Berkeley,No,Mitxxxxx,Stxxx,xxxxxxxxxxerkeley.edu,UC Berkeley,No,Dxx,Klxxx,xxxxxxxxxxerkeley.edu,UC Berkeley,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxx,Rabxxxxxxx,UC Berkeley,,,,,,xxxxxxxxxxxxxxs.berkeley.edu,,,,,United States,,Maxxx Rabxxxxxxx;Mitxxxxx Stxxx;Dxx Klxxx,xxxxxxxxxxxxxxs.berkeley.edu;xxxxxxxxxxxerkeley.edu;xxxxxxxxxxxerkeley.edu,Abstract Syntax Networks for Code Generation and Semantic Parsing,Abstract Syntax Networks for Code Generation and Semantic Parsing,11,Maxim Rabinovich,,"Computer Science Division
UC Berkeley
Soda Hall
Berkeley, CA 94720",,,No. Do not include my submission in this dataset.,No,None,None
708,708X-A9J4G9B7D3,Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture,Yuaxxxxxx Mexx;Anxx Rumxxxxxx and Alxxxx Roxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we propose to use a set of simple, uniform in architecture
LSTM-based models to recover different kinds of temporal relations from text.
Using the shortest dependency path between entities as input, the same
architecture is used to extract intra-sentence, cross-sentence, and document
creation time relations. A ``double-checking'' technique reverses entity pairs
in classification, boosting the recall of positive cases and reducing
misclassifications between opposite classes. An efficient pruning algorithm
resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our
proposed technique outperforms state-of-the-art methods by a large margin.",7 Feb 2017 10:39:32 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction;  temporal/spatial information extraction,Yuaxxxxxx,Mexx,xxxxxxxxxxxxng@gmail.com,University of Massachusetts Lowell,No,Anxx,Rumxxxxxx,xxxxxxx.uml.edu,University of Massachusetts Lowell,No,Alxxxx,Romxxxx,xxxxxxxxxtlook.com,University of Massachusetts Lowell,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuaxxxxxx,Mexx,University of Massachusetts Lowell,,,,,,xxxxxxxxxxxxng@gmail.com,,,,,United States,,Yuaxxxxxx Mexx;Anxx Rumxxxxxx;Alxxxx Romxxxx,xxxxxxxxxxxxng@gmail.com;xxxxxxxx.uml.edu;xxxxxxxxxutlook.com,,,,,,,,,Only include my submission if it is accepted.,No,None,None
709,709X-D3J5F5E3C8,Chinese Word Segmentation with Document-level Optimization,Shoxxxxx Lx;Qixx Yxx;Jinxxxxx Waxx and Guoxxxx Zxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Previous studies normally formulate Chinese word segmentation as a character
sequence labeling task and optimize the solution in sentence-level. In this
paper, we address Chinese word segmentation as a document-level optimization
problem. First, we apply a state-of-the-art approach, i.e., long short-term
memory (LSTM), to perform character classification; Then, we propose a global
objective function on the basis of character classification and achieve global
optimization via Integer Linear Programming (ILP). Specifically, we propose
several kinds of global constrains in ILP to capture various segmentation
knowledge, such as segmentation consistency and domain-specific regulation, to
achieve document-level optimization, besides label transition knowledge to
achieve sentence-level optimization. Empirical studies demonstrate the
effectiveness of the proposed approach to Chinese word segmentation.",7 Feb 2017 09:28:40 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",word segmentation,Shoxxxxx,Lx,xxxxxxxxxxxsuda.edu.cn,Soochow University,No,Qixx,Yxx,xxxxxxxx47@qq.com,Soochow University,No,Jinxxxxx,Waxx,xxxxxxxxx@gmail.com,Soochow University,No,Guoxxxx,Zhxx,xxxxxxxxxda.edu.cn,Soochow University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixx,Yxx,Soochow University,,,,,,xxxxxxxx47@qq.com,,,,,China,,Shoxxxxx Lx;Qixx Yxx;Jinxxxxx Waxx;Guoxxxx Zhxx,xxxxxxxxxxxsuda.edu.cn;xxxxxxxxx47@qq.com;xxxxxxxxxx@gmail.com;xxxxxxxxxuda.edu.cn,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
710,710X-C7H3J7D3P9,Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,Gabxxxx Doxxx;Amxx Golxxxxx;Saxxxx Srixxxxxxx and Micxxxx Frxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Accept - Oral Tuesday,,Undecided (Cognitive Modelling and Psycholinguistics),"Cultural fit is widely believed to affect the success of individuals and the
groups to which they belong. Yet it remains an elusive, poorly measured
construct. Recent research draws on computational linguistics to measure
cultural fit but overlooks asymmetries in cultural adaptation. By contrast, we
develop a directed, dynamic measure of cultural fit based on linguistic
alignment, which estimates the influence of one person's word use on another's
and distinguishes between two enculturation mechanisms: internalization and
self-regulation. We use this measure to trace employees' enculturation
trajectories over a large, multi-year corpus of corporate emails and find that
patterns of alignment in the first six months of employment are predictive of
individuals’ downstream outcomes, especially involuntary exit. Further
predictive analyses suggest referential alignment plays an overlooked role in
linguistic alignment.",23 Apr 2017 01:52:12 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,,Gabxxxx,Doxxx,xxxxxxxxxanford.edu,Stanford University,No,Amxx,Golxxxxx,xxxxxxxxxanford.edu,Stanford University,No,Saxxxx,Srixxxxxxx,xxxxxxxxxxxxxxs.berkeley.edu,UC Berkeley,No,Micxxxx,Frxxx,xxxxxxxxxxanford.edu,Stanford University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gabxxxx,Doxxx,San Diego State University,,,,,,xxxxxxxsdsu.edu,,,,,United States,,Gabxxxx Doxxx;Amxx Golxxxxx;Saxxxx Srixxxxxxx;Micxxxx Frxxx,xxxxxxxxxanford.edu;xxxxxxxxxxanford.edu;xxxxxxxxxxxxxxas.berkeley.edu;xxxxxxxxxxtanford.edu,Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,10,Gabriel Doyle,,"Stanford University, Department of Psychology
Stanford, CA 94305",on,,Only include my submission if it is accepted.,No,None,None
711,711X-H3A2J4J8E8,End-to-end Mapping of Instructions to Actions with Reinforcement Learning from Demonstrations,Dipxxxxx Kuxxx;Joxx Lanxxxxx and Yoxx Arxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"We present an end-to-end approach for executing natural language instructions 
with reinforcement learning.  The approach uses a neural network architecture 
taking an instruction and an image of the current environment as input while 
outputting the next action to execute. Learning uses a variant of policy 
gradient for a contextual bandit setting to train.  To effectively learn,
reward shaping 
guides the agent's exploration using demonstrations of the desired system 
behavior.  The approach effectively learns to execute actions and 
significantly outperforms  supervised learning.",7 Feb 2017 11:50:33 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",reinforcement learning,Dipenxxxxxxxxx,Mixxx,xxxxxxxxxrnell.edu,Cornell University,No,Joxx,Lanxxxxx,xxxxxxch.net,Microsoft Research,No,Yoxx,Arxxx,xxxxxxxxxornell.edu,Cornell University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dipxxxxx,Mixxx,Cornell University,,,607xxxxxxx,,,xxxxxxxxxrnell.edu,,,,,United States,,Dipxxxxx Kuxxx;Joxx Lanxxxxx;Yoxx Arxxx,xxxxxxxxxrnell.edu;xxxxxxnch.net;xxxxxxxxxxornell.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
713,713X-A2A7B2H3P7,Combating Human Trafficking with Multimodal Deep Models,Edxxxx Toxx;Amxx Zaxxx;Caxx Joxxx and Louisxxxxxxxxx Moxxxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Accept - Poster Monday,,Undecided (Multidisciplinary),"Human trafficking is a global epidemic affecting millions of people across the
planet. Sex trafficking, the dominant form of human trafficking, has seen a
significant rise mostly due to the abundance of escort websites, where human
traffickers can openly advertise among at-will escort advertisements. In this
paper, we take a major step in the automatic detection of advertisements
suspected to pertain to human trafficking. We present a novel dataset called
Trafficking-10k, with more than 10,000~advertisements annotated for this task.
The dataset contains two sources of information per advertisement: text and
images. For the accurate detection of trafficking advertisements, we designed
and trained a deep multimodal model called the Human Trafficking Deep Network
(HTDN).",23 Apr 2017 03:18:52 GMT,Applications/Tools,Other,,Edxxxx,Toxx,xxxxxxxcmu.edu,CMU LTI,No,Amxx,Zaxxx,xxxxxxxxxxxdrew.cmu.edu,CMU,No,Caxx,Joxxx,xxxxxxxxxxxxanalytics.com,Marinus Analytics,No,Louisxxxxxxxxx,Morxxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Amxx,Zaxxx,CMU,,,,,,xxxxxxxxxxxdrew.cmu.edu,,,,,United States,,Edxxxx Toxx;Amxx Zaxxx;Caxx Joxxx;Louisxxxxxxxxx Morxxxx,xxxxxxxcmu.edu;xxxxxxxxxxxxdrew.cmu.edu;xxxxxxxxxxxxxanalytics.com;xxxxxxxxxcs.cmu.edu,Combating Human Trafficking with Multimodal Deep Models,Combating Human Trafficking with Multimodal Deep Models,10,Amir Zadeh,,"LTI, CMU, 5000 Forbest Avenue Pittsburgh PA 15213",on,on,No. Do not include my submission in this dataset.,No,None,None
714,714X-A6P3D9B3B3,"Using millions of emoji occurrences to pretrain any-domain models for detecting emotion, sentiment and sarcasm",Bjxxxx Fexxx;Alxx Misxxxx;Anxxxx Søxxxxx;Iyxx Raxxxx and Suxx Lexxxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"NLP tasks such as emotion detection, sarcasm detection, and sentiment analysis
aim to infer information about an author's emotional state and intentions,
often conveyed indirectly. There are many ways to signal emotion or sentiment,
but learning the complex relation between implicit cues and an author's
intentions is difficult due to the scarcity of manually annotated data. This
paper shows that by harvesting millions of emoji occurrences and learning to
predict them in context, we can learn text representations that are less
dependent on large volumes of annotated data. Using emoji prediction to
pretrain our model we obtain improvements over the state of the art on 5 out of
6 benchmark datasets.",7 Feb 2017 08:00:49 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  unsupervised and semi-supervised learning;  NLP applications;  learning with small datasets;  NLP on noisy unstructured text;  text classification;  figurative language,Bjxxxx,Fexxx,xxxxxxmit.edu,Massachusetts Institute of Technology,No,Alxx,Misxxxx,xxxxxxxxxxcs.neu.edu,Northeastern University,No,Anxxxx,Søxxxxx,xxxxxxxx@di.ku.dk,University of Copenhagen,No,Iyxx,Raxxxx,xxxxxxx@mit.edu,Massachusetts Institute of Technology,No,Suxx,Lehxxxx,xxxxxdtu.dk,Technical University of Denmark,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Bjxxxx,Fexxx,Massachusetts Institute of Technology,,,,,,xxxxxxmit.edu,,,,,United States,,Bjxxxx Fexxx;Alxx Misxxxx;Anxxxx Søxxxxx;Iyxx Raxxxx;Suxx Lehxxxx,xxxxxxmit.edu;xxxxxxxxxxccs.neu.edu;xxxxxxxxx@di.ku.dk;xxxxxxxx@mit.edu;xxxxxxdtu.dk,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
715,715X-B3G8G3A7C9,Reading Wikipedia to Answer Open-Domain Questions,Daxxx Chxx;Adxx Fixxx;Jaxxx Wexxxx and Antxxxx Boxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"This paper proposes to tackle open-domain question answering using Wikipedia as
the unique knowledge source: the answer to any factoid question is a text span
in a Wikipedia article. This task of machine reading at scale combines the
challenges of document retrieval (finding the relevant articles) with that of
machine comprehension of text (identifying the answer spans from those
articles). Our approach combines a search component based on bigram hashing and
TF-IDF matching with a multi-layer recurrent neural network model trained to
detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA
datasets indicate that (1) both modules are highly competitive with respect to
existing counterparts and (2) multitask learning using distant supervision on
their combination is an effective complete system on this challenging task.",1 May 2017 17:56:02 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Daxxx,Chxx,xxxxxxxxxxtanford.edu,Stanford University,No,Adxx,Fixxx,xxxxxx@fb.com,Facebook,No,Jaxxx,Wexxxx,xxxxxxxxxx@gmail.com,Facebook,No,Antxxxx,Boxxxx,xxxxxxx@fb.com,Facebook,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxx,Chxx,Stanford University,,,,,,xxxxxxxxxxtanford.edu,,,,,United States,,Daxxx Chxx;Adxx Fixxx;Jaxxx Wexxxx;Antxxxx Boxxxx,xxxxxxxxxxtanford.edu;xxxxxxx@fb.com;xxxxxxxxxxn@gmail.com;xxxxxxxs@fb.com,Reading Wikipedia to Answer Open-Domain Questions,Reading Wikipedia to Answer Open-Domain Questions,10,Danqi Chen,,"Danqi Chen, Stanford University",,,Only include my submission if it is accepted.,No,None,None
716,716X-A9P7B2C7H6,Concept Transfer Learning for Adaptive Language Understanding,Sx Zxx and Kxx Yx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"A major challenge of the language understanding (LU) is the domain adaptation
problem. In this paper, we propose an atomic-concept-tree-based semantic
representation, which indicates the inner relation of different semantic slots.
Specifically, we propose the concept transfer learning methods for slot filling
in adaptive LU. The concept transfer learning makes use of the common ground of
concepts shown in the literal description. We evaluate our methods on the
benchmark LU datasets: ATIS and DSTC 2&3. The experiment results show that the
concept transfer learning is very efficient for domain adaptation in the LU.
Especially, we achieve a new state-of-the-art performance (F1-score 96.08%) on
the ATIS with the lexicon features.",7 Feb 2017 11:45:50 GMT,Empirical/Data-Driven,Semantics,domain adaptation;  spoken language understanding;  semantic relations,Sx,Zxx,xxxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong University,No,Kxx,Yx,xxxxxxxxxtu.edu.cn,Shanghai Jiao Tong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sx,Zxx,Shanghai Jiao Tong University,,,,,,xxxxxxxxxxjtu.edu.cn,,,,,China,,Sx Zxx;Kxx Yx,xxxxxxxxxxjtu.edu.cn;xxxxxxxxxjtu.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
717,717X-G7G3D3D3D6,Domain-Aware Dependency Parsing for Questions,Apxxxx Garxxxxxx;laxxx chixxxxxxx and Yuxxxx L,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Parsing natural language questions over specific domains is essential to a wide
range of emerging applications from question answering to dialog systems.
Unfortunately, pre-trained parsers are usually trained with corpora dominated
by non-questions and thus perform poorly over domain questions. One possible
but expensive solution is to retrain parsers with manually annotated
domain-specific questions. In this paper, we describe an approach to
automatically generate labeled domain questions by leveraging domain knowledge,
seed domain questions, and general questions. We use our approach to generate
labeled datasets in two domains, which we release to the community.
Our experimental results demonstrate the value of the auto-generated questions
in improving parsing accuracy over domain questions.",10 Feb 2017 05:57:01 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",corpus development;  language generation;  domain adaptation;  learning with small datasets;  rule-based/symbolic learning methods;  syntax;  parsing,Apxxxx,Garxxxxxx,xxxxxxxxumich.edu,University of Michigan,No,laxxx,chixxxxxxx,xxxxxxxx.ibm.com,IBM Research - Almaden,No,Yuxxxx,Lx,xxxxxxxxxus.ibm.com,IBM Research - Almaden,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Apxxxx,Garxxxxxx,University of Michigan,,,,,,xxxxxxxxumich.edu,,,,,United States,,Apxxxx Garxxxxxx;laxxx chixxxxxxx;Yuxxxx Lx,xxxxxxxxumich.edu;xxxxxxxxs.ibm.com;xxxxxxxxxxus.ibm.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
718,718X-E5A3A3F2C3,Capturing Stacked Multi-Order Dependencies on Recurrent Neural Networks: A Practically Scalable Solution,Xx Sxx and Yaxx Yaxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Recurrent neural networks (e.g., LSTM) are widely used for NLP tasks, and
various methods have been proposed to capture tag dependencies (e.g.,
LSTM-CRF). However, there are two limitations of those methods: the scalability
and the single-order setting. Those methods require dynamic programming in
training, which is not scalable when increase the order. Besides, the existing
methods adopt the single order setting, which is easy to be overfitting. To
deal with the problems, we propose to capture multi-order information in
decoding rather than training. The proposed method, stacked multi-order
decoding (STAD), is a scalable solution that can capture combined rich
information from multiple orders, resulting in higher accuracies than existing
methods with single order setting. In various NLP tasks, we show that STAD is
efficient in both training and decoding, and it achieves significantly higher
accuracies than existing methods.",7 Feb 2017 08:06:04 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",part-of-speech tagging;  named entity recognition;  scalability/efficiency of ML methods;  discriminative learning methods;  structured input/output;  experimental evaluation/comparison of ML methods;  chunking,Xx,Sxx,xxxxxxxxu.edu.cn,Peking University,No,Yaxx,Yaxx,xxxxxxxxxxxx6@hotmail.com,Peking U,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xx,Sxx,Peking University,,,1881xxxxxxx,,,xxxxxxxxu.edu.cn,,,,,China,,Xx Sxx;Yaxx Yaxx,xxxxxxxxu.edu.cn;xxxxxxxxxxxxx6@hotmail.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
719,719X-P4H3C5B6F4,Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains,Liaxxxxx Waxx;Jixx Jixxx;Hxx Lexxx;Chxx Hxx; x and ax Soxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Poster Monday,,Undecided (Generation Summarization),"In this paper, we study how to improve the
domain adaptability of a deletion-based
Long Short-Term Memory (LSTM) neural network model for sentence compression. We
hypothesize that syntactic information helps in making such models
more robust across domains. We propose two major changes to the model: using
explicit syntactic features and introducing syntactic constraints through
Integer Linear Programming (ILP). Our evaluation
shows that the proposed model works better than the original model as well as a
traditional non-neural-network-based model
in a cross-domain setting.",23 Apr 2017 05:56:15 GMT,Empirical/Data-Driven,Summarization,,Liaxxxxx,Waxx,xxxxxxxxxxxxxguo@bit.edu.cn,Beijing Institute of Technology,No,Jixx,Jixxx,xxxxxxxxxxsmu.edu.sg,Singapore Management University,No,Haixxxxxx,Chxxx,xxxxxxxxxdso.org.sg,DSO National Laboratories,No,Chexxxxx,Oxx,xxxxxxxxxdso.org.sg,DSO National Laboratories,No,Daxxxx,Soxx,xxxxxxx.edu.cn,Beijing Institute of Technology,No,Lexxxx,Lixx,xxxxxxxxit.edu.cn,Beijing Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,Liaxxxxx,Waxx,Beijing Institute of Technology,,,,,,xxxxxxxxxxxxxguo@bit.edu.cn,,Beijing,,,China,,Liaxxxxx Waxx;Jixx Jixxx;Hxx Lexxx;Chxx Hxx;Daxxxx Soxx;Lexxxx Lixx,xxxxxxxxxxxxxguo@bit.edu.cn;xxxxxxxxxx@smu.edu.sg;xxxxxxxxxxdso.org.sg;xxxxxxxxxxdso.org.sg;xxxxxxxt.edu.cn;xxxxxxxxxit.edu.cn,Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains,Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains,9,wangliangguo,,"School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China",,on,No. Do not include my submission in this dataset.,No,None,None
720,720X-D6A6A3D4H9,Generating Long and Diverse Responses with Neural Conversation Models,Yuaxxxxx Shxx;Stexxxx Goxxx;Dexxx Brxxx;Anxx Goxxxx;Brxxx Stxxxx and Rxx Kurxxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Building general-purpose conversation agents is a very challenging task, but
necessary on the road toward intelligent agents that can interact with humans
in natural language. Neural conversation models -- purely data-driven systems
trained end-to-end on dialogue corpora -- have shown great promise recently,
yet they often produce short and generic responses. This work presents new
training and decoding methods that improve the quality, coherence, and
diversity of long responses generated using sequence-to-sequence models. Our
approach adds self-attention to the decoder to maintain coherence in longer
responses, and we propose a practical approach, called the glimpse-model, for
scaling to large datasets. We introduce a stochastic beam-search algorithm with
segment-by-segment reranking which lets us inject diversity earlier in the
generation process. We trained on a combined data set of over 2.3B conversation
messages mined from the web. In human evaluation studies, our method produces
longer responses overall, with a higher proportion rated as acceptable and
excellent as length increases, compared to baseline sequence-to-sequence models
with explicit length-promotion. A back-off strategy produces better responses
overall, in the full spectrum of lengths.",10 Feb 2017 05:59:54 GMT,Empirical/Data-Driven,Dialog and interactive systems,generative models;  language generation;  dialogue;  open-domain question answering,Yuaxxxxx,Shxx,xxxxxxxxxgoogle.com,Google Inc.,No,Stexxxx,Goxxx,xxxxxxxxoogle.com,Google Inc.,No,Dexxx,Brxxx,xxxxxxxxxx@google.com,Google Inc.,No,Anxx,Goxxxx,xxxxxxxxxoogle.com,Google Inc.,No,Brxxx,Stxxxx,xxxxxxxgle.com,Google Inc.,No,Rxx,Kurxxxxx,xxxxxxxxxxx@google.com,Google Inc.,No,,,,,,,,,,,,,,,,,,,,,,Yuaxxxxx,Shxx,Google Inc.,,,650xxxxxxx,,,xxxxxxxxxgoogle.com,,Mountain View,CA,,United States,,Yuaxxxxx Shxx;Stexxxx Goxxx;Dexxx Brxxx;Anxx Goxxxx;Brxxx Stxxxx;Rxx Kurxxxxx,xxxxxxxxxgoogle.com;xxxxxxxxxoogle.com;xxxxxxxxxxx@google.com;xxxxxxxxxgoogle.com;xxxxxxxogle.com;xxxxxxxxxxxl@google.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
721,721X-G7A3P3J8A4,How to do semantic SMT for low resource languages without SRL,Mexxxx Belxxxxx and Dexxx Wx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"We show consistent translation improvements
    across multiple challenging low resource input languages---Uzbek and
Uyghur,
    as well as simulated low resource conditions with Chinese and Turkish---via
    a new semantic statistical machine translation approach that does not
    require semantic role labeling for low resource input languages (which
    generally does not exist) but instead requires only monolingual semantic
    role labeling in the high resource output English language. Semantic
    statistical machine translation for low resource languages has been a
    difficult challenge due to the unavailability of semantic resources, and
the
    results we report are the best by far to date for this type of approach.
The
    advances we report here exploit the novel realization that the
monolingual
    shallow semantic parse coverage is an excellent way to semantically
    bias expectation-maximization induction even for low resource languages
    where no SRL is available. The results show that our proposed method leads
    to better translation quality than conventional inversion transduction
    grammars and traditional GIZA++ based word alignments on different
datasets,
    corroborating our hypothesis.",7 Feb 2017 12:13:44 GMT,Empirical/Data-Driven,Machine translation,alignment;  semantic role labelling,Mexxxx,Belxxxxx,xxxxxxxxxxcse.ust.hk,HKUST,No,Dexxx,Wx,xxxxxxxs.ust.hk,HKUST,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mexxxx,Belxxxxx,University of Copenhagen,,,,,,xxxxxxi.ku.dk,,,,,Denmark,Postdoc researcher,Mexxxx Belxxxxx;Dexxx Wx,xxxxxxxxxxcse.ust.hk;xxxxxxxxs.ust.hk,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
722,722X-A5A5E7D6J6,Incorporating Relation Paths in Neural Relation Extraction,Wenxxxx Zexx;Yaxxxx Lxx;Zhixxxx Lxx and Maoxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Distantly supervised relation extraction has been widely used to find novel
relational facts from plain text. To predict the relation between a pair of two
target entities, existing methods solely rely on those direct sentences
containing both entities. In fact, there are also many sentences containing
only one of the target entities, which also provide rich useful information but
not yet employed by relation extraction. To address this issue, we build
inference chains between two target entities via intermediate entities, and
propose a path-based neural relation extraction model to encode the relational
semantics from both direct sentences and inference chains. Experimental results
on real-world datasets show that, our model can make full use of those
sentences containing only one target entity, and achieves significant and
consistent improvements on relation extraction as compared with baselines.",7 Feb 2017 11:54:18 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relational Learning;  relation/event extraction,Wenxxxx,Zexx,xxxxxxxxxxxx995@gmail.com,Tsinghua University,No,Yaxxxx,Lxx,xxxxxxxxxxxxxxtsinghua.edu.cn,Tsinghua University,No,Zhixxxx,Lxx,xxxxxxxxxxghua.edu.cn,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wenxxxx,Zexx,Tsinghua University,,,,,,xxxxxxxxxxxx995@gmail.com,,,,,China,,Wenxxxx Zexx;Yaxxxx Lxx;Zhixxxx Lxx;Maoxxxx Sxx,xxxxxxxxxxxx995@gmail.com;xxxxxxxxxxxxxxxtsinghua.edu.cn;xxxxxxxxxxxghua.edu.cn;xxxxxxxxxxhua.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
723,723X-H2E6B9C6H3,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,Taxxx Sakxxxxx;Suxx Bhxx and Prxxxx Visxxxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Accept - Oral Monday,,,"We present in this paper a novel framework for morpheme segmentation which uses
the morpho-syntactic regularities preserved by word representations, in
addition to orthographic features, to segment words into morphemes. This
framework is the first to consider vocabulary-wide syntactico-semantic 
information for this task. We also analyze the deficiencies  of  available
benchmarking datasets and introduce our own dataset that was created on the
basis of compositionality.  We validate our algorithm across datasets and
present state-of-the-art results.",23 Apr 2017 08:25:23 GMT,Applications/Tools,"Phonology, morphology, and word segmentation",,Taxxx,Sakxxxxx,xxxxxxxxxxxxni@gmail.com,University of Illinois at Urbana-Champaign,No,Suxx,Bhxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana-Champaign,No,Prxxxx,Visxxxxxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana-Champaign,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Taxxx,Sakxxxxx,University of Illinois at Urbana-Champaign,,,1-217xxxxxxxxx,,,xxxxxxxxxxxxni@gmail.com,,Urbana,IL,,United States,,Taxxx Sakxxxxx;Suxx Bhxx;Prxxxx Visxxxxxx,xxxxxxxxxxxxni@gmail.com;xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,10,Tarek Sakakini,,"University of Illinois at Urbana-Champaign
1308 W Main St,
Urbana, IL
61801",on,on,Only include my submission if it is accepted.,No,None,None
724,724X-P3J7J6P3D9,Extracting Temporally-Anchored Spatial Knowledge Between Pairs of Named Entities,Alxxxx and a Vemxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"This paper presents an approach to extract temporally-anchored spatial
knowledge between pairs of named entities. Specifically, we determine whether
entities are or are not located somewhere, and specify for how long with
respect to events. We present crowdsourced annotations using coarse- and
fine-grained labels, and results using both gold-standard and predicted
linguistic annotations.",7 Feb 2017 09:43:45 GMT,Empirical/Data-Driven,Semantics,semantic relations;  temporal/spatial information extraction,Alaxxxxxxx,Vemxxxx,xxxxxxxxxxxxxxala@my.unt.edu,University Of North Texas,No,Eduxxxx,Blxxxx,xxxxxxxxxxxnco@unt.edu,University of North Texas,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Eduxxxx,Blxxxx,University of North Texas,,,,,,xxxxxxxxxxxnco@unt.edu,,,,,United States,,Alaxxxxxxx Vemxxxx;Eduxxxx Blxxxx,xxxxxxxxxxxxxxala@my.unt.edu;xxxxxxxxxxxanco@unt.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
726,726X-G6G2E4B5D9,Learning a Neural Semantic Parser from User Feedback,Srixxxxxxx Iyxx;Ioaxxxx Konxxxx;Alxxx Chxxxx;Jaxxxx Krisxxxxxxxxx and Luxx Zetxxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Oral Tuesday,,Undecided (Semantics),"We present an approach to rapidly and easily build natural language interfaces
to databases for new domains, whose performance improves over time based on
user feedback, and requires minimal intervention. To achieve this, we adapt
neural sequence models to map utterances directly to SQL with its full
expressivity, bypassing any intermediate meaning representations. These models
are immediately deployed online to solicit feedback from real users to flag
incorrect queries. Finally, the popularity of SQL facilitates gathering
annotations for incorrect predictions using the crowd, which is directly used
to improve our models. This complete feedback loop, without intermediate
representations or database specific engineering, opens up new ways of building
high quality semantic parsers. Experiments suggest that this approach can be
deployed quickly for any new target domain, as we show by learning a semantic
parser for an online academic database from scratch.",22 Apr 2017 19:24:16 GMT,Empirical/Data-Driven,Semantics,,Srixxxxxxx,Iyxx,xxxxxx@uw.edu,University of Washington,No,Ioaxxxx,Konxxxx,xxxxxxxxxxxxxashington.edu,University of Washington,No,Alxxx,Chxxxx,xxxxxxxxxxxxxashington.edu,University of Washington,No,Jaxxxx,Krisxxxxxxxxx,xxxxxxxxxxxxxxurthy@gmail.com,Allen Institute for Artificial Intelligence,No,Luxx,Zettxxxxxxx,xxxxxxxxxxhington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Srixxxxxxx,Iyxx,University of Washington,,,,,,xxxxxx@uw.edu,,,,,United States,,Srixxxxxxx Iyxx;Ioaxxxx Konxxxx;Alxxx Chxxxx;Jaxxxx Krisxxxxxxxxx;Luxx Zettxxxxxxx,xxxxxx@uw.edu;xxxxxxxxxxxxxwashington.edu;xxxxxxxxxxxxxwashington.edu;xxxxxxxxxxxxxxxurthy@gmail.com;xxxxxxxxxxxhington.edu,Learning a Neural Semantic Parser from User Feedback,Learning a Neural Semantic Parser from User Feedback,11,Srinivasan Iyer,,"Paul G. Allen School of Computer Science & Engineering, Univ. of Washington, Seattle, WA",on,on,Only include my submission if it is accepted.,No,None,None
727,727X-A2H5E9F5J5,Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,Krixxxx Johxxxx;Dx Jxx and Dxx Golxxxxxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Accept - Oral Tuesday,,Undecided (Social Media),"Framing is a political strategy in which politicians carefully word their
statements in order to control public perception of issues. Previous works
exploring political framing typically analyze frame usage in longer texts, such
as congressional speeches. We present a collection of weakly supervised models
which harness collective classification to predict the frames used in political
discourse on the microblogging platform, Twitter. Our global probabilistic
models show that by combining both lexical features of tweets and network-based
behavioral features of Twitter, we are able to increase the average,
unsupervised F1 score by 21.52 points over a lexical baseline alone.",30 Apr 2017 08:14:22 GMT,Empirical/Data-Driven,Social media,,Krixxxx,Johxxxx,xxxxxxxxxpurdue.edu,Purdue University,No,Dx,Jxx,xxxxxxxrdue.edu,Purdue University,No,Dxx,Golxxxxxxx,xxxxxxxxxgmail.com,Purdue University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Krixxxx,Johxxxx,Purdue University,,,,,,xxxxxxxxxpurdue.edu,,,IN,,United States,,Krixxxx Johxxxx;Dx Jxx;Dxx Golxxxxxxx,xxxxxxxxxpurdue.edu;xxxxxxxxrdue.edu;xxxxxxxxx@gmail.com,Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,12,Kristen M Johnson,,"Purdue University
610 Purdue Mall, West Lafayette, IN 47907",on,on,Only include my submission if it is accepted.,No,None,None
728,728X-C7J4A7H3J9,Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings,Joxx Wiexxxx and Kexxx Gixxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Tuesday,,Undecided (Semantics),"We consider the problem of learning general-purpose, paraphrastic sentence
embeddings, revisiting the setting of Wieting et al. (2016b). While they found
LSTM recurrent networks to underperform word averaging, we present several
developments that together produce the opposite conclusion. These include
training on sentence pairs rather than phrase pairs, averaging states to
represent sequences, and regularizing aggressively. These improve LSTMs in both
transfer learning and supervised settings. We also introduce a new recurrent
architecture, the Gated Recurrent Averaging Network, that is inspired by
averaging and LSTMs while outperforming them both. We analyze our learned
models, finding evidence of preferences for particular parts of speech and
dependency relations.",23 Apr 2017 01:03:21 GMT,Empirical/Data-Driven,Semantics,,Joxx,Wiexxxx,xxxxxxxxxxllinois.edu,University of Illinois; TTI-Chicago,No,Kexxx,Gixxxx,xxxxxxxxttic.edu,Toyota Technological Institute at Chicago,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Joxx,Wiexxxx,University of Illinois; TTI-Chicago; CMU,,,,,,xxxxxxxxxcs.cmu.edu,,,,,United States,,Joxx Wiexxxx; TTIxxxxxxxxx;Kexxx Gixxxx,xxxxxxxxxxllinois.edu;xxxxxxxx@ttic.edu,Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings,Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings,11,John Wieting,,,on,on,No. Do not include my submission in this dataset.,No,None,None
729,729X-P2F6B3E3J3,Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning,Jixx Mx;Wxx Gxx and Kamxxxx Wxx,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Accept - Oral Tuesday,,Undecided (Social Media),"How fake news goes viral via social media? How does its propagation pattern
differ from real stories? In this paper, we attempt to address the problem of
identifying rumors, i.e., fake information, out of microblog posts based on
their propagation structure. We firstly model microblog posts diffusion with
propagation trees, which provide valuable clues on how an original message is
transmitted and developed over time. We then propose a kernel-based method
called Propagation Tree Kernel, which captures high-order patterns
differentiating different types of rumors by evaluating the similarities
between their propagation tree structures. Experimental results on two
real-world datasets demonstrate that the proposed kernel-based approach can
detect rumors more quickly and accurately than state-of-the-art rumor detection
models.",18 Apr 2017 15:13:39 GMT,Empirical/Data-Driven,Social media,,Jixx,Mx,xxxxxxxxxxcuhk.edu.hk,the Chinese University of Hong Kong,No,Wxx,Gxx,xxxxxxx.org.qa,Qatar Computing Research Institute,No,Kamxxxx,Woxx,xxxxxxxxxxcuhk.edu.hk,"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wxx,Gxx,Victoria University of Wellington,,,,,,xxxxxxxxx@gmail.com,,,,,New Zealand,"Wei Gao is currently a Senior Lecturer of Information Systems in the School of Information Management, Victoria University of Wellington in New Zealand. Previously, he was a Scientist in Qatar Computing Research Institute during 2011-2017, a Research Assistant Professor in the Chinese University of Hong Kong in 2010-2011, and a Research Fellow in the Institute for Infocomm Research in Singapore in 2010. His research interests lie in information retrieval, natural language processing, social media analytics, and artificial intelligence. He published over 60 papers and articles in the major international conferences and journals including ACL, EMNLP, SIGIR, CIKM, WSDM, IJCAI, ACM TOIS, ACM TIST, etc. He served in the program committees of a good number of international conferences and worked as the reviewer for top-tier journals in the relevant research areas. He also served as the workshop co-chair of BigComp 2016, the session chair in ASONAM 2015, the area co-chair in NLPCC2015, and the tutorial co-chair in IJCNLP 2011. He received his PhD and MPhil degrees of Information Systems from The Chinese University of Hong Kong.",Jixx Mx;Wxx Gxx;Kamxxxx Woxx and Engixxxxxxx Manxxxxxxx,xxxxxxxxxxcuhk.edu.hk;xxxxxxxf.org.qa;xxxxxxxxxxxcuhk.edu.hk,Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning,Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning,10,Wei Gao,,Qatar Computing Research Institute,on,on,Only include my submission if it is accepted.,No,None,None
730,730X-A9D2H3J2F6,A Knowledge-Grounded Neural Conversation Model,Maxxxx Ghazxxxxxxxxx;Chxxx Broxxxxx;Minxxxxx Chxxx;Bixx Doxxx;Jiaxxxxx Gxx;Wenxxxx Yxx and Mixxxx Gaxxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Neural network models are capable of generating extremely natural sounding
conversational interactions. Nevertheless, these models have yet to demonstrate
that they can incorporate content in the form of factual information or
entity-grounded opinion that would enable them to serve in more task-oriented
conversational applications. This paper presents a novel, fully data-driven,
and knowledge-grounded neural conversation model aimed at producing more
contentful responses without slot filling. We generalize the widely-used
Seq2Seq approach by conditioning responses on both conversation history and
external ""facts"", allowing the model to be versatile and applicable in an
open-domain setting. Our approach yields significant improvements over a
competitive Seq2Seq baseline. Human judges found that our outputs are
significantly more informative.",10 Feb 2017 06:00:19 GMT,Applications/Tools,Dialog and interactive systems,dialogue;  NLP in social networking media;  spoken language  generation;  language generation;  social network,Maxxxx,Ghazxxxxxxxxx,xxxxxxxx@isi.edu,USC,No,Chxxx,Broxxxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Minxxxxx,Chxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Bixx,Doxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,Jiaxxxxx,Gxx,xxxxxxxxxrosoft.com,"Microsoft Research, Redmond",No,Wenxxxx,Yxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Mixxxx,Gaxxxx,xxxxxxxxxxcrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,Maxxxx,Ghazxxxxxxxxx,USC,,,,,,xxxxxxxx@isi.edu,,,,,United States,,Maxxxx Ghazxxxxxxxxx;Chxxx Broxxxxx;Minxxxxx Chxxx;Bixx Doxxx;Jiaxxxxx Gxx;Wenxxxx Yxx;Mixxxx Gaxxxx,xxxxxxxx@isi.edu;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxcrosoft.com;xxxxxxxxxxrosoft.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxcrosoft.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
731,731X-A8B2G8P5H5,Active Sentiment Domain Adaptation,Fanxxxxx Wx;Yonxxxxx Huxxx and Jxx Yx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Poster Monday,,Undecided (Sentiment Analysis Opinion Mining),"Domain adaptation is an important technology to handle domain dependence
problem in sentiment analysis field. Existing methods usually rely on sentiment
classifiers trained in source domains. However, their performance may heavily
decline if the distributions of sentiment features in source and target domains
have significant difference. In this paper, we propose an active sentiment
domain adaptation approach to handle this problem. Instead of the source domain
sentiment classifiers, our approach adapts the general-purpose sentiment
lexicons to target domain with the help of a small number of labeled samples
which are selected and annotated in an active learning mode, as well as the
domain-specific sentiment similarities among words mined from unlabeled samples
of target domain. A unified model is proposed to fuse different types of
sentiment information and train sentiment classifier for target domain.
Extensive experiments on benchmark datasets show that our approach can train
accurate sentiment classifier with less labeled samples.",23 Apr 2017 06:58:32 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Fanxxxxx,Wx,xxxxxxxxxx@gmail.com,Tsinghua Univeristy,No,Yonxxxxx,Huxxx,xxxxxxxxxxxnghua.edu.cn,Tsinghua University,No,Jxx,Yxx,xxxxxxxxxxrosoft.com,Microsoft Research Asia,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fanxxxxx,Wx,Microsoft Research Asia,,,,,,xxxxxxxxxx@gmail.com,,,,,China,,Fanxxxxx Wx;Yonxxxxx Huxxx;Jxx Yxx,xxxxxxxxxx@gmail.com;xxxxxxxxxxxxnghua.edu.cn;xxxxxxxxxxcrosoft.com,Active Sentiment Domain Adaptation,Active Sentiment Domain Adaptation,11,Fangzhao Wu,,,on,on,Only include my submission if it is accepted.,No,None,None
732,732X-P6P6B4J4A7,Probabilistic Typology: Deep Generative Models of Vowel Inventories,Ryxx Cotxxxxxx and Jaxxx Eixxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Wednesday,,Undecided (Machine Learning),"Linguistic typology studies the range of structures present in human language.
The main goal of the field is to discover which sets of possible phenomena are
universal, and which are merely frequent. For example, all languages have
vowels, while most---but not all---languages have an /u/ sound. In this paper
we present the first probabilistic treatment of a basic question in
phonological typology: What makes a natural vowel inventory?  We introduce a
series of deep stochastic point processes, and contrast them with previous
computational, simulation-based approaches.  We provide a comprehensive suite
of experiments on over 200 distinct languages.",9 May 2017 13:14:01 GMT,Empirical/Data-Driven,Machine learning,,Ryxx,Cotxxxxxx,xxxxxxxxxxxxll@gmail.com,Johns Hopkins University,No,Jaxxx,Eixxxx,xxxxxxxx.jhu.edu,Johns Hopkins University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ryxx,Cotxxxxxx,Johns Hopkins University,,,213xxxxxxx,,,xxxxxxxxxxxxll@gmail.com,,Baltimore,MD,,United States,,Ryxx Cotxxxxxx;Jaxxx Eixxxx,xxxxxxxxxxxxll@gmail.com;xxxxxxxxs.jhu.edu,Probabilistic Typology: Deep Generative Models of Vowel Inventories,Probabilistic Typology: Deep Generative Models of Vowel Inventories,11,Jason Eisner,,,on,on,No. Do not include my submission in this dataset.,No,None,None
733,733X-J5J3J6G6J6,Visualizing and Understanding Neural Machine Translation,Yanxxxx Dixx;Yaxx Lxx;Huxxxx Luxx and Maoxxxx Sx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Oral Wednesday,,Undecided (Machine Translation),"While neural machine translation (NMT) has made remarkable progress in recent
years, it is hard to interpret its internal workings due to the continuous
representations and non-linearity of neural networks. In this work, we propose
to use layer-wise relevance propagation (LRP) to compute the contribution of
each contextual word to arbitrary hidden states in the attention-based
encoder-decoder framework. We show that visualization with LRP helps to
interpret the internal workings of NMT and analyze translation errors.",23 Apr 2017 09:33:29 GMT,Empirical/Data-Driven,Machine translation,,Yanxxxx,Dixx,xxxxxxxyeah.net,Tsinghua University,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Huxxxx,Luxx,xxxxxxxxxx@gmail.com,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yanxxxx,Dixx,Tsinghua University,,,1881xxxxxxx,,,xxxxxxxyeah.net,,Beijing,,,China,,Yanxxxx Dixx;Yaxx Lxx;Huxxxx Luxx;Maoxxxx Sxx,xxxxxxxyeah.net;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxo@gmail.com;xxxxxxxxxxhua.edu.cn,Visualizing and Understanding Neural Machine Translation,Visualizing and Understanding Neural Machine Translation,10,Yanzhuo Ding,,"Tsinghua University, Beijing, China",on,on,No. Do not include my submission in this dataset.,No,None,None
734,734X-H6J6H8D5G9,Mixture Modeling for Neural Machine Translation,Mieraxxxxxxxxx Maixxxxx;Yaxx Lxx;Huxxxx Luxx and Maoxxxx Sx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Although neural machine translation (NMT) has made remarkable progress in
recent several years, it suffers from the domain imbalance problem: languages
vary across different domains but available parallel corpora are mainly
restricted to governmental documents. To address this problem, we propose a
mixture modeling approach to domain adaptation for neural machine translation.
Our mixture neural model is capable of automatically discovering latent topics
of source sentences tailored to neural translation and dynamically mixing
translation sub-models according to the topic distribution. Experiments on
Chinese-English datasets show that our approach achieves significant
improvements over a conventional mixture modeling approach.",7 Feb 2017 11:30:35 GMT,Empirical/Data-Driven,Machine translation,domain adaptation,Mieraxxxxxxxxx,Maixxxxx,xxxxxxxxxxxxxxxx.tsinghua.edu.cn,Tsinghua University,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Huxxxx,Luxx,xxxxxxxxxx@gmail.com,Tsinghua University,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mieraxxxxxxxxx,Maixxxxx,Tsinghua University,,,,,,xxxxxxxxxxxxxxxx.tsinghua.edu.cn,,,,,China,,Mieraxxxxxxxxx Maixxxxx;Yaxx Lxx;Huxxxx Luxx;Maoxxxx Sxx,xxxxxxxxxxxxxxxx.tsinghua.edu.cn;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxo@gmail.com;xxxxxxxxxxhua.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
736,736X-H7G7C9F6H5,Taxonomy Driven Deep Learning Approach for Semantic Question Matching,Rajxxxxx Puxxxx;Dexxxx Guxxx;Anuxxxx Maxxxx;Asxx Ekxxx;Txx Gxx and Pusxxxx Bhatxxxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Semantically similar questions with same or almost similar answers appear in
various lexical forms. Identifying already answered semantically similar
questions greatly increases the accuracy of a question answering (QA) system.
In this paper, we propose an effective approach for question-question (QQ)
matching by exploiting various deep learning (DL) models and a linguistically
motivated taxonomy. Experiments performed on a newly created data set
demonstrate the effectiveness of our proposed approach. Empirically, we show
that deep learning representations, when augmented with taxonomy classes
improve the performance on semantic question matching significantly.",7 Feb 2017 12:02:14 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  lexical semantics;  rule-based/symbolic learning methods;  semantic relations;  question interpretation;  text classification;  ontology development;  open-domain question answering;  ontological semantics,Rajxxxxx,Puxxxx,xxxxxxxxxxxxxrian@gmail.com,iitb.ac.in,No,Dexxxx,Guxxx,xxxxxxxxxxxx651@gmail.com,IIT Patna,No,Anuxxxx,Maxxxx,xxxxxxxxxxxxxx@accenture.com,"Accenture Labs, Bengaluru",No,Asxx,Ekxxx,xxxxxxxxxx@gmail.com,Indian Institute of Technology Patna,No,Txx,Geoxxxxx,xxxxxxxxxxxxxaccenture.com,"Accenture Labs, Bengaluru",No,Pusxxxx,Bhatxxxxxxxxx,xxxxxxxxx@gmail.com,"CSE Department, IIT Bombay",No,,,,,,,,,,,,,,,,,,,,,,Rajxxxxx,Puxxxx,purdue.edu,,,1765xxxxxxx,,,xxxxxxxxxurdue.edu,,Mumbai,Maharashtra,,United States,,Rajxxxxx Puxxxx;Dexxxx Guxxx;Anuxxxx Maxxxx;Asxx Ekxxx;Txx Gxx;Pusxxxx Bhatxxxxxxxxx,xxxxxxxxxxxxxrian@gmail.com;xxxxxxxxxxxxx651@gmail.com;xxxxxxxxxxxxxxa@accenture.com;xxxxxxxxxxl@gmail.com;xxxxxxxxxxxxx@accenture.com;xxxxxxxxxx@gmail.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
737,737X-A7E2A2E7D6,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,Maxxxx Joxxx;Euxxxx Chxx;Daxxxx Wexx and Luxx Zetxxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"We present TriviaQA, a challenging reading comprehension dataset containing
over 650K question-answer-evidence triples. TriviaQA includes 95K
question-answer  pairs authored by trivia enthusiasts and independently
gathered evidence documents, six per question on average, that provide high
quality distant supervision for answering the questions. We show that, in
comparison to other recently introduced large-scale datasets, TriviaQA (1) has
relatively complex, compositional questions,  (2)  has considerable 
syntactic and  lexical                                      variability  between     
     
questions and 
corresponding 
answer-evidence  sentences,  and  (3) requires more cross sentence reasoning to
find answers.  We also present two baseline algorithms: a feature-based
classifier and a state-of-the-art neural network, that performs well on SQuAD
reading comprehension. Neither approach comes close to human performance (23%
and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is
worth significant future study.",28 Jun 2017 05:21:34 GMT,Resources/Evaluation,Semantics,,Maxxxx,Joxxx,xxxxxxxxxxxxxashington.edu,University of Washington,No,Euxxxx,Chxx,xxxxxxxxxxxxshington.edu,University of Washington,No,Daxxxx,Wexx,xxxxxxxxxxxhington.edu,University of Washington,No,Luxx,Zettxxxxxxx,xxxxxxxxxxhington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxxxx,Joxxx,University of Washington,,,,,,xxxxxxxxxxxxxashington.edu,,Seattle,WA,,United States,,Maxxxx Joxxx;Euxxxx Chxx;Daxxxx Wexx;Luxx Zettxxxxxxx,xxxxxxxxxxxxxashington.edu;xxxxxxxxxxxxashington.edu;xxxxxxxxxxxshington.edu;xxxxxxxxxxxhington.edu,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,TriviaQA: A Large Dataset for Reading Comprehension,11,Mandar Joshi,,"Computer Science & Engineering
University of Washington
AC101 Paul G. Allen Center for
     Computer Science & Engineering
Box 352350
185 Stevens Way
Seattle WA 98195-2350",on,on,No. Do not include my submission in this dataset.,No,None,None
738,738X-E7H5D9A8P2,Global Recurrent Structure for Modeling Dynamic Boundary Features Addressing Domain Adaption for ChineseWord Segmentation,Shxx Huxxx and Houxxxx WAxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,,"Recently, various neural network methods for Chinese word segmentation have
achieved performance competitive with state-of-the-art systems. However,
constrained by the domain and size of the training corpus, these methods do not
work well in domain adaptation. In this paper, we propose a novel BLSTM-based
neural network model which incorporates a global recurrent structure for
modeling
boundary features dynamically. Experiments show that the structure can
effectively
polish up the performance of Chinese word segmentation, especially OOV Recall,
which is of great benefit to cross-domain corpora. On the SIGHAN Bakeoff
2010 data, our results are competitive to the best reported.",7 Feb 2017 09:11:52 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",domain adaptation;  word segmentation,Shxx,Huxxx,xxxxxxxxxxx1@pku.edu.cn,Peking University,No,Houxxxx,WAxx,xxxxxxxxku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shxx,Huxxx,Peking University,,,(+86)xxxxxxxxxxx,,,xxxxxxxxxxx1@pku.edu.cn,,Beijing,Beijing,,China,,Shxx Huxxx;Houxxxx WAxx,xxxxxxxxxxx1@pku.edu.cn;xxxxxxxxxku.edu.cn,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
739,739X-H9C2A5P9P3,Adversarial Examples for Evaluating Reading Comprehension Systems,Roxxx Jxx and Pexxx Lixxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Conventional accuracy metrics indicate that recent systems for reading
comprehension are making rapid progress, but the extent to which these systems
truly understand language remains unclear. To probe deeper, we propose an
adversarial evaluation in which systems must answer questions about paragraphs
that contain adversarially inserted sentences. These sentences are carefully
constructed to distract computer systems without misleading humans about the
correct answer. On the SQuAD dataset, we show that the performance of existing
models drops from an average of 76% F1 score to 30% in this adversarial
setting; when the adversary is allowed to add ungrammatical sequences of words,
this number decreases further to 6%. We conclude that existing models are
overly reliant on superficial measures of similarity, and do not yet understand
language at a deep level.",7 Feb 2017 11:32:53 GMT,Resources/Evaluation,"Information extraction, text mining, and question answering",experimental evaluation/comparison of ML methods;  open-domain question answering;  evaluation metrics,Roxxx,Jxx,xxxxxxxxxxtanford.edu,Stanford University,No,Pexxx,Lixxx,xxxxxxxxxxxtanford.edu,Stanford University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Roxxx,Jxx,Stanford University,,,,,,xxxxxxxxxxtanford.edu,,,,,United States,,Roxxx Jxx;Pexxx Lixxx,xxxxxxxxxxtanford.edu;xxxxxxxxxxxstanford.edu,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
740,740X-F8D8P3G3J6,Automatic Extraction of Correction Patterns from Expert-Revised Corpora,Gioxxxxx Sirxxxxx and Luxxx Dx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"In this paper, we first present the task of automatically extracting correction
patterns from texts which have been manually revised by domain experts. In real
industrial scenarios, raw texts obtained via surveys or web crawling often
require manual intervention to flatten word capitalization, punctuation,
linguistic variability and entity naming. In this context, we propose a
distributional and language-independent approach that learns revision rules
that also manages errors introduced by the experts themselves. We extensively
evaluated our approach on more than 300,000 expert-revised sentences, showing
promising and reliable results.",7 Feb 2017 11:45:07 GMT,Empirical/Data-Driven,Machine learning,unsupervised and semi-supervised learning;  NLP applications;  distributional similarity;  NLP for expert domains;  alignment,Gioxxxxx,Sirxxxxx,xxxxxxxxxxi.unito.it,University of Turin - Unito,No,Luxxx,Di xxxx,xxxxxxxxx.unito.it,University of Turin,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gioxxxxx,Sirxxxxx,University of Turin - Unito,,,,,,xxxxxxxxxxi.unito.it,,,,,Italy,,Gioxxxxx Sirxxxxx;Luxxx Dx,xxxxxxxxxxi.unito.it;xxxxxxxxxi.unito.it,,,,,,,,,Only include my submission if it is accepted.,No,None,None
741,741X-F7J2G3D4P4,Automatic Induction of Synsets from a Graph of Synonyms,Dmxxxx Ustxxxx; Axxx and ex Panxxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Accept - Poster Monday,,Undecided (Semantics),"This paper presents a new graph-based approach that induces synsets using
synonymy dictionaries and word embeddings. First, we build a weighted graph of
synonyms extracted from commonly available resources, such as Wiktionary.
Second, we apply word sense induction to deal with ambiguous words. Finally, we
cluster the disambiguated version of the ambiguous input graph into synsets.
Our meta-clustering approach lets us use an efficient hard clustering algorithm
to perform a fuzzy clustering of the graph. Despite its simplicity, our
approach shows excellent results, outperforming five competitive
state-of-the-art methods in terms of F-score on three gold standard datasets
for English and Russian derived from large-scale manually constructed lexical
resources.",22 Apr 2017 22:09:29 GMT,Empirical/Data-Driven,Semantics,,Dmxxxx,Ustxxxx,xxxxxxxxxxxxov@gmail.com,Krasovskii Institute of Mathematics and Mechanics,No,Alexxxxxx,Panxxxxxx,xxxxxxxxxxxxxxander@gmail.com,University of Hamburg,No,Chxxx,Biexxxx,xxxxxxxxxxxxxxxxik.uni-hamburg.de,University of Hamburg,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alexxxxxx,Panxxxxxx,University of Hamburg,,,,,,xxxxxxxxxxxxxxander@gmail.com,,Darmstadt,,,Germany,,Dmxxxx Ustxxxx;Alexxxxxx Panxxxxxx;Chxxx Biexxxx,xxxxxxxxxxxxov@gmail.com;xxxxxxxxxxxxxxxander@gmail.com;xxxxxxxxxxxxxxxxxik.uni-hamburg.de,Automatic Induction of Synsets from a Graph of Synonyms,Automatic Induction of Synsets from a Graph of Synonyms,12,Alexander Panchenko,,"University of Hamburg
Faculty of Mathematics, Informatics and Natural Sciences
Department of Informatics, Language Technology Group 
Vogt-Kölln-Str. 30, F-416, 22527 Hamburg
Tel: +49 40 428 832 368",on,on,Only include my submission if it is accepted.,No,None,None
742,742X-A3C6J5G6H2,Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization,Jiaxxxxx Zhxxx;Yaxx Lxx;Huxxxx Luxx;Jinxxxxx Xx and Maoxxxx Sx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Monday,,Undecided (Machine Translation),"Although neural machine translation has made significant progress recently, how
to integrate multiple overlapping, arbitrary prior knowledge sources remains a
challenge. In this work, we propose to use posterior regularization to provide
a general framework for integrating prior knowledge into neural machine
translation. We represent prior knowledge sources as features in a log-linear
model, which guides the learning processing of the neural translation model.
Experiments on Chinese-English dataset show that our approach leads to
significant improvements.",23 Apr 2017 08:40:52 GMT,Empirical/Data-Driven,Machine translation,,Jiaxxxxx,Zhxxx,xxxxxxx126.com,Tsinghua University,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Huxxxx,Luxx,xxxxxxxxxx@gmail.com,Tsinghua University,No,Jinxxxxx,Xx,xxxxxxxxxxxxogou-inc.com,Sogou,No,Maoxxxx,Sxx,xxxxxxxxxhua.edu.cn,Tsinghua University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Jiaxxxxx,Zhxxx,Tsinghua University,,,,,,xxxxxxx126.com,,,,,China,,Jiaxxxxx Zhxxx;Yaxx Lxx;Huxxxx Luxx;Jinxxxxx Xx;Maoxxxx Sxx,xxxxxxx126.com;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxo@gmail.com;xxxxxxxxxxxxsogou-inc.com;xxxxxxxxxxhua.edu.cn,Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization,Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization,10,Jiacheng Zhang,,"Tsinghua University, Beijing, China",on,,No. Do not include my submission in this dataset.,No,None,None
743,743X-G8J6P5F6J4,A Comparative Study of Online Document Clustering Algorithms in a Unified Framework,Arxxxx Znoxxxx;Sebxxxxxxx Mirxxxx;Shxx Bx and Guxxxx Barxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"We study the problem of online document clustering, such as with a stream of
incoming news articles. We compare and contrast two algorithms in a unified
framework: an online latent semantic analysis (LSA) algorithm and the online
keyword-based algorithm of Aggarwal and Yu (2006). On a set of multilingual
datasets in English, Spanish and German, we find that each algorithm has its
own advantages and disadvantages relating to efficiency, performance and
stability. State-of-art crosslingual clustering is enabled by crosslingual word
embeddings.",7 Feb 2017 10:51:47 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",cross-lingual approaches;  distributional similarity;  document clustering,Arxxxx,Znoxxxx,xxxxxxxxxxxins@leta.lv,"LETA, University of Latvia",No,Sebxxxxxxx,Mirxxxx,xxxxxxxxxxxxxxnda@priberam.pt,Priberam,No,Shaxxxx,Coxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,Guxxxx,Barxxxxx,xxxxxxxxxxxxins@lumii.lv,"IMCS, University of Latvia",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Arxxxx,Znoxxxx,"LETA, University of Latvia",,,3712xxxxxxx,,,xxxxxxxxxxxins@leta.lv,,Rīga,Latvia,,Latvia,,Arxxxx Znoxxxx;Sebxxxxxxx Mirxxxx;Shxx Bx;Guxxxx Barxxxxx,xxxxxxxxxxxins@leta.lv;xxxxxxxxxxxxxxxnda@priberam.pt;xxxxxxxxxxf.ed.ac.uk;xxxxxxxxxxxxdins@lumii.lv,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
744,744X-D3D7J3F3A7,Translating Neuralese,Jaxxx Andxxxx;Anxx Drxxxx and Dxx Klxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Accept - Oral Monday,,Undecided (Vision Robots Grounding),"Several approaches have recently been proposed for learning decentralized deep
multiagent policies that coordinate via a differentiable communication channel.
While these policies are effective for many tasks, interpretation of their
induced communication strategies has remained a challenge. Here we propose to
interpret agents' messages by translating them.  Unlike in typical machine
translation problems, we have no parallel data to learn from. Instead we
develop
a translation model based on the insight that agent messages and natural
language strings mean the same thing if they induce the same belief about
the world in a listener.  We present theoretical guarantees and empirical
evidence that our approach preserves both the semantics and pragmatics of
messages by ensuring that players communicating through a translation layer do
not suffer a substantial loss in reward relative to players with a common
language.",21 Apr 2017 21:08:06 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",,Jaxxx,Andxxxx,xxxxxxxxxrkeley.edu,Berkeley,No,Anxx,Drxxxx,xxxxxxxxxxrkeley.edu,UC Berkeley,No,Dxx,Klxxx,xxxxxxxxxxerkeley.edu,UC Berkeley,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jaxxx,Andxxxx,Berkeley,,,,,,xxxxxxxxxrkeley.edu,,,CA,,United States,,Jaxxx Andxxxx;Anxx Drxxxx;Dxx Klxxx,xxxxxxxxxrkeley.edu;xxxxxxxxxxerkeley.edu;xxxxxxxxxxxerkeley.edu,Translating Neuralese,Translating Neuralese,11,JA,,,on,on,No. Do not include my submission in this dataset.,No,None,None
746,746X-C9C4J9A6B9,Learning Text Summarization using Semantic Units based LSTM Model,Shexxxx Soxx;Yisxxxx Lxx and Tonxxxxx Rxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"Text Summarization is condensing the source text into a shorter version
preserving its information content and overall meaning. It is very difficult
for human beings to manually summarize large documents of text. Various kinds
of text summarization models have been proposed to mine the semantics in the
original texts and paraphrase them. However, most of models only focus on one
of semantics and syntactic structure, while few of models focus on both of
these two parts. In this paper, on the basis of previous studies, we propose a
text summarization model which combines deep learning model and semantic
relation extraction, called Semantic Units based LSTM Model. It is composed of
two main stages, which are extracting semantic units from sentences and
generating text summaries by deep learning model. Finally, experiments and
comparisons have been presented to show the better performance of our model.",7 Feb 2017 09:40:44 GMT,Empirical/Data-Driven,Summarization,document summarization,Shexxxx,Soxx,xxxxxxxxxxdian.edu.cn,"Software Engineering Institute, Xidian University",No,Yisxxxx,Lxx,xxxxxxxxxian.edu.cn,"Software Engineering Institute, Xidian University",No,Tonxxxxx,Ruxx,xxxxxxxxxxxxxxxu.xidian.edu.cn,"School of Computer Science and Technology, Xidian University",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shexxxx,Soxx,"Software Engineering Institute, Xidian University",,,,,,xxxxxxxxxxdian.edu.cn,,Xi'an,Shaanxi,,China,,Shexxxx Soxx;Yisxxxx Lxx;Tonxxxxx Ruxx and Tecxxxxxxx Xixxxx,xxxxxxxxxxdian.edu.cn;xxxxxxxxxxian.edu.cn;xxxxxxxxxxxxxxxtu.xidian.edu.cn,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
749,749X-D3G3P9D5G4,Unsupervised Word Sense Disambiguation using Topic Models,Devxxxxx Sixxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Word Sense Disambiguation is an open problem in Natural Language Processing
which is particularly challenging and useful in the unsupervised setting where
all the words in any given text need to be disambiguated without using any
labelled data. In this paper, we propose a WSD system which uses topic models
to leverage the whole document as the context for a word to be disambiguated as
opposed to just the sentence in the current WSD systems. The proposed method is
a variant of Latent Dirichlet Allocation in which the topic proportions for a
document are replaced by synset proportions. We further utilize the information
in the WordNet by assigning non-uniform prior to synset distribution over words
and logistic-normal prior for document distribution over synsets. The proposed
method outperforms the current state-of-the-art unsupervised WSD system on the
SensEval-2, SensEval-3 and SemEval 2007 English All-Word WSD datasets by a
significant margin.",7 Feb 2017 09:47:29 GMT,Empirical/Data-Driven,Semantics,word sense disambiguation;  graphical models,Devenxxxxxxxxx,Chaxxxx,xxxxxxxxxxxxlot@gmail.com,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Devenxxxxxxxxx,Chaxxxx,Carnegie Mellon University,,,1415xxxxxxx,,,xxxxxxxxxs.cmu.edu,,Pittsburgh,PA,,United States,,Devxxxxx Sixxx,xxxxxxxxxxxxlot@gmail.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
750,750X-F9G9B3P3A6,A Mention-Ranking Model for Propositional Anaphora Resolution,Axx Marxxxxxx;Juxx Opxxx;Lxx Boxx and Anxxxx Frxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Resolving propositional anaphora is an important, but difficult task for text
understanding. With recent advances in representation learning, resolving this
type of anaphora becomes a tangible aim. A central property of propositional
anaphora is that it establishes a relation between the anaphor contained in the
anaphoric sentence and its (typically sentential) antecedent. 
We propose an LSTM-based mention-ranking model that learns how propositional
anaphors relate to their antecedent sentences in a Siamese network
architecture. 
We overcome the lack of training resources by generating large numbers of
artificial anaphoric sentence--antecedent pairs. Our model outperforms
state-of-the-art results on a Shell noun resolution dataset. We  also report
the first benchmark results on a propositional anaphora subset of the ARRAU
corpus. Our results show that the model learns to identify the true antecedent
using sentence representations without the guide of syntactic information.",7 Feb 2017 10:47:12 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse;  coreference resolution,Axx,Marxxxxxx,xxxxxxxxxxxxxxxxx.uni-heidelberg.de,"Institut für Computerlinguistik, Universität Heidelberg",No,Juxx,Opxxx,xxxxxxxxxxxxxheidelberg.de,Heidelberg University,No,Lxx,Boxx,xxxxxxxxxxxxheidelberg.de,Heidelberg University,No,Anxxxx,Frxxx,xxxxxxxxxxxxxheidelberg.de,Heidelberg Universiy,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxxxx,Frxxx,Heidelberg Universiy,,,+49 6xxxxxxxxxx,,,xxxxxxxxxxxxxheidelberg.de,,,,,Germany,,Axx Marxxxxxx;Juxx Opxxx;Lxx Boxx;Anxxxx Frxxx,xxxxxxxxxxxxxxxxx.uni-heidelberg.de;xxxxxxxxxxxxx-heidelberg.de;xxxxxxxxxxxxxheidelberg.de;xxxxxxxxxxxxx-heidelberg.de,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
752,752X-E8E5E7F5P3,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,Ioaxxxx Konxxxx;Srixxxxxxx Iyxx;Maxx Yatxxxx;Yexxx Chxx and Luxx Zetxxxxxxx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Monday,,Undecided (Generation Summarization),"Sequence-to-sequence models have shown strong performance across a broad range
of applications. However, their application to parsing and generating text
using Abstract Meaning Representation (AMR) has been limited, due to the
relatively limited amount of labeled data and the non-sequential nature of the
AMR graphs. 
We present a novel training procedure that can lift this limitation using
millions of unlabeled sentences and careful preprocessing of the AMR graphs. 
For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the
current best score reported without significant use of external semantic
resources.
For AMR generation, our model establishes a new state-of-the-art performance of
BLEU 33.8. 
We present extensive ablative and qualitative analysis including strong
evidence that sequence-based AMR models are robust against ordering variations
of graph-to-sequence conversions.",23 Apr 2017 02:35:34 GMT,Empirical/Data-Driven,Generation,,Ioaxxxx,Konxxxx,xxxxxxxxxxxxxashington.edu,University of Washington,No,Srixxxxxxx,Iyxx,xxxxxx@uw.edu,University of Washington,No,Maxx,Yatxxxx,xxxxxxxxxxxhington.edu,University of Washington,No,Yexxx,Chxx,xxxxxxxxxxxshington.edu,University of Washington,No,Luxx,Zettxxxxxxx,xxxxxxxxxxhington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxx,Yatxxxx,University of Washington,,,650xxxxxxx,,,xxxxxxxxxxxhington.edu,,,,,United States,,Ioaxxxx Konxxxx;Srixxxxxxx Iyxx;Maxx Yatxxxx;Yexxx Chxx;Luxx Zettxxxxxxx,xxxxxxxxxxxxxashington.edu;xxxxxxx@uw.edu;xxxxxxxxxxxshington.edu;xxxxxxxxxxxxshington.edu;xxxxxxxxxxxhington.edu,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,12,Ioannis Konstas,,"Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA",on,,"Yes, include my submission even if the paper is rejected.",No,None,None
753,753X-D5F4A5B7P6,Structural Embedding of Syntactic Trees for Machine Comprehension,Rxx Lxx;Juxxxx Hx;zx yaxx and wxx wx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"This paper develops a model that addresses syntactic embedding for machine
comprehension, a key task of natural language understanding. 
Our proposed model, \textit{structural embedding of syntactic trees} (SEST),
takes each word in a sentence, constructs a sequence of syntactic nodes
extracted from syntactic parse trees, and encodes the sequence into a vector
representation. The learned vector is then incorporated into neural attention
models, which allows learning the mapping of syntactic structures between
question and context pairs.
We evaluate our approach on SQuAD dataset and demonstrate that our model can
accurately identify the syntactic boundaries of the sentences and to extract
answers that are syntactically coherent over the baseline methods.",7 Feb 2017 10:39:46 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",context-aware question answering;  answer extraction;  open-domain question answering,Rxx,Lxx,xxxxxxx.cmu.edu,CMU LTI,No,Juxxxx,Hx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,zx,yaxx,xxxxxxxcmu.edu,Carnegie Mellon University,No,wxx,wxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rxx,Lxx,CMU LTI,,,412xxxxxxx,,,xxxxxxx.cmu.edu,,Pittsburgh,PA,,United States,,Rxx Lxx;Juxxxx Hx;zx yaxx;wxx wxx,xxxxxxx.cmu.edu;xxxxxxxxxcs.cmu.edu;xxxxxxx.cmu.edu;xxxxxxxxxs.cmu.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
754,754X-B9B5P5H8P5,Taking into Account Natural Language Fuzziness and Variability when Defining Syntax Constraints: Subject without Nominal Construction?,Adxxxx Torxxxx;Maxxx Dolxxxx and Phixxxxx Blxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"The aim of this work is to explain nominal constructions while taking into
account gradience and fuzziness. More specifically, it deals with specific
phenomena linked to the possibility of having a Subject without a Nominal
Construction. Due to this, the focus of this paper will be on presenting the
main constraints of Subject Construction and Nominal Constructions in Spanish
and special attention will be given to variability present in the extracted
Pronoun data. In describing the constructions, Property Grammars will be used
as
the formal theory. Data has been extracted from the Spanish Universal
Dependency Treebank Corpus and MarsaGram. It will be shown how the work
presented here could be applied in the form of an algorithm for parsing which
could be of benefit to various areas of language and technology such as
developing self-taught language learning software (in which the violations and
degree of violation could be tagged), data mining or human-machine interfaces.
The work presented here could also be key in explaining complexity in language
processing.",7 Feb 2017 10:56:35 GMT,Theoretical,"Tagging, chunking, syntax, and parsing",grammatical formalisms;  NLP applications;  NLP on noisy unstructured text;  syntax;  parsing;  adaptation to noisy data,Adxxxx,Torrexxxxxxxxxx,xxxxxxxxxxxxxxxxtudiants.urv.cat,Universitat Rovira i Virgili,No,Marixxxxxxxxx,Jiméxxxxxxxxxx,xxxxxxxxxxxxxximenez@urv.cat,Universitat Rovira i Virgili,No,Phixxxxx,Blxxxx,xxxxxxxxpl-aix.fr,CNRS & Université de Provence,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adxxxx,Torrexxxxxxxxxx,Universitat Rovira i Virgili,,,,,,xxxxxxxxxxxxxxxxtudiants.urv.cat,,,,,Spain,,Adxxxx Torxxxx;Maxxx Dolxxxx;Phixxxxx Blxxxx,xxxxxxxxxxxxxxxxtudiants.urv.cat;xxxxxxxxxxxxxxjimenez@urv.cat;xxxxxxxxxpl-aix.fr,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
755,755X-C3J3D7G2B9,Recurrent Attention Networks for Aspect Sentiment Analysis,Pexx Chxx;Zhoxxxxxx Sxx;Lixxxx Bixx and Wxx Yxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"We propose a novel framework based on neural networks to identify the sentiment
of opinion targets in one comment/review. Our framework adopts
multiple-attention mechanism to capture sentiment features separated by a long
distance, so that it is more robust against irrelevant information. The results
of multiple attentions are non-linearly combined with a GRU network, which
strengthens the expressive power of our model for handling more complications.
The weighted-memory mechanism not only helps us avoid the labor-intensive
feature engineering work, but also provides a tailor-made memory for different
opinion targets of a sentence. We examine the merit of our model on four
datasets: two are from SemEval 2014, consisting of reviews of restaurants and
laptops; a twitter dataset, for testing its performance on social media data;
and a Chinese news comment dataset, for testing our model's language
sensitivity. The experimental results show that our model outperforms the
state-of-the-art methods on different types of data.",7 Feb 2017 10:04:10 GMT,Applications/Tools,Sentiment analysis and opinion mining,sentiment analysis,Pexx,Chxx,xxxxxxxxxencent.com,Tencent Inc.,No,Zhoxxxxxx,Sxx,xxxxxxxxxxtencent.com,Tencent Inc.,No,Lixxxx,Bixx,xxxxxxxxxx@gmail.com,Tencent Inc.,No,Wxx,Yaxx,xxxxxxxxxxencent.com,Tencent Inc.,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lixxxx,Bixx,Tencent AI Lab,,,,,,xxxxxxxxxx@gmail.com,,,,,China,"I am a senior researcher at Tencent AI Lab. Before joining Tencent, I was a postdoc in Machine Learning Department at Carnegie Mellon University. I received my Ph.D. degree from The Chinese University of Hong Kong, and my M.Phil. degree from Peking University. I have interests in several broad research directions, such as natural language processing, information retrieval, and machine learning.",Pexx Chxx;Zhoxxxxxx Sxx;Lixxxx Bixx;Wxx Yaxx,xxxxxxxxxencent.com;xxxxxxxxxxxtencent.com;xxxxxxxxxxg@gmail.com;xxxxxxxxxxtencent.com,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
756,756X-A8E8F8H2E4,Transition-Based Disfluency Detection with LSTMs,Shaxxxx Waxx;Wanxxxxx Cxx;Meixxxx Zhxxx and Tixx Lx,Speech,Chxxxx Hoxx;Chixxxxxx Lxx,Reject,,Reject (Speech),"Disfluency detection is the task of recognizing infelicity spans in spoken
language transcripts. In this paper, we model the problem of disfluency
detection using a transition-based framework, which  has the ability to capture
global segment-level features. Our method incrementally constructs and labels
the disfluency segment of input sentences using a new transition system without
syntax information. Experiments show that our model achieves the
state-of-the-art f-score of  87.5\% on the commonly used English Switchboard
test set.",7 Feb 2017 11:55:45 GMT,Empirical/Data-Driven,Speech,spoken language understanding,Shaxxxx,Waxx,xxxxxxxxxxhit.edu.cn,Harbin Institute of Technology,No,Wanxxxxx,Cxx,xxxxxxxxxgmail.com,Harbin Institute of Technology,No,Meixxxx,Zhxxx,xxxxxxxxx@gmail.com,"Heilongjiang University, China",No,Tixx,Lxx,xxxxxxxxxp.126.com,Harbin Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shaxxxx,Waxx,Harbin Institute of Technology,,,,,,xxxxxxxxxxhit.edu.cn,,,,,China,,Shaxxxx Waxx;Wanxxxxx Cxx;Meixxxx Zhxxx;Tixx Lxx,xxxxxxxxxxhit.edu.cn;xxxxxxxxx@gmail.com;xxxxxxxxxx@gmail.com;xxxxxxxxxip.126.com,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
757,757X-D3A7P9B6G5,Translation Dictionary Expansion via Hierarchical Cognate and Language Relationship Models,Winxxxx Wx and Daxxx Yarxxxxx,Multilingual,Omxx Abxxx;Moxx Dixx,Reject,,Undecided (Multilingual),"Low-resource languages often suffer from a lack of high-coverage lexical
resources. In this paper, we propose a method to generate cognate lists by
clustering words from lexical resources. We then employ character-based machine
translation methods in solving the task of cognate chain completion by inducing
missing word translations from lower-coverage  dictionaries to fill gaps in the
cognate chain. We also show improvements over single language pair baselines
when employing novel but readily reproducible multi-language system combination
on the Romance and Turkic language families. For the Romance family, we show
that system combination using the results of clustering outperforms weights
derived from the historical-linguistic scholarship on language phylogenies. Our
approach is applicable to any language family and has not been previously
performed at such scale.",7 Feb 2017 11:49:44 GMT,Resources/Evaluation,Multilinguality,transliteration;  lexicon development;  multilingual applications;  multilingual resources;  lexical borrowing,Winxxxx,Wx,xxxxxxhu.edu,Johns Hopkins University,No,Daxxx,Yarxxxxx,xxxxxxxx@jhu.edu,Johns Hopkins University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Winxxxx,Wx,Johns Hopkins University,,,,,,xxxxxxhu.edu,,,MD,,United States,,Winxxxx Wx;Daxxx Yarxxxxx,xxxxxxhu.edu;xxxxxxxxy@jhu.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
759,759X-B9B6B5J9C3,Joint Modeling of Content and Discourse Relations in Dialogues,Kexxxx Qxx;Lx Waxx and Joxxxx Kx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Tuesday,,Undecided (Dialog Interactive Systems),"We present a joint modeling approach to identify salient discussion points in
spoken meetings as well as to label the discourse relations between speaker
turns. A variation of our model is also discussed when discourse relations are
treated as latent variables. Experimental results on two popular meeting
corpora show that our joint model can outperform state-of-the-art approaches
for both phrase-based content selection and discourse relation prediction
tasks. We also evaluate our model on predicting the consistency among team
members' understanding of their group decisions. Classifiers trained with
features constructed from our model achieve significant better predictive
performance than the state-of-the-art.",21 Apr 2017 18:32:58 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Kexxxx,Qxx,xxxxxxxxxx@gmail.com,Northeastern University,No,Lx,Waxx,xxxxxxxxxs.neu.edu,Northeastern University,No,Joxxxx,Kxx,xxxxxxmit.edu,Massachusetts Institute of Technology,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lx,Waxx,Northeastern University,,,607xxxxxxx,,,xxxxxxxxxs.neu.edu,,Boston,MA,,United States,,Kexxxx Qxx;Lx Waxx;Joxxxx Kxx,xxxxxxxxxx@gmail.com;xxxxxxxxxcs.neu.edu;xxxxxxxmit.edu,Joint Modeling of Content and Discourse Relations in Dialogues,Joint Modeling of Content and Discourse Relations in Dialogues,11,Lu Wang,,"Northeastern University
360 Huntington Ave., Boston, MA 02115, USA",,on,Only include my submission if it is accepted.,No,None,None
760,760X-C6B2H9H7F7,Learning to Skim Text,Adxxx Wxx;Honxxxx Lxx and Quxx L,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Poster Tuesday,,Undecided (Machine Learning),"Recurrent Neural Networks are showing much promise in many sub-areas of natural
language processing, ranging from document classification to machine
translation to automatic question answering. Despite their promise, many
recurrent models have to read the whole text word by word, making it slow to
handle long documents. For example, it is difficult to use a recurrent network
to read a book and answer questions about it. In this paper, we present an
approach of reading text while skipping irrelevant information if needed. The
underlying model is a recurrent network that learns how far to jump after
reading a few words of the input text. We employ a standard policy gradient
method to train the model to make discrete jumping decisions. In our benchmarks
on four different tasks, including number prediction, sentiment analysis, news
article classification and automatic Q\&A, our proposed model, a modified LSTM
with jumping, is up to 6 times faster than the standard sequential LSTM, while
maintaining the same or even better accuracy.",23 Apr 2017 02:59:20 GMT,Applications/Tools,Machine learning,,Adaxxxxxx,Yx,xxxxxxxx.cmu.edu,Carnegie Mellon University,No,Honxxxx,Lxx,xxxxxxxxogle.com,Google,No,Quxx,Lx,xxxxxxxgle.com,Google,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wxx,Yx,Carnegie Mellon University,,,,,,xxxxxxxxxx@gmail.com,,,PA,,United States,,Adxxx Wxx;Honxxxx Lxx;Quxx Lx,xxxxxxxx.cmu.edu;xxxxxxxxoogle.com;xxxxxxxogle.com,Learning to Skim Text,Learning to Skim Text,11,Adams Wei Yu,,"Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA, 15213.",,on,Only include my submission if it is accepted.,No,None,None
761,761X-C6J9A6J6F9,Get To The Point: Summarization with Pointer-Generator Networks,Abixxxx Sxx;Pexxx Jx and Chrixxxxxxx Dx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Accept - Oral Tuesday,,Undecided (Generation Summarization),"Neural sequence-to-sequence models have provided a viable new approach for
abstractive text summarization (meaning they are not restricted to simply
selecting and rearranging passages from the original text). However, these
models have two shortcomings: they are liable to reproduce factual details
inaccurately, and they tend to repeat themselves. In this work we propose a
novel architecture that augments the standard sequence-to-sequence attentional
model in two orthogonal ways. First, we use a hybrid pointer-generator network
that can copy words from the source text via pointing, which aids accurate
reproduction of information, while retaining the ability to produce novel words
through the generator. Second, we use coverage to keep track of what has been
summarized, which discourages repetition. We apply our model to the CNN / Daily
Mail summarization task, outperforming the current abstractive state-of-the-art
by at least 2 ROUGE points.",26 Apr 2017 02:05:46 GMT,Empirical/Data-Driven,Summarization,,Abixxxx,Sxx,xxxxxxxxxanford.edu,Stanford University,No,Petxxxxx,Lxx,xxxxxxxxxxgoogle.com,Google Brain,No,Chrisxxxxxxxxx,Manxxxx,xxxxxxxxxxxstanford.edu,Stanford University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Abixxxx,Sxx,Stanford University,,,,,,xxxxxxxxxanford.edu,,,CA,,United States,,Abixxxx Sxx;Pexxx Jx;Chrixxxxxxx Dx,xxxxxxxxxanford.edu;xxxxxxxxxx@google.com;xxxxxxxxxxxxstanford.edu,Get To The Point: Summarization with Pointer-Generator Networks,Get To The Point: Summarization with Pointer-Generator Networks,11,Abigail See,,,on,on,No. Do not include my submission in this dataset.,No,None,None
762,762X-B2G4D5C8B7,DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks,Linxxxxx Koxx;Chxxx Albxxxx;Daxxxx Anxxx;Ivxx Bogxxxx and Daxxx Wexx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"In this work, we present a compact, modular framework for constructing novel
recurrent neural architectures. Our basic module is a new generic unit, the
Transition Based Recurrent Unit (TBRU). In addition to hidden layer
activations, TBRUs have discrete state dynamics that allow network connections
to be built dynamically as a function of intermediate activations. By
connecting multiple TBRUs, we can extend and combine commonly used
architectures such as sequence-to-sequence, attention mechanisms, and recursive
tree-structured models. A TBRU can also serve as both an encoder for downstream
tasks and as a decoder for its own task simultaneously, resulting in more
accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic
Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more
accurate and efficient than seq2seq with attention for syntactic dependency
parsing and yields more accurate multi-task learning for extractive
summarization tasks.",7 Feb 2017 11:10:42 GMT,Empirical/Data-Driven,Machine learning,discriminative learning methods;  parsing,Linxxxxx,Koxx,xxxxxxxxxcs.cmu.edu,Carnegie Mellon University,No,Chxxx,Albxxxx,xxxxxxxxxxxti@gmail.com,Google,No,Daxxxx,Anxxx,xxxxxxxxxxx@google.com,Google Inc.,No,Ivxx,Bogxxxx,xxxxxxxxxoogle.com,Google Inc.,No,Daxxx,Wexxx,xxxxxxxxxoogle.com,Google,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Linxxxxx,Koxx,Carnegie Mellon University,,,412-xxxxxxxx,,,xxxxxxxxxcs.cmu.edu,,,,,United States,,Linxxxxx Koxx;Chxxx Albxxxx;Daxxxx Anxxx;Ivxx Bogxxxx;Daxxx Wexxx,xxxxxxxxxcs.cmu.edu;xxxxxxxxxxxxti@gmail.com;xxxxxxxxxxxr@google.com;xxxxxxxxxgoogle.com;xxxxxxxxxgoogle.com,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
763,763X-A3H9P3A6D6,Bayesian Modeling of Lexical Resources for Low-Resource Settings,Nicxxxxx Andxxxx;Maxx Drxxxx;Benxxxxx Vxx and Jaxxx Eixxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Tuesday,,Undecided (Machine Learning),"Lexical resources such as dictionaries and gazetteers are often used
as auxiliary data for tasks such as part-of-speech induction and named-entity
recognition. However, discriminative training with lexical features requires
annotated data to reliably estimate the lexical feature weights and may result
in overfitting the lexical features at the expense of features which generalize
better.
In this paper, we investigate a more robust approach: we stipulate
that the lexicon is the result of an assumed generative
process. Practically, this means that we may treat the lexical
resources as observations under the proposed generative model.
The lexical resources provide training data for the generative model
without requiring separate data to estimate lexical feature
weights. We evaluate the proposed approach in two settings:
part-of-speech induction and low-resource named-entity recognition.",27 Apr 2017 00:43:54 GMT,Theoretical,Machine learning,,Nicxxxxx,Andxxxx,xxxxxhu.edu,Johns Hopkins University,No,Maxx,Drxxxx,xxxxxxxxxs.jhu.edu,Johns Hopkins University,No,Benxxxxx,Vanxxxxxx,xxxxxxxxxcs.jhu.edu,JHU,No,Jaxxx,Eixxxx,xxxxxxxx.jhu.edu,Johns Hopkins University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nicxxxxx,Andxxxx,Johns Hopkins University,,,443xxxxxxx,,,xxxxxhu.edu,,,,,United States,,Nicxxxxx Andxxxx;Maxx Drxxxx;Benxxxxx Vxx;Jaxxx Eixxxx,xxxxxhu.edu;xxxxxxxxxcs.jhu.edu;xxxxxxxxxxcs.jhu.edu;xxxxxxxxs.jhu.edu,Bayesian Modeling of Lexical Resources for Low-Resource Settings,Bayesian Modeling of Lexical Resources for Low-Resource Settings,11,Nicholas Andrews,,"Johns Hopkins University
Baltimore, MD",on,,No. Do not include my submission in this dataset.,No,None,None
764,764X-D3E6C9D5B3,Search-based Neural Structured Learning for Sequential Question Answering,Moxxx Iyxxx;Wenxxxx Yxx and Minxxxxx Chxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Poster Tuesday,,Undecided (IE QA Text Mining Applications),"Recent work in semantic parsing for question answering has focused on long and
complicated questions, many of which would seem unnatural if asked in a normal
conversation between two humans. In an effort to explore a conversational QA
setting, we present a more realistic task: answering sequences of simple but
inter-related questions. We collect a dataset of 6,066 question sequences that
inquire about semi-structured tables from Wikipedia, with 17,553
question-answer pairs in total. To solve this sequential question answering
task, we propose a novel dynamic neural semantic parsing framework trained
using a weakly supervised reward-guided search. Our model effectively leverages
the sequential context to outperform state-of-the-art QA systems that are
designed to answer highly complex questions.",21 Apr 2017 22:46:25 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Moxxx,Iyxxx,xxxxxxxxgmail.com,"University of Maryland, College Park",No,Wenxxxx,Yxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Minxxxxx,Chxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Moxxx,Iyxxx,Allen Institute for Artificial Intelligence,,,571xxxxxxx,,,xxxxxxxxgmail.com,,,,,United States,,Moxxx Iyxxx;Wenxxxx Yxx;Minxxxxx Chxxx,xxxxxxxxgmail.com;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxicrosoft.com,Search-based Neural Structured Learning for Sequential Question Answering,Search-based Neural Structured Learning for Sequential Question Answering,11,Mohit Iyyer,,,,on,Only include my submission if it is accepted.,No,None,None
765,765X-C3E9B4A8J4,Unsupervised Pre-training for Deep Relation Extraction Models,Zhxxxx Lx;Lixxxx Qx;Qioxxxxx Xx and Maxx Joxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Relation extraction models based on deep learning have been attracting a lot of
attention recently. Little research is carried out to reduce their need of
labelled training data. In this work, we propose three unsupervised
pre-training methods customized for relation extraction. The pre-trained models
need only half or even less training data to achieve equivalent performance as
the same models without pre-training.",7 Feb 2017 12:00:41 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  relation/event extraction,Zhxxxx,Lx,xxxxxxxxxx4@gmail.com,DATA61,No,Lixxxx,Qx,xxxxxxxxxxxxta61.csiro.au,Data61,No,Qioxxxxx,Xx,xxxxxxxxxxxxxata61.csiro.au,DATA61,No,Maxx,Johxxxx,xxxxxxxxxxxn@mq.edu.au,Macquarie University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lixxxx,Qx,Data61,,,,,,xxxxxxxxxxxxta61.csiro.au,,Canberra,ACT,,Australia,,Zhxxxx Lx;Lixxxx Qx;Qioxxxxx Xx;Maxx Johxxxx,xxxxxxxxxx4@gmail.com;xxxxxxxxxxxxxta61.csiro.au;xxxxxxxxxxxxxxata61.csiro.au;xxxxxxxxxxxon@mq.edu.au,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
766,766X-J2C8F5P9E8,Learning What is Important: Towards Informed Question Answering,Daxxxx Khaxxxxx;Tuxxxx Khxx;Asxxxx Sabxxxxxx and Dxx Rxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Many  natural  language  question  answering  (QA)  systems  are  easily 
distracted by irrelevant or redundant terms in questions,          especially 
when  dealing  with
 long or  multi-sentence  questions.                 To  address this  challenge, 
we 
investigate 
a  general notion of ""essentiality"" for each element of a          question.   
To  circumvent 
the  inherent ambiguity  in  what  is  essential,  we  resort to  group 
intelligence  via 
crowd-sourced question annotation, asking workers to annotate terms that they
believe to be essential for answering about 2200 elementary-level  science 
questions.   We 
demonstrate that this notion of essentiality is learnable. Our                   
max-margin 
essential  term  classifier trained on this annotated dataset substantially
outperforms both PMI-based (unsupervised) as well as frequency-based
(supervised)                baselines  by  5%  each  on  the per-token F1 score and
per-sentence MAP. Employing this component’s output in two state-of-the-art
solvers for science QA results              in  more        informed 
decision-making, as  evidenced by  higher  accuracy  for  one solver  and 
fewer  lucky  guesses  (hence more robust predictions) for the other.",7 Feb 2017 11:25:18 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",scalability and portability of question answering systems;  context-aware question answering,Daxxxx,Khaxxxxx,xxxxxxxxxxxxabi@gmail.com,"University of Illinois, Urbana-Champaign",No,Tuxxxx,Khxx,xxxxxxxxxllenai.org,Allen Institute for Artificial Intelligence,No,Asxxxx,Sabxxxxxx,xxxxxxxxxllenai.org,Allen Institute for Artificial Intelligence,No,Dxx,Roxx,xxxxxxxxinois.edu,"University of Illinois, Urbana-Champaign",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Daxxxx,Khaxxxxx,University of Pennsylvania,,,,,,xxxxxxxxxxxxabi@gmail.com,,,,,United States,PhD Student at UPenn,Daxxxx Khaxxxxx;Tuxxxx Khxx;Asxxxx Sabxxxxxx;Dxx Roxx,xxxxxxxxxxxxabi@gmail.com;xxxxxxxxxxllenai.org;xxxxxxxxxxllenai.org;xxxxxxxxxinois.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
767,767X-E9C2G5A7C7,Modeling Essay Writing Traits Using Recurrent Neural Networks,Kaxxx Tagxxxxxx and Hwxx Txx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Providing proper feedback about different dimensions of essay writing is
essential for improving this skill in self-learning students. Several datasets
have been created in recent years to address various aspects of essay writing.
However, the performance of the existing systems is still far from perfect. In
this paper, we propose a novel framework based on recurrent neural networks to
model these aspects in student essays, without manually designed task-specific
features. Among various writing aspects, we have used our framework to model
argument strength and essay organization in student essays. The experiments
show that our method outperforms strong state-of-the-art systems and leads to
relative error reductions of 7.0% and 13.5% (in terms of mean squared error) in
argument strength and essay organization tasks, respectively.",7 Feb 2017 11:52:21 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",educational applications,Kaxxx,Tagxxxxxx,xxxxxxxxxxxxour@gmail.com,National University of Singapore,No,Hwexxxxx,Nx,xxxxxxxxxxnus.edu.sg,National University of Singapore,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kaxxx,Tagxxxxxx,National University of Singapore,,,659xxxxxxx,,,xxxxxxxxxxxxour@gmail.com,,,,,Singapore,,Kaxxx Tagxxxxxx;Hwxx Txx,xxxxxxxxxxxxour@gmail.com;xxxxxxxxxx.nus.edu.sg,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
768,768X-E2E7G4A6H9,Detecting Lexical Entailment in Context,Yogxxxxx Vyxx and Maxxxx Carxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Detecting entailment between words is a key task for several NLP applications.
Previous work has largely focused on entailment between words out of context.
We propose, instead, to address lexical entailment in context, providing
exemplar sentences to ground the meaning of words considered in the entailment
relation. We show that contextualized word representations constructed from
existing word embeddings, and word-context similarity features lead to
significant improvements over context-agnostic models on two novel entailment
test sets, and also improve the state-of-the-art on the related task of
detecting semantic relations in context.",7 Feb 2017 11:33:27 GMT,Empirical/Data-Driven,Semantics,cross-lingual approaches;  lexical semantics;  multilingual applications;  textual entailment and paraphrasing;  distributional similarity;  semantic relations,Yogxxxxx,Vyxx,xxxxxxxxxcs.umd.edu,University of Maryland,No,Maxxxx,Carxxxx,xxxxxxxxs.umd.edu,University of Maryland,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yogxxxxx,Vyxx,University of Maryland,,,,,,xxxxxxxxxcs.umd.edu,,College Park,MD,,United States,,Yogxxxxx Vyxx;Maxxxx Carxxxx and ),xxxxxxxxxcs.umd.edu;xxxxxxxxxs.umd.edu,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
769,769X-C6G6D6D8B3,Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,Hx Hx;Anxxxx Balaxxxxxxxx;Mixxxx Erxx and Pexxx Lixx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Poster Tuesday,,Undecided (Dialog Interactive Systems),"We study a \emph{symmetric collaborative dialogue} setting
in which two agents, each with private knowledge,
must strategically communicate to achieve a common goal.
The open-ended dialogue state in this setting poses new challenges for existing
dialogue systems.
We collected a dataset of 11K human-human dialogues,
which exhibits interesting lexical, semantic, and strategic elements.
To model
both structured knowledge and unstructured language,
we propose a neural model with dynamic knowledge graph embeddings
that evolve as the dialogue progresses.
Automatic and human evaluations show that our model is both more effective
at achieving the goal and more human-like than baseline neural and rule-based
models.",24 Apr 2017 07:28:43 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Hx,Hx,xxxxxxxxgmail.com,Stanford University,No,Anxxxx,Balaxxxxxxxx,xxxxxxxxxxx28@gmail.com,Stanford University,No,Mixxxx,Erxx,xxxxxxxxxxtanford.edu,Stanford University,No,Pexxx,Lixxx,xxxxxxxxxxxtanford.edu,Stanford University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hx,Hx,Stanford University,,,,,,xxxxxxxxgmail.com,,,AL,,United States,,Hx Hx;Anxxxx Balaxxxxxxxx;Mixxxx Erxx;Pexxx Lixxx,xxxxxxxxgmail.com;xxxxxxxxxxxx28@gmail.com;xxxxxxxxxxxtanford.edu;xxxxxxxxxxxstanford.edu,Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,11,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
770,770X-F6H6A7G3G3,Learning Word Embeddings for Textual Entailment,Yuxxxx Lxx and Jixx Jixxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"In this paper, we study how to train special word embeddings to encode lexical
entailment relations and how to use such word embeddings in a neural network
model for textual entailment.
We consider four lexical entailment relations and automatically derive word
pairs with their relation labels from WordNet.
We then use either a standard neural network model or a binary vector model
proposed by us to learn word embeddings from these labeled word pairs.
When these special word embeddings are used in combination with word2vec
embeddings in a decomposable attention model for textual entailment on the SICK
dataset, we find that the model works better than using purely word2vec
embeddings.",7 Feb 2017 11:53:39 GMT,Applications/Tools,Semantics,lexical semantics;  textual entailment and paraphrasing,Yuxxxx,Lxx,xxxxxxxxxxxxxdis.smu.edu.sg,Singapore Management University,No,Jixx,Jixxx,xxxxxxxxxxsmu.edu.sg,Singapore Management University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuxxxx,Lxx,Singapore Management University,,,,,,xxxxxxxxxxxxxdis.smu.edu.sg,,,,,Singapore,,Yuxxxx Lxx;Jixx Jixxx,xxxxxxxxxxxxxdis.smu.edu.sg;xxxxxxxxxx@smu.edu.sg,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
771,771X-C7E5F2H6P3,Don't understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures,Irxxx Hapxxxxxx and  Axxxx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Oral Tuesday,,Undecided (Discourse Pragmatics),"An interesting aspect of structured prediction is the evaluation of an output
structure against the gold standard. Especially in the loss-augmented setting,
the need of finding the max-violating constraint has severely limited the
expressivity of effective loss functions.
In this paper, we trade off exact computation for enabling the use and study of
more complex loss functions for coreference resolution. Most interestingly, we
show that such functions can be (i) automatically learned also from
controversial but commonly accepted coreference measures, e.g., MELA, and (ii)
successfully used in learning algorithms. The accurate model comparison on the
standard CoNLL-2012 setting shows the benefit of more expressive loss
functions.",30 Apr 2017 23:00:36 GMT,Empirical/Data-Driven,Discourse and pragmatics,,Irxxx,Hapxxxxxx,xxxxxxxxxxxxhyk@unitn.it,University of Trento,No,Alexxxxxxx,Mosxxxxxx,xxxxxxxxxx@gmail.com,Qatar Computing Research Institute (prof. in Computer Science at University of Trento),No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Irxxx,Hapxxxxxx,University of Trento,,,3934xxxxxxxx,,,xxxxxxxxxxxxhyk@unitn.it,,Trento,TN,,Italy,,Irxxx Hapxxxxxx; Axxxx and rx Mosxxxxxx,xxxxxxxxxxxxhyk@unitn.it;xxxxxxxxxxi@gmail.com,Don't understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures,Don't understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures,11,Iryna Haponchyk,,,,on,No. Do not include my submission in this dataset.,No,None,None
772,772X-F7P3E3E7P6,HieraVec: A Flexible Method for Joint Learning of Topic and Embedding Model,Bx Zhxx;Qixx Xixx and Yixxx Huxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Topic models and embedding models are two most popular categories of techniques
to learn latent 
semantics from text. In topic models each word is generated according to its
global context such as the document or sentence, while in embedding models each
word occurrence is measured by its local context such as surrounding words. So
it is expected to train topic and embedding model jointly by utilizing
multi-context information to learn better representations. In this paper, we
design a flexible method named HieraVec to achieve this goal, which can
integrate various kinds of topic and embedding models together. Experimental
results show that HieraVec achieves improvements in both individual components;
in some text classification tasks, it achieves state-of-the-art results. What's
more, we implement HieraVec on Spark that can train on a large-scale corpus and
achieves good scalability on cluster.",7 Feb 2017 11:56:34 GMT,Theoretical,"Document analysis including text categorization, topic models, and retrieval",unsupervised and semi-supervised learning;  text mining;  document mining,Bx,Zhxx,xxxxxxxxxxxxxail.nju.edu.cn,Nanjing University,No,Qixx,Xixx,xxxxxxxxx76@163.com,Nanjing University,No,Yixxx,Huxxx,xxxxxxxxju.edu.cn,Nanjing University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bx,Zhxx,Nanjing University,,,,,,xxxxxxxxxxxxxail.nju.edu.cn,,,,,China,,Bx Zhxx;Qixx Xixx;Yixxx Huxxx,xxxxxxxxxxxxxail.nju.edu.cn;xxxxxxxxxx76@163.com;xxxxxxxxxju.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
774,774X-P8E6B9E5E7,From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood,Kexxxx Gxx;Panxxxxx Pasxxxx;Evxx Lxx and Pexxx Lixx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Oral Tuesday,,Undecided (Machine Learning),"Our goal is to learn a semantic parser that maps natural language utterances
into executable programs when only indirect supervision is available: examples
are labeled with the correct execution result, but not the program itself.
Consequently, we must search the space of programs for those that output the
correct result, while not being misled by \emph{spurious programs}: incorrect
programs that coincidentally output the correct result. We connect two common
learning paradigms, reinforcement learning (RL) and maximum marginal likelihood
(MML), and then present a new learning algorithm that combines the strengths of
both. The new algorithm guards against spurious programs by combining the
systematic search traditionally employed in MML with the randomized exploration
of RL, and by updating parameters such that probability is spread more evenly
across consistent programs. We apply our learning algorithm to a new neural
semantic parser and show significant gains over existing state-of-the-art
results on a recent context-dependent semantic parsing task.",23 Apr 2017 10:48:52 GMT,Empirical/Data-Driven,Machine learning,,Kexxxx,Gxx,xxxxxxxxnford.edu,Stanford University,No,Panxxxxx,Pasxxxx,xxxxxxxxxxxxstanford.edu,Stanford University,No,Evxx,Lxx,xxxxxxxxxxanford.edu,Stanford,No,Pexxx,Lixxx,xxxxxxxxxxxtanford.edu,Stanford University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kexxxx,Gxx,Stanford University,,,+1 91xxxxxxxxxx,,,xxxxxxxxnford.edu,,Stanford,CA,,United States,,Kexxxx Gxx;Panxxxxx Pasxxxx;Evxx Lxx;Pexxx Lixxx,xxxxxxxxnford.edu;xxxxxxxxxxxx.stanford.edu;xxxxxxxxxxtanford.edu;xxxxxxxxxxxstanford.edu,From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood,From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood,12,Kelvin Guu,,"Stanford University
450 Serra Mall, Stanford, CA 94305",on,on,No. Do not include my submission in this dataset.,No,None,None
775,775X-H3P2E3H2J8,A Geometric Contextual Model for Identifying Unseen Metaphors,Stexxxx McGxxxxx;Matxxxx Puxxxx and Gerxxxx Wixxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"We present a method for metaphor identification based on an explicitly
geometric approach to modelling salient lexical associations, and test it on a
task of distinguishing metaphoric from literal uses of adjective-noun phrases.
By contextually projecting candidate word pairs into interpretably geometric
spaces, we show that it obtains state-of-the-art performance, while providing a
method which is effectively \emph{zero-shot} -- able to be applied to new
unseen phrases. Our dynamically context-sensitive model is inspired by
theoretical insight into the situational nature of language and cognition.",7 Feb 2017 11:54:11 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,multiword semantics/compositionality;  lexical semantics;  semantic relations;  figurative language;  NLP on Wikipedia and other collaboratively constructed resources;  pragmatics,Stexxxx,McGxxxxx,xxxxxxxxxxxr@qmul.ac.uk,Queen Mary University of London,No,Matxxxx,Puxxxx,xxxxxxxxxqmul.ac.uk,Queen Mary University of London,No,Gerxxxx,Wigxxxx,xxxxxxxxxxxxxns@qmul.ac.uk,Queen Mary University of London,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Stexxxx,McGxxxxx,Queen Mary University of London,,,075 xxxxxxxxx,,,xxxxxxxxxxxr@qmul.ac.uk,,,,,United Kingdom,,Stexxxx McGxxxxx;Matxxxx Puxxxx;Gerxxxx Wigxxxx,xxxxxxxxxxxr@qmul.ac.uk;xxxxxxxxxxqmul.ac.uk;xxxxxxxxxxxxxins@qmul.ac.uk,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
776,776X-C7A9C4C4D6,Unsupervised Extraction of Co-Hyponym Sets,Alexxxxxx Panxxxxxx;Dmxxxx Ustxxxx;Stexxxx Farxxxx;Sixxxx Paxxx and Chxxx Bixxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"We introduce a new lexical-semantic structure called co-set (a set of
co-hyponyms), which consists of closely related co-hyponyms and their common
hypernyms and provide an unsupervised method for efficient extraction of this
structure from corpora. Creation of this structure is motivated by the
hypothesis that joint extraction is more robust than the extraction of binary
relations. Evaluations against WordNet and BabelNet as well as two large-scale
crowdsourcing studies indicates a high quality of the extracted semantic
relations. Besides, we show that co-sets give rise to a powerful method for
taxonomy induction, outperforming all participants of the TExEval task at
SemEval 2016 by a large margin.",7 Feb 2017 11:41:33 GMT,Empirical/Data-Driven,Semantics,graph-based algorithms;  distributional similarity;  semantic relations;  word sense induction;  ontological semantics;  semantic knowledge induction,Alexxxxxx,Panxxxxxx,xxxxxxxxxxxxxxander@gmail.com,TU Darmstadt,No,Dmxxxx,Ustxxxx,xxxxxxxxxxxxov@gmail.com,Krasovskii Institute of Mathematics and Mechanics,No,Stexxxx,Farxxxx,xxxxxxxxxxxxxxxxxk.uni-mannheim.de,University of Mannheim,No,Simoxxxxxxxx,Ponxxxxx,xxxxxxxxxxxxxxxxk.uni-mannheim.de,University of Mannheim,No,Chxxx,Biexxxx,xxxxxxxxxxxxxxxxik.uni-hamburg.de,University of Hamburg,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Alexxxxxx,Panxxxxxx,University of Hamburg,,,,,,xxxxxxxxxxxxxxander@gmail.com,,Darmstadt,,,Germany,,Alexxxxxx Panxxxxxx;Dmxxxx Ustxxxx;Stexxxx Farxxxx;Sixxxx Paxxx;Chxxx Biexxxx,xxxxxxxxxxxxxxander@gmail.com;xxxxxxxxxxxxlov@gmail.com;xxxxxxxxxxxxxxxxxik.uni-mannheim.de;xxxxxxxxxxxxxxxxxk.uni-mannheim.de;xxxxxxxxxxxxxxxxxik.uni-hamburg.de,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
777,777X-B2C4F2D6B6,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,Akxxx Saxxxx;Kazxxxx Haxxxx;Naxxxx Okaxxxx and Kenxxxx Ixx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Accept - Oral Monday,,Undecided (Sentiment Analysis Opinion Mining),"We presents in this paper our approach for modeling inter-topic preferences of
Twitter
users: for example, ""those who agree with the Trans-Pacific Partnership (TPP)
also agree
with free trade"". This kind of knowledge is useful not only for stance
detection across multiple topics but also for various real-world applications
including public opinion survey,
electoral prediction, electoral campaigns, and online debates. In order to
extract
users' preferences on Twitter, we design linguistic patterns in which people
agree
and disagree about specific topics (e.g., ""A is completely wrong'').
By applying these linguistic patterns to a collection of tweets, we extract
statements agreeing and disagreeing with various topics. Inspired by previous
work on
item recommendation, we formalize the task of modeling inter-topic preferences
as matrix factorization: representing users' preference as a user-topic matrix
and mapping both users and topics onto a latent feature space that abstracts
the preferences. Our experimental results demonstrate both that our presented
approach is useful in predicting missing preferences of users and that the
latent vector representations of topics successfully encode inter-topic
preferences.",22 Apr 2017 07:23:41 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,,Akxxx,Saxxxx,xxxxxxxxxxxtohoku.ac.jp,Tohoku University,No,Kazxxxx,Haxxxx,xxxxxxxxxxxxtohoku.ac.jp,Tohoku University,No,Naxxxx,Okaxxxx,xxxxxxxxxxxx.tohoku.ac.jp,Tohoku University,No,Kenxxxx,Inxx,xxxxxxxxxxxohoku.ac.jp,Tohoku University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Akxxx,Saxxxx,"Recruit Technologies Co., Ltd.",,,+81-8xxxxxxxxxxx,,,xxxxxxxxxxxcruit.co.jp,,,,,Japan,,Akxxx Saxxxx;Kazxxxx Haxxxx;Naxxxx Okaxxxx;Kenxxxx Inxx,xxxxxxxxxxxtohoku.ac.jp;xxxxxxxxxxxx.tohoku.ac.jp;xxxxxxxxxxxxx.tohoku.ac.jp;xxxxxxxxxxxtohoku.ac.jp,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,11,Akira Sasaki,,"Graduate School of Information Sciences, Tohoku University
6-3-09 Aoba, Aramaki-aza Aoba-ku, Sendai, 980-8579, Japan",on,,"Yes, include my submission even if the paper is rejected.",No,None,None
778,778X-F7J4G3C8H3,Automatic Generation of Tunable Analogy Benchmarks for Word Representations,Taxxx Sakxxxxx;Suxx Bhxx and Prxxxx Visxxxxx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"In this paper we present a method to automatically generate syntactic analogy
datasets for the evaluation of word representations in an unsupervised manner.
This also allows for customization in terms of word-frequencies, syntactic
rules, part-of-speech tags and size of the dataset. We show the ability of our
method to generate cross-lingual analogy task datasets for languages other than
English, where evaluation datasets are limited if not nonexistent, by
constructing datasets for French, German, Spanish, Arabic and Hebrew. 

Our method clusters pairs of words into morphological rules in an unsupervised
manner, using which we generate analogy questions for different rules. We show
the quality of an automatically generated dataset by checking the correlation
of the performance of different word representations on it with the performance
of the same representations on the Google analogy dataset. The values exhibited
a high correlation of 0.95. Moreover, we showcase the benefits of customization
through studying the performance of different word representations when varying
the frequency of words in the dataset.",7 Feb 2017 11:08:32 GMT,Applications/Tools,Resources and evaluation,morphology;  distributional similarity;  evaluation metrics,Taxxx,Sakxxxxx,xxxxxxxxxxxxni@gmail.com,University of Illinois at Urbana-Champaign,No,Suxx,Bhxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana-Champaign,No,Prxxxx,Visxxxxxx,xxxxxxxxxxlinois.edu,University of Illinois at Urbana-Champaign,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Taxxx,Sakxxxxx,University of Illinois at Urbana-Champaign,,,1-217xxxxxxxxx,,,xxxxxxxxxxxxni@gmail.com,,Urbana,IL,,United States,,Taxxx Sakxxxxx;Suxx Bhxx;Prxxxx Visxxxxxx,xxxxxxxxxxxxni@gmail.com;xxxxxxxxxxllinois.edu;xxxxxxxxxxllinois.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
779,779X-F8P2C6P3F2,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,Yxx Chxx;Yaxx Lxx;Yoxx Chxxx and Vixxxx O.xx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Accept - Poster Tuesday,,Undecided (Machine Translation),"While end-to-end neural machine translation (NMT) has made remarkable progress
recently, it still suffers from the data scarcity problem for low-resource
language pairs and domains. In this paper, we propose a method for
zero-resource NMT by assuming that parallel sentences have close probabilities
of generating a sentence in a third language. Based on the assumption, our
method is able to train a source-to-target NMT model (``student'') without
parallel corpora available guided by an existing pivot-to-target NMT model
(``teacher'') on a source-pivot parallel corpus. Experimental results show that
the proposed method significantly improves over a baseline pivot-based model by
+3.0 BLEU points across various language pairs.",23 Apr 2017 11:48:59 GMT,Theoretical,Machine translation,,Yxx,Chxx,xxxxxxxxxxxek@gmail.com,The University of Hong Kong,No,Yaxx,Lxx,xxxxxxxxxxxxxsinghua.edu.cn,Tsinghua University,No,Yoxx,Chxxx,xxxxxxxxxxx01@gmail.com,Tsinghua University,No,Victxxxxxxx,Lx,xxxxxxx.hku.hk,The University of Hong Kong,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yxx,Chxx,The University of Hong Kong,,,,,,xxxxxxxxxxxek@gmail.com,,,,,Hong Kong Special Administrative Region of China,,Yxx Chxx;Yaxx Lxx;Yoxx Chxxx;Vixxxx O.xx,xxxxxxxxxxxek@gmail.com;xxxxxxxxxxxxxxsinghua.edu.cn;xxxxxxxxxxxx01@gmail.com;xxxxxxxe.hku.hk,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,11,Yun Chen,,"The University of Hong Kong, Pokfolam Road, Hong Kong",on,on,Only include my submission if it is accepted.,No,None,None
780,780X-D2P4C6J8C3,Learning to Solve Math Word Problems in Two Stages,Danxxxx Huxxx;Chixxxxx Lxx;Jixx Yxx and Shuxxxx Sx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"A math word problems can be formulated as a combination of a math problem type
with a theme, e.g. scientific fiction or bank investment. The key to solve a
math word problem is to decide its problem type expressed in the theme. This
paper presents a two-stage approach for automatically solving math word
problems. Our method retrieves relevant equation system templates derived from
training data and aligns numbers in math word problems to the templates. It
then generates candidate equations and solves the math problems. We models
problem types with templates which can be obtained from problem-equation pair
annotations, and design features to abstract math word problems from various
themes. Experimental results show that our method achieves an accuracy of
28.4\% on the full Dolphin18K benchmark, which is 10\% (54\% relative) higher
than previous state-of-art systems while obtains an accuracy of 20\% (71\%
relative) increase on the TS6 benchmark subset.",7 Feb 2017 11:54:38 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",question answering in restricted domains,Danxxxx,Huxxx,xxxxxxxx6@qq.com,Sun Yet-Sen University,No,Chixxxxx,Lxx,xxxxxxxxosoft.com,Micorsoft Research,No,Jixx,Yxx,xxxxxxxxxxxx.sysu.edu.cn,Sun Yet-Sen University,No,Shuxxxx,Sxx,xxxxxxxxtmail.com,Tencent,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Danxxxx,Huxxx,Sun Yat-sen University,,,1560xxxxxxx,,,xxxxxxxxxxxxx2.sysu.edu.cn,,Guangzhou,Guangdong,,China,,Danxxxx Huxxx;Chixxxxx Lxx;Jixx Yxx;Shuxxxx Sxx,xxxxxxxx6@qq.com;xxxxxxxxxosoft.com;xxxxxxxxxxxxl.sysu.edu.cn;xxxxxxxxxtmail.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
781,781X-B6G8P3D8F9,Deep Reinforcement Learning for Dialogue Management,Peixxxx Sx;Paxxxx Budzxxxxxxxx;Mixxxx Gaxxx;Nixxxx Mrkxxxxx;Lixx Mx;Stxxxx Ulxxx;Tsunxxxxxxx Wxx and Stxxx Yoxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"A neural network-based dialogue manager for task-oriented spoken dialogue
systems is presented, which maps from the dialogue state to a distribution over
system responses. 
The proposed model is shown to be robust in interaction with human users as
well as being able to operate on a significantly larger action space than
previous dialogue systems.
Additionally, we introduce a novel prioritised learning algorithm in episodic
tasks. 
This framework is first evaluated with various deep reinforcement learning
algorithms on a smaller action set in simulation, both from scratch and from a
model pre-trained with a small corpus of dialogue data.
The later mitigates the problems of data sparsity when relying only on off-line
learning using a limited coverage dataset and poor initial performance when
training from scratch. 
The proposed system achieves robust state-of-the-art performance in larger
action space and in interaction with human subjects.",7 Feb 2017 11:59:03 GMT,Empirical/Data-Driven,Dialog and interactive systems,dialogue;  reinforcement learning;  NLP applications;  dialogue control,Peixxxx,Sx,xxxxxxxam.ac.uk,University of Cambridge,No,Paxxxx,Budzxxxxxxxx,xxxxxxxxxxxxng.cam.ac.uk,University of Cambridge,No,Mixxxx,Gaxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Nixxxx,Mrkxxxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Linxxxx,Rojasxxxxxxxxx,xxxxxxxxxxxxng.cam.ac.uk,University of Cambridge.,No,Stxxxx,Ulxxx,xxxxxxxam.ac.uk,University of Cambridge,No,Tsunxxxxxxx,Wxx,xxxxxxxam.ac.uk,University of Cambridge,No,Stxxx,Yoxxx,xxxxxxxxcam.ac.uk,Cambridge University,No,,,,,,,,,,,,Peixxxx,Sx,PolyAI,,,4475xxxxxxxx,,,xxxxxxxxxly-ai.com,,,,,United Kingdom,,Peixxxx Sx;Paxxxx Budzxxxxxxxx;Mixxxx Gaxxx;Nixxxx Mrkxxxxx;Lixx Mx;Stxxxx Ulxxx;Tsunxxxxxxx Wxx;Stxxx Yoxxx,xxxxxxxam.ac.uk;xxxxxxxxxxxxeng.cam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxxxxxeng.cam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxam.ac.uk;xxxxxxxxxcam.ac.uk,,,,,,,,,Only include my submission if it is accepted.,No,None,None
782,782X-B5D4A6G7A8,Dependency Grammar Induction with (Neural) Lexicalization and Big Training Data,Wenxxxx Hxx;Yoxx Jixxx and Kexxx T,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"We study the impact of big models (in terms of the degree of lexicalization)
and big data (in terms of the training corpus size) on dependency grammar
induction. We find that the traditional learning approach of Dependency Model
with Valence of Klein and Manning only benefits from very small degrees of
lexicalization and moderate sizes of training corpora; but with good
initialization of the model, learning can benefit from moderate degrees of
lexicalization and large training data. We then extend the Neural Dependency
Model with Valence to take more advantage of lexicalization and big data and we
achieve a result that is competitive with the current state-of-the-art.",7 Feb 2017 12:03:57 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",unsupervised and semi-supervised learning;  syntax;  parsing,Wenxxxx,Hxx,xxxxxxxxxxxxaitech.edu.cn,ShanghaiTech University,No,Yoxx,Jixxx,xxxxxxxxxxxxxxghaitech.edu.cn,ShanghaiTech University,No,Kexxx,Tx,xxxxxxxxxxxxitech.edu.cn,ShanghaiTech University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wenxxxx,Hxx,ShanghaiTech University,,,,,,xxxxxxxxxxxxaitech.edu.cn,,,,,China,,Wenxxxx Hxx;Yoxx Jixxx;Kexxx Tx,xxxxxxxxxxxxaitech.edu.cn;xxxxxxxxxxxxxxxghaitech.edu.cn;xxxxxxxxxxxxaitech.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
783,783X-P9E9B9G3H4,Khmer POS Tagging Using Conditional Random Fields,Sokxxxxxxx Sanxxxx and Chaxxxxxx Pluempxxxxxxxxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"The transformation-based approach with hybrid of rule-based and tri-gram have
already been introduced for Khmer part-of-speech (POS) tagging. In this study,
in order to further explore this topic, we present an alternative approach to
Khmer POS tagging using Conditional Random Fields (CRFs). Since the features
greatly affect the tagging accuracy, we investigate four groups of features and
use them with the CRF model. First, we study different con-textual information
and use it as our base-line model. We then analyze the characteristics of Khmer
and came up with two additional groups of language-related features including
morphemes and word-shapes. We also explore the use of lexicon as features to
further improve the accuracy of our tagger. Our proposed approach has been
evaluated on a corpus of 41,058 words and 27 POS tags. The comparative study
has shown that our proposed approach produce a competitive accuracy compared to
other Khmer POS tagging approaches.",7 Feb 2017 11:24:29 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",part-of-speech tagging,Sokxxxxxxx,Sanxxxx,xxxxxxxxxxxxxxxxent.mahidol.ac.th,Mahidol University,No,Chaxxxxxx,Pluempxxxxxxxxxxxx,xxxxxxxxxxxxx@mahidol.ac.th,Mahidol University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sokxxxxxxx,Sanxxxx,Mahidol University,,,995xxxxxx,,,xxxxxxxxxxxxxxxxent.mahidol.ac.th,,Salaya,Nakhon Pathom,,Thailand,,Sokxxxxxxx Sanxxxx;Chaxxxxxx Pluempxxxxxxxxxxxx,xxxxxxxxxxxxxxxxent.mahidol.ac.th;xxxxxxxxxxxxxx@mahidol.ac.th,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
784,784X-A2G6J4D7P6,Learning Syntactically Plausible Word Representations by Solving Word Ordering,Noxxxx Nisxxxx and Hixxxx Nakxxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Syntactic information is crucial to capture sentence structures such as word
order. However,recent works on learning distributed word representations do not
model word order explicitly. As the ability to represent syntactic information
is missing in these embeddings models, the vectors learned with these methods
are suboptimal for syntax-related tasks such as part-of-speech tagging and
dependency parsing. In this paper, we propose a new approach to learning
syntactically plausible word representations. The proposed method learns word
embeddings by solving word ordering problems using pointer networks. We
quantitatively and qualitatively evaluate our approach in comparison with prior
works. The experimental results demonstrate that the proposed method produces
vector spaces that is capable of capturing syntactic regularities better than
existing methods.",7 Feb 2017 11:58:07 GMT,Empirical/Data-Driven,Semantics,unsupervised and semi-supervised learning;  part-of-speech tagging;  syntax,Noxxxx,Nisxxxx,xxxxxxxxxxxxxxx.i.u-tokyo.ac.jp,The University of Tokyo,No,Hixxxx,Nakxxxxx,xxxxxxxxxxxxxxxx.i.u-tokyo.ac.jp,The University of Tokyo,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Noxxxx,Nisxxxx,The University of Tokyo,,,,,,xxxxxxxxxxxxxxx.i.u-tokyo.ac.jp,,,,,Japan,,Noxxxx Nisxxxx;Hixxxx Nakxxxxx,xxxxxxxxxxxxxxx.i.u-tokyo.ac.jp;xxxxxxxxxxxxxxxxi.i.u-tokyo.ac.jp,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
785,785X-B2D6B4P2J6,Source Encoding Enhancement for Neural Machine Translation,Qixxx Waxx;Toxx Xixx;Fuxxx Lx and Jixxxx Zx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"In this paper we propose a source encoding enhancement (SEE) method to
represent the source sentence in a precise and linguistically-motivated way for
neural machine translation. Unlike previous work, we do not resort to the
summary of an entire sentence at the position of each word. Instead, we
explicitly model words and n-grams which are meaningful linguistic units in
translation. Plus, we develop two methods to introduce the new embed-ding into
a neural machine translation system. Experimental results show that the SEE
method yields an improvement of 2.5+ BLEU points over the baseline on NIST
Chinese-English MT tasks.",7 Feb 2017 13:11:13 GMT,Theoretical,Machine translation,statistical machine translation,Qixxx,Waxx,xxxxxxxxx@gmail.com,Northeastern University,No,Toxx,Xixx,xxxxxxxxxxxxl.neu.edu.cn,Northestern University,No,Fuxxx,Lx,xxxxxxxxx9@163.com,Northeastern University,No,Jixxxx,Zxx,xxxxxxxxxxxxil.neu.edu.cn,Northeastern University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Qixxx,Waxx,Northeastern University,,,,,,xxxxxxxxxxxu@gmail.com,,,,,China,,Qixxx Waxx;Toxx Xixx;Fuxxx Lx;Jixxxx Zxx,xxxxxxxxx@gmail.com;xxxxxxxxxxxxil.neu.edu.cn;xxxxxxxxx19@163.com;xxxxxxxxxxxxxil.neu.edu.cn,,,,,,,,,Only include my submission if it is accepted.,No,None,None
786,786X-J4H6F3E6H9,Multilingual Event Extraction without Manual Annotation and Feature Engineering,Benxxxxx Yxx;Ngxx Nxx;Hwxx Txx and Daxxxx Dahxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this paper, we focus on the task of multilingual event extraction without
the use of manually annotated training data and feature engineering, as they
are labor-intensive and often domain- or language-specific. We propose a
language-agnostic method which combines distant supervision and bootstrapping
to automatically extract training data, then use a long short-term memory
(LSTM) network to automatically build a model for event extraction. We evaluate
our method on the widely used MUC-4 and MUC-6 English event extraction
benchmark datasets, as well as a manually constructed test set in Chinese. Our
LSTM model built using automatically extracted training data outperforms a
traditional machine learning classifier built with manually annotated training
data on two of our three datasets, and performs competitively on the third
dataset.",7 Feb 2017 11:56:58 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",information extraction;  cross-language information extraction;  relation/event extraction,Benxxxxx,Yxx,xxxxxxxxxxxxp.nus.edu.sg,National University of Singapore,No,Ngocxxxxxxxxx,Ngxxxx,xxxxxxxxxxxxxmp.nus.edu.sg,National University of Singapore,No,Hwexxxxx,Nx,xxxxxxxxxxnus.edu.sg,National University of Singapore,No,Daxxxx,Dahxxxxxx,xxxxxxxxxer@sap.com,SAP Research & Innovation,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benxxxxx,Yxx,NUS,,,,,,xxxxxxxxxxxxp.nus.edu.sg,,,,,Singapore,,Benxxxxx Yxx;Ngxx Nxx;Hwxx Txx;Daxxxx Dahxxxxxx,xxxxxxxxxxxxp.nus.edu.sg;xxxxxxxxxxxxxomp.nus.edu.sg;xxxxxxxxxx.nus.edu.sg;xxxxxxxxxxer@sap.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
787,787X-J4J8P5A3B2,Question Classification in the Travel Domain,Dilxxxx Patxxxxxx;Paxxxx Lixxxx;Hasxxxx Kahxxxxx;Vixxxx Dixx;Upxxx Kohxxxxx and Surxxxxxx Ranxxxxxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In question answering systems, identification of the Expected Answer Type (EAT)
of a question imposes some constraints when determining the possible answer,
thus  increasing the probability of finding the answer to a particular
question. This paper presents the first study on semantic classification of 
questions into EATs in the travel domain. However, the set of EATs are
inherently domain-dependent, meaning that existing research on taxonomies, data
sets, and classification techniques is insufficient/inapplicable in the context
of new domains. Thus a new two-level taxonomy for the travel domain is
introduced, along with a dataset annotated with the same. A machine learning
approach is used for question classification, which gives very promising
results even with the use of syntactic and semantic features.",7 Feb 2017 11:23:24 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",corpus development;  NLP applications;  context-aware question answering;  text classification;  question answering in restricted domains,Dilxxxx,Patxxxxxx,xxxxxxxxxxxxxcse.mrt.ac.lk,"University of Moratuwa, Sri Lanka",No,Paxxxx,Liyanxxxxxxxxxx,xxxxxxxxxxxse.mrt.ac.lk,"University of Moratuwa, Sri Lanka",No,Hasxxxx,Kahxxxxx,xxxxxxxxxxxxcse.mrt.ac.lk,"University of Moratuwa, Sri Lanka",No,Vixxxx,Dixx,xxxxxxxxxxxse.mrt.ac.lk,"University of Moratuwa, Sri Lanka",No,Upxxx,Kohxxxxx,xxxxxxxxxegen.co.uk,CodeGen International (Pvt) Ltd.,No,Surxxxxxx,Ranxxxxxxx,xxxxxxxxxxxse.mrt.ac.lk,university of moratuwa,No,,,,,,,,,,,,,,,,,,,,,,Paxxxx,Liyanxxxxxxxxxx,"University of Moratuwa, Sri Lanka",,,,,,xxxxxxxxxxxse.mrt.ac.lk,,,,,Sri Lanka,,Dilxxxx Patxxxxxx;Paxxxx Lixxxx;Hasxxxx Kahxxxxx;Vixxxx Dixx;Upxxx Kohxxxxx;Surxxxxxx Ranxxxxxxx,xxxxxxxxxxxxxcse.mrt.ac.lk;xxxxxxxxxxxxse.mrt.ac.lk;xxxxxxxxxxxxxcse.mrt.ac.lk;xxxxxxxxxxxxse.mrt.ac.lk;xxxxxxxxxxegen.co.uk;xxxxxxxxxxxxse.mrt.ac.lk,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
789,789X-J6B5G6C6C6,Building a Semagram Base from Scratch,Gioxxxxx Sirxxxxx;Luxxx Dx;Robxxxx Navxxxx and Valxxxxxx Lexx,Resources Evaluation,Soxxxx Roxxxx;Waxxx Zagxxxxxx,Reject,,Undecided (Resources Evaluation),"Word senses are typically defined with textual definitions for human
consumption and, in computational lexicons, put in context via lexical-semantic
relations such as synonymy, antonymy, hypernymy, etc. In this paper we embrace
a radically different paradigm that provides a slot-filler structure, called a
""semagram"", to define the meaning of words in terms of their prototypical
semantic information.
We propose a semagram-based knowledge model composed of 41 semantic
relationships which integrates features from a range of different sources, such
as computational lexicons and property norms. We describe an annotation
exercise regarding 50 concepts over 10 different categories and put forward an
Open Information Extraction approach to the extraction of new semagram values
from
large corpora.",7 Feb 2017 11:46:23 GMT,Resources/Evaluation,Resources and evaluation,information extraction;  relational Learning;  information retrieval;  semantic relations;  relation/event extraction;  ontology development;  NLP on Wikipedia and other collaboratively constructed resources,Gioxxxxx,Sirxxxxx,xxxxxxxxxxi.unito.it,University of Turin - Unito,No,Luxxx,Di xxxx,xxxxxxxxx.unito.it,University of Turin,No,Robxxxx,Navxxxx,xxxxxxxxxxxuniroma1.it,Sapienza University of Rome,No,Valxxxxxx,Lexxx,xxxxxxxxxxxxxne13@gmail.com,University of Turin,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gioxxxxx,Sirxxxxx,University of Turin - Unito,,,,,,xxxxxxxxxxi.unito.it,,,,,Italy,,Gioxxxxx Sirxxxxx;Luxxx Dx;Robxxxx Navxxxx;Valxxxxxx Lexxx,xxxxxxxxxxi.unito.it;xxxxxxxxxi.unito.it;xxxxxxxxxxx.uniroma1.it;xxxxxxxxxxxxxxne13@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
790,790X-A8A6G6E4D6,Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence,Caxx Coxxx;Joxxxx Lx and Matxxxx Laxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"Discontinuous phrase-structure parsing either relies on formal grammars such as
LCFRS that come with high parsing complexity, or on reductions to
non-projective dependency parsing with complex label schemes that encode phrase
combinations. We propose an alternative approach based on a discontinuous
variation of spinal TAGs. The main contributions of this paper are (1) a
reduction of the general problem of joint sequence labeling and non-projective
dependency parsing to the Generalized Maximum Spanning Arborescence problem,
(2) a novel decoding algorithm of the later through Lagrangian relaxation (3)
an application to discontinuous spine-based parsing. Our model is tested on the
Discontinuous PTB corpus with promising results despite a basic statistical
model.",7 Feb 2017 11:49:56 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",syntax;  parsing;  ILP-based approaches (ILP=Integer Linear Programming),Caxx,Coxxx,xxxxxxxxxxxxxiv-paris13.fr,Laboratoire d'Informatique de Paris Nord,No,Joxxxx,Le xxxx,xxxxxxxxxxxxux@gmail.com,Université Paris Nord,No,Matxxxx,Lacxxxx,xxxxxxxxxxxxxxniv-paris13.fr,LIPN,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Caxx,Coxxx,University of Amsterdam,,,,,,xxxxxxxxo@uva.nl,,,,,Netherlands,,Caxx Coxxx;Joxxxx Lx;Matxxxx Lacxxxx,xxxxxxxxxxxxxiv-paris13.fr;xxxxxxxxxxxxoux@gmail.com;xxxxxxxxxxxxxxuniv-paris13.fr,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
791,791X-G8C3F6G5G9,Document-Level Multi-Aspect Sentiment Classification with Hierarchical and Iterative Attention,Yixxxx Yxx;Yanxxxx Soxx and Mixx Zhxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Document-level multi-aspect sentiment classification is an important task for
customer relation management. In this paper, we model the task as a machine
comprehension problem where pseudo questionanswer pairs are constructed by a
small number of aspect-related keywords and aspect ratings. A hierarchical
iterative attention model is proposed to build aspectspecific representations
by frequent and repeated interactions between documents and aspect questions.
We adopt a hierarchical architecture to represent both word level and sentence
level information, and use the attention operations for aspect questions and
documents alternatively with the multiple hop mechanism. Experimental results
on the TripAdvisor and BeerAdvocate datasets show that our model outperforms
other classical baselines. Moreover, the attention visualization
reveals that our model learns reasonable weights of words and sentences for
each aspect.",7 Feb 2017 11:52:57 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  text classification,Yixxxx,Yxx,xxxxxxxxxxpku.edu.cn,Peking University,No,Yanxxxx,Soxx,xxxxxxxxse.ust.hk,HKUST,No,Mixx,Zhxxx,xxxxxxxxxxpku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yixxxx,Yxx,Peking University,,,,,,xxxxxxxxxxpku.edu.cn,,Peking,Peking,,China,,Yixxxx Yxx;Yanxxxx Soxx;Mixx Zhxxx,xxxxxxxxxxpku.edu.cn;xxxxxxxxxse.ust.hk;xxxxxxxxxx@pku.edu.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
792,792X-G9D4C9H3C9,LSTMEmbed: a Lexical and SemanTic Model of Embeddings with a bidirectional LSTM,Ignxxxx Iacxxxxxx and Robxxxx Navxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"The Long Short-Term Memory (LSTM) architecture for recurrent neural networks
has become the state-of-the-art model for a range of different Natural Language
Processing (NLP) tasks especially in language modeling and sequence to sequence
learning. In this paper we leverage a bidirectional LSTM while at the same time
taking advantage of other semantic resources in order to create a vector space
model for words and senses that outperforms most popular algorithms for
learning embeddings.  We evaluate our approach on the most well-known
benchmarks on vector space representations.",7 Feb 2017 11:53:48 GMT,Empirical/Data-Driven,Semantics,lexical semantics;  distributional similarity;  semantic relations,Ignxxxx,Iacxxxxxx,xxxxxxxxxxxx.uniroma1.it,Sapienza University of Rome,No,Robxxxx,Navxxxx,xxxxxxxxxxxuniroma1.it,Sapienza University of Rome,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ignxxxx,Iacxxxxxx,Sapienza University of Rome,,,,,,xxxxxxxxxxxx.uniroma1.it,,,,,Italy,,Ignxxxx Iacxxxxxx;Robxxxx Navxxxx,xxxxxxxxxxxx.uniroma1.it;xxxxxxxxxxx.uniroma1.it,,,,,,,on,on,"Yes, include my submission even if the paper is rejected.",No,None,None
793,793X-J8E6P3J7P5,Attentional Pointer Networks for Answer Span Extraction,Soxxxx Kuxxx;Taxxx Naxxx and Hwxx Txx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Most of the previous machine comprehension datasets focus only on factoid
questions where the answer is usually one word or one phrase. Recently, the
Stanford Question Answering Dataset (SQuAD) has been created where the queries
are not necessarily factoid-based and the answers can be any span of text from
the corresponding passage. In this paper, we propose a Max-Attentional
Query-based Pointer Network (MAQPN) approach for answer span extraction.
Experimental results on the SQuAD dataset have shown that our proposed approach
achieves comparable performance to the state of the art. We also extend the
MAQPN architecture to tackle the situation when there is no answer in a
passage. In practice, given a query, a question answering (QA) system finds the
answer from a huge collection of text documents, rather than just from one
given passage. By incorporating a front-end passage ranker, we also analyze the
performance of MAQPN on past TREC QA tracks.",7 Feb 2017 11:36:26 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",answer extraction;  open-domain question answering,Soxxxx,Kuxxx,xxxxxxxxxgmail.com,National University of Singapore,No,Taxxx,Naxxx,xxxxxxxxxgmail.com,National University of Singapore,No,Hwexxxxx,Nx,xxxxxxxxxxnus.edu.sg,National University of Singapore,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Soxxxx,Kuxxx,National University of Singapore,,,,,,xxxxxxxxxgmail.com,,,,,Singapore,,Soxxxx Kuxxx;Taxxx Naxxx;Hwxx Txx,xxxxxxxxxgmail.com;xxxxxxxxx@gmail.com;xxxxxxxxxx.nus.edu.sg,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
794,794X-P7C3A3G5A5,Domain Attention with an Ensemble of Experts,Youxxxxxx Kxx;Kaxx Strxxxx and Donxxxxx Kx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Accept - Oral Tuesday,,Undecided (Dialog Interactive Systems),"An important problem in domain adaptation is to quickly generalize to a new
domain with limited supervision given K existing domains. One approach is to
retrain a global model across all K + 1 domains using standard techniques, for
instance Daum´e III (2009). However, it is desirable to adapt without having
to re-estimate a global model from scratch each time a new domain with
potentially new intents and slots is added. We describe a solution based on
attending an ensemble of domain experts. We assume K domain specific intent and
slot models trained on respective domains. When given domain K + 1, our model
uses a weighted combination of the K domain experts’ feedback along with its
own opinion to make predictions on the new domain. In experiments,
the model significantly outperforms baselines that do not use domain adaptation
and also performs better than the full retraining approach.",26 Apr 2017 01:36:33 GMT,Empirical/Data-Driven,Dialog and interactive systems,,Youxxxxxx,Kxx,xxxxxxxxx@gmail.com,Microsoft,No,Kaxx,Strxxxx,xxxxxxxxxratos.com,Toyota Technological Institute at Chicago,No,Donxxxxx,Kxx,xxxxxxxxxxxxxmicrosoft.com,Microsoft AI and Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Youxxxxxx,Kxx,Amazon Alexa Brain,,,608xxxxxxx,,,xxxxxxxxx@gmail.com,,Redmond,WA,,United States,I currently lead the Scientist team @ Amazon Alexa Brain .,Youxxxxxx Kxx;Kaxx Strxxxx;Donxxxxx Kxx and  Rexxxxxxx,xxxxxxxxx@gmail.com;xxxxxxxxxtratos.com;xxxxxxxxxxxxx@microsoft.com,Domain Attention with an Ensemble of Experts,Domain Attention with an Ensemble of Experts,11,,,,,,No. Do not include my submission in this dataset.,No,None,None
795,795X-F7G4F3D8H3,Canonical Correlation Analysis for Extractive Document Summarization,Nixxx Papasaxxxxxxxxxxx;Shxxxx Narxxxx and Shxx Bx,Generation Summarization,Wexxxx Lx;Vexxxx Rixxxx;Alxx and ex Ruxx,Reject,,Undecided (Generation Summarization),"We describe a method for extractive single document summarization based on
Canonical Correlation Analysis. Within this method, texts and corresponding
summaries are represented in two different feature spaces, which, in turn, are
projected to one common text-highlight low-dimensional space. Distance in the
common space is used to rank document sentences and generate a summary for each
document. We report results on two summarization corpora, one commonly used for
summarization evaluation (DUC-2002) and one large scale (CNN/Daily Mail).",7 Feb 2017 11:55:30 GMT,Applications/Tools,Summarization,document summarization,Nixxx,Papasaxxxxxxxxxxx,xxxxxxxxxxxa@gmail.com,University of Edinburgh,No,Shxxxx,Narxxxx,xxxxxxxxxxnf.ed.ac.uk,University of Edinburgh,No,Shaxxxx,Coxxx,xxxxxxxxxf.ed.ac.uk,University of Edinburgh,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nixxx,Papasaxxxxxxxxxxx,University of Edinburgh,,,,,,xxxxxxxxxxxa@gmail.com,,,,,United Kingdom,,Nixxx Papasaxxxxxxxxxxx;Shxxxx Narxxxx;Shxx Bx,xxxxxxxxxxxa@gmail.com;xxxxxxxxxxxnf.ed.ac.uk;xxxxxxxxxxf.ed.ac.uk,,,,,,,,,Only include my submission if it is accepted.,No,None,None
796,796X-H2B9H3G9A7,Measuring Thematic Fit with Distributional Feature Overlap,Enxxxx Saxxxx;Emmxxxxxx Chexxxxx; Axxxx and rx Lexxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"In this paper, we introduce a new distributional method for modeling
predicate-argument thematic fit judgments, inspired by several cognitive and
psycholinguistic findings. 
We use a dependency-based DSM to build a prototypical representation of
verb-specific roles: first, we extract the most salient second order contexts
for each role, i.e. the most salient semantic dimensions of typical role
fillers, and then we compute thematic fit as a weighted overlap between the top
features of the candidate fillers and of the prototypes.
Our experiments show that our method achieves competitive results with
state-of-the-art systems.",7 Feb 2017 11:52:12 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,unsupervised and semi-supervised learning;  language generation;  lexical semantics;  selectional preferences;  semantic role labelling;  semantic knowledge induction,Enxxxx,Saxxxx,xxxxxxxxxxxxs@sutd.edu.sg,Singapore University of Technology and Design,No,Emmxxxxxx,Chexxxxx,xxxxxxxxxxxxxsoni@gmail.com,Aix-Marseille University,No,Alexxxxxxx,Lexxx,xxxxxxxxxxxxenci@unipi.it,University of Pisa,No,Phixxxxx,Blxxxx,xxxxxxxxpl-aix.fr,LPL CNRS,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enxxxx,Saxxxx,MIT,,,+852xxxxxxxxx,,,xxxxxxx@mit.edu,,,,,United States,,Enxxxx Saxxxx;Emmxxxxxx Chexxxxx;Alexxxxxxx Lexxx;Phixxxxx Blxxxx,xxxxxxxxxxxxs@sutd.edu.sg;xxxxxxxxxxxxxxsoni@gmail.com;xxxxxxxxxxxxxenci@unipi.it;xxxxxxxxxpl-aix.fr,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
797,797X-J6J7A3F6G6,A Neural Language Model that Infers Meanings of Unknown Words Dynamically in Discourse,Soxxxx Kobxxxxxx;Naxxxx Okaxxxx and Kenxxxx Ixx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"This study addresses the problem of the meanings of unknown
words or entities in a discourse with respect to the word embeddings approaches
in neural language models. In this study, a method is proposed to construct and
exploit on-the-fly word embeddings in not only input layer but also output
layer of neural model, and this is extended from a dynamic entity
representation as discussed in a study by Kobayashi et al. (2016) with
incorporating a copy mechanism proposed independently in studies by Gu et al.
(2016) and Gulcehre et al. (2016). Additionally, the present study also
includes building a new task and dataset, Anonymized Language Modeling to
evaluate the ability to grasp word meanings while reading. Experiments on the
new dataset, i.e. the RNN language model extended with the proposed method,
indicate that the proposed model outperforms baseline models. Further analysis
indicates that the method proposed in the present study successfully captures a
better meaning the representation from context involving a few instances.",7 Feb 2017 13:08:42 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse;  word sense induction,Soxxxx,Kobxxxxxx,xxxxxxxxferred.jp,Preferred Networks,No,Naxxxx,Okaxxxx,xxxxxxxxxxxx.tohoku.ac.jp,Tohoku University,No,Kenxxxx,Inxx,xxxxxxxxxxxohoku.ac.jp,Tohoku University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Soxxxx,Kobxxxxxx,"Preferred Networks, Inc.",,,,,,xxxxxxxxferred.jp,,,,,Japan,,Soxxxx Kobxxxxxx;Naxxxx Okaxxxx;Kenxxxx Inxx,xxxxxxxxferred.jp;xxxxxxxxxxxxx.tohoku.ac.jp;xxxxxxxxxxxtohoku.ac.jp,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
798,798X-C9A6E2G2C8,Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix,Binxxxxx Lxx;Yanxxxx Fexx;Zhxxx Waxx;Zhaxxxxx Zxx;Sonxxxxx Huxxx;Rxx Yxx and Donxxxx Zxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Accept - Oral Monday,,Undecided (IE QA Text Mining Applications),"Distant supervision significantly reduces human efforts in building training
data for many classification tasks. While promising, this technique often
introduces noise to the generated training data, which can severely affect the
model performance. In this paper, we take a deep look at the application of
distant supervision in relation extraction. We show that the dynamic transition
matrix can effectively characterize the noise in the training data built by
distant supervision. The transition matrix can be effectively trained using a
novel curriculum learning based method without any direct supervision about the
noise. We thoroughly evaluate our approach under a wide range of extraction
scenarios. Experimental results show that our approach consistently improves
the extraction results and outperforms the state-of-the-art in various
evaluation scenarios.",23 Apr 2017 02:31:01 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",,Binxxxxx,Lxx,xxxxxxxxxxxo@pku.edu.cn,"Institute of Computer Science and Technology, Peking University",No,Yanxxxx,Fexx,xxxxxxxxxxx@pku.edu.cn,Peking University,No,Zhxxx,Waxx,xxxxxxxxxxxaster.ac.uk,Lancaster University,No,Zhaxxxxx,Zxx,xxxxxxxxxxxu@pku.edu.cn,Peking University,No,Sonxxxxx,Huxxx,xxxxxxxxgmail.com,IBM China Research Lab,No,Rxx,Yxx,xxxxxxxxxxu@gmail.com,Peking University,No,Donxxxx,Zhxx,xxxxxxxxxxx@pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,Yanxxxx,Fexx,Peking University,,,0086-xxxxxxxxxxx,,,xxxxxxxxxxx@pku.edu.cn,,,,,China,,Binxxxxx Lxx;Yanxxxx Fexx;Zhxxx Waxx;Zhaxxxxx Zxx;Sonxxxxx Huxxx;Rxx Yxx;Donxxxx Zhxx,xxxxxxxxxxxo@pku.edu.cn;xxxxxxxxxxxg@pku.edu.cn;xxxxxxxxxxxcaster.ac.uk;xxxxxxxxxxxxu@pku.edu.cn;xxxxxxxxxgmail.com;xxxxxxxxxxxu@gmail.com;xxxxxxxxxxxn@pku.edu.cn,Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix,Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix,10,Bingfeng Luo,,"Institute of Computer Science and Technology, Peking University",on,on,No. Do not include my submission in this dataset.,No,None,None
799,799X-C9C9A6D6G3,Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing,Anxxx Ylixxxxxx and Caxxxx Gómezxxxxxxxxxxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Poster Monday,,Undecided (Tagging Chunking Syntax Parsing),"We present a simple encoding for unlabeled noncrossing graphs and show how its
latent counterpart helps us to represent several families of directed and
undirected graphs used in syntactic and semantic parsing of natural language as
context-free languages.  The families are separated purely on the basis of
forbidden patterns in latent encoding, eliminating the need to differentiate
the families of non-crossing graphs in inference algorithms: one algorithm
works for all when the search space can be controlled in parser input.",22 Apr 2017 21:05:08 GMT,Theoretical,"Tagging, chunking, syntax, and parsing",,Anxxx,Ylixxxxxx,xxxxxxxxxxxxxa@helsinki.fi,University of Helsinki,No,Caxxxx,Gómezxxxxxxxxxxx,xxxxxxx@udc.es,Universidade da Coruña,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Anxxx,Ylixxxxxx,University of Helsinki,,,+358-xxxxxxxxxx,,,xxxxxxxxxxxxxa@helsinki.fi,,,,,Finland,"Phd 2005 in Language Technology.  
I have specialized in Finite-State Algrithms, Transducers, Morphology and Parsing.
I am currently a research fellow working on probabilistic finite-state dependency parsing.",Anxxx Ylixxxxxx;Caxxxx Gómezxxxxxxxxxxx,xxxxxxxxxxxxxa@helsinki.fi;xxxxxxxr@udc.es,Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing,Generic Axiomatization of Noncrossing Dependency Graphs,11,Anssi Yli-Jyrä,,"The Department of Modern Languages, PO Box 24, 00014 University of Helsinki, Finland",on,on,No. Do not include my submission in this dataset.,No,None,None
800,800X-A8E4D3F6P8,A Minimal Span-Based Neural Constituency Parser,Mitxxxxx Stxxx;Jaxxx Andxxxx and Dxx Klxx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Oral Tuesday,,Undecided (Tagging Chunking Syntax Parsing),"In this work, we present a minimal neural model for constituency parsing based
on independent scoring of labels and spans. We show that this model is not only
compatible with classical dynamic programming techniques, but also admits a
novel greedy top-down inference algorithm based on recursive partitioning of
the input. We demonstrate empirically that both prediction schemes are
competitive with recent work, and when combined with basic extensions to the
scoring model are capable of achieving state-of-the-art single-model
performance on the Penn Treebank (91.79 F1) and strong performance on the
French Treebank (82.23 F1).",23 Apr 2017 05:39:00 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Mitxxxxx,Stxxx,xxxxxxxxxxerkeley.edu,UC Berkeley,No,Jaxxx,Andxxxx,xxxxxxxxxrkeley.edu,Berkeley,No,Dxx,Klxxx,xxxxxxxxxxerkeley.edu,UC Berkeley,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mitxxxxx,Stxxx,UC Berkeley,,,,,,xxxxxxxxxxerkeley.edu,,,,,United States,,Mitxxxxx Stxxx;Jaxxx Andxxxx;Dxx Klxxx,xxxxxxxxxxerkeley.edu;xxxxxxxxxxrkeley.edu;xxxxxxxxxxxerkeley.edu,A Minimal Span-Based Neural Constituency Parser,A Minimal Span-Based Neural Constituency Parser,10,Mitchell Stern,,"Computer Science Division
University of California, Berkeley
Sutardja Dai Hall
Berkeley, CA 94720",,,No. Do not include my submission in this dataset.,No,None,None
801,801X-B6H4P4E8A9,Lightly-Supervised Model for Persuasion,Isxxx Perxxxx and Vinxxxx Nx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"We propose a weakly-supervised approach to model persuasion. Persuasion is an
under-investigated yet very practical problem in pragmatics. Given the
difficulty in obtaining data for studying persuasion-related tasks, we
hypothesize that a weakly-supervised approach is appropriate for this task. In
an evaluation on common corpora of persuasion, our approach achieves good
performance.",7 Feb 2017 12:08:20 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse;  pragmatics,Isxxx,Perxxxx,xxxxxxxxxxotmail.com,University of Texas at Dallas,No,Vinxxxx,Nx,xxxxxxxxxxxtdallas.edu,University of Texas at Dallas,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vinxxxx,Nx,University of Texas at Dallas,,,,,,xxxxxxxxxxxtdallas.edu,,,,,United States,,Isxxx Perxxxx;Vinxxxx Nx,xxxxxxxxxxotmail.com;xxxxxxxxxxxutdallas.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
802,802X-B3C8J9E7B3,Transfer Learning with Separation of Domain-general and Domain-specific Representations for Slot Filling,Jooxxxxxx Kxx and Youxxxxxx Kxx,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"We propose a transfer learning method separately representing domain-general
information and domain-specific information applied to slot filling. We show
the effectiveness of encouraging domain-general representations for all the
domains to be domain and domain-specific representations to be less correlated
to the domain-general representations on transfer learning for slot filling
tasks. Our model can be effectively generalized to transfer learning cases with
more than one source domains by using multi-domain adversarial training. We
demonstrate that our approach can improve the performance of transfer learning
for sequence models given multiple domains with different label spaces. Our
model shows the best F1-scores compared to other recently introduced transfer
learning models for slot filling.",7 Feb 2017 13:09:55 GMT,Empirical/Data-Driven,Dialog and interactive systems,part-of-speech tagging;  named entity recognition;  spoken language understanding,Jooxxxxxx,Kxx,xxxxxxxxxxxxxhio-state.edu,The Ohio State University,No,Youxxxxxx,Kxx,xxxxxxxxxrosoft.com,Microsoft,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Youxxxxxx,Kxx,Amazon Alexa Brain,,,608xxxxxxx,,,xxxxxxxxx@gmail.com,,Redmond,WA,,United States,I currently lead the Scientist team @ Amazon Alexa Brain .,Jooxxxxxx Kxx;Youxxxxxx Kxx,xxxxxxxxxxxxxhio-state.edu;xxxxxxxxxxrosoft.com,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
803,803X-E3J3P2C4H5,Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision,Haxxxx Pexx;Minxxxxx Chxxx and Wenxxxx Yx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Neural networks have achieved state-of-the-art performance on several
structure-output prediction tasks, trained in a fully supervised fashion.
However, because annotated examples in structured domains are often costly to
obtain,
the applications of neural networks are thus limited.
In this work, we propose Maximum Margin Reward Networks (MMRN), which is a
neural
network based framework that aims to learn from both explicit (full
structures) and implicit supervision signals (partial feedback on the
correctness of the predicted structure).
We demonstrate the effectiveness of MMRN in two applications:
Named Entity Recognition (NER) and Knowledge Base QA (KBQA), outperforming
previous systems on two benchmark datasets, CoNLL-2003 and WebQuestionsSP.",7 Feb 2017 12:54:55 GMT,Empirical/Data-Driven,Machine learning,NLP applications;  interactive question answering;  named entity recognition;  entity disambiguation;  context-aware question answering;  reinforcement learning;  open-domain question answering,Haxxxx,Pexx,xxxxxxxxxlinois.edu,UIUC,No,Minxxxxx,Chxxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,Wenxxxx,Yxx,xxxxxxxxxxxcrosoft.com,Microsoft Research,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Haxxxx,Pexx,UIUC,,,,,,xxxxxxxxxlinois.edu,,,,,United States,,Haxxxx Pexx;Minxxxxx Chxxx;Wenxxxx Yxx,xxxxxxxxxlinois.edu;xxxxxxxxxxxicrosoft.com;xxxxxxxxxxxicrosoft.com,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
804,804X-F3H7C5A2F8,"Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video",Haxxxx Lx;Juxxxx Zxx;Coxx Mx;Jixxxx Zhxxx and Chexxxxxx Zxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Reject,,Undecided (Vision Robots Grounding),"The rapid increase of the multimedia data over the Internet necessitates
multi-modal summarization from collections of text, image, audio and video. In
this work, we propose an extractive Multi-modal Summarization (MMS) method
which can automatically generate a textual summary given a set of documents,
images, audios and videos related to a specific topic. The key idea is to
bridge the semantic gaps between multi-modal contents. For audio information,
we design an approach to selectively use its transcription. For vision
information, we learn joint representations of texts and images using a neural
network. Finally, all the multi-modal aspects are considered to generate the
textural summary by maximizing the salience, non-redundancy, readability and
coverage through budgeted optimization of submodular functions.  We further
introduce an MMS corpus in English and Chinese (will be released upon
publication). The experimental results on this dataset demonstrate that our
method outperforms other competitive baseline methods.",10 Feb 2017 05:58:48 GMT,Theoretical,"Vision, robots, and other grounding",multimodal representations and processing;  document summarization;  multi-document summarization,Haxxxx,Lx,xxxxxxxxxxxlpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Juxxxx,Zxx,xxxxxxxxxxxxlpr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Coxx,Mx,xxxxxxxxx7@ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,Jixxxx,Zhxxx,xxxxxxxxxxng@ia.ac.cn,Institute of Automation Chinese Academy of Sciences,No,Chexxxxxx,Zoxx,xxxxxxxxxxr.ia.ac.cn,"Institute of Automation, Chinese Academy of Sciences",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Haxxxx,Lx,"Institute of Automation, Chinese Academy of Sciences",,,1560xxxxxxx,,,xxxxxxxxxxxlpr.ia.ac.cn,,Beijing,Beijing,,China,,Haxxxx Lx;Juxxxx Zxx;Coxx Mx;Jixxxx Zhxxx;Chexxxxxx Zoxx,xxxxxxxxxxxlpr.ia.ac.cn;xxxxxxxxxxxxnlpr.ia.ac.cn;xxxxxxxxxx7@ia.ac.cn;xxxxxxxxxxxng@ia.ac.cn;xxxxxxxxxxpr.ia.ac.cn,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
805,805X-C3P7D6E9D6,TextFlow: A Text Similarity Measure based on Continuous Sequences,Yasxxxx Mrxxxx;Haxxx Kilxxxxxx and Dixx Demnxxxxxxxxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Accept - Oral Tuesday,,Undecided (Multidisciplinary),"Text similarity measures are used in multiple tasks such as plagiarism
detection, information ranking and recognition of paraphrases and textual
entailment. While recent advances in deep learning highlighted the relevance of
sequential models in natural language generation, existing similarity measures
do not fully exploit the sequential nature of language. Examples of such
similarity measures include n-grams and skip-grams overlap which rely on
distinct slices of the input texts. In this paper we present a novel text
similarity measure inspired from a common representation in DNA sequence
alignment algorithms. The new measure, called TextFlow, represents input text
pairs as continuous curves and uses both the actual position of the words and
sequence matching to compute the similarity value. Our experiments on 8
different datasets show very encouraging results in paraphrase detection,
textual entailment recognition and ranking relevance.",7 May 2017 06:05:48 GMT,Empirical/Data-Driven,Multidisciplinary,,Yasxxxx,Mrxxxx,xxxxxxxxgmail.com,U.S. National Library of Medicine,No,Haxxx,Kilxxxxxx,xxxxxxxxxxxmail.nih.gov,National Library of Medicine,No,Dixx,Demnexxxxxxxxx,xxxxxxxxxxil.nih.gov,NLM,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yasxxxx,Mrxxxx,U.S. National Library of Medicine,,,,,,xxxxxxxxgmail.com,,Bethesda,MD,,United States,,Yasxxxx Mrxxxx;Haxxx Kilxxxxxx;Dixx Demnexxxxxxxxx,xxxxxxxxgmail.com;xxxxxxxxxxxxmail.nih.gov;xxxxxxxxxxail.nih.gov,TextFlow: A Text Similarity Measure based on Continuous Sequences,TextFlow: A Text Similarity Measure based on Continuous Sequences,10,not copyrighted (government employees),,,on,on,Only include my submission if it is accepted.,No,None,None
806,806X-E8G6F7G2E6,Joint Learning for Event Coreference Resolution,Jixx Lx and Vinxxxx Nx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Accept - Oral Monday,,Undecided (Discourse Pragmatics),"While joint models have been developed for many NLP tasks, the vast majority of
event coreference resolvers, including the top-performing resolvers competing
in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, are
pipeline-based, where the propagation of errors from the trigger detection
component to the event coreference component is a major performance limiting
factor. To address this problem, we propose a model for jointly learning event
coreference, trigger detection, and event anaphoricity. Our joint model is
novel in its choice of tasks and its features for capturing cross-task
interactions. To our knowledge, this is the first attempt to train a
mention-ranking model and employ event anaphoricity for event coreference. Our
model achieves the best results to date on the KBP 2016 English and Chinese
datasets.",3 May 2017 04:45:15 GMT,Empirical/Data-Driven,Discourse and pragmatics,,Jixx,Lx,xxxxxxxxxxxtdallas.edu,University of Texas at Dallas,No,Vinxxxx,Nx,xxxxxxxxxxxtdallas.edu,University of Texas at Dallas,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jixx,Lx,University of Texas at Dallas,,,,,,xxxxxxxxxxxtdallas.edu,,,,,United States,,Jixx Lx;Vinxxxx Nx,xxxxxxxxxxxtdallas.edu;xxxxxxxxxxxutdallas.edu,Joint Learning for Event Coreference Resolution,Joint Learning for Event Coreference Resolution,12,Jing Lu,,"Computer Science Department
The University of Texas at Dallas
800 W. Campbell Rd., MS EC31
Richardson, TX 75080, USA",on,on,No. Do not include my submission in this dataset.,No,None,None
807,807X-H3D2E8B6C6,Failure Prognosis Companion Teaching for On-line Dialogue Policy Learning,Chxxx Chxxx;Ruxxxx Yaxx;Lx Chxx;Xixxx Zhxx and Kxx Y,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"The key to building evolvable dialogue system in real-world scenarios is
ensuring safe and efficient on-line dialogue policy learning. Considering
actualities that user feedback will only be given to the system as a success
signal in a fraction of sessions, the policy will evolve very slowly from poor
initial policy, which can easily lead to bad user experience and even result in
a failure of attracting sufficient real users for sustainable policy training.
To solve this cold start problem, we propose a complete framework of companion
teaching to integrate a human teacher into the on-line dialogue policy learning
process and further answer the question of how and when for human to teach.
Especially for answering ""when"", a novel failure prognosis based teaching
heuristic is presented. Simulation experiments with occasional user feedback
show significant improvements have been made by the proposed approach, measured
by well-designed quantitive metrics on safety and efficiency.",7 Feb 2017 11:56:32 GMT,Empirical/Data-Driven,Dialog and interactive systems,dialogue control;  dialogue;  evaluation metrics,Chxxx,Chxxx,xxxxxxxxxxx@sjtu.edu.cn,Shanghai Jiao Tong University,No,Ruxxxx,Yaxx,xxxxxxxxxxx@sjtu.edu.cn,Shanghai Jiao Tong University,No,Lx,Chxx,xxxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong University,No,Xixxx,Zhxx,xxxxxxxxxtu.edu.cn,Shanghai Jiao Tong University,No,Kxx,Yx,xxxxxxxxxtu.edu.cn,Shanghai Jiao Tong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Ruxxxx,Yaxx,Shanghai Jiao Tong University,,,,,,xxxxxxxxxxx@sjtu.edu.cn,,,,,China,,Chxxx Chxxx;Ruxxxx Yaxx;Lx Chxx;Xixxx Zhxx;Kxx Yx,xxxxxxxxxxx@sjtu.edu.cn;xxxxxxxxxxxx@sjtu.edu.cn;xxxxxxxxxxsjtu.edu.cn;xxxxxxxxxjtu.edu.cn;xxxxxxxxxjtu.edu.cn,,,,,,,,on,Only include my submission if it is accepted.,No,None,None
808,808X-J5C6H3A2F6,Unsupervised Quasi-Identity Event Coreference,Daxxxx Daxxxx;Dhexxxx Rajxxxxxx;Edxxxx Hoxx and Texxxx Mitxxxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Events provide the core semantic content of most texts. Many NLP tasks
necessitate resolving coreferent event mentions in any given domain. We propose
a novel approach for resolving event coreference when the trigger word refers
to multiple (and often non-contiguous) clauses together. This quite common
phenomenon has not been studied before. Our approach is completely
unsupervised.  The method, applied on two different datasets (biomedical
research papers and political blogs), achieves 70% and 59% accuracy
respectively against human labeled  gold annotations.",7 Feb 2017 11:48:48 GMT,Empirical/Data-Driven,Semantics,coreference resolution,Daxxxx,Daxxxx,xxxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Dhexxxx,Rajxxxxxx,xxxxxxxxxxxal@gmail.com,Carnegie Mellon University,No,Edxxxx,Hoxx,xxxxxxmu.edu,CMU,No,Texxxx,Mitxxxxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dhexxxx,Rajxxxxxx,Carnegie Mellon University,,,,,,xxxxxxxxxxxal@gmail.com,,,,,United States,,Daxxxx Daxxxx;Dhexxxx Rajxxxxxx;Edxxxx Hoxx;Texxxx Mitxxxxx,xxxxxxxxxs.cmu.edu;xxxxxxxxxxxxal@gmail.com;xxxxxxcmu.edu;xxxxxxxxxs.cmu.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
809,809X-B8F7P5H3C3,Comparing language related issues for NMT and PBMT between German and English,Maxx Popxxxxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"This work presents an extensive comparison of language related problems for
neural machine translation and phrase-based machine translation between German
and English. The explored issues are related both to the language
characteristics as well as to the machine translation process and, although
related, are going beyond typical translation error classes.
 It is shown that the main advantage of the NMT system consists of better
handling of verbs, English noun collocations, German compound words, phrase
structure as well as articles. Although in total there are less obstacles for
the NMT system than for the PBMT system, they are complementary -- only about
one third of the sentences deals with the same issues, and for about 40% of the
sentences the issues are completely different.        This means that
combination/hybridisation of the NMT and PBMT approaches is a promising
direction for improving both types of systems. In addition, it is shown that
the main issues for the NMT system are prepositions, translation of English
(source) ambiguous words and generating English (target) continuous tenses.",7 Feb 2017 11:49:37 GMT,Resources/Evaluation,Machine translation,MT evaluations;  statistical machine translation,Maxx,Popxxxxx,xxxxxxxxxxxx@hu-berlin.de,Humboldt University of Berlin,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxx,Popxxxxx,ADAPT Centre @ DCU,,,,,,xxxxxxxxxxxxx166@gmail.com,,,,,Ireland,,Maxx Popxxxxx,xxxxxxxxxxxx@hu-berlin.de,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
810,810X-H2F6F6F6E8,SanSplit: Learning sandhi splitting process in Sanksrit using Deep Attentional Recurrent Neural Networks,Neelxxxxxxx Ganxxxxx;Naxxxx Paxxxx;Raxxx Araxxxxxxx;Anxxx Sanxxxxx;Raxxx Gaxx and Suxxxx Agxxxx,Phonology Morphology Word Segmentation,Jaxxx Eixxxx;Hinxxxx Schxxxxx,Reject,,Undecided (Tagging Chunking Syntax Parsing),"In Sanskrit language, small words (morphemes) are combined together through a
phonological process called \textit{Sandhi} to form arbitrarily long compound
words. Sandhi splitting is the process of splitting a given compound word into
its constituent morphemes. Although the rules governing the splitting of words
exist, it is highly challenging to identify where the split(s) occur, as the
same compound word might be broken down in multiple ways to provide partly
correct splits. Although existing systems explore incorporating these
pre-defined splitting rules, they have been unable to satisfactorily address
the fundamental problem of identifying the split location.

        In this research, we formulate a Sandhi split location prediction
algorithm for a compound Sanskrit word using attention based deep
bi-directional recurrent neural network with LSTM units (AB-RNN). The proposed
AB-RNN takes a compound word as input in the form of a character sequence and
predicts the probability of split after each character. The advantage of the
proposed AB-RNN is its ability to remember long term dependencies in a given
sequence, thus, providing better performance than existing tools. We further
create a large scale public corpus for evaluating sandhi splitting techniques
and benchmark the performance of the proposed approach with existing systems.",10 Feb 2017 04:55:16 GMT,Empirical/Data-Driven,"Phonology, morphology, and word segmentation",corpus development;  language modeling for spoken language;  word segmentation;  chunking,Neelxxxxxxx,Ganxxxxx,xxxxxxxxxxxg@gmail.com,IBM Research,No,Naxxxx,Paxxxx,xxxxxxxxxin.ibm.com,IBM Research,No,Raxxx,Araxxxxxxx,xxxxxxxxxxin.ibm.com,IBM Research,No,Anxxx,Sanxxxxx,xxxxxxxxxin.ibm.com,IBM Research,No,Raxxx,Gaxx,xxxxxxxxxxxxe.iitd.ac.in,IIT Delhi,No,Suxxxx,Agaxxxx,xxxxxxxxxxiitd.ac.in,IIT Delhi,No,,,,,,,,,,,,,,,,,,,,,,Neelxxxxxxx,Ganxxxxx,IBM Research,,,,,,xxxxxxxxxxxg@gmail.com,,,,,India,,Neelxxxxxxx Ganxxxxx;Naxxxx Paxxxx;Raxxx Araxxxxxxx;Anxxx Sanxxxxx;Raxxx Gaxx;Suxxxx Agaxxxx,xxxxxxxxxxxg@gmail.com;xxxxxxxxxxin.ibm.com;xxxxxxxxxx@in.ibm.com;xxxxxxxxxxin.ibm.com;xxxxxxxxxxxxse.iitd.ac.in;xxxxxxxxxx.iitd.ac.in,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
811,811X-E8E6D4A3B2,Towards a Universal Sentiment Classifier in Multiple languages,Kxx Xx and Xiaxxxx Wxx,Sentiment Analysis Opinion Mining,Alexxxxxx Balxxxx;Lunxxxx Kx;Saxx Mohxxxxx,Reject,,Undecided (Sentiment Analysis Opinion Mining),"Existing sentiment classifiers usually work for only one specific language, and
different classification models are used in different languages. In this paper
we aim to build a universal sentiment classifier with a single classification
model in multiple different languages. In order to achieve this goal, we
propose to learn multilingual sentiment-aware word embeddings simultaneously
based only on the labeled reviews in English and unlabeled parallel data
available in a few language pairs. It is not required that the parallel data
exist between English and any other language, because the sentiment information
can be transferred into any language via pivot languages. We present the
evaluation results of our universal sentiment classifier in five languages, and
the results are very promising even when the parallel data between English and
the target languages are not used. Furthermore, the universal single classifier
is compared with a few cross-language sentiment classifiers relying on direct
parallel data between the source and target languages, and the results show
that the performance of our universal sentiment classifier is very competitive
with that of different cross-language classifiers in multiple target languages.",7 Feb 2017 13:03:24 GMT,Empirical/Data-Driven,Sentiment analysis and opinion mining,sentiment analysis;  multilingual applications,Kxx,Xx,xxxxxxxxu.edu.cn,Peking University,No,Xiaxxxx,Wxx,xxxxxxxxxx@pku.edu.cn,Peking University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Xiaxxxx,Wxx,Peking University,,,,,,xxxxxxxxxx@pku.edu.cn,,,,,China,,Kxx Xx;Xiaxxxx Wxx,xxxxxxxxu.edu.cn;xxxxxxxxxxx@pku.edu.cn,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
812,812X-P8H9F3E7B4,Rule-Guided Safe and Efficient On-line Dialogue Policy Learning,Lx Chxx;Xixxx Zhxx;Chxxx Chxxx;Ruxxxx Yaxx and Kxx Y,Dialog Interactive Systems,Rxx Artxxxxx;Raxxxx Ferxxxxxxx;Olxxxx Lexxx,Reject,,Undecided (Dialog Interactive Systems),"Hand-crafted rules and reinforcement learning (RL) are two popular choices to
obtain dialogue policy.  Whereas rule-based policy is often reliable but not
self-adapting, RL employees optimization to improve policy but suffers bad
initial performance. In this paper we propose a novel on-line learning method
to integrate these two approaches with companion learning, in which the
pre-defined rule-based policy guides the statistical system as a teacher.
Besides reward from the users, the statistical dialogue policy receives example
action as well as extra reward from the rule. Based on the uncertainty
estimated using a dropout Q-Network, two companion functions are proposed to
control the teaching time. Simulation experiments showed that our proposed
framework and the companion functions can significantly improve the safety and
efficiency of on-line policy optimization.",7 Feb 2017 13:11:51 GMT,Empirical/Data-Driven,Dialog and interactive systems,dialogue control;  dialogue;  reinforcement learning,Lx,Chxx,xxxxxxxxxxjtu.edu.cn,Shanghai Jiao Tong University,No,Xixxx,Zhxx,xxxxxxxxxtu.edu.cn,Shanghai Jiao Tong University,No,Chxxx,Chxxx,xxxxxxxxxxx@sjtu.edu.cn,Shanghai Jiao Tong University,No,Ruxxxx,Yaxx,xxxxxxxxxxx@sjtu.edu.cn,Shanghai Jiao Tong University,No,Kxx,Yx,xxxxxxxxxtu.edu.cn,Shanghai Jiao Tong University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Lx,Chxx,Shanghai Jiao Tong University,,,+86 1xxxxxxxxxx,,,xxxxxxxxxxjtu.edu.cn,,Shanghai,Shanghai,,China,,Lx Chxx;Xixxx Zhxx;Chxxx Chxxx;Ruxxxx Yaxx;Kxx Yx,xxxxxxxxxxjtu.edu.cn;xxxxxxxxxjtu.edu.cn;xxxxxxxxxxxx@sjtu.edu.cn;xxxxxxxxxxxx@sjtu.edu.cn;xxxxxxxxxjtu.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
813,813X-D6F8F7C9B5,Optimizing Language Learning from Reading the Web,Igxx Labxxxx,Multidisciplinary,Kaxxxx Foxx;Micxxxx Pioxxxxxxx,Reject,,Undecided (Multidisciplinary),"In this paper, we explore the potential of the web to be the utilized as a
resource for teaching foreign language vocabulary. As more and more people
shift to reading electronic media (e.g., Wikipedia, news articles, e-books), we
are presented with an opportunity to exploit that content as a vehicle for
teaching a foreign language. Specifically, in this paper, we explore the
potential of teaching readers vocabulary in a new language by deliberately
replacing words in the reading with their translation in the foreign language.
Our hypothesis is that the reader will implicitly pick-up the meaning of the
foreign words from context, while reading content that they are interested in
and understand. Based on studies that we conduct, we develop a computational
approach for modeling word learning from context, and utilize this model to
optimize a set of web pages to present to the reader to maximize the number of
acquired words.",7 Feb 2017 12:02:34 GMT,Empirical/Data-Driven,Other,NLP applications;  educational applications;  code-switching,Igxx,Labxxxx,xxxxxxxxxxxv@gmail.com,Cornell University,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Igxx,Labxxxx,Cornell University,,,646xxxxxxx,,,xxxxxxxxxxxv@gmail.com,,,,,United States,,Igxx Labxxxx,xxxxxxxxxxxv@gmail.com,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
814,814X-C6P5H9E8E5,Graph Based Decoding for Event Sequencing and Coreference Resolution,Zhexxxxxxx Lxx;Texxxx Mitxxxxx and Edxxxx Hxx,Semantics,Maxxxx Farxxxx;Hanxxxxx Hajxxxxxxx;Prexxxx Naxxx;Mehxxxxxx Sadxxxxxx;Alxxx Villxxxxxxxxx,Reject,,Undecided (Semantics),"Events in text document often interact with each other with complex temporal
and logical relations. Automatic identification and classification of such
relations is an important step for document understanding. Recent studies on
event relation classification only focus on the temporal aspects of events. In
this paper, we study event coreference and a new event sequencing task under
the script (Schank and Abelson, 1977)  framework. The latter requires causal
and logical inference in addition to temporal reasoning. We propose a new
unified structured-graph decoding algorithm that allows us to decode directed
acyclic graphs to solve event sequencing. Our event coreference system has
achieved state-of-the-art performance on TAC-KBP event coreference task and our
event sequencing system beats an informed baseline. Despite the good
performance, we argue that complex semantic reasoning is required to further
improve system performance on both tasks.",7 Feb 2017 12:51:07 GMT,Empirical/Data-Driven,Semantics,structured input/output;  relation/event extraction;  coreference resolution,Zhexxxxxxx,Lxx,xxxxxxxcmu.edu,Carnegie Mellon University,No,Texxxx,Mitxxxxx,xxxxxxxxs.cmu.edu,Carnegie Mellon University,No,Edxxxx,Hoxx,xxxxxxmu.edu,Carnegie Mellon University,No,,,,,,,,,,,,,,,,,,,,,Hidxxxxx,Shixxxx,xxxxxxxxxxxxeis.ynu.ac.jp,,,,,,,,,,,,,,Zhexxxxxxx,Lxx,Carnegie Mellon University,,,,,,xxxxxxxcmu.edu,,,,,United States,,Zhexxxxxxx Lxx;Texxxx Mitxxxxx;Edxxxx Hoxx,xxxxxxxcmu.edu;xxxxxxxxxs.cmu.edu;xxxxxxcmu.edu,,,,,,,on,,No. Do not include my submission in this dataset.,No,None,None
815,815X-E7A6D3G6E3,Divergence Characterization for Impaired Speech in ALS,Arxxxx Bhxxxx;Boxxxx Doxx;Krxxxx Hollxxxxxxxx;Barxxxx McKxxxxx and Saxxxx Lx,Biomedical,Aurxxxxx Néxxxxx;Kaxxx Verxxxxx,Reject,,Undecided (Biomedical),"Approximately 80\% to 95\% of patients with Amyotrophic Lateral Sclerosis (ALS)
eventually develop speech impairments \cite{Beu11}, such as defective
articulation, slow laborious speech and hypernasality \cite{Duf13}. We adopt
the view that the relationship between impaired speech and asymptomatic speech
may be seen as a {\it divergence\/} from a baseline. This relationship can be
characterized in terms of measurable combinations of phonological
characteristics that are indicative of the degree to which the two diverge.
While certain components of speech such as speaking rate, breathing patterns,
and voice loudness have proven too variable to provide a reliable marker
\cite{Gre13}, we demonstrate that divergence measurements based on phonological
characteristics of speech correlate with physiological assessments of ALS.
Speech-based assessments offer benefits over commonly-used physiological
assessments in that they are inexpensive, non-intrusive, and do not require
trained clinical personnel for administering and interpreting the results. This
paper lays the theoretical foundation for characterizing divergences from
phonological characteristics and demonstrates empirically---through analysis of
ALS speech from 10 subjects---that it is possible to identify divergences with
high reliability and to correlate speech divergences with outcomes of
physiological assessments.",7 Feb 2017 12:19:29 GMT,Theoretical,Biomedical,NLP applications;  phonology;  transcription,Arxxxx,Bhxxxx,xxxxxxx@ihmc.us,Florida Institute for Human and Machine Cognition,No,Boxxxx,Doxx,xxxxxxxxxxacs.umd.edu,Florida Institute for Human-Machine Cognition,No,Krxxxx,Hollxxxxxxxx,xxxxxxxxxgmail.com,IHMC,No,Barxxxx,McKxxxxx,xxxxxxxxxxxenzie@va.gov,"James A. Haley VA Hospital, Tampa, FL",No,Samxxxxxx,Phixxxxx,xxxxxxxxxxxlips@va.gov,"James A. Haley VA Hospital, Tampa, FL",No,,,,,,,,,,,,,,,,,,,,,,,,,,,Arxxxx,Bhxxxx,Florida Institute for Human and Machine Cognition,,,,,,xxxxxxx@ihmc.us,,,FL,,United States,,Arxxxx Bhxxxx;Boxxxx Doxx;Krxxxx Hollxxxxxxxx;Barxxxx McKxxxxx;Saxxxx Lx,xxxxxxx@ihmc.us;xxxxxxxxxxxacs.umd.edu;xxxxxxxxx@gmail.com;xxxxxxxxxxxxenzie@va.gov;xxxxxxxxxxxllips@va.gov,,,,,,,,,Only include my submission if it is accepted.,No,None,None
816,816X-D9J3F5P8E6,Harnessing Model Uncertainty for Understanding and Regularizing Neural Predictions,izzxxxxx gxx;sexxx yaxxx;Yx Sx and xixxxx yx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Reject,,Undecided (Machine Learning),"Complexity of deep neural networks make it difficult to justify the underlying
predictions. This drawback limits progressive development of new models and
further
applicability. To alleviate these problems, we learn to pinpoint variables of a
neural network model as neural indicators that cause high uncertainty in the
network. Our
approach utilizes two different views of model uncertainty, forward and
backward, to generate the neural indicators. Forward view uncertainty observes
the variation in the output distribution, while backward view uncertainty
examines the error estimate of the model when output distribution varies. We
develop a new objective using adversarial examples generated via model
uncertainty to regularize confident output distributions. We evaluate the
approach on factoid question answering. Identified neural indicators show a
high semantic similarity with the corresponding knowledge-base paths. Our
adversarial training approach outperforms a
multiencoder approach significantly. We also observe similar improvements on a
sentiment
classification task.",7 Feb 2017 12:34:14 GMT,Theoretical,Machine learning,unsupervised and semi-supervised learning;  theoretical aspects of machine learning,izzxxxxx,gxx,xxxxxxxxxxxxxmail.ucsb.edu,university of california santa barbara,No,sexxx,yaxxx,xxxxxxxxx.ucsb.edu,university of california santa barbara,No,Yx,Sx,xxxxxxxucsb.edu,University of California Santa Barbara,No,xixxxx,yxx,xxxxxxxxucsb.edu,university of california santa barbara,No,,,,,,,,,,,,,,,,Wexxxx,Jxx,xxxxxxxxxxsjtu.edu.cn,,,,,,,,,,,,,,Izzxxxxx,Gxx,University of California Santa Barbara,,,,,,xxxxxxxxxxr@gmail.com,,,,,United States,,izzxxxxx gxx;sexxx yaxxx;Yx Sx;xixxxx yxx,xxxxxxxxxxxxxmail.ucsb.edu;xxxxxxxxxs.ucsb.edu;xxxxxxxxucsb.edu;xxxxxxxx.ucsb.edu,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
817,817X-D5F7C3A3D6,Supervised Models for Argument Mining in Essays,Isxxx Perxxxx and Vinxxxx Nx,Discourse Pragmatics,Yanxxxxx Jx;Suxxxx Lx;Boxxxx Wexxxx,Reject,,Undecided (Discourse Pragmatics),"Understanding the argumentative structure of
a persuasive essay involves addressing two
challenging tasks: identifying the components
of the essay’s argument and identifying the
relations that occur between them.  We                    
examine the under-investigated task of
argument mining in persuasive 
student essays, where we  
train a perceptron that allows
us to output 
the tree associated with each paragraph.
We evaluate our approach on  a              
publicly-available corpus of 402 essays, where it yields
a 10.7% relative error reduction in F-score
at the relation identification task 
over a state-of-the-art system.",7 Feb 2017 11:59:31 GMT,Empirical/Data-Driven,Discourse and pragmatics,discourse;  pragmatics,Isxxx,Perxxxx,xxxxxxxxxxotmail.com,University of Texas at Dallas,No,Vinxxxx,Nx,xxxxxxxxxxxtdallas.edu,University of Texas at Dallas,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Isxxx,Perxxxx,University of Texas at Dallas,,,,,,xxxxxxxxxxotmail.com,,,,,United States,,Isxxx Perxxxx;Vinxxxx Nx,xxxxxxxxxxotmail.com;xxxxxxxxxxxutdallas.edu,,,,,,,on,on,No. Do not include my submission in this dataset.,No,None,None
818,818X-F6D9A3H7G4,Verb Physics: Relative Physical Knowledge of Actions and Objects,Maxxxxx Foxxxx and Yexxx Chxx,Vision Robots Grounding,Moxxx Baxxxx;Naxx Kusxxxx,Accept - Oral Monday,,Undecided (Vision Robots Grounding),"Learning commonsense knowledge from natural language text is nontrivial due to
reporting bias: people rarely state the obvious, e.g., “My house is bigger
than me.” However, while rarely stated explicitly, this trivial everyday
knowledge does influence the way people talk about the world, which provides
indirect clues to reason about the world. For example, a statement like,
“Tyler entered his house” implies that his house is bigger than Tyler.

In this paper, we present an approach to infer relative physical knowledge of
actions and objects along five dimensions (e.g., size, weight, and strength)
from unstructured natural language text. We frame knowledge acquisition as
joint inference over two closely related problems: learning (1) relative
physical knowledge of object pairs and (2) physical implications of actions
when applied to those object pairs. Empirical results demonstrate that it is
possible to extract knowledge of actions and objects from language and that
joint inference over different types of knowledge improves performance.",23 Apr 2017 10:54:35 GMT,Empirical/Data-Driven,"Vision, robots, and other grounding",,Maxxxxx,Foxxxx,xxxxxxxxxcs.uw.edu,University of Washington,No,Yexxx,Chxx,xxxxxxxxxxxshington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,Takxxxx,Egxxxx,xxxxxxxxxxuchi@alt.ai,,,,,,,,,,,,,,Maxxxxx,Foxxxx,University of Washington,,,,,,xxxxxxxxxcs.uw.edu,,,,,United States,,Maxxxxx Foxxxx;Yexxx Chxx,xxxxxxxxxcs.uw.edu;xxxxxxxxxxxxshington.edu,Verb Physics: Relative Physical Knowledge of Actions and Objects,Verb Physics: Relative Physical Knowledge of Actions and Objects,11,Maxwell Forbes,Graduate student,"Paul G. Allen School of Computer Science & Engineering
University of Washington
Box 352350
185 E Stevens Way NE
Seattle, WA 98195",on,on,Only include my submission if it is accepted.,No,None,None
819,819X-J8E7A8G7F9,Improving Factoid Question Answering by Query Revision,Sexxx Yaxxx;Izzxxxxx Gxx;Yx Sx and Xixxxx Yx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"In this work, we propose revising factoid questions to include entity and
relation context information in knowledge bases and demonstrate the revised
questions are able to help improving the quality of question answering. We
identify three kinds of \textit{question revisions} that augment question
statement with textual evidences corresponding to relation, type of topic
entity, and type of answer. A bidirectional Long Short-Term Memory (LSTM)
network is employed to encode revised questions. We use a margin-based loss
objective is to train a scoring mechanism over these representations. It
determines whether we shall adjust the results returned by an existing QA
system. We evaluate the approach on STAGG, one of the leading question
answering systems. Our approach improves the F1 score from 52.5\% to 53.9\% on
WEBQUESTIONS data.",7 Feb 2017 13:08:27 GMT,Empirical/Data-Driven,"Information extraction, text mining, and question answering",question interpretation;  open-domain question answering,Sexxx,Yaxxx,xxxxxxxxx.ucsb.edu,University of California Santa Barbara,No,Izzxxxxx,Gxx,xxxxxxxxxxx@cs.ucsb.edu,UC Santa Barbara,No,Yx,Sx,xxxxxxxucsb.edu,UC Santa Barbara,No,Xixxxx,Yxx,xxxxxxxxucsb.edu,UC Santa Barbara,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sexxx,Yaxxx,University of California Santa Barbara,,,,,,xxxxxxxxx.ucsb.edu,,,,,United States,,Sexxx Yaxxx;Izzxxxxx Gxx;Yx Sx;Xixxxx Yxx,xxxxxxxxx.ucsb.edu;xxxxxxxxxxxx@cs.ucsb.edu;xxxxxxxxucsb.edu;xxxxxxxx.ucsb.edu,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
820,820X-P2H3G7F5E6,A Syntactic Approach to Domain-Specific Automatic Question Generation,Gxx Daxxx and Maxx Laxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Factoid questions are questions that require short fact-based answers.
Automatic generation (AQG) of factoid questions from a given text can
contribute to educational activities, interactive question answering systems,
search engines, and other applications.  
  The goal of our research is to generate factoid source-question-answer
triplets based on a specific domain.  We propose a four-component pipeline,
which obtains as input a training corpus of domain-specific documents, along
with a set of declarative sentences from the same domain, and generates as
output a set of factoid questions that refer to the source sentences but are
slightly different from them, so that a question-answering system or a person
can be asked a question that requires a deeper understanding and knowledge than
a simple word-matching. Contrary to existing domain-specific AQG systems that
utilize the template-based approach to question generation, we propose to
transform each source sentence into a set of questions by applying a series of
rules (a syntactic-based approach).
  Our pipeline was evaluated in the domain of cyber security using a series of
experiments on each component of the pipeline separately and on the end-to-end
system. The proposed approach generated a higher percentage of acceptable
questions than a prior state-of-the-art AQG system.",7 Feb 2017 11:56:46 GMT,Applications/Tools,"Information extraction, text mining, and question answering",NLP applications;  language generation;  word sense disambiguation;  semantic relations;  ontological semantics;  question answering in restricted domains,Gxx,Daxxx,xxxxxxxxx@gmail.com,Ben-Gurion University of the Negev,No,Maxx,Laxx,xxxxxxxgu.ac.il,Ben-Gurion University of the Negev,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Maxx,Laxx,Ben-Gurion University of the Negev,,,,,,xxxxxxxgu.ac.il,,,,,Israel,,Gxx Daxxx;Maxx Laxx,xxxxxxxxx@gmail.com;xxxxxxxxgu.ac.il,,,,,,,on,,Only include my submission if it is accepted.,No,None,None
821,821X-F8H8F7D8B7,The Bechdel Test is Not Enough: Gender Biases in the Language and Story Structure of Modern Films,Maaxxxx Sxx;Arxxx Holxxxxx;Cixxx Marxxxxx;Haxxxx Rasxxxx and Yexxx Cxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Movies have considerable influence over cultural norms and social identities.  
    Thus, any implicit bias portrayed in movies runs the risk of reinforcing
existing bias found in society. 
    In this paper, we investigate systematic gender bias in modern films  
    Specifically, we characterize different types of bias in three elements of
movie scripts: narration (character portrayal in 3rd person), dialog
(character's self-expression and interactions), and story structure. 
    Our contribution includes a new extension to the connotation frames of
\newcite{rashkinconnotation} with two new connotative dimensions: agency and
power dynamics implied by specific verb choices, which allow us to study more
nuanced bias than previously possible.         
    We find marked differences in the way narration, dialogue, and the
underlying structure of scripts tell the stories of men and women and develop
automatic methods of inspecting these factors and the bias they present.",7 Feb 2017 11:57:12 GMT,Applications/Tools,"Document analysis including text categorization, topic models, and retrieval",sentiment analysis;  NLP applications,Maaxxxx,Sxx,xxxxxxxxxxxhington.edu,University of Washington,No,Arxxx,Holxxxxx,xxxxxxxxxxxhington.edu,University of Washington,No,Cindyxxxxxxxxx,Praxxxxxx,xxxxxxxxxxxxxtio@gmail.com,University of Washington,No,Haxxxx,Rasxxxx,xxxxxxxxxxxxxashington.edu,University of Washington,No,Yexxx,Chxx,xxxxxxxxxxxshington.edu,University of Washington,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Maaxxxx,Sxx,University of Washington,,,,,,xxxxxxxxxxxhington.edu,,Seattle,WA,,United States,,Maaxxxx Sxx;Arxxx Holxxxxx;Cixxx Marxxxxx;Haxxxx Rasxxxx;Yexxx Chxx,xxxxxxxxxxxhington.edu;xxxxxxxxxxxshington.edu;xxxxxxxxxxxxxetio@gmail.com;xxxxxxxxxxxxxwashington.edu;xxxxxxxxxxxxshington.edu,,,,,,,,,Only include my submission if it is accepted.,No,None,None
822,822X-J6A3J6H6B6,Semantic Parsing of Pre-university Math Problems,Taxxxx Matxxxxxx;Taxxxx Ixx;Hidxxxx Iwxxx;Hirxxxxx Anxx and Noxxxx Hx,Tagging Chunking Syntax Parsing,Emxxx Pixxxx;Barxxxx Plxxx;Yxx Zhxxx;Hxx Zhxx,Accept - Poster Tuesday,,Undecided (Tagging Chunking Syntax Parsing),"We have been developing an end-to-end math problem solving system that accepts
natural language input.
The current paper focuses on how we analyze the problem sentences to produce
logical forms.
We chose a hybrid approach combining a shallow syntactic analyzer and a
manually-developed lexicalized grammar.
A feature of the grammar is that it is extensively typed on the basis of a
formal ontology for pre-university math.
These types are helpful in semantic disambiguation inside and across sentences.
Experimental results show that the hybrid system produces a well-formed logical
form with 88\% precision and 56\% recall.",20 Apr 2017 15:05:16 GMT,Empirical/Data-Driven,"Tagging, chunking, syntax, and parsing",,Taxxxx,Matxxxxxx,xxxxxxxxxxxxxxnagoya-u.ac.jp,Nagoya University,No,Taxxxx,Ixx,xxxxxxxxxxxxxxnagoya-u.ac.jp,Nagoya University,No,Hidxxxx,Iwxxx,xxxxxxxxxxujitsu.com,Fujitsu Lab. Ltd.,No,Hirxxxxx,Anxx,xxxxxxxxxujitsu.com,Fujitsu Lab. Ltd,No,Noxxxx,H. xxxx,xxxxxxxi.ac.jp,National Institute of Informatics,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Taxxxx,Matxxxxxx,Nagoya University,,,+81-5xxxxxxxxxx,,,xxxxxxxxxxxxxxnagoya-u.ac.jp,,,,,Japan,,Taxxxx Matxxxxxx;Taxxxx Ixx;Hidxxxx Iwxxx;Hirxxxxx Anxx;Noxxxx Hx,xxxxxxxxxxxxxxnagoya-u.ac.jp;xxxxxxxxxxxxxx.nagoya-u.ac.jp;xxxxxxxxxxfujitsu.com;xxxxxxxxxxujitsu.com;xxxxxxxii.ac.jp,Semantic Parsing of Pre-university Math Problems,Semantic Parsing of Pre-university Math Problems,11,Takuya Matsuzaki,,"Nagoya University
Furo-cho, Chikusa-ku, Nagoya, Aichi, Japan",,on,No. Do not include my submission in this dataset.,No,None,None
823,823X-C5C2C2D8G5,Mr. Gee: Learning Multi-relational Graph Embedding for Stance Classification,Weixxxx Chxx;Yixxxx Chxx and Lunxxxx K,Social Media,Zhixxxx Lxx;Shxxxx Pxx;Svixxxxx Volxxxx,Reject,,Undecided (Social Media),"Knowledge is usually presented as a graph containing entities and their
relations; this type of information, however, is not easily integrated with
feature vectors. In this paper, we propose Mr. Gee, a novel approach for
generating multi-relational graph embeddings which effectively captures the
graph structures within the generated embeddings. By encoding the graph without
task labels, the reliability of generated embeddings does not depend on the
amount of gold instances. We evaluated the  Mr. Gee embeddings using a stance
classification task on web posts. A post engagement graph is defined and
constructed to show for each post the word-inclusion and user-engagement
relations for this purpose. Chinese Facebook and English CreateDebate datasets
were selected as experimental materials, on which Mr. Gee achieved
state-of-the-art performance of 87.6% and 80.7% accuracy, respectively. When
the likers are available, this novel embedding yields performance gains of
23.59% compared to the best related work; this demonstrates its power to model
useful relations.",7 Feb 2017 12:33:27 GMT,Empirical/Data-Driven,Social media,sentiment analysis;  Web mining;  graph-based algorithms;  text classification;  NLP in social networking media,Weixxxx,Chxx,xxxxxxxxxxxxxsinica.edu.tw,Academia Sinica,No,Yixxxx,Chxx,xxxxxxxxxxxinica.edu.tw,Academia Sinica,No,Lunxxxx,Kx,xxxxxxxxxxxnica.edu.tw,Academia Sinica,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yixxxx,Chxx,Academia Sinica,,,,,,xxxxxxxxxxxinica.edu.tw,,,,,Taiwan,,Weixxxx Chxx;Yixxxx Chxx;Lunxxxx Kx,xxxxxxxxxxxxxsinica.edu.tw;xxxxxxxxxxxxinica.edu.tw;xxxxxxxxxxxinica.edu.tw,,,,,,,on,on,Only include my submission if it is accepted.,No,None,None
824,824X-B9D3B8D4E2,Revealing Underlying Text Semantics: Automatic Mind-map Generating with Recurrent Neural Networks,Yaxx Wxx and Honxxxx Gxx,IE QA Text Mining Applications,Maxxxx -;Euxxxx Agixxxxxx;Chixxxxx Chxxx;Zorxxxxx Kozxxxxx;Kaxx Lxx,Reject,,Undecided (IE QA Text Mining Applications),"Seeking methods to assistant people understanding an article is always a
hotspot. We suggest an automatic mind-map generating algorithm would be of
great help to this purpose as a mind-map can illustrate both the salient
semantics of an article and the relationship between the semantics. We define
the relationship as the governing relationship that the governor contains the
information which, from the author’s perspective, is the origin of the
contents described by the governed semantics. To evaluate the governing
relatedness, we adopt the memory and gate mechanism of the Long Short-Term
Memory (LSTM) to extract the salient information the governed semantics inherit
from the governor. The proposed method has achieved promising experimental
results on comparison with the artificial mind-maps.",7 Feb 2017 13:06:06 GMT,Empirical/Data-Driven,"Document analysis including text categorization, topic models, and retrieval",information extraction;  text mining;  document summarization;  document mining,Yaxx,Wxx,xxxxxxxxxotmail.com,Nankai University,No,Honxxxx,Gxx,xxxxxxxx.ibm.com,IBM Research - China,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Wxx,Nankai University,,,+86 1xxxxxxxxxx,,,xxxxxxxxxotmail.com,,Tianjin,,,China,,Yaxx Wxx;Honxxxx Gxx,xxxxxxxxxotmail.com;xxxxxxxxn.ibm.com,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
825,825X-B4D7A6A8P5,An Algebra for Feature Extraction,Vixxx Srixxxxx,Machine Learning,Grzxxxxx Chrxxxxxx;Amxx Gloxxxxxx;Toxxx Jaaxxxxx;Suxxxx Raxx;Wilxxxx Yaxx,Accept - Poster Tuesday,,Undecided (Machine Learning),"Though feature extraction is a necessary first step in statistical NLP, it is
often seen as a mere preprocessing step. Yet, it can dominate computation time,
both during training, and especially at deployment. In this paper, we formalize
feature extraction from an algebraic perspective. Our formalization allows us
to define a message passing algorithm that can restructure feature templates to
be more computationally efficient. We show via experiments on text chunking and
relation extraction that this restructuring does indeed speed up feature
extraction in practice by reducing redundant computation.",24 Apr 2017 14:42:45 GMT,Theoretical,Machine learning,,Vixxx,Srixxxxx,xxxxxxxxx.utah.edu,University of Utah,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vixxx,Srixxxxx,University of Utah,,,,,,xxxxxxxxx.utah.edu,,,UT,,United States,,Vixxx Srixxxxx,xxxxxxxxx.utah.edu,An Algebra for Feature Extraction,An Algebra for Feature Extraction,10,Vivek Srikumar,,,,,No. Do not include my submission in this dataset.,No,None,None
827,827X-F2A3P9G8E3,Memory-augmented Neural Machine Translation,Yaxx Fexx;Shxxxx Zhxxx;Doxx Waxx and Anxx Zhxx,Machine Translation,Yaxx Lxx;Minxxxxxxx Luxxx;Haxxxx Mx;Grxxxx Nexxxx;Dexx Xixxx,Reject,,Undecided (Machine Translation),"Neural machine translation (NMT), due to its highly compact neural model 
and the smooth translation functions, is short in dealing with unfrequent words
and
pairs. This paper presents a new memory-augmented NMT architecture, which
stores 
the information about how unfrequent words should be translated in a memory and

utilizes them in the decoding process. Our experiments on two translation tasks
demonstrated that the presented architecture can noticeably improve performance
of NMT systems.",7 Feb 2017 13:07:44 GMT,Empirical/Data-Driven,Machine translation,,Yaxx,Fexx,xxxxxxxxp@163.com,Baidu,No,Shxxxx,Zhxxx,xxxxxxxx@163.com,BUPT,No,Doxx,Waxx,xxxxxxxxxxxxxxxx.tsinghua.edu.cn,Tsinghua University,No,Anxx,Zhxxx,xxxxxxxxxxxxxxxxxit.tsinghua.edu.cn,BUPT,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yaxx,Fexx,"Institute of Computing Technology, Chinese Academy of Sciences",,,,,,xxxxxxxxxict.ac.cn,,,,,China,,Yaxx Fexx;Shxxxx Zhxxx;Doxx Waxx;Anxx Zhxxx,xxxxxxxxp@163.com;xxxxxxxx7@163.com;xxxxxxxxxxxxxxxxs.tsinghua.edu.cn;xxxxxxxxxxxxxxxxxxit.tsinghua.edu.cn,,,,,,,,,No. Do not include my submission in this dataset.,No,None,None
3181,3181X-A3H3G4F3P6,Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers,Saxxx Štxxxxx;Vicxxxxx Yaxxxx;Ruxxxx Mixxxx;Sixxxx Paxxx and Hexxxx Stucxxxxxxxxx,Cognitive Modelling and Psycholinguistics,Roxxx Lexx;Anxxxx Søxxxxx,Reject,,Undecided (Cognitive Modelling and Psycholinguistics),"Eye tracking studies from the past few decades have shaped the way we think of
word complexity and cognitive load: words that are long, rare and ambiguous are
more difficult to read. However, online processing techniques have been
scarcely applied to investigating the reading difficulties of people with
autism and what vocabulary is challenging for them. Are word length and word
frequency still the most important features of complex words? We present
parallel gaze data obtained from adult readers with autism and a control group
of neurotypical readers and show that the former required higher cognitive
effort to comprehend the texts as evidenced by three gaze-based measures. We
divide all words into four classes based on their viewing times for both groups
and investigate the relationship between longer viewing times and several
cognitively-based measures including word concreteness, familiarity, age of
acquisition and imagability.",6 Feb 2017 10:41:13 GMT,Empirical/Data-Driven,Cognitive modeling and psycholinguistics,language acquisition,Saxxx,Štxxxxx,xxxxxxxxxxxja@gmail.com,University of Mannheim,No,Vicxxxxx,Yaxxxx,xxxxxxxxxwlv.ac.uk,University of Wolverhampton,No,Ruxxxx,Mixxxx,xxxxxxxxxwlv.ac.uk,University of Wolverhampton,No,Simoxxxxxxxx,Ponxxxxx,xxxxxxxxxxxxxxxxk.uni-mannheim.de,University of Mannheim,No,Hexxxx,Stuckxxxxxxxxx,xxxxxxxxxxxxxxxxk.uni-mannheim.de,University of Mannheim,No,,,,,,,,,,,,,,,,,,,,,,,,,,,Saxxx,Štxxxxx,University of Mannheim,,,,,,xxxxxxxxxxxja@gmail.com,,,,,Germany,,Saxxx Štxxxxx;Vicxxxxx Yaxxxx;Ruxxxx Mixxxx;Sixxxx Paxxx;Hexxxx Stuckxxxxxxxxx,xxxxxxxxxxxja@gmail.com;xxxxxxxxx@wlv.ac.uk;xxxxxxxxx@wlv.ac.uk;xxxxxxxxxxxxxxxxxk.uni-mannheim.de;xxxxxxxxxxxxxxxxxk.uni-mannheim.de,,,,,,,,on,No. Do not include my submission in this dataset.,No,None,None
