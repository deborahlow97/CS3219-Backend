5,4,326,Jacxxxx Saxxx,1,"(OVERALL EVALUATION) The topics of the paper is clearly appropriate for the JCDL conference.  The authors presents a study of the effectiveness of four learning schemes used to predict the quality of a citation (binary:  important vs. marginal) in an corps of scientific papers (ACL corpus on computational linguistics). The experiment is based on a set of 450 citations (not so big).  

In a second part, the authors propose two new learning strategies (one based on SVM and random forest, the second on a deep learning architecture).  The experiment shows that a better performance (precision, Recall, F1) can be achieved by the two new proposed schemes.  A statistical test is missing to confirm this findings.    

minor points
Who are the experts (Page 3, Section 3).
Is the citation collection available? (Page 3, Section 3)
Section 4.1. not fully clear:  ""Each feature is divided into four categories""  Each feature or the whole feature set?
Section ""4 Results ..."" must be ""5. Results ..""
In Fig 2 and 3 :  add a space before ""(area =""
Page 7:  ""by using the-fold"" -> ""by using the three-fold""
""6 Conclusion""  -> ""7. Conclusion""
In the conclusion, the support for your learning scheme is a single collection... maybe another scientific domain can have a different citation pattern..","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-01-24,16:50,no,,5,4,326,Jacques Savoy,1,"(OVERALL EVALUATION) The topics of the paper is clearly appropriate for the JCDL conference.  The authors presents a study of the effectiveness of four learning schemes used to predict the quality of a citation (binary:  important vs. marginal) in an corps of scientific papers (ACL corpus on computational linguistics). The experiment is based on a set of 450 citations (not so big).  

In a second part, the authors propose two new learning strategies (one based on SVM and random forest, the second on a deep learning architecture).  The experiment shows that a better performance (precision, Recall, F1) can be achieved by the two new proposed schemes.  A statistical test is missing to confirm this findings.    

minor points
Who are the experts (Page 3, Section 3).
Is the citation collection available? (Page 3, Section 3)
Section 4.1. not fully clear:  ""Each feature is divided into four categories""  Each feature or the whole feature set?
Section ""4 Results ..."" must be ""5. Results ..""
In Fig 2 and 3 :  add a space before ""(area =""
Page 7:  ""by using the-fold"" -> ""by using the three-fold""
""6 Conclusion""  -> ""7. Conclusion""
In the conclusion, the support for your learning scheme is a single collection... maybe another scientific domain can have a different citation pattern..","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-01-24,no
9,4,8,Ghxxxx Abdxxxx,4,"(OVERALL EVALUATION) the authors apply the Extra-Trees classifier to extract 29 best new features and create a new model that they use to apply Random Forest and Support Vector Machine to classify reference as important or not important. The new model improves on the state of the art by 11.25 according to the experimental results.
The paper is technically sound, the approach is described in details and the results are reasonable.
The related work section is a little long for a technical paper, however, it provided a nice survey of the previous work.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-24,07:20,no,,9,4,8,Ghaleb Abdulla,4,"(OVERALL EVALUATION) the authors apply the Extra-Trees classifier to extract 29 best new features and create a new model that they use to apply Random Forest and Support Vector Machine to classify reference as important or not important. The new model improves on the state of the art by 11.25 according to the experimental results.
The paper is technically sound, the approach is described in details and the results are reasonable.
The related work section is a little long for a technical paper, however, it provided a nice survey of the previous work.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-24,no
11,5,326,Jacxxxx Saxxx,1,"(OVERALL EVALUATION) Interesting paper.  The authors have analyzed 75 papers on experiments / presentation / descriptions of deploying cloud services for libraries. The main results focus on data, patrons, library staff, IT infrastructure, cloud services, costs, and policies & contracts. Besides these main aspects, the authors underline that the librarians must understand the IT processes.  In addition, the authors mention clearly the problems related to the legal aspects, the risks, and the importance for librarians to support the new web services. 

In a second part, the paper presents seven main recommendations to help librarians in selecting / developing a set of cloud services (the service scope, managing the time, the costs, the quality, the human resource, the communication and the risks).   

The style and writing of the paper is clear and easy to follow.  The organisation of the prevention is good and examples are used to help the reader to understand the underlying problems / issues / possible solutions. 

This paper should generate questions and discussions during the conference.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-01-24,16:51,no,,11,5,326,Jacques Savoy,1,"(OVERALL EVALUATION) Interesting paper.  The authors have analyzed 75 papers on experiments / presentation / descriptions of deploying cloud services for libraries. The main results focus on data, patrons, library staff, IT infrastructure, cloud services, costs, and policies & contracts. Besides these main aspects, the authors underline that the librarians must understand the IT processes.  In addition, the authors mention clearly the problems related to the legal aspects, the risks, and the importance for librarians to support the new web services. 

In a second part, the paper presents seven main recommendations to help librarians in selecting / developing a set of cloud services (the service scope, managing the time, the costs, the quality, the human resource, the communication and the risks).   

The style and writing of the paper is clear and easy to follow.  The organisation of the prevention is good and examples are used to help the reader to understand the underlying problems / issues / possible solutions. 

This paper should generate questions and discussions during the conference.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-01-24,no
13,5,411,Zhxxx Xxx,2,"(OVERALL EVALUATION) The paper summarizes challenges reported in papers related to library cloud adaptions, then offers some general guidelines to address these challenges. While the topic is relevant and of interest to JCDL audiences, the paper does not go deep enough to reveal much new information not already well known. The methods section lacks details, making it difficult to evaluate the validity of the results. The recommendations section makes all identified challenges a project management issue, which lacks support from the literature and experience. A few other issues:

1. Figure 1 is too small. The clustering and the linking could be a major contribution of the paper but are not sufficiently addressed in the paper.
2. It is not clear how many of the identfied challenges are library specific. It feels like that they are in general limitations of any SaaS. These issues are being addressed by the industry, e.g., via govcloud, better tooled management interfaces, and better cost estimation, etc. It’s not clear if the challenges remain unresolved or have been alleviated by these new developments.
3. Are library journals the best place to mine these challenges? It may be the case that libraries lag behind the IT industry in cloud adoptions such that library SaaS is implemented poorer than industry average and then library personnel is poorer prepared for the transition? Or the quality of these papers is lacking? For example, the UWHS example cited in the paper and its conclusion that ‘UWHS is less likely to rely on any free, cloud application for any “critical project again”’clearly ignores the fact that a significant number of universities have already outsourced their email services to free google or Microsoft cloud services.
4. The focus on SaaS, in particular ILS, does not take into considerations of many other library cloud adoptions. Many institutional repositories, including Dspace and fedora, has cloud images and are widely used.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Could still be accepted as a poster.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-15,22:40,no,,13,5,411,Zhiwu Xie,2,"(OVERALL EVALUATION) The paper summarizes challenges reported in papers related to library cloud adaptions, then offers some general guidelines to address these challenges. While the topic is relevant and of interest to JCDL audiences, the paper does not go deep enough to reveal much new information not already well known. The methods section lacks details, making it difficult to evaluate the validity of the results. The recommendations section makes all identified challenges a project management issue, which lacks support from the literature and experience. A few other issues:

1. Figure 1 is too small. The clustering and the linking could be a major contribution of the paper but are not sufficiently addressed in the paper.
2. It is not clear how many of the identfied challenges are library specific. It feels like that they are in general limitations of any SaaS. These issues are being addressed by the industry, e.g., via govcloud, better tooled management interfaces, and better cost estimation, etc. It’s not clear if the challenges remain unresolved or have been alleviated by these new developments.
3. Are library journals the best place to mine these challenges? It may be the case that libraries lag behind the IT industry in cloud adoptions such that library SaaS is implemented poorer than industry average and then library personnel is poorer prepared for the transition? Or the quality of these papers is lacking? For example, the UWHS example cited in the paper and its conclusion that ‘UWHS is less likely to rely on any free, cloud application for any “critical project again”’clearly ignores the fact that a significant number of universities have already outsourced their email services to free google or Microsoft cloud services.
4. The focus on SaaS, in particular ILS, does not take into considerations of many other library cloud adoptions. Many institutional repositories, including Dspace and fedora, has cloud images and are widely used.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Could still be accepted as a poster.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-15,no
16,6,2,Trxxx Aalxxxx,1,"(OVERALL EVALUATION) This short paper presents a mobile device application that supports the exploration of the physical collection in a library building by recommending relevant subject areas in other areas. Log file analysis is used to answer some specific research questions such as what trends can be identified for this kind of browsing.

I find this research is interesting and relevant for the conference, but although the experiment is somewhat easy to understand, the presentation is unfocused and not yet at the level that is required for a conference publication. 

In the abstract and the beginning of the paper, the paper claims to present the development and evaluation of the topic space recommendation model. I do not find that this model is presented properly anywhere in the paper, and do not see any references to other papers presenting it. It is partially described in the background section, but difficult to identify what this model actually does. In the conclusion, the name of the app that implements this module is mentioned but it is unclear if this app module is the same as the model initially indicated as the topic of the paper. 

The introduction of the paper (as well as the abstract) lacks the contextualization and motivation for this experiment. I also find it inappropriate to include authors contention and personal position, as it only weakens the contribution.  In general, the paper lacks a good description with examples on how the system works and what the actual contribution is.

A main contribution of this paper seems to be the log analysis that is used to answer 2 research questions. The first RQ is well formulated, but the second is hard to figure out and the author can improve the contribution from this research by more carefully designing a set of well-defined research questions. 

The graphs do not render well on my print, and I suspect that this also will be a problem in the final print. Even in the pdf on screen, the images do not render well because of low resolution. 

The analysis in the finding seems to be focused on what recommendations that have been made - or followed by end users. Given that the log is covering 2 years, I find the number of users and records recommended surprisingly low and indicates a mobile app that is rather infrequently used? It is somewhat unclear if they at all investigated how successful the recommendations where. Phrases like ""checkout records"" and ""strong enough for circulation"" may make sense for a librarian but is not a precise way of describing the limitations of the research. The recommendations are described as having a ""long tail power law distribution"" but the research would have been much more interesting if the authors succeeded in explaining this phenomena in terms of user experience. All in all, I find it hard to figure out what the actual findings are.

The paper has a good list of publications, although the formatting needs to be improved.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-13,18:20,no,,16,6,2,Trond Aalberg,1,"(OVERALL EVALUATION) This short paper presents a mobile device application that supports the exploration of the physical collection in a library building by recommending relevant subject areas in other areas. Log file analysis is used to answer some specific research questions such as what trends can be identified for this kind of browsing.

I find this research is interesting and relevant for the conference, but although the experiment is somewhat easy to understand, the presentation is unfocused and not yet at the level that is required for a conference publication. 

In the abstract and the beginning of the paper, the paper claims to present the development and evaluation of the topic space recommendation model. I do not find that this model is presented properly anywhere in the paper, and do not see any references to other papers presenting it. It is partially described in the background section, but difficult to identify what this model actually does. In the conclusion, the name of the app that implements this module is mentioned but it is unclear if this app module is the same as the model initially indicated as the topic of the paper. 

The introduction of the paper (as well as the abstract) lacks the contextualization and motivation for this experiment. I also find it inappropriate to include authors contention and personal position, as it only weakens the contribution.  In general, the paper lacks a good description with examples on how the system works and what the actual contribution is.

A main contribution of this paper seems to be the log analysis that is used to answer 2 research questions. The first RQ is well formulated, but the second is hard to figure out and the author can improve the contribution from this research by more carefully designing a set of well-defined research questions. 

The graphs do not render well on my print, and I suspect that this also will be a problem in the final print. Even in the pdf on screen, the images do not render well because of low resolution. 

The analysis in the finding seems to be focused on what recommendations that have been made - or followed by end users. Given that the log is covering 2 years, I find the number of users and records recommended surprisingly low and indicates a mobile app that is rather infrequently used? It is somewhat unclear if they at all investigated how successful the recommendations where. Phrases like ""checkout records"" and ""strong enough for circulation"" may make sense for a librarian but is not a precise way of describing the limitations of the research. The recommendations are described as having a ""long tail power law distribution"" but the research would have been much more interesting if the authors succeeded in explaining this phenomena in terms of user experience. All in all, I find it hard to figure out what the actual findings are.

The paper has a good list of publications, although the formatting needs to be improved.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-13,no
18,6,138,Danxxxx Gixx,2,"(OVERALL EVALUATION) In the digital era, the huge number of books gets harder and harder to manage over time. In order to solve this problem, the author of this paper presents a new application in order to develop and evaluate the topic space recommendation model. 
Also, this study presents some interesting results about how this application increases awareness and access to library services. 
The structure of this paper is clearly and  well proportioned. Also, the method is presented in detail, giving enough information about the methods used. 
However, a few comments should be taken into account by author:
- Figure 1 should be placed in the paper near where it is first discussed, rather than at the end of Introduction, if possible.
- Proofreading is necessary to correct some minor errors (pg. 1 “to scholarly inquiry”, “has been renewed interest on” – the use of the article is recommended; “a collections”).
The paper describe how to develop and evaluate the topic space recommendation model as an alternative to the personalization algorithms. Anyway, in order to develop this model by using a mobile application, this research has a good practical implication.
With suggested minor improvements along these lines, this paper can make a good case for digital library development.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-14,17:50,no,,18,6,138,Daniela Gifu,2,"(OVERALL EVALUATION) In the digital era, the huge number of books gets harder and harder to manage over time. In order to solve this problem, the author of this paper presents a new application in order to develop and evaluate the topic space recommendation model. 
Also, this study presents some interesting results about how this application increases awareness and access to library services. 
The structure of this paper is clearly and  well proportioned. Also, the method is presented in detail, giving enough information about the methods used. 
However, a few comments should be taken into account by author:
- Figure 1 should be placed in the paper near where it is first discussed, rather than at the end of Introduction, if possible.
- Proofreading is necessary to correct some minor errors (pg. 1 “to scholarly inquiry”, “has been renewed interest on” – the use of the article is recommended; “a collections”).
The paper describe how to develop and evaluate the topic space recommendation model as an alternative to the personalization algorithms. Anyway, in order to develop this model by using a mobile application, this research has a good practical implication.
With suggested minor improvements along these lines, this paper can make a good case for digital library development.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-14,no
21,7,49,Juxxxx Fx,1,"(OVERALL EVALUATION) This paper presents a method of generating a representative document with a canonical timeline of events that are cataloged on Twitter by users sharing URIs of articles. In other words, the authors are using sharing activity on social media to generate a timeline for events being discussed at high volumes.

The authors use hashtags, n-grams, and their associated shared URIs to generate topical indexes, and use a retweet and sharing count to identify popular topics under the assumption that popular means the event should be cataloged. 

The approaches to identifying topical events is very intriguing. However, the fine-grained intent of the authors is unclear. one sentence in particular in the introduction (""While one could argue that editorial content and wikipedia pages contain the best information, many other perspectives are left behind due to attention, bias, and human scalability."") suggests that this approach is designed to incorporate multiple biases or viewpoints. However, the approach used by the authors has the potential to introduce bias. One can imagine that tweets using hashtags regarding #benghazi or #clintonemails would center around content vastly different than #trumptaxreturns or #mexicowall. Rather, timelines regarding #rogueone would be much more comprehensive. In other words, I didn't see any evaluations regarding the completeness of the timelines using this selection method to discredit any bias introduced into the resulting documents. I would have preferred to see examples of bias being removed or countered to ease my discomfort with the potential for this phenomenon to occur.

My concerns about bias regardless of popularity of content is amplified in the evaluation section. The authors aim to generate a timeline of events, but only use one wikipedia article (out of 9!) that has a comparable timeline. When comparing against the wikipedia articles, the inferred timelines have a very broad set of precision and recall measures (e.g., 0.07 vs 0.80) based on the topics. It seems difficult to draw conclusions about algorithmic effectiveness given this breadth.

Further, the authors use humans to identify defects in the constructed timelines. This seems to excuse low recall of events in the timeline. In other words, their evaluation identifies the number of irrelevant events in a timeline; if the recall of the timeline is low, the algorithm may perform better. However, f-measure is more indicative of the algorithm's success in this case. I would have preferred to see a canonical reference timeline (potentially generated by human experts) and the precision *and* recall and associated defects measured. 

Figures 6 and 7 were not fully understandable; what is ""T"" and ""W"" on the X-axis?

In summary, this paper is extremely interesting, but the evaluation and test dataset evaluation falls short.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I hate to be the ""non-committal reviewer"", but I truly am on the proverbial ""fence"" about this paper. I see a high value in the work (it's really neat that they are working on automatic story generation given the shutdown of storify), but think there are moderately concerning shortcomings with the evaluation. I look forward to my peers' input and will revise my review if others' have compelling arguments that I have not considered for either acceptance or rejection.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-08,20:28,no,,21,7,49,Justin F. Brunelle,1,"(OVERALL EVALUATION) This paper presents a method of generating a representative document with a canonical timeline of events that are cataloged on Twitter by users sharing URIs of articles. In other words, the authors are using sharing activity on social media to generate a timeline for events being discussed at high volumes.

The authors use hashtags, n-grams, and their associated shared URIs to generate topical indexes, and use a retweet and sharing count to identify popular topics under the assumption that popular means the event should be cataloged. 

The approaches to identifying topical events is very intriguing. However, the fine-grained intent of the authors is unclear. one sentence in particular in the introduction (""While one could argue that editorial content and wikipedia pages contain the best information, many other perspectives are left behind due to attention, bias, and human scalability."") suggests that this approach is designed to incorporate multiple biases or viewpoints. However, the approach used by the authors has the potential to introduce bias. One can imagine that tweets using hashtags regarding #benghazi or #clintonemails would center around content vastly different than #trumptaxreturns or #mexicowall. Rather, timelines regarding #rogueone would be much more comprehensive. In other words, I didn't see any evaluations regarding the completeness of the timelines using this selection method to discredit any bias introduced into the resulting documents. I would have preferred to see examples of bias being removed or countered to ease my discomfort with the potential for this phenomenon to occur.

My concerns about bias regardless of popularity of content is amplified in the evaluation section. The authors aim to generate a timeline of events, but only use one wikipedia article (out of 9!) that has a comparable timeline. When comparing against the wikipedia articles, the inferred timelines have a very broad set of precision and recall measures (e.g., 0.07 vs 0.80) based on the topics. It seems difficult to draw conclusions about algorithmic effectiveness given this breadth.

Further, the authors use humans to identify defects in the constructed timelines. This seems to excuse low recall of events in the timeline. In other words, their evaluation identifies the number of irrelevant events in a timeline; if the recall of the timeline is low, the algorithm may perform better. However, f-measure is more indicative of the algorithm's success in this case. I would have preferred to see a canonical reference timeline (potentially generated by human experts) and the precision *and* recall and associated defects measured. 

Figures 6 and 7 were not fully understandable; what is ""T"" and ""W"" on the X-axis?

In summary, this paper is extremely interesting, but the evaluation and test dataset evaluation falls short.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I hate to be the ""non-committal reviewer"", but I truly am on the proverbial ""fence"" about this paper. I see a high value in the work (it's really neat that they are working on automatic story generation given the shutdown of storify), but think there are moderately concerning shortcomings with the evaluation. I look forward to my peers' input and will revise my review if others' have compelling arguments that I have not considered for either acceptance or rejection.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-08,no
24,7,25,Yaxxxx Alnxxxxxx,3,"(OVERALL EVALUATION) The paper presents a framework that automatically generates a timeline for an event or an evolution of a story from the online stream of social media. The product of this research is similar to a wiki page in a couple of aspects: table of content, story evolution or timeline, references, related work. As the authors mentioned, a Wikipedia page usually is more than this; it usually has a history of the events, causes, and consequences.  The authors performed three evaluation methods on the page they generate an offline evaluation, a Wikipedia evaluation, and a diversity evaluation.   


The paper is well written and the methodology was clear. However, some parts of the evaluations and the results need more clarification. For example, section 6.1 that explains the offline evaluation doesn’t show enough details about table 3, especially the length. I couldn’t get exactly how the evaluation was done, who are the workers who evaluated D2 and what are their background, etc.?","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,19:51,no,,24,7,25,Yasmin Alnoamany,3,"(OVERALL EVALUATION) The paper presents a framework that automatically generates a timeline for an event or an evolution of a story from the online stream of social media. The product of this research is similar to a wiki page in a couple of aspects: table of content, story evolution or timeline, references, related work. As the authors mentioned, a Wikipedia page usually is more than this; it usually has a history of the events, causes, and consequences.  The authors performed three evaluation methods on the page they generate an offline evaluation, a Wikipedia evaluation, and a diversity evaluation.   


The paper is well written and the methodology was clear. However, some parts of the evaluations and the results need more clarification. For example, section 6.1 that explains the offline evaluation doesn’t show enough details about table 3, especially the length. I couldn’t get exactly how the evaluation was done, who are the workers who evaluated D2 and what are their background, etc.?","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,no
26,8,327,Jacxxxx Saxxx,1,"(OVERALL EVALUATION) The objective of this paper is to present a multi-disciplinary methodological framework.
The current version of the paper is a description of a experimental design of a digital libraries (cultural heritage) for a Inuit community (North of Canada).  The author must clearly justify the choice and explain some of the used term (e.g., multi method) and how this can be integrated into a framework.  I'm expecting reading the advantages and limitations of the proposed new methodical considerations.  But many interesting questions can be found from this experiment (what are the specific cultural heritage objects that must be preserved? in which form and why?, etc.).

The writing is clear and organisation of the paper is good.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very limited interest, and the title does not correspond to the content","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-01-24,16:47,no,,26,8,327,Jacques Savoy,1,"(OVERALL EVALUATION) The objective of this paper is to present a multi-disciplinary methodological framework.
The current version of the paper is a description of a experimental design of a digital libraries (cultural heritage) for a Inuit community (North of Canada).  The author must clearly justify the choice and explain some of the used term (e.g., multi method) and how this can be integrated into a framework.  I'm expecting reading the advantages and limitations of the proposed new methodical considerations.  But many interesting questions can be found from this experiment (what are the specific cultural heritage objects that must be preserved? in which form and why?, etc.).

The writing is clear and organisation of the paper is good.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very limited interest, and the title does not correspond to the content","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-01-24,no
28,8,400,Mixx Wrxxxx,2,"(OVERALL EVALUATION) The paper outlines the methodological framework used to develop a community digital library (Digital Library North - DLN)  for an indigenous community in the north of Canada. The introduction, context and related work sections provide decent background. The core is section 4 which outlines the framework key elements; environmental scan, formative usability study, surveys , community leader engagement, information audit, community workshops and photography, What has already been reported in other references are the development of the model for the environmental scan and results of a community survey.

In the introduction and in the beginning of the conclusion, the author (project team) asserts that to develop a community DL for indigenous communities, you need a diverse methodological framework, and that is what the paper lays as the framework for the DLN.

The conclusions, however,  jump the reader from a review of the framework (with, what I'd expect as limited evaluation of its parts), to asserting three key lessons for developing a cultural digital heritage library without presenting or referring to specific results analysis - e.g. the final conclusion is the “DLN framework allows for a deeper and a more accurate perspective of how to develop community DLs … for indigenous and aboriginal communities”. It seems a leap from the framework discussion and note of some initial results to these assertions for what is a work in progress - I can understand these key issues, but it seems to get to assertion based on evaluation would be future work (or work that’s been done, but not reported yet).

That said, I’m intrigued to hear more of the project, and I think this project is definitely within the audience interest of JCDL. As this short paper only outlines the framework of community engagement, I hope we could look forward to reports on the environmental scan and how well it functions for a DL, more analysis on the usability data (and the comment of “less formal ways to collect usability data” would be interesting to develop), and what was learned from the leader engagement as a long paper. The information audit appears to have identified quite a large corpus of material, and it would seem an archival digitization task is in the offing.

Section 4.3 - end of section states survey data were analyzed and presented in another paper, but that paper is not in the references.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,00:19,no,,28,8,400,Mike Wright,2,"(OVERALL EVALUATION) The paper outlines the methodological framework used to develop a community digital library (Digital Library North - DLN)  for an indigenous community in the north of Canada. The introduction, context and related work sections provide decent background. The core is section 4 which outlines the framework key elements; environmental scan, formative usability study, surveys , community leader engagement, information audit, community workshops and photography, What has already been reported in other references are the development of the model for the environmental scan and results of a community survey.

In the introduction and in the beginning of the conclusion, the author (project team) asserts that to develop a community DL for indigenous communities, you need a diverse methodological framework, and that is what the paper lays as the framework for the DLN.

The conclusions, however,  jump the reader from a review of the framework (with, what I'd expect as limited evaluation of its parts), to asserting three key lessons for developing a cultural digital heritage library without presenting or referring to specific results analysis - e.g. the final conclusion is the “DLN framework allows for a deeper and a more accurate perspective of how to develop community DLs … for indigenous and aboriginal communities”. It seems a leap from the framework discussion and note of some initial results to these assertions for what is a work in progress - I can understand these key issues, but it seems to get to assertion based on evaluation would be future work (or work that’s been done, but not reported yet).

That said, I’m intrigued to hear more of the project, and I think this project is definitely within the audience interest of JCDL. As this short paper only outlines the framework of community engagement, I hope we could look forward to reports on the environmental scan and how well it functions for a DL, more analysis on the usability data (and the comment of “less formal ways to collect usability data” would be interesting to develop), and what was learned from the leader engagement as a long paper. The information audit appears to have identified quite a large corpus of material, and it would seem an archival digitization task is in the offing.

Section 4.3 - end of section states survey data were analyzed and presented in another paper, but that paper is not in the references.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,no
35,10,326,Jacxxxx Saxxx,1,"(OVERALL EVALUATION) This paper focus on the problem of identifying temporal street names (a street name with a date reference) in various languages.  The paper is well structured and clearly written.  To automatically determine whether a street name is a temporal street name, the description of the proposed system is well presented.  Various analysis are provided (are some dates more frequent than others? Are some month more frequently used than other, etc.).  The precision achieved by the system is rather high (97%) and 62% when providing an explanation. 

The main drawback of this paper is the topic.  very specific and not directly with DL.   


Minor comments.
Beginning of Section 2.1  the duration and set type is not explained
last paragrph in Section 2.5.  We could have a street and avenue with the same date, but both are distinct street (the same in German with Strasse, Weg, Gasse).
Beginning of Section 5.1.  It seems that the street names appearing in the test set were already used when generating the system.  need to confirm/infirm this point.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very specific topic, and if you have room, why not.  The paper is clearly written with an evaluation","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-01-24,16:52,no,,35,10,326,Jacques Savoy,1,"(OVERALL EVALUATION) This paper focus on the problem of identifying temporal street names (a street name with a date reference) in various languages.  The paper is well structured and clearly written.  To automatically determine whether a street name is a temporal street name, the description of the proposed system is well presented.  Various analysis are provided (are some dates more frequent than others? Are some month more frequently used than other, etc.).  The precision achieved by the system is rather high (97%) and 62% when providing an explanation. 

The main drawback of this paper is the topic.  very specific and not directly with DL.   


Minor comments.
Beginning of Section 2.1  the duration and set type is not explained
last paragrph in Section 2.5.  We could have a street and avenue with the same date, but both are distinct street (the same in German with Strasse, Weg, Gasse).
Beginning of Section 5.1.  It seems that the street names appearing in the test set were already used when generating the system.  need to confirm/infirm this point.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very specific topic, and if you have room, why not.  The paper is clearly written with an evaluation","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-01-24,no
40,12,330,Chrxxxxx Seixxxx,2,"(OVERALL EVALUATION) The paper investigates the influence of corpus fragmentation on the quality of text embeddings using standard evaluation sets. 
This topic is highly interesting, and obtained results would help researchers in choosing good parameters for corpus construction and model training. Word embeddings for encoding text semantics is of interest to the community. 
The chosen methodology is generally sound, the paper nicely written and the steps are understandable. However, I do have some question about details/choices and are not convinced about some conclusions the authors made. Especially, the conclusion of ""3-grams are best"" and the deviation from the notion of window size of Mikolovs paper might cause confusion and hinder correct uptake of results from the community. 
Publishing source code and data sets is definitely a plus and increases reproducibility of the study.
 
Detailed comments can be found below.


Background
==========
- Formula (1): I think this is a simplification. I would add a parameter \theta for model-specific hyperparameters (e.g. negative sampling in word2ve). Among the hyperparameters are then d, epoch_nr, win). C and dict_size are parameters for the training data and not the model itself.
Also the fragmentation (k in k-gram) would be such a parameter.
- 2.1.2. I was wondering why the authors chose CBOW. since already in Mikolov's original paper [citation 21 in the paper] skip-gram outperforms cbow.
- Footnote 2: For me, it is not clear how you treat sentences. For sentence ""A b c d. E"" and 3-grams would you generate abc, bcd or anything else? An Example would have helped.
- Section 2.3. 
 - It seems that you use a different notion of window size and context than word2vec. A window size of 2 would generate 4 context words in word2vec/cbow. For a 5-gram a b c d e, the training example generated would be a b d e -> c (only the middle word is predicted). In you example you 1) consider a different notion of window size and 2) also create training examples a b c d -> f.
First, I would like to know the reasoning for these choices and second it has to be made clear in the paper that this differs from word2vec notion of window-size, otherwise readers might misinterpret the results. Your window size of 2 is Mikolov's window size of 1.

Experiment Setup
=================
- Figure 1 interpretation: It seems from figure 1 that - other than described in the text - the model is best at 7 epochs, and after getting worse, accuracy seems to increase again after 10 epochs. So, I am not convinced of your choice of 5 epochs.
- I wondered for the analogy tasks how you counted test cases for which a, b, and/or c were not part of the vocabulary. Did this happen?
- It would be helpful for result interpretation to report the values achieved in other studies on the different corpora. Just to see, whether your corpus+model+fragmentation-methods are way off or very close to what has been achieved elsewhere.


Experiment Questions
====================
- experiment question 1: 
 - ""..size of any n-gram corpus increases exponentially with large n"". I am not convinced of that. The number of tokens is nearly the same. Of we move a sliding window over a text of size s, we get s-n+1 tokens (if we ignore sentence boundaries). The corpus size is then the number of distinct tokens. As I agree, that the number of bigrams is larger than the number of unigrams, I am not convinced that this relation also holds for 10-grams and 11-grams, for instance. Seeing, that we stop to generate the n-gram at sentence boundaries. (**)
 - Justification of the question: I think this question needs to be limited to corpora of specific sizes. For a small corpus, but large n, we will have a lot of n-grams with a very low match count. Thus, the question (and your answers) need to be constrained to ""sufficiently large corpora"" (as you also argue in section 3.1.)
- experiment question 2:
 - the same argument for the size of the corpus as above holds here

Experiment Results
==================
- Section 5.2.1. n-gram size, referring to table 5:
 - I would disagree with the interpretation that 3-gram is the best. The total loss for 2-grams compared to 3-grams is 50% (17% to 6.7%). From 3-grams to 5-grams we again half the loss (6.7% to 2.3%). For the analogy tasks, results will even be better than full-text with 5-grams.  Only the difference from 5-grams to 8-grams is nearly neglect-able on average. Thus I would go for 5-grams. However, if a second parameter (e.g. the corpus size and thereby the training efficiency) needs to be considered, this choice might be different (see also my other comment marked with **)
  - I would use a similar argument w.r.t results in table 6.
- Section 5.3. 
  - It would be helpful to get a more detailed knowledge why min-count is so important. Which examples (or how many) could not be solved in the analogy task because a word from the test set was not part of the training set due to this threshold? Could you give examples?
  - Further, it would be interesting to see the different final corpus sizes with a specific miscount parameter (how many n-grams will be excluded?)
- Section 5.3.3.
  - what is an existing match count compared to the actual value for the match count? Do you mean theoretical value an actual value? Like x-axis are all natural numbers between 1 and max-match-count and y-axis is the observed frequency? Please clarify, to help interpret figure 2



Language/Format/Structure
=========================
- It took me some time to relate the results in Table 1-4 to the benchmark set described in section 3.3. It would be helpful to have the names you used for the columns of the table in section 3.3 (some are there but not all), and also have an additional heading in the tables saying which are similarity and which are analogy test sets. Further, it would also have been helpful to read the evaluation metric (Spearman vs. accuracy) in the table (either in the caption or in the column heading. 
- The same (heading, aggregation) applies for tables 7 and 8.
- I suggest to aggregate Tables 1-4 were aggregated, having an additional column/indicator for the respective window size. This would allow to better compare the results w.r.t. window sizes.
- above table 5: ""as explained Example 3"" -> as explained in Example
- Section 5.2.: You already introduced the parameter \emp{win} as window size. Please use the same parameter in section 5.2. (instead of $j$).
- Table 5: I was confused that the bold values (largest) ones actually encode the second-to-worst values. This does not make sense to me (apart that in the interpretation that this is the method you would chose, but I would disagree). Please reconsider the coding. 
- Table 10: please include the distance measure (cosine) in the table header
- Figure 2 should be placed on the page before, at least before table 10
- Figures 1 and 2 have some artefacts in print-out. Could you include a vector graphics version instead?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-09,13:27,no,,40,12,330,Christin Seifert,2,"(OVERALL EVALUATION) The paper investigates the influence of corpus fragmentation on the quality of text embeddings using standard evaluation sets. 
This topic is highly interesting, and obtained results would help researchers in choosing good parameters for corpus construction and model training. Word embeddings for encoding text semantics is of interest to the community. 
The chosen methodology is generally sound, the paper nicely written and the steps are understandable. However, I do have some question about details/choices and are not convinced about some conclusions the authors made. Especially, the conclusion of ""3-grams are best"" and the deviation from the notion of window size of Mikolovs paper might cause confusion and hinder correct uptake of results from the community. 
Publishing source code and data sets is definitely a plus and increases reproducibility of the study.
 
Detailed comments can be found below.


Background
==========
- Formula (1): I think this is a simplification. I would add a parameter \theta for model-specific hyperparameters (e.g. negative sampling in word2ve). Among the hyperparameters are then d, epoch_nr, win). C and dict_size are parameters for the training data and not the model itself.
Also the fragmentation (k in k-gram) would be such a parameter.
- 2.1.2. I was wondering why the authors chose CBOW. since already in Mikolov's original paper [citation 21 in the paper] skip-gram outperforms cbow.
- Footnote 2: For me, it is not clear how you treat sentences. For sentence ""A b c d. E"" and 3-grams would you generate abc, bcd or anything else? An Example would have helped.
- Section 2.3. 
 - It seems that you use a different notion of window size and context than word2vec. A window size of 2 would generate 4 context words in word2vec/cbow. For a 5-gram a b c d e, the training example generated would be a b d e -> c (only the middle word is predicted). In you example you 1) consider a different notion of window size and 2) also create training examples a b c d -> f.
First, I would like to know the reasoning for these choices and second it has to be made clear in the paper that this differs from word2vec notion of window-size, otherwise readers might misinterpret the results. Your window size of 2 is Mikolov's window size of 1.

Experiment Setup
=================
- Figure 1 interpretation: It seems from figure 1 that - other than described in the text - the model is best at 7 epochs, and after getting worse, accuracy seems to increase again after 10 epochs. So, I am not convinced of your choice of 5 epochs.
- I wondered for the analogy tasks how you counted test cases for which a, b, and/or c were not part of the vocabulary. Did this happen?
- It would be helpful for result interpretation to report the values achieved in other studies on the different corpora. Just to see, whether your corpus+model+fragmentation-methods are way off or very close to what has been achieved elsewhere.


Experiment Questions
====================
- experiment question 1: 
 - ""..size of any n-gram corpus increases exponentially with large n"". I am not convinced of that. The number of tokens is nearly the same. Of we move a sliding window over a text of size s, we get s-n+1 tokens (if we ignore sentence boundaries). The corpus size is then the number of distinct tokens. As I agree, that the number of bigrams is larger than the number of unigrams, I am not convinced that this relation also holds for 10-grams and 11-grams, for instance. Seeing, that we stop to generate the n-gram at sentence boundaries. (**)
 - Justification of the question: I think this question needs to be limited to corpora of specific sizes. For a small corpus, but large n, we will have a lot of n-grams with a very low match count. Thus, the question (and your answers) need to be constrained to ""sufficiently large corpora"" (as you also argue in section 3.1.)
- experiment question 2:
 - the same argument for the size of the corpus as above holds here

Experiment Results
==================
- Section 5.2.1. n-gram size, referring to table 5:
 - I would disagree with the interpretation that 3-gram is the best. The total loss for 2-grams compared to 3-grams is 50% (17% to 6.7%). From 3-grams to 5-grams we again half the loss (6.7% to 2.3%). For the analogy tasks, results will even be better than full-text with 5-grams.  Only the difference from 5-grams to 8-grams is nearly neglect-able on average. Thus I would go for 5-grams. However, if a second parameter (e.g. the corpus size and thereby the training efficiency) needs to be considered, this choice might be different (see also my other comment marked with **)
  - I would use a similar argument w.r.t results in table 6.
- Section 5.3. 
  - It would be helpful to get a more detailed knowledge why min-count is so important. Which examples (or how many) could not be solved in the analogy task because a word from the test set was not part of the training set due to this threshold? Could you give examples?
  - Further, it would be interesting to see the different final corpus sizes with a specific miscount parameter (how many n-grams will be excluded?)
- Section 5.3.3.
  - what is an existing match count compared to the actual value for the match count? Do you mean theoretical value an actual value? Like x-axis are all natural numbers between 1 and max-match-count and y-axis is the observed frequency? Please clarify, to help interpret figure 2



Language/Format/Structure
=========================
- It took me some time to relate the results in Table 1-4 to the benchmark set described in section 3.3. It would be helpful to have the names you used for the columns of the table in section 3.3 (some are there but not all), and also have an additional heading in the tables saying which are similarity and which are analogy test sets. Further, it would also have been helpful to read the evaluation metric (Spearman vs. accuracy) in the table (either in the caption or in the column heading. 
- The same (heading, aggregation) applies for tables 7 and 8.
- I suggest to aggregate Tables 1-4 were aggregated, having an additional column/indicator for the respective window size. This would allow to better compare the results w.r.t. window sizes.
- above table 5: ""as explained Example 3"" -> as explained in Example
- Section 5.2.: You already introduced the parameter \emp{win} as window size. Please use the same parameter in section 5.2. (instead of $j$).
- Table 5: I was confused that the bold values (largest) ones actually encode the second-to-worst values. This does not make sense to me (apart that in the interpretation that this is the method you would chose, but I would disagree). Please reconsider the coding. 
- Table 10: please include the distance measure (cosine) in the table header
- Figure 2 should be placed on the page before, at least before table 10
- Figures 1 and 2 have some artefacts in print-out. Could you include a vector graphics version instead?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-09,no
44,13,149,Swxxxx Gotxxxxxx,2,"(OVERALL EVALUATION) 1.	An interesting task as current NERs are not such elaborative in terms of languages and html content as targeted in this paper.
2.	Can also talk about how this can be useful in the education environment applications.
3.	More details about labelling and statistics of the languages and faculty counts will be useful.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-16,00:07,no,,44,13,149,Swapna Gottipati,2,"(OVERALL EVALUATION) 1.	An interesting task as current NERs are not such elaborative in terms of languages and html content as targeted in this paper.
2.	Can also talk about how this can be useful in the education environment applications.
3.	More details about labelling and statistics of the languages and faculty counts will be useful.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-16,no
45,13,149,Swxxxx Gotxxxxxx,2,"(OVERALL EVALUATION) 1. An interesting task as current NERs are not such elaborative in terms of languages and html content as targeted in this paper. The paper focus on extracting names from faculty websites. The traditional NER taggers are more focused on the grammatical structure and are not suitable for faculty websites. 
2. The paper uses a methodology of the textual content and structural content to achieve the precision on the task. 	
3. This is a useful problem and can be applied in many education application. The authors should also talk about how this can be useful in the education environment applications. 
4. The current models fail in such tasks. So authors should provide a comparison experiments of using standard NER techniques on this tasks. 
5. More details about labeling approach, and statistics of the languages and faculty counts will be useful. It is unclear from the paper the details of the dataset for training and testing. Also, it would be good to provide some details of how the model performed on various languages. On which languages it performed better vs which languages it failed? What is the analysis?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have seen that standard NERs models have many limitations and had to be tweeked for problems like this. The authors used text and structural features for better accuracy and on multiple languages. I would say that more details on the experiments should be sufficient to accept this paper.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-20,00:52,no,,45,13,149,Swapna Gottipati,2,"(OVERALL EVALUATION) 1. An interesting task as current NERs are not such elaborative in terms of languages and html content as targeted in this paper. The paper focus on extracting names from faculty websites. The traditional NER taggers are more focused on the grammatical structure and are not suitable for faculty websites. 
2. The paper uses a methodology of the textual content and structural content to achieve the precision on the task. 	
3. This is a useful problem and can be applied in many education application. The authors should also talk about how this can be useful in the education environment applications. 
4. The current models fail in such tasks. So authors should provide a comparison experiments of using standard NER techniques on this tasks. 
5. More details about labeling approach, and statistics of the languages and faculty counts will be useful. It is unclear from the paper the details of the dataset for training and testing. Also, it would be good to provide some details of how the model performed on various languages. On which languages it performed better vs which languages it failed? What is the analysis?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have seen that standard NERs models have many limitations and had to be tweeked for problems like this. The authors used text and structural features for better accuracy and on multiple languages. I would say that more details on the experiments should be sufficient to accept this paper.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-20,no
47,13,324,Haxx Salaxxxxxxx,3,"(OVERALL EVALUATION) In this work, the authors present one of the problems of web data extraction, namely entity name extraction of authors in faculty directories with the purpose of enriching public bibliographic databases. They propose a statistical approach that combines textual and structural features of html web pages which produced high precision and recall upon testing it against a dataset they created of +11000 researchers.

The flow of the paper is good, with high clarity and organization. The authors covered the prior contributions extensively and with breadth. They also spread a good amount of the paper explaining the problem, and their approach and methodology to solve it. I was most impressed with the dataset that they collected and manually labelled. I hope the authors will publish it as well as this work so that the scientific community could benefit from it. Finally the experiments were sufficient and well documented. All in all this is a good balanced paper in my opinion. My only take is that it would have been much more impactful if they ran their experiments on the same testing dataset but using a couple of the other prior approaches from their related works section. Their 0.95 F-score is good but would have been more impactful if it was placed against the other methods results.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: yes",2,,,,,2018-02-17,11:37,no,,47,13,324,Hany Salaheldeen,3,"(OVERALL EVALUATION) In this work, the authors present one of the problems of web data extraction, namely entity name extraction of authors in faculty directories with the purpose of enriching public bibliographic databases. They propose a statistical approach that combines textual and structural features of html web pages which produced high precision and recall upon testing it against a dataset they created of +11000 researchers.

The flow of the paper is good, with high clarity and organization. The authors covered the prior contributions extensively and with breadth. They also spread a good amount of the paper explaining the problem, and their approach and methodology to solve it. I was most impressed with the dataset that they collected and manually labelled. I hope the authors will publish it as well as this work so that the scientific community could benefit from it. Finally the experiments were sufficient and well documented. All in all this is a good balanced paper in my opinion. My only take is that it would have been much more impactful if they ran their experiments on the same testing dataset but using a couple of the other prior approaches from their related works section. Their 0.95 F-score is good but would have been more impactful if it was placed against the other methods results.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: yes",2,,,,,2018-02-17,no
51,14,339,S and ipxx Sixxxx,3,"(OVERALL EVALUATION) This paper proposes a community-driven data curation system to enhance data understandability and reusability. The system focuses on linking relevant literature to the data as use contextual information to help users understand the data. I think this could be a very useful system to the research community. Although there are only a handful of data repositories available, metadata are missing in many which makes them unfit for reuse. This system could be an important step towards eliminating this.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,05:47,no,,51,14,339,Sandipan Sikdar,3,"(OVERALL EVALUATION) This paper proposes a community-driven data curation system to enhance data understandability and reusability. The system focuses on linking relevant literature to the data as use contextual information to help users understand the data. I think this could be a very useful system to the research community. Although there are only a handful of data repositories available, metadata are missing in many which makes them unfit for reuse. This system could be an important step towards eliminating this.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,no
59,16,386,Anxxx Velxxxx,4,"(OVERALL EVALUATION) This paper should be of considerable interest to the JCDL community and I was compelled by the claims made for this work: a simple, easy to understand and effective author-name disambiguation.  

Although the overall method is pretty clear and offers a promising strategy to tackle this problem, I don't think that the paper lives up to it's claims of simplicity. The account of agglomerative clustering and its implementation are not obvious, the discussion of quality limits for clustering moves in section 3.6 sounds like the author(s) are convincing themselves that their formula for ""l"" just works and that the reader should just take it on faith that, for example ""the exact values [for alpha and beta] are not particularly important - only the order of magnitude"" and the Implementation details section is sparse.  The discussion of the experiments in section 4.4 leaves the impression that the authors have not had much time to systematically analyze the result to understand why their choices of alpha, beta and epsilon are effective.

In short - I think this approach to the problem is very promising and I trust that the author(s) are onto an effective strategy. However, as a long paper for this conference, I think this research isn't quite mature yet. For this conference, this paper might better be summarized for a short paper (without all the graphs etc.) and the final results published in journal with a proper comparison with the other methods mentioned in the literature review, in order to  substantiate the claims to simplicity and effectiveness.

Below are a few suggestions for improvement for the next version of this paper.

P. 1 Col 2. ""We note that when disambiguating a name {\it name}, we do not need to consider any other names {\it name'} (in) R""

I am not sure (a) I understand what that means or (b) why this is worthy of note.  Is it because other approaches (e.g. ones that use pair-wise comparisons of names) do not consider each name one at a time?

Since the authors of this paper seem to put some weight on their (simpler) approach.

P.2 Col 1. ""She shows that the extend of ambiguity has a direct influence on the scientic performance"" - typo with ""extend""

P.2 Col 1. ""They show among others that the top-ranked researchers are so high up the ranking due to a lack of author name disambiguation."" 

Suggest a rephrasing ""They show among other things that one reason that the top-ranked
researchers are so high up the ranking is due to the lack of author name disambiguation.""

P.2 contains four occurences of the phrase ""very good results"" or ""very good results"" to describe other researchers work but with no quantitative measures to back it up (except for one reference to and F1 measure).

P.2 Col 2. ""Depending on supervision and what they call ’rules’ renders their method rather complicated"".

What aspect of supervision makes their method complicated?  The fact that it *depends* on supervision or that their supervisory training method depends on 'rules'... I am willing to accept that the method is complicated but it isn't clear why.

P.2. Col 2. ""there is exactly one document d(x) that this mention appears on."" Suggest:
""there is exactly one document d(x) in which this mention appears.""

P.3. Col 1. ""All categories assigned to d(x)""... by whom? Elsevier's subject categories? It isn't clear from the description of Fcat(X) whether or not the authors of this article decided on the categories.  Same comment about keywords (although it seems likely that these are they keyword chosen by the articles' authors.

P.3. Col 1. ""All names given as authors of all documents d' referenced by d"".  Does this mean the same thing as ""All the names of the authors in the list of references in d"" or does it mean ""All the names of the authors in the list of references in d that are also authors in the collection.""?  In other words, are you considering author names for works referenced by d but which (the works) are not also in the collection you were studying?

P.3. Col 1. The sentence structure for Feature 8 is unclear.

Section 3.2 describing the Agglomerative clustering algorithm seems could be clarified to substantiate the claim in section 2 that ""blocking"" is being used. If, as the authors state in 3.2 ""the initial state where each mention x is in its own cluster C = {x}"" and ""Then, pairs (C,C') of clusters are merged"", surely this amounts to pairwise comparisons of {x} / {x'} in the limiting case where each cluster is a singleton of one.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I didn't want to say it point-blank to the author(s) but it seems to me that there is a lot of ""hand-waving"" in this paper.  It also reads like ""notes to myself about why I did what I did"" rather than a research paper.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-17,16:27,no,,59,16,386,Andre Vellino,4,"(OVERALL EVALUATION) This paper should be of considerable interest to the JCDL community and I was compelled by the claims made for this work: a simple, easy to understand and effective author-name disambiguation.  

Although the overall method is pretty clear and offers a promising strategy to tackle this problem, I don't think that the paper lives up to it's claims of simplicity. The account of agglomerative clustering and its implementation are not obvious, the discussion of quality limits for clustering moves in section 3.6 sounds like the author(s) are convincing themselves that their formula for ""l"" just works and that the reader should just take it on faith that, for example ""the exact values [for alpha and beta] are not particularly important - only the order of magnitude"" and the Implementation details section is sparse.  The discussion of the experiments in section 4.4 leaves the impression that the authors have not had much time to systematically analyze the result to understand why their choices of alpha, beta and epsilon are effective.

In short - I think this approach to the problem is very promising and I trust that the author(s) are onto an effective strategy. However, as a long paper for this conference, I think this research isn't quite mature yet. For this conference, this paper might better be summarized for a short paper (without all the graphs etc.) and the final results published in journal with a proper comparison with the other methods mentioned in the literature review, in order to  substantiate the claims to simplicity and effectiveness.

Below are a few suggestions for improvement for the next version of this paper.

P. 1 Col 2. ""We note that when disambiguating a name {\it name}, we do not need to consider any other names {\it name'} (in) R""

I am not sure (a) I understand what that means or (b) why this is worthy of note.  Is it because other approaches (e.g. ones that use pair-wise comparisons of names) do not consider each name one at a time?

Since the authors of this paper seem to put some weight on their (simpler) approach.

P.2 Col 1. ""She shows that the extend of ambiguity has a direct influence on the scientic performance"" - typo with ""extend""

P.2 Col 1. ""They show among others that the top-ranked researchers are so high up the ranking due to a lack of author name disambiguation."" 

Suggest a rephrasing ""They show among other things that one reason that the top-ranked
researchers are so high up the ranking is due to the lack of author name disambiguation.""

P.2 contains four occurences of the phrase ""very good results"" or ""very good results"" to describe other researchers work but with no quantitative measures to back it up (except for one reference to and F1 measure).

P.2 Col 2. ""Depending on supervision and what they call ’rules’ renders their method rather complicated"".

What aspect of supervision makes their method complicated?  The fact that it *depends* on supervision or that their supervisory training method depends on 'rules'... I am willing to accept that the method is complicated but it isn't clear why.

P.2. Col 2. ""there is exactly one document d(x) that this mention appears on."" Suggest:
""there is exactly one document d(x) in which this mention appears.""

P.3. Col 1. ""All categories assigned to d(x)""... by whom? Elsevier's subject categories? It isn't clear from the description of Fcat(X) whether or not the authors of this article decided on the categories.  Same comment about keywords (although it seems likely that these are they keyword chosen by the articles' authors.

P.3. Col 1. ""All names given as authors of all documents d' referenced by d"".  Does this mean the same thing as ""All the names of the authors in the list of references in d"" or does it mean ""All the names of the authors in the list of references in d that are also authors in the collection.""?  In other words, are you considering author names for works referenced by d but which (the works) are not also in the collection you were studying?

P.3. Col 1. The sentence structure for Feature 8 is unclear.

Section 3.2 describing the Agglomerative clustering algorithm seems could be clarified to substantiate the claim in section 2 that ""blocking"" is being used. If, as the authors state in 3.2 ""the initial state where each mention x is in its own cluster C = {x}"" and ""Then, pairs (C,C') of clusters are merged"", surely this amounts to pairwise comparisons of {x} / {x'} in the limiting case where each cluster is a singleton of one.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I didn't want to say it point-blank to the author(s) but it seems to me that there is a lot of ""hand-waving"" in this paper.  It also reads like ""notes to myself about why I did what I did"" rather than a research paper.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-17,no
71,21,20,Suxxx Alxxxx,1,"(OVERALL EVALUATION) relevance to JCDL: This study explores an important topic -- what metric companies can use as a guide for strategic decisions about research investments. Linking research and outcomes is important for all sectors and makes this relevant to JCDL. In addition the  research approach is likely of interest to the JCDL community.

 novelty/originality: The approach used by the researchers -- linking a comprehensive review of patents with revenue generation as reported in the Fortune 500 rankings is sound and extends work undertaken in the past. It does offer a novel approach adding the concept of temporal buckets.

 methodology: The methodology is well thought out and has checks in place to achieve rigor. The study works with a substantial dataset (2.6 million full txt articles and 93 million patent citations) and identifies rigorous processes including a full suite of preprocessing steps.

 style/quality of writing: This paper is very well written both in terms of its organization and style.  The argument is clearly stated following a logical progression that provides the reader with scaffolding for understanding the impetus for the work, the design of the research, the experimental setup and the results.  In addition the writing is easy to read with terms being defined and concepts being described in easy-to-understand language. 

evaluation: the study results are interesting and well presented. The possible explanations offer interesting analyses of the results and insights that are useful. The addition of case studies offers further insight and suggests the authors were working to triangulate results which adds further credibility to their work.

 replicability: The description of the study  appears to be thorough and explicit which suggests it would be replicable. For example the description of bucket construction, which is a unique piece of this study, appears to be sufficiently detailed for replication. I use the word ""appears"" since my own experience is that until replication is tried it is not possible to imagine every condition that may diminish replicability.

 adequacy of references: References are acceptable for this paper which wisely uses much of its specified space on this study after providing an adequate lit review to set context","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-15,21:15,no,,71,21,20,Suzie Allard,1,"(OVERALL EVALUATION) relevance to JCDL: This study explores an important topic -- what metric companies can use as a guide for strategic decisions about research investments. Linking research and outcomes is important for all sectors and makes this relevant to JCDL. In addition the  research approach is likely of interest to the JCDL community.

 novelty/originality: The approach used by the researchers -- linking a comprehensive review of patents with revenue generation as reported in the Fortune 500 rankings is sound and extends work undertaken in the past. It does offer a novel approach adding the concept of temporal buckets.

 methodology: The methodology is well thought out and has checks in place to achieve rigor. The study works with a substantial dataset (2.6 million full txt articles and 93 million patent citations) and identifies rigorous processes including a full suite of preprocessing steps.

 style/quality of writing: This paper is very well written both in terms of its organization and style.  The argument is clearly stated following a logical progression that provides the reader with scaffolding for understanding the impetus for the work, the design of the research, the experimental setup and the results.  In addition the writing is easy to read with terms being defined and concepts being described in easy-to-understand language. 

evaluation: the study results are interesting and well presented. The possible explanations offer interesting analyses of the results and insights that are useful. The addition of case studies offers further insight and suggests the authors were working to triangulate results which adds further credibility to their work.

 replicability: The description of the study  appears to be thorough and explicit which suggests it would be replicable. For example the description of bucket construction, which is a unique piece of this study, appears to be sufficiently detailed for replication. I use the word ""appears"" since my own experience is that until replication is tried it is not possible to imagine every condition that may diminish replicability.

 adequacy of references: References are acceptable for this paper which wisely uses much of its specified space on this study after providing an adequate lit review to set context","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-15,no
75,21,99,Jx Stexxxx,4,"(OVERALL EVALUATION) This is an interesting bibliometric analysis of the relationships among large corporations and patent production and use. As such, it definitely has many merits and has and audience that would appreciated it. However, after three readings of the paper, I cannot find myself making a case that this paper is in scope for JCDL. This paper is better suited for a informetrics conference or a business conference innovation conference.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2018-02-18,02:23,no,,75,21,99,J. Stephen Downie,4,"(OVERALL EVALUATION) This is an interesting bibliometric analysis of the relationships among large corporations and patent production and use. As such, it definitely has many merits and has and audience that would appreciated it. However, after three readings of the paper, I cannot find myself making a case that this paper is in scope for JCDL. This paper is better suited for a informetrics conference or a business conference innovation conference.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2018-02-18,no
78,22,10,Maxx Agxxx,2,"(OVERALL EVALUATION) The research topic is relevant to JCDL and literature is well reviewed. 

Research hypotheses look reasonable and interesting, but the survey design doesn’t look fit very well for the research questions. The fact that all of the 30 participants are graduate students (22 master degree holder and 8 PhD candidate, and 14 participants major in social science) means that they are likely to have higher information literacy than the general public and to be critical to the rumors, thus they are unlikely to share general attitude toward online health rumors, as the authors briefly mentioned. Therefore, a direct comparison between precedent studies is difficult. Making a similar survey with younger students with presumably lower information literacy and comparing the result with that of the present study would give insight into a role of information literacy education. 

The selection of the rumors used in the survey requires a more detailed explanation, especially because the rumors database (reference 33) is not accessible. Authors used such a broad word “health” as a keyword and“randomly” selected rumors “which include the proposed rumor presentation (picture, verification or hyperlink)”, but a criteria for selection, for example, the number of comments/retweets, classification of dread rumors and wish rumors (which are common classification for online health rumors), or categories of the health topic (such as dentists, beauty care, child care, diseases of aging people, etc.) could have improved the survey plan and been useful for data analysis. 

With these limitations in mind, the present study still gives us a useful piece of knowledge about relationship between users’ trust in online health rumors and rumor presentation.

A little work for the presentation of the article would be required; tables shouldn’t divided into different pages/columns (tables 1 and 2), figures in table 3 should be right-aligned, the caption for the figure 3 should be beneath the figure rather than in the next column, and contents of sections 3.1 and 3.2 are partially redundant.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-13,05:11,no,,78,22,10,Mari Agata,2,"(OVERALL EVALUATION) The research topic is relevant to JCDL and literature is well reviewed. 

Research hypotheses look reasonable and interesting, but the survey design doesn’t look fit very well for the research questions. The fact that all of the 30 participants are graduate students (22 master degree holder and 8 PhD candidate, and 14 participants major in social science) means that they are likely to have higher information literacy than the general public and to be critical to the rumors, thus they are unlikely to share general attitude toward online health rumors, as the authors briefly mentioned. Therefore, a direct comparison between precedent studies is difficult. Making a similar survey with younger students with presumably lower information literacy and comparing the result with that of the present study would give insight into a role of information literacy education. 

The selection of the rumors used in the survey requires a more detailed explanation, especially because the rumors database (reference 33) is not accessible. Authors used such a broad word “health” as a keyword and“randomly” selected rumors “which include the proposed rumor presentation (picture, verification or hyperlink)”, but a criteria for selection, for example, the number of comments/retweets, classification of dread rumors and wish rumors (which are common classification for online health rumors), or categories of the health topic (such as dentists, beauty care, child care, diseases of aging people, etc.) could have improved the survey plan and been useful for data analysis. 

With these limitations in mind, the present study still gives us a useful piece of knowledge about relationship between users’ trust in online health rumors and rumor presentation.

A little work for the presentation of the article would be required; tables shouldn’t divided into different pages/columns (tables 1 and 2), figures in table 3 should be right-aligned, the caption for the figure 3 should be beneath the figure rather than in the next column, and contents of sections 3.1 and 3.2 are partially redundant.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-13,no
79,22,10,Maxx Agxxx,2,"(OVERALL EVALUATION) The research topic is relevant to JCDL and literature is well reviewed. 

Research hypotheses look reasonable and interesting, but the survey design doesn’t look fit very well for the research questions. The fact that all of the 30 participants are graduate students (22 master degree holder and 8 PhD candidate, and 14 participants major in social science) means that they are likely to have higher information literacy than the general public and to be critical to the rumors, thus they are unlikely to share general attitude toward online health rumors, as the authors briefly mentioned. Therefore, a direct comparison between precedent studies is difficult. Making a similar survey with younger students with presumably lower information literacy and comparing the result with that of the present study would give insight into a role of information literacy education. 

The selection of the rumors used in the survey requires a more detailed explanation, especially because the rumors database (reference 33) is not accessible. Authors used such a broad word “health” as a keyword and“randomly” selected rumors “which include the proposed rumor presentation (picture, verification or hyperlink)”, but a criteria for selection, for example, the number of comments/retweets, classification of dread rumors and wish rumors (which are common classification for online health rumors), or categories of the health topic (such as dentists, beauty care, child care, diseases of aging people, etc.) could have improved the survey plan and been useful for data analysis. 

A little work for the presentation of the article would be required; tables shouldn’t divided into different pages/columns (tables 1 and 2), figures in table 3 should be right-aligned, the caption for the figure 3 should be beneath the figure rather than in the next column, and contents of sections 3.1 and 3.2 are partially redundant.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-13,05:48,no,,79,22,10,Mari Agata,2,"(OVERALL EVALUATION) The research topic is relevant to JCDL and literature is well reviewed. 

Research hypotheses look reasonable and interesting, but the survey design doesn’t look fit very well for the research questions. The fact that all of the 30 participants are graduate students (22 master degree holder and 8 PhD candidate, and 14 participants major in social science) means that they are likely to have higher information literacy than the general public and to be critical to the rumors, thus they are unlikely to share general attitude toward online health rumors, as the authors briefly mentioned. Therefore, a direct comparison between precedent studies is difficult. Making a similar survey with younger students with presumably lower information literacy and comparing the result with that of the present study would give insight into a role of information literacy education. 

The selection of the rumors used in the survey requires a more detailed explanation, especially because the rumors database (reference 33) is not accessible. Authors used such a broad word “health” as a keyword and“randomly” selected rumors “which include the proposed rumor presentation (picture, verification or hyperlink)”, but a criteria for selection, for example, the number of comments/retweets, classification of dread rumors and wish rumors (which are common classification for online health rumors), or categories of the health topic (such as dentists, beauty care, child care, diseases of aging people, etc.) could have improved the survey plan and been useful for data analysis. 

A little work for the presentation of the article would be required; tables shouldn’t divided into different pages/columns (tables 1 and 2), figures in table 3 should be right-aligned, the caption for the figure 3 should be beneath the figure rather than in the next column, and contents of sections 3.1 and 3.2 are partially redundant.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-13,no
84,23,60,Joxx Hx,3,"(OVERALL EVALUATION) The paper describes a mapping between ontological concepts and the object-oriented model. The goal of the authors is to provide rich descriptions of real life objects, based on the Human Activities and Infrastructures Foundry of ontologies. There is a good intellectual work behind the paper, that follows previous works of the authors. HOwever, the paper includes several inconsistencies from the object-oriented perspective that need to be pointed out.

First of all, the paper is confusing and merges concepts of very different levels of abstraction without providing a clear rationale. 
The goal of the work is very ambitious since the authors try to build structured models of complex historical situations. They propose to use the conceptualization made in the HAIF, which seems to be a good option. However, at the same time, they put the focus on how its concepts map to Object-oriented concepts, making assumptions that are not exact. For instance, the authors claim that the goal of encapsulation in object-oriented programming is pairing of objects with methods, which is too simplistic and out of focus. Encapsulation is a programming technique used to ensure information hiding as a way of reducing coupling between modules, and later classes, in classical programming languages. Pairing objects woth methods would be a mean to ensure encapsulation, but not a goal on itself.

Quallities are clearly properties of objects that, surprisingly, are not mapped to attributes in object-orientes languages. Relational qualities, in turn, could be seen as associations (in UML terminology). Finally, there is another mistake when the authors associate UML with BPMN. There are completely different notations. In fact, UML has a much woder scope than BPMN, to the point that the so-called Diagram activity in UML covers the same domain as BPMN, namely process modelling.

There is extensive literature on formal and object oriented modelling languages proposed in the early 1990s that explored the view of objects as observable processes. H. D. Erich, A. Sernadas, G. Saake and others developed families of languages that covered this view). Also, Yair Wand published a famous paper where he defined a direct mappingbetween the Mario BUnge’s ontology and the OO model:

An ontological model of an information system
Y Wand, R Weber - IEEE transactions on software engineering, 1990

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is confusing, with notorius inconsistencies regarding the Object oriented model.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-14,17:24,no,,84,23,60,Jose H. Canós-Cerdá,3,"(OVERALL EVALUATION) The paper describes a mapping between ontological concepts and the object-oriented model. The goal of the authors is to provide rich descriptions of real life objects, based on the Human Activities and Infrastructures Foundry of ontologies. There is a good intellectual work behind the paper, that follows previous works of the authors. HOwever, the paper includes several inconsistencies from the object-oriented perspective that need to be pointed out.

First of all, the paper is confusing and merges concepts of very different levels of abstraction without providing a clear rationale. 
The goal of the work is very ambitious since the authors try to build structured models of complex historical situations. They propose to use the conceptualization made in the HAIF, which seems to be a good option. However, at the same time, they put the focus on how its concepts map to Object-oriented concepts, making assumptions that are not exact. For instance, the authors claim that the goal of encapsulation in object-oriented programming is pairing of objects with methods, which is too simplistic and out of focus. Encapsulation is a programming technique used to ensure information hiding as a way of reducing coupling between modules, and later classes, in classical programming languages. Pairing objects woth methods would be a mean to ensure encapsulation, but not a goal on itself.

Quallities are clearly properties of objects that, surprisingly, are not mapped to attributes in object-orientes languages. Relational qualities, in turn, could be seen as associations (in UML terminology). Finally, there is another mistake when the authors associate UML with BPMN. There are completely different notations. In fact, UML has a much woder scope than BPMN, to the point that the so-called Diagram activity in UML covers the same domain as BPMN, namely process modelling.

There is extensive literature on formal and object oriented modelling languages proposed in the early 1990s that explored the view of objects as observable processes. H. D. Erich, A. Sernadas, G. Saake and others developed families of languages that covered this view). Also, Yair Wand published a famous paper where he defined a direct mappingbetween the Mario BUnge’s ontology and the OO model:

An ontological model of an information system
Y Wand, R Weber - IEEE transactions on software engineering, 1990

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is confusing, with notorius inconsistencies regarding the Object oriented model.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-14,no
87,24,148,Sujxxxx Dxx,1,"(OVERALL EVALUATION) The proposed topic is of interest to the JCDL community and as the authors point out, the number of images being made available online and part of digital library collections is increasing.

Consequently, it is good to have a venue where people working on related topics and listed problems can gather to discuss status of research, practical aspects, and commercial solutions in these areas.

The workshop proposal should include details on whether this specific workshops or very similar ones have been held before. In particular, were the listed authors involved in similar workshop organization. It is also important to include short bios of the proposers to enable reviewers to gauge without too much effort whether the proposers have requisite background and expertise for holding such a workshop.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The workshop sounds interesting and will be of value to the JCDL community.
However, I could not figure out the proposers' background and expertise on these specific topics and if they have organized similar workshops previously.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2018-01-25,04:43,no,,87,24,148,Sujatha Das Gollapalli,1,"(OVERALL EVALUATION) The proposed topic is of interest to the JCDL community and as the authors point out, the number of images being made available online and part of digital library collections is increasing.

Consequently, it is good to have a venue where people working on related topics and listed problems can gather to discuss status of research, practical aspects, and commercial solutions in these areas.

The workshop proposal should include details on whether this specific workshops or very similar ones have been held before. In particular, were the listed authors involved in similar workshop organization. It is also important to include short bios of the proposers to enable reviewers to gauge without too much effort whether the proposers have requisite background and expertise for holding such a workshop.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The workshop sounds interesting and will be of value to the JCDL community.
However, I could not figure out the proposers' background and expertise on these specific topics and if they have organized similar workshops previously.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2018-01-25,no
96,26,397,Nicxxxxx Woxxx,3,"(OVERALL EVALUATION) The authors are reporting on a national initiative to establish a theoretical framework for developing digital scholarship services within Chinese university libraries. The initiative itself could be relevant to JCDL audiences; however, the content of this paper is mostly a preliminary literature review. There are distracting grammatical errors and the discussion is fairly short. Table 1 wastes space that could be used for more detailed discussion. There is nothing novel or original in the conclusion. There is little regarding the unique or interesting factors affecting providing digital scholarship services in China. As the authors note, further empirical work needs to be done to support their framework.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-15,19:39,no,,96,26,397,Nicholas Worby,3,"(OVERALL EVALUATION) The authors are reporting on a national initiative to establish a theoretical framework for developing digital scholarship services within Chinese university libraries. The initiative itself could be relevant to JCDL audiences; however, the content of this paper is mostly a preliminary literature review. There are distracting grammatical errors and the discussion is fairly short. Table 1 wastes space that could be used for more detailed discussion. There is nothing novel or original in the conclusion. There is little regarding the unique or interesting factors affecting providing digital scholarship services in China. As the authors note, further empirical work needs to be done to support their framework.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-15,no
102,28,413,Wexxxx Xx,2,"(OVERALL EVALUATION) This paper presented an investigation of introducing sentiment of a citation into the ranking of scientific papers. The paper first investigated methods of assessing a sentimental score for each citation sentences. Then the authors proposed two basic ranking schema and compared ranking results with/without considering sentiment score. Finally, the impact of the sentiment influence in the index is closely inspected with one paper as an example. 

The paper is generally well written with experiments towards sentiment analysis accuracy and differences of ranking results. But the overall contribution and conclusion are not clearly presented due to the list of concerns listed as follows. 

- The sentiment analysis accuracy is about 80.61% which doesn't seem very high. The author also did not compare their results with other approaches. As an active research field, many open source packages and pre-trained models are available to support sentiment analysis. The author should consider adding comparisons of their proposed method with some established one, such as Stanford coreNLP for sentiment analysis.  This can help authors better understand their accuracy results and contributions. 

-  The author trained and tuned based on a smaller curated dataset (corpus 1) and applied it to a larger corpus. As shown in table 1, the distribution of positive/negative instances is quite different between two corpora. Additional explanation and analysis may be needed. One potential reason is due to the bias introduced by oversampling of minority classes. 

- The comparison results of different ranking results may need more discussion. What is the goal of such comparisons?  It is not clear what conclusion authors trying to draw besides the four indexes are different. On the other hand, how the sentiment scores are introduced to build indices are not clearly presented. Different weighting schema used may change the ranking results greatly. 

- The analysis of example paper also seems inclusive. Introducing the sentiment score displayed different impacts in different indexing schema. The research questions 3 need further studies.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-16,06:56,no,,102,28,413,Weijia Xu,2,"(OVERALL EVALUATION) This paper presented an investigation of introducing sentiment of a citation into the ranking of scientific papers. The paper first investigated methods of assessing a sentimental score for each citation sentences. Then the authors proposed two basic ranking schema and compared ranking results with/without considering sentiment score. Finally, the impact of the sentiment influence in the index is closely inspected with one paper as an example. 

The paper is generally well written with experiments towards sentiment analysis accuracy and differences of ranking results. But the overall contribution and conclusion are not clearly presented due to the list of concerns listed as follows. 

- The sentiment analysis accuracy is about 80.61% which doesn't seem very high. The author also did not compare their results with other approaches. As an active research field, many open source packages and pre-trained models are available to support sentiment analysis. The author should consider adding comparisons of their proposed method with some established one, such as Stanford coreNLP for sentiment analysis.  This can help authors better understand their accuracy results and contributions. 

-  The author trained and tuned based on a smaller curated dataset (corpus 1) and applied it to a larger corpus. As shown in table 1, the distribution of positive/negative instances is quite different between two corpora. Additional explanation and analysis may be needed. One potential reason is due to the bias introduced by oversampling of minority classes. 

- The comparison results of different ranking results may need more discussion. What is the goal of such comparisons?  It is not clear what conclusion authors trying to draw besides the four indexes are different. On the other hand, how the sentiment scores are introduced to build indices are not clearly presented. Different weighting schema used may change the ranking results greatly. 

- The analysis of example paper also seems inclusive. Introducing the sentiment score displayed different impacts in different indexing schema. The research questions 3 need further studies.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-16,no
123,37,189,Nabxxxx Jaxxxx,3,(OVERALL EVALUATION) The paper introduces the concept of Micrawler and how can it be beneficial to Micro Archives. It gives a good overview of the architecture. The methodology is well defined. The author talks about the opportunities and ends the paper with a nice conclusion.,"Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-15,19:12,no,,123,37,189,Nabeela Jaffer,3,(OVERALL EVALUATION) The paper introduces the concept of Micrawler and how can it be beneficial to Micro Archives. It gives a good overview of the architecture. The methodology is well defined. The author talks about the opportunities and ends the paper with a nice conclusion.,"Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-15,no
125,38,271,Anixxxx Mukxxxxxx,1,"(OVERALL EVALUATION) I liked the overall idea of the paper. The experiments have also been done quite rigorously. However, the major problem of this approach is that the authors approximate all their quantities, using the nodes only in set C. For instance they assume that the fidelty of all the nodes in T can be approximated by the fidelty of the nodes in C. Similarly, they extrapolate the impact of the ghost vertices through the impact of the vertices C. These approximations could have serious repercussions and an exhaustive error analysis experiment is needed to support such choices for real networks. For instance, the error for various sub-cases need to be studied |T|~|C|, |T|>|C|, |T|>>|C| etc. If the error values diverge then that is fatal for the system. In case the paper is accepted, I would like to see a complete treatment of errors in the camera ready version of the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am giving a weak accept because the identification of the problem is good and the experimentation is exhaustive. However, the error due to their assumptions might make their system unusable. In case of acceptance, the authors should be advised to report error analysis experiments.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2018-02-10,14:05,no,,125,38,271,Animesh Mukherjee,1,"(OVERALL EVALUATION) I liked the overall idea of the paper. The experiments have also been done quite rigorously. However, the major problem of this approach is that the authors approximate all their quantities, using the nodes only in set C. For instance they assume that the fidelty of all the nodes in T can be approximated by the fidelty of the nodes in C. Similarly, they extrapolate the impact of the ghost vertices through the impact of the vertices C. These approximations could have serious repercussions and an exhaustive error analysis experiment is needed to support such choices for real networks. For instance, the error for various sub-cases need to be studied |T|~|C|, |T|>|C|, |T|>>|C| etc. If the error values diverge then that is fatal for the system. In case the paper is accepted, I would like to see a complete treatment of errors in the camera ready version of the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am giving a weak accept because the identification of the problem is good and the experimentation is exhaustive. However, the error due to their assumptions might make their system unusable. In case of acceptance, the authors should be advised to report error analysis experiments.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2018-02-10,no
132,40,124,Nuxx Frxxxx,1,"(OVERALL EVALUATION) This paper still requires much effort from the authors to be ready for publication. Interpreting the description of the research is extremely hard due to the immature state of the text. As a reviewer, I'm not able to review the work in detail given the current state of the text. 
The authors should think more deeply about the right terms to use to describe the concepts involved in their work, and also the English language use needs revision.
The paper also does not strictly follow the template and is not well formatted in many places.

One comment on the title, is that the use of acronyms (the SQA) should be avoided.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-12,04:15,no,,132,40,124,Nuno Freire,1,"(OVERALL EVALUATION) This paper still requires much effort from the authors to be ready for publication. Interpreting the description of the research is extremely hard due to the immature state of the text. As a reviewer, I'm not able to review the work in detail given the current state of the text. 
The authors should think more deeply about the right terms to use to describe the concepts involved in their work, and also the English language use needs revision.
The paper also does not strictly follow the template and is not well formatted in many places.

One comment on the title, is that the use of acronyms (the SQA) should be avoided.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-12,no
135,40,314,Saxxxx Rxx,3,"(OVERALL EVALUATION) This paper analyzes the Chinese social network Zhihu. The users are classified into four types: knowledge seeker, contributor, browser and mixed behavior user. The category of a user is found deterministically depending on how many posts they write or read. Finally, the ""score"" for a user is calculated using an entropy-based method. 

The paper is very readable but has many formatting issues. There are many spaces and I am certain that the ACM format is not followed here. Some information is irrelevant such as: a. ""Python's pandas object is used to group large sample data"" or ""The Python pandas object was used to separate the sample data and grouped by using the grouped"". Figure 3 should be a histogram instead of a scatter plot. Figure 4 is not at all clear: the proportion data should have been explained in other ways.  

But I have a major concern with the way the ""user performance quality assessment indicator system"" is created, which is the actual contribution of the paper. Table 2 possibly lists the features used to calculate the user performance. But it is not well explained what the ""indicator weights"" for these features are (other than the statement ""The entropy method uses the concept of information entropy to calculate the weight of the indicator system, that is, the greater the difference between each values of an index is, the smaller the entropy is, and the greater the weight of the index is"").  This should have been explained in greater details. Also, it is unclear what the last column in table 2 is (""0.36"", page 5, last paragraph). Finally, the statement ""Due to the large number of users, we use the average score of the four categories users to represents the user's level of performance quality"" isn't very clear to me. Does this mean all user's in the same group have the same level of performance quality? The granularity is fairly shallow in that case. For figure 5, there's no X-axis or Y-axis label, so hard to understand what it represents. 

The contributions listed by the authors in the conclusion are as follows:

1. ""This paper uses Python to crawl the behavior data of more than 4 million users on Zhihu platform""
-   This is not really a research contribution. 

2. ""Classifies the users into four categories based on
different Q&A behavior: Knowledge Seeker, Knowledge Contributor, Mixed Behavior User, Browser.""
-  Knowledge seeker and knowledge contributors are well-known categories reported in prev. literature.  However, I am not sure how the authors differentiate between the browser and the inactive users since no browsing data is analyzed. More confusing is the mixed behavior user: wouldn't it be more intuitive to consider a user with more answers than questions to be ""knowledge contributor""? This classification scheme artificially boosts the performance score of one class: the mixed behavior user because from table  3 it is clear that they have higher content contribution score than even the ""knowledge contributor"" class. 

3. ""We constructed a user performance quality
assessment indicator system, using entropy method to
calculate the weight and find out the user's comprehensive performance quality score, we analyzed the scores based on the user-layer.""
- I consider this to be the most important contribution of the paper, but as mentioned before, it is not very well explained. Also, is the goal here to say which type of user is better than the other? That is obviously going to be the mixed behavior class because they are more active, therefore have more content contribution. 

Considering these arguments I would suggest a rejection, or, in the best case acceptance as a short paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would suggest a rejection, or, in the best case acceptance as a short paper.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-21,07:18,no,,135,40,314,Sagnik Ray Choudhury,3,"(OVERALL EVALUATION) This paper analyzes the Chinese social network Zhihu. The users are classified into four types: knowledge seeker, contributor, browser and mixed behavior user. The category of a user is found deterministically depending on how many posts they write or read. Finally, the ""score"" for a user is calculated using an entropy-based method. 

The paper is very readable but has many formatting issues. There are many spaces and I am certain that the ACM format is not followed here. Some information is irrelevant such as: a. ""Python's pandas object is used to group large sample data"" or ""The Python pandas object was used to separate the sample data and grouped by using the grouped"". Figure 3 should be a histogram instead of a scatter plot. Figure 4 is not at all clear: the proportion data should have been explained in other ways.  

But I have a major concern with the way the ""user performance quality assessment indicator system"" is created, which is the actual contribution of the paper. Table 2 possibly lists the features used to calculate the user performance. But it is not well explained what the ""indicator weights"" for these features are (other than the statement ""The entropy method uses the concept of information entropy to calculate the weight of the indicator system, that is, the greater the difference between each values of an index is, the smaller the entropy is, and the greater the weight of the index is"").  This should have been explained in greater details. Also, it is unclear what the last column in table 2 is (""0.36"", page 5, last paragraph). Finally, the statement ""Due to the large number of users, we use the average score of the four categories users to represents the user's level of performance quality"" isn't very clear to me. Does this mean all user's in the same group have the same level of performance quality? The granularity is fairly shallow in that case. For figure 5, there's no X-axis or Y-axis label, so hard to understand what it represents. 

The contributions listed by the authors in the conclusion are as follows:

1. ""This paper uses Python to crawl the behavior data of more than 4 million users on Zhihu platform""
-   This is not really a research contribution. 

2. ""Classifies the users into four categories based on
different Q&A behavior: Knowledge Seeker, Knowledge Contributor, Mixed Behavior User, Browser.""
-  Knowledge seeker and knowledge contributors are well-known categories reported in prev. literature.  However, I am not sure how the authors differentiate between the browser and the inactive users since no browsing data is analyzed. More confusing is the mixed behavior user: wouldn't it be more intuitive to consider a user with more answers than questions to be ""knowledge contributor""? This classification scheme artificially boosts the performance score of one class: the mixed behavior user because from table  3 it is clear that they have higher content contribution score than even the ""knowledge contributor"" class. 

3. ""We constructed a user performance quality
assessment indicator system, using entropy method to
calculate the weight and find out the user's comprehensive performance quality score, we analyzed the scores based on the user-layer.""
- I consider this to be the most important contribution of the paper, but as mentioned before, it is not very well explained. Also, is the goal here to say which type of user is better than the other? That is obviously going to be the mixed behavior class because they are more active, therefore have more content contribution. 

Considering these arguments I would suggest a rejection, or, in the best case acceptance as a short paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would suggest a rejection, or, in the best case acceptance as a short paper.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-21,no
138,41,190,Nabxxxx Jaxxxx,1,"(OVERALL EVALUATION) This paper is relevant to one of the JCDL topics: Personal Digital Information Management. It gives a good overview of how personal digital information management tools can be used but it lacks the comparison of the tools in depth.There are many tools, which the author listed but did not talk about how and when those could be effective. It is focused on only Evernote as digital information management tool. The abstract says the paper is about how and to what extent digital tools can be used. Although the author has described the Evernote tool in detail, the aspect of how it has made a significant improvement in data management and how it has transformed the way data is managed is not well addressed. The methodology used is case studies research which is all based on one tool, Evernote, and does not address the user’s experience with other tools, as suggested in the topic.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-12,15:23,no,,138,41,190,Nabeela Jaffer,1,"(OVERALL EVALUATION) This paper is relevant to one of the JCDL topics: Personal Digital Information Management. It gives a good overview of how personal digital information management tools can be used but it lacks the comparison of the tools in depth.There are many tools, which the author listed but did not talk about how and when those could be effective. It is focused on only Evernote as digital information management tool. The abstract says the paper is about how and to what extent digital tools can be used. Although the author has described the Evernote tool in detail, the aspect of how it has made a significant improvement in data management and how it has transformed the way data is managed is not well addressed. The methodology used is case studies research which is all based on one tool, Evernote, and does not address the user’s experience with other tools, as suggested in the topic.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-12,no
142,42,377,Ricxxxx Toxxxx,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the creation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to “encyclopedic image database” and “deep learning”. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of “SRU”.
3.	Provide a reference to “Tesseract”.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-10,12:56,no,,142,42,377,Ricardo Torres,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the creation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to “encyclopedic image database” and “deep learning”. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of “SRU”.
3.	Provide a reference to “Tesseract”.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-10,no
143,42,377,Ricxxxx Toxxxx,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the creation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to “encyclopedic image database” and “deep learning”. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of “SRU”.
3.	Provide a reference to “Tesseract”.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-10,12:58,no,,143,42,377,Ricardo Torres,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the creation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to “encyclopedic image database” and “deep learning”. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of “SRU”.
3.	Provide a reference to “Tesseract”.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-10,no
144,42,377,Ricxxxx Toxxxx,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the implementation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to “encyclopedic image database” and “deep learning”. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of “SRU”.
3.	Provide a reference to “Tesseract”.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-15,12:12,no,,144,42,377,Ricardo Torres,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the implementation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to “encyclopedic image database” and “deep learning”. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of “SRU”.
3.	Provide a reference to “Tesseract”.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-15,no
150,43,61,Vitxxxx Casxxxxx,2,"(OVERALL EVALUATION) The paper presents the results of a survey (based on 185 valid questionnaires) addressing Information Professionals (IPs) in the LAM sector (Libraries, Archives, Museums) to understand their knowledge, use and requirements about Linked Data (LD) and Semantic web (SW). 
The topic and the approach are not new and the results bring little additional knowledge in this field. However, the topics of LD and SW are clearly relevant, especially for the Library domain, and it might be worth to present the results of the survey as a short paper, to try and stir some discussions in the audience and renew the interest in these topics.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-16,23:19,no,,150,43,61,Vittore Casarosa,2,"(OVERALL EVALUATION) The paper presents the results of a survey (based on 185 valid questionnaires) addressing Information Professionals (IPs) in the LAM sector (Libraries, Archives, Museums) to understand their knowledge, use and requirements about Linked Data (LD) and Semantic web (SW). 
The topic and the approach are not new and the results bring little additional knowledge in this field. However, the topics of LD and SW are clearly relevant, especially for the Library domain, and it might be worth to present the results of the survey as a short paper, to try and stir some discussions in the audience and renew the interest in these topics.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-16,no
153,44,77,Timxxxx Coxx,1,"(OVERALL EVALUATION) This submission is well-written and strongly in scope for JCDL. However, especially given the long paper and poster about Quill presented at last year's JCDL, this submission is insufficient in terms of novelty/originality to warrant acceptance. The incremental work reported is simply not enough for a new long paper. Sections 1, 2, and 3 (and multiple of the figures) largely reiterate and recap the 2017 jcdl paper and poster and do so in greater depth than necessary for a follow-up. The rest of the submission focuses on an ad hoc assessment of Quill over the last year and on changes made in response to this assessment, but the assessment reported is immature, incomplete and weak methodologically (not unrelated). While section 4 provides some new insights gleaned from observations of users wanting to use Quill to better understand 19th century negotiations, the discussion provides insufficient context about these users and the conventions or other negotiations they were seeking to analyze (the introduction mentions Utah's constitutional convention, but this use case is never mentioned again in the body of the paper). This is a problem that recurs throughout the remaining sections of the paper. Clearly the workshops were a source of inspiration for the updates made to Quill over the last year, but the observations reported from the workshop seem anecdotal and potentially random. It does not appear that user feedback was gathered in a structured manner, such that feedback could have been coded and analyzed systematically. At the very least it would be better to see the observations reported as case studies. Better yet would be if the workshops had led to formal surveys, focus groups or user studies, the results of which could then have been coded, analyzed and reported systematically using established methodologies. The overall impression is that the testing, iterative improvements, and gathering of user feedback is very much a work in progress, more suitable for reporting at this stage in a short paper. This impression is reinforced by how sparsely some of the 'enhancements' of Quill are reported, e.g, the implementation of 'quick jump' codes instead of reliance on standard URLs as a solution to a perceived weakness in using URLs for certain classroom settings.  This is reported in the paper as an assertion with no documentation as to the magnitude of problem, nor follow-up as to whether quick jumps actually solve the problem. The intuitions and changes to Quill discussed in sections 5, 6, and 7 are certainly plausible, but in a full paper for JCDL further evidence of confirmation is the expectation. A plausible short paper could be distilled out of this submission (as a follow-up to last year's paper and poster), but in my opinion there's just not enough there yet to warrant acceptance as a full paper.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-15,16:44,no,,153,44,77,Timothy Cole,1,"(OVERALL EVALUATION) This submission is well-written and strongly in scope for JCDL. However, especially given the long paper and poster about Quill presented at last year's JCDL, this submission is insufficient in terms of novelty/originality to warrant acceptance. The incremental work reported is simply not enough for a new long paper. Sections 1, 2, and 3 (and multiple of the figures) largely reiterate and recap the 2017 jcdl paper and poster and do so in greater depth than necessary for a follow-up. The rest of the submission focuses on an ad hoc assessment of Quill over the last year and on changes made in response to this assessment, but the assessment reported is immature, incomplete and weak methodologically (not unrelated). While section 4 provides some new insights gleaned from observations of users wanting to use Quill to better understand 19th century negotiations, the discussion provides insufficient context about these users and the conventions or other negotiations they were seeking to analyze (the introduction mentions Utah's constitutional convention, but this use case is never mentioned again in the body of the paper). This is a problem that recurs throughout the remaining sections of the paper. Clearly the workshops were a source of inspiration for the updates made to Quill over the last year, but the observations reported from the workshop seem anecdotal and potentially random. It does not appear that user feedback was gathered in a structured manner, such that feedback could have been coded and analyzed systematically. At the very least it would be better to see the observations reported as case studies. Better yet would be if the workshops had led to formal surveys, focus groups or user studies, the results of which could then have been coded, analyzed and reported systematically using established methodologies. The overall impression is that the testing, iterative improvements, and gathering of user feedback is very much a work in progress, more suitable for reporting at this stage in a short paper. This impression is reinforced by how sparsely some of the 'enhancements' of Quill are reported, e.g, the implementation of 'quick jump' codes instead of reliance on standard URLs as a solution to a perceived weakness in using URLs for certain classroom settings.  This is reported in the paper as an assertion with no documentation as to the magnitude of problem, nor follow-up as to whether quick jumps actually solve the problem. The intuitions and changes to Quill discussed in sections 5, 6, and 7 are certainly plausible, but in a full paper for JCDL further evidence of confirmation is the expectation. A plausible short paper could be distilled out of this submission (as a follow-up to last year's paper and poster), but in my opinion there's just not enough there yet to warrant acceptance as a full paper.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-15,no
155,44,4,Juxx Abxxx,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. Further they describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collection.

The paper does not present technical details of modifications but does provide screens shots of interface design changes and additional user-centered screens added to provide new ways to add metadata and comments (keywords) to the records.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study or present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. 

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-15,17:46,no,,155,44,4,June Abbas,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. Further they describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collection.

The paper does not present technical details of modifications but does provide screens shots of interface design changes and additional user-centered screens added to provide new ways to add metadata and comments (keywords) to the records.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study or present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. 

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-15,no
156,44,4,Juxx Abxxx,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. However what is missing from the paper is any discussion on how the data was gathered during the workshops or how it was analyzed. This is definitely a weakness of the paper. The authors do describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collection.

The paper does not present technical details of modifications but does provide screens shots of interface design changes and additional user-centered screens added to provide new ways to add metadata and comments (keywords) to the records.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study or present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. 

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-15,17:53,no,,156,44,4,June Abbas,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. However what is missing from the paper is any discussion on how the data was gathered during the workshops or how it was analyzed. This is definitely a weakness of the paper. The authors do describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collection.

The paper does not present technical details of modifications but does provide screens shots of interface design changes and additional user-centered screens added to provide new ways to add metadata and comments (keywords) to the records.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study or present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. 

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-15,no
157,44,4,Juxx Abxxx,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. However what is missing from the paper is any discussion on how the data was gathered during the workshops or how it was analyzed. This is definitely a weakness of the paper. The authors do describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collection.

The paper does not present technical details of modifications but does provide screens shots of interface design changes and additional user-centered screens added to provide new ways to add metadata and comments (keywords) to the records.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study or present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. 

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-18,21:55,no,,157,44,4,June Abbas,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. However what is missing from the paper is any discussion on how the data was gathered during the workshops or how it was analyzed. This is definitely a weakness of the paper. The authors do describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collection.

The paper does not present technical details of modifications but does provide screens shots of interface design changes and additional user-centered screens added to provide new ways to add metadata and comments (keywords) to the records.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study or present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. 

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-18,no
159,44,260,Roxxxx Hx,3,"(OVERALL EVALUATION) This paper is a continuation of the JCDL award winning Quill paper contribution from JCDL 2017. It focuses on the following extensions of the Quill  platform to assist researchers to collaborate with Quill on the study of a wider range of negotiated discourse material. This will further enable use of Quill in the classroom and in a wider range of academic pursuits that can utilize third-party digital archives as source material for Quill analysis. Furthermore the articles discusses community development of the Quill platform through the creation of the Negotiated Text Network.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very effective non-technical paper that further refines the Quill framework and offers opportunity for expanded Quill community building. This should allow for improved overall functionality of the platform along with opportunities for sustainability of the application among the negotiated texts research community.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-16,19:24,no,,159,44,260,Robert H. Mcdonald,3,"(OVERALL EVALUATION) This paper is a continuation of the JCDL award winning Quill paper contribution from JCDL 2017. It focuses on the following extensions of the Quill  platform to assist researchers to collaborate with Quill on the study of a wider range of negotiated discourse material. This will further enable use of Quill in the classroom and in a wider range of academic pursuits that can utilize third-party digital archives as source material for Quill analysis. Furthermore the articles discusses community development of the Quill platform through the creation of the Negotiated Text Network.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very effective non-technical paper that further refines the Quill framework and offers opportunity for expanded Quill community building. This should allow for improved overall functionality of the platform along with opportunities for sustainability of the application among the negotiated texts research community.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-16,no
163,45,291,Denxxxxx Perxxxx,2,"(OVERALL EVALUATION) The paper proposes a system that recommends books based on their authors’ writing style. It focuses on the analysis of the textual content of books to improve their recommendations. The system transfers information learned by an authorship identification (AuthId) classifier to a book recommendation module by using a neural network.

The subject is relevant to digital libraries. The paper is well written and with a good bibliographical review.

An issue not evaluated in the work was the efficiency of the approach. Processing the whole text of books using neural networks may not be efficient, and infeasible to big data.
Is it really necessary to analyze the whole text of books to identify the authors' writing style?
Maybe less than 100,000 words are sufficient for identification. It would be interesting to add experiments with fewer words per book in order to verify this assumption.

The baselines used for comparison are general information retrieval approaches. It would be interesting also to make a comparison of the proposed approach with other specific baselines to recommend books found in the literature. This would be important to ensure that writing style recommendation is actually the best recommendation strategy.

According to Figure 2, although the proposed approach achieves better results than the baselines, the results for precision and recall are very low. Does this occur in other works of the literature? Or is it a feature of the dataset and the way the experiments were done?
Although the authors have done a good analysis of the results, they should better justify the low values.

The Conclusions section is very simple. There is plenty of room for increments.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-09,18:50,no,,163,45,291,Denilson Pereira,2,"(OVERALL EVALUATION) The paper proposes a system that recommends books based on their authors’ writing style. It focuses on the analysis of the textual content of books to improve their recommendations. The system transfers information learned by an authorship identification (AuthId) classifier to a book recommendation module by using a neural network.

The subject is relevant to digital libraries. The paper is well written and with a good bibliographical review.

An issue not evaluated in the work was the efficiency of the approach. Processing the whole text of books using neural networks may not be efficient, and infeasible to big data.
Is it really necessary to analyze the whole text of books to identify the authors' writing style?
Maybe less than 100,000 words are sufficient for identification. It would be interesting to add experiments with fewer words per book in order to verify this assumption.

The baselines used for comparison are general information retrieval approaches. It would be interesting also to make a comparison of the proposed approach with other specific baselines to recommend books found in the literature. This would be important to ensure that writing style recommendation is actually the best recommendation strategy.

According to Figure 2, although the proposed approach achieves better results than the baselines, the results for precision and recall are very low. Does this occur in other works of the literature? Or is it a feature of the dataset and the way the experiments were done?
Although the authors have done a good analysis of the results, they should better justify the low values.

The Conclusions section is very simple. There is plenty of room for increments.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-09,no
170,47,399,Mixx Wrxxxx,2,"(OVERALL EVALUATION) The core of the paper is reporting a technique to potentially help editors assemble a good set of reviewers for a submission based on reviewers’ past review history. They utilize a genetic algorithm to perform analysis and present possible sets of reviewers based on use of GA’s in putting together teams in other fields.  They have access to the corpus of reviews of both JHEP and JSTAT, two leading journals in physics. Using these corpus, they do analysis of reviews and accept/reject results for both single reviews and multi-reviewed cases (apparently both journals do have single-review submissions), and use this to build a hypothesis for their algorithm. They use part of the corpus for training, and test over the larger corpus. The goal is to select review teams to reduce the number of discordant sets of reviewers (i.e. review ends with no resolution on disagreements) after observing what they believe are issues of some less impactful papers being accepted, and some more impactful being rejected (but then published elsewhere) as measured in long term citation count. 

I do wonder about the authors synonymous use of high quality with high impact through use of long term citation count - in my own thinking, it’s the issue of just what a citation count really means even though we use this as the core measure of long term impact.  In thinking this, I wonder if the approach to reducing discordance in reviews is necessarily the ultimate goal as doesn’t discordance at times highlight where there may be still unsettled approaches?  Yet striving to get consensus is part of reviewing, and helping with that is a useful goal.  While the paper elicited this tension for me, and it may be interesting to hear how the authors think about this, I don’t think it undermines their current work.

This work builds on their previous work - one referenced from the ACM conference on Information and Knowledge Management, and another, surprisingly not referenced in related work, from last year’s JCDL “Influence of Reviewer Interaction Network on Long-term Citations: A Case Study of the Scientific Peer-Review System of the Journal of High Energy Physics” which looked at a variety of features that may impact acceptance/rejection, with particular attention to the reviewer network, and just focused on JHEP.  I would like to see this reflected in related work.

Section 5.2, part way through paragraph (LL, MM, HH, LH, MH, LH) should read (LL, MM, HH, LH, MH, LM).

Section 6, last paragraph before section 6.1, state “we argue that assigning multiple referees to a submission is similar to forming compatible referee groups” - that “forming compatible referee groups” would seem to be wrong as the comparison was to a collaborative learning setting, so what was the desired collaborative learning group you’re comparing to?","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-12,16:52,no,,170,47,399,Mike Wright,2,"(OVERALL EVALUATION) The core of the paper is reporting a technique to potentially help editors assemble a good set of reviewers for a submission based on reviewers’ past review history. They utilize a genetic algorithm to perform analysis and present possible sets of reviewers based on use of GA’s in putting together teams in other fields.  They have access to the corpus of reviews of both JHEP and JSTAT, two leading journals in physics. Using these corpus, they do analysis of reviews and accept/reject results for both single reviews and multi-reviewed cases (apparently both journals do have single-review submissions), and use this to build a hypothesis for their algorithm. They use part of the corpus for training, and test over the larger corpus. The goal is to select review teams to reduce the number of discordant sets of reviewers (i.e. review ends with no resolution on disagreements) after observing what they believe are issues of some less impactful papers being accepted, and some more impactful being rejected (but then published elsewhere) as measured in long term citation count. 

I do wonder about the authors synonymous use of high quality with high impact through use of long term citation count - in my own thinking, it’s the issue of just what a citation count really means even though we use this as the core measure of long term impact.  In thinking this, I wonder if the approach to reducing discordance in reviews is necessarily the ultimate goal as doesn’t discordance at times highlight where there may be still unsettled approaches?  Yet striving to get consensus is part of reviewing, and helping with that is a useful goal.  While the paper elicited this tension for me, and it may be interesting to hear how the authors think about this, I don’t think it undermines their current work.

This work builds on their previous work - one referenced from the ACM conference on Information and Knowledge Management, and another, surprisingly not referenced in related work, from last year’s JCDL “Influence of Reviewer Interaction Network on Long-term Citations: A Case Study of the Scientific Peer-Review System of the Journal of High Energy Physics” which looked at a variety of features that may impact acceptance/rejection, with particular attention to the reviewer network, and just focused on JHEP.  I would like to see this reflected in related work.

Section 5.2, part way through paragraph (LL, MM, HH, LH, MH, LH) should read (LL, MM, HH, LH, MH, LM).

Section 6, last paragraph before section 6.1, state “we argue that assigning multiple referees to a submission is similar to forming compatible referee groups” - that “forming compatible referee groups” would seem to be wrong as the comparison was to a collaborative learning setting, so what was the desired collaborative learning group you’re comparing to?","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-12,no
172,47,86,Pexxx Daxxx,3,"(OVERALL EVALUATION) This paper presents findings from a study of peer-review practices in two physics journals, in particular focusing on relationships between referees’ evaluations of articles and these articles’ subsequent success (measured through number of citations). The study finds that accepted articles that were reviewed by a single referee were subsequently more highly-cited than accepted articles reviewed by multiple referees, and that articles where there was a higher level of discordance between referees tend to have lower citation rates. Based on this study, the paper then outlines and evaluates an approach for allocating sets of reviewers to articles to improve review processes. 

This is an interesting paper. However, I do not believe it is relevant to JCDL: it has no obvious link to digital libraries, and further, the authors do not attempt to link the topic to digital libraries. This paper would be more suitable for a scholarly communications conference or journal. 

In addition, while the dataset used in the analysis is fairly large (approx. 36,000 papers), it is unclear whether the findings (based only on two journals in a highly-specialized domain of physics) are generalizable across a broad range of disciplines. 

There are a few other points that the authors could clarify in a revision:
•	What is meant by the term “compatible referee groups” (p. 7 and elsewhere)?;
•	What basis do the authors have for assuming that referees with similar opinions of an article will tend to produce reviews of similar length (p. 4)? Surely there are many other factors that affect the length of a review?;
•	When a referee is classified as anomalous, how many reviews must a referee have performed to be classified with any degree of significance? Do referees, in reality, actually perform sufficient reviews such that the system of allocating referees to articles is practical?","Overall evaluation: -2
Reviewer's confidence: 2
Recommend for best paper: no",-2,,,,,2018-02-16,22:45,no,,172,47,86,Peter Darch,3,"(OVERALL EVALUATION) This paper presents findings from a study of peer-review practices in two physics journals, in particular focusing on relationships between referees’ evaluations of articles and these articles’ subsequent success (measured through number of citations). The study finds that accepted articles that were reviewed by a single referee were subsequently more highly-cited than accepted articles reviewed by multiple referees, and that articles where there was a higher level of discordance between referees tend to have lower citation rates. Based on this study, the paper then outlines and evaluates an approach for allocating sets of reviewers to articles to improve review processes. 

This is an interesting paper. However, I do not believe it is relevant to JCDL: it has no obvious link to digital libraries, and further, the authors do not attempt to link the topic to digital libraries. This paper would be more suitable for a scholarly communications conference or journal. 

In addition, while the dataset used in the analysis is fairly large (approx. 36,000 papers), it is unclear whether the findings (based only on two journals in a highly-specialized domain of physics) are generalizable across a broad range of disciplines. 

There are a few other points that the authors could clarify in a revision:
•	What is meant by the term “compatible referee groups” (p. 7 and elsewhere)?;
•	What basis do the authors have for assuming that referees with similar opinions of an article will tend to produce reviews of similar length (p. 4)? Surely there are many other factors that affect the length of a review?;
•	When a referee is classified as anomalous, how many reviews must a referee have performed to be classified with any degree of significance? Do referees, in reality, actually perform sufficient reviews such that the system of allocating referees to articles is practical?","Overall evaluation: -2
Reviewer's confidence: 2
Recommend for best paper: no",-2,,,,,2018-02-16,no
188,54,358,Kazxxxxx Sugxxxxx,2,"(OVERALL EVALUATION) The authors propose a method for recommending co-authors or collaborators by taking this task 
as link prediction problem. The authors plan to use machine machine learning to predict new edges 
in temporal structure of co-authorship network. 

While this work is well-motivated by the current situation in the National Research University 
Higher School of Economics (HSE) as described in Section 3.3, the authors just describe the plan 
for experiments. To make this paper publishable, the authors should propose a novel approach, 
evaluate it with a relevant measure, and then discuss the obtained results. 

(1) Novelty/Originality
- The authors try to apply just regression or classification models as described in Sec 4. 
So if the authors propose a novel approach, for example, neural network-based model, this paper 
would be intersting. 


(2) Methodology
- The authors should detail more about their proposed approach not just listing features, 
similarity scores, and symmetric binary functions. 


(3) Assessment/Evaluation/Comparison
- As pointed out above, the authors need to quantitatively evaluate their proposed approach. 
It is also important to compare their proposed approach with some state-of-the-arts. 


(4) Style/Quality of Writing
This paper has some wrong expressions as follows: 

[Sec 3.1]
- infromation on ... => in*for*mation on ... 

- Figure ?? => Figure 1
(Need to specify Figure ID) 

[Sec 3.2]
- In certain cases when the author is ... => In certain ca => ses *where* the author is ... 

- Last name and initials => *l*ast name and initials 


(5) Replicability
- The authors plan to use classical regression or classification models as described in Sec 4. 
So if the authors clearly describe their features, it is eacy to reproduce their experiments. 


(6) References
- The authors need to survey related work that is more specific to ""identifying collaborators."" 
For example, they need to cite the following papers: 

H.-H. Chen,  L. Gou, X. Zhang, and C. Lee Giles: 
""CollabSeer: A Search Engine for Collaboration Discovery"" (JCDL2011)

H. Deng, J. Han, M. R. Lyu, and I. King: 
""Modeling and Exploiting Heterogeneous Bibliographic Networks for Expertise Ranking"" (JCDL2012)

J. Tang, S. Wu, J. Sun, and Hang Su: 
""Cross-domain Collaboration Recommendation"" (KDD2012)

S. S. Rangapuram, T. Bühler, M. Hein
""Towards realistic team formation in social networks based on densest subgraphs"" (WWW2013)

S. H. Hashemi, M. Neshati, and H. Beigy: 
""Expertise Retrieval in Bibliographic Network: A Topic Dominance Learning Approach"" (CIKM2013)

M. Y. Allaho, W.-C. Lee: 
""Increasing the Responsiveness of Recommended Expert Collaborators for Online Open Projects"" (CIKM2014)","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-12,13:16,no,,188,54,358,Kazunari Sugiyama,2,"(OVERALL EVALUATION) The authors propose a method for recommending co-authors or collaborators by taking this task 
as link prediction problem. The authors plan to use machine machine learning to predict new edges 
in temporal structure of co-authorship network. 

While this work is well-motivated by the current situation in the National Research University 
Higher School of Economics (HSE) as described in Section 3.3, the authors just describe the plan 
for experiments. To make this paper publishable, the authors should propose a novel approach, 
evaluate it with a relevant measure, and then discuss the obtained results. 

(1) Novelty/Originality
- The authors try to apply just regression or classification models as described in Sec 4. 
So if the authors propose a novel approach, for example, neural network-based model, this paper 
would be intersting. 


(2) Methodology
- The authors should detail more about their proposed approach not just listing features, 
similarity scores, and symmetric binary functions. 


(3) Assessment/Evaluation/Comparison
- As pointed out above, the authors need to quantitatively evaluate their proposed approach. 
It is also important to compare their proposed approach with some state-of-the-arts. 


(4) Style/Quality of Writing
This paper has some wrong expressions as follows: 

[Sec 3.1]
- infromation on ... => in*for*mation on ... 

- Figure ?? => Figure 1
(Need to specify Figure ID) 

[Sec 3.2]
- In certain cases when the author is ... => In certain ca => ses *where* the author is ... 

- Last name and initials => *l*ast name and initials 


(5) Replicability
- The authors plan to use classical regression or classification models as described in Sec 4. 
So if the authors clearly describe their features, it is eacy to reproduce their experiments. 


(6) References
- The authors need to survey related work that is more specific to ""identifying collaborators."" 
For example, they need to cite the following papers: 

H.-H. Chen,  L. Gou, X. Zhang, and C. Lee Giles: 
""CollabSeer: A Search Engine for Collaboration Discovery"" (JCDL2011)

H. Deng, J. Han, M. R. Lyu, and I. King: 
""Modeling and Exploiting Heterogeneous Bibliographic Networks for Expertise Ranking"" (JCDL2012)

J. Tang, S. Wu, J. Sun, and Hang Su: 
""Cross-domain Collaboration Recommendation"" (KDD2012)

S. S. Rangapuram, T. Bühler, M. Hein
""Towards realistic team formation in social networks based on densest subgraphs"" (WWW2013)

S. H. Hashemi, M. Neshati, and H. Beigy: 
""Expertise Retrieval in Bibliographic Network: A Topic Dominance Learning Approach"" (CIKM2013)

M. Y. Allaho, W.-C. Lee: 
""Increasing the Responsiveness of Recommended Expert Collaborators for Online Open Projects"" (CIKM2014)","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-12,no
190,54,174,Draxxxxxx Herrxxxxxxx,3,"(OVERALL EVALUATION) This paper presents an approach for predicting and recommending co-authors based on various network-based features. The problem is formulated as link prediction task and is tackled by combining approaches — author similarity measures and more traditional graph-based recommendations with network embeddings. I think this is a very interesting problem and potentially a useful solution, however, the paper seems incomplete, which is why I cannot recommend it for acceptance. In particular, in section 4 the authors mention they measure AUC, however, the results are not presented in the paper and it seems the entire results section is missing. Unfortunately, without any results, it is hard assessing the method. I have some general comments and questions regarding the remainder of the text.

First, in the introduction, the authors seem to jump around different topics, which made the text harder to follow. In the related work section the authors mention several terms without explaining them (e.g. attribute-based vs. self-organizing networks). While these terms may be well known in graph analysis, I think it may still be worth providing a brief explanation to help the reader.

There are a number of statements in the data collection section which may need more explanation. In particular, the authors state that they “manually input at the personal web-page of researcher research interest list according to RSCI categorization”. This sounds very time consuming. For how many records was the manual processing done? 

The authors state duplicate records were merged, how was this done? Was this done based on title match, similarity, or other criteria? 

Next, the authors state that “All the missing fields were omitted during computational part of filed with median over respective category of articles and authors.” — I’m not sure I understand this sentence, does it say some fields were omitted if empty and some filled with median values? It would be useful to state which fields were omitted and which were replaced with median values. 

It would also be useful to provide some statistics for the author disambiguation results to give the reader a sense of how well did the disambiguation work. 

Regarding the quality metrics, it would be helpful to state which specific metrics were imported (was it JIF, eigenfactor, or other metrics?) and again provide some basic statistics for these metrics. Finally, I would appreciate if the authors could share some overview statistics of the dataset. I assume this was partially done in figure 1, however, the figure is quite difficult to read. I thought the commas are used as decimal separators instead of colons, however, in that case, some of the values don’t make sense to me (e.g. I would expect the number of authors to always be a natural number).

Regarding the choice of the dataset, I think it would be particularly interesting to perform the experiment on a dataset which includes authors from different institutes rather than authors from a single institute only. I would expect the authors within the institute to more likely form connections, since there is a higher likelihood that they already know each other. From this perspective I find the choice of the dataset somewhat limiting. Forming connections across institutes and maybe even across countries is much harder and is therefore a task which may much more benefit from recommendations.

Finally, the paper would greatly benefit from proofreading.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-17,06:10,no,,190,54,174,Drahomira Herrmannova,3,"(OVERALL EVALUATION) This paper presents an approach for predicting and recommending co-authors based on various network-based features. The problem is formulated as link prediction task and is tackled by combining approaches — author similarity measures and more traditional graph-based recommendations with network embeddings. I think this is a very interesting problem and potentially a useful solution, however, the paper seems incomplete, which is why I cannot recommend it for acceptance. In particular, in section 4 the authors mention they measure AUC, however, the results are not presented in the paper and it seems the entire results section is missing. Unfortunately, without any results, it is hard assessing the method. I have some general comments and questions regarding the remainder of the text.

First, in the introduction, the authors seem to jump around different topics, which made the text harder to follow. In the related work section the authors mention several terms without explaining them (e.g. attribute-based vs. self-organizing networks). While these terms may be well known in graph analysis, I think it may still be worth providing a brief explanation to help the reader.

There are a number of statements in the data collection section which may need more explanation. In particular, the authors state that they “manually input at the personal web-page of researcher research interest list according to RSCI categorization”. This sounds very time consuming. For how many records was the manual processing done? 

The authors state duplicate records were merged, how was this done? Was this done based on title match, similarity, or other criteria? 

Next, the authors state that “All the missing fields were omitted during computational part of filed with median over respective category of articles and authors.” — I’m not sure I understand this sentence, does it say some fields were omitted if empty and some filled with median values? It would be useful to state which fields were omitted and which were replaced with median values. 

It would also be useful to provide some statistics for the author disambiguation results to give the reader a sense of how well did the disambiguation work. 

Regarding the quality metrics, it would be helpful to state which specific metrics were imported (was it JIF, eigenfactor, or other metrics?) and again provide some basic statistics for these metrics. Finally, I would appreciate if the authors could share some overview statistics of the dataset. I assume this was partially done in figure 1, however, the figure is quite difficult to read. I thought the commas are used as decimal separators instead of colons, however, in that case, some of the values don’t make sense to me (e.g. I would expect the number of authors to always be a natural number).

Regarding the choice of the dataset, I think it would be particularly interesting to perform the experiment on a dataset which includes authors from different institutes rather than authors from a single institute only. I would expect the authors within the institute to more likely form connections, since there is a higher likelihood that they already know each other. From this perspective I find the choice of the dataset somewhat limiting. Forming connections across institutes and maybe even across countries is much harder and is therefore a task which may much more benefit from recommendations.

Finally, the paper would greatly benefit from proofreading.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-17,no
194,56,103,Patxxxx Fxx,3,"(OVERALL EVALUATION) The paper studies an interesting problem.
The approach they proposed is rather piecemeal.
It is hard to know the novelty of the proposed approach.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-21,21:18,no,,194,56,103,Patrick Fan,3,"(OVERALL EVALUATION) The paper studies an interesting problem.
The approach they proposed is rather piecemeal.
It is hard to know the novelty of the proposed approach.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-21,no
195,56,103,Patxxxx Fxx,3,"(OVERALL EVALUATION) The paper studies an interesting problem.
The approach they proposed is rather piecemeal.
It is hard to know the novelty of the proposed approach.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-22,11:30,no,,195,56,103,Patrick Fan,3,"(OVERALL EVALUATION) The paper studies an interesting problem.
The approach they proposed is rather piecemeal.
It is hard to know the novelty of the proposed approach.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-22,no
197,56,204,Minxxxx Kxx,4,"(OVERALL EVALUATION) This paper examines the problem of detecting academic plagiarism using image based methodologies for plagiarism of figures/charts and other graphical contents, using both ratio and perceptual based hashing, involving the repurposing of the AlexNet 2012 architectured convolutional network net (CNN) for one of the 4 methods ensembled to make the system.

The image-based approach to plagiarism is a good idea that has been raised but little practical studies have gone onto working on this area to my knowledge.  It is heartening to know that this work has been executed.  The paper is well structured and argued, and the model and architecture is sound and reasonable.

Minor points.  I'm not sure that I would agree with ""Strongly disguised images"" as plagiarism.  Certainly photographs of different landmarks don't constitute plagiarism and I think it would be good for the authors to state why the VroniPlag collection deems such cases as plagiarism.

Figure 3 can be better drawn to be more vertically compact and use space more efficiently.  The aspect ratio looks off.

I'm not sure that the implementation details (in Py 2.7) would be necessary details to discuss.  The algorithmic steps in 3.4 however, are nicely described and reduced to a practical description, which is helpful.

It would be great if the authors could address how their system would scale and the constants modified when dealing with a larger corpus.  For example, in S 3.11 there are some constants (for #m of strongly related images) that would seem to be quite corpus and scale specific.  How to select these well would be a useful side discussion if there is sufficient room for discussion.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-23,15:10,no,,197,56,204,Min-Yen Kan,4,"(OVERALL EVALUATION) This paper examines the problem of detecting academic plagiarism using image based methodologies for plagiarism of figures/charts and other graphical contents, using both ratio and perceptual based hashing, involving the repurposing of the AlexNet 2012 architectured convolutional network net (CNN) for one of the 4 methods ensembled to make the system.

The image-based approach to plagiarism is a good idea that has been raised but little practical studies have gone onto working on this area to my knowledge.  It is heartening to know that this work has been executed.  The paper is well structured and argued, and the model and architecture is sound and reasonable.

Minor points.  I'm not sure that I would agree with ""Strongly disguised images"" as plagiarism.  Certainly photographs of different landmarks don't constitute plagiarism and I think it would be good for the authors to state why the VroniPlag collection deems such cases as plagiarism.

Figure 3 can be better drawn to be more vertically compact and use space more efficiently.  The aspect ratio looks off.

I'm not sure that the implementation details (in Py 2.7) would be necessary details to discuss.  The algorithmic steps in 3.4 however, are nicely described and reduced to a practical description, which is helpful.

It would be great if the authors could address how their system would scale and the constants modified when dealing with a larger corpus.  For example, in S 3.11 there are some constants (for #m of strongly related images) that would seem to be quite corpus and scale specific.  How to select these well would be a useful side discussion if there is sufficient room for discussion.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-23,no
202,58,179,Hexxx Hocxxxxx,1,"(OVERALL EVALUATION) The paper describes a collaborative digitisation project between a municipality, a historical society, two universities, and local citizenry. The author has done a good job - readers are able to get a clear sense of how the project was organised and how it progressed, including the roles of each partner involved in the project.

It would be a much better paper if the author could leave out some of the details and reflect on what went well and what went wrong, and what are the lessons learnt. Many digitisation projects take place but what are the specifics about this project that is relevant to the conference or interesting to the readers? The author should also discuss long-term digital preservation considerations or arrangement related to the project. 

It would have been interesting to include details on contribution from city residents and volunteers, and discussion on how they might play a bigger role in any future effort of digitising the remainder of the GNMHS collections.

The paper could benefit from e.g. a workflow diagram which would reduce the detailed textual description.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-09,16:14,no,,202,58,179,Helen Hockx-Yu,1,"(OVERALL EVALUATION) The paper describes a collaborative digitisation project between a municipality, a historical society, two universities, and local citizenry. The author has done a good job - readers are able to get a clear sense of how the project was organised and how it progressed, including the roles of each partner involved in the project.

It would be a much better paper if the author could leave out some of the details and reflect on what went well and what went wrong, and what are the lessons learnt. Many digitisation projects take place but what are the specifics about this project that is relevant to the conference or interesting to the readers? The author should also discuss long-term digital preservation considerations or arrangement related to the project. 

It would have been interesting to include details on contribution from city residents and volunteers, and discussion on how they might play a bigger role in any future effort of digitising the remainder of the GNMHS collections.

The paper could benefit from e.g. a workflow diagram which would reduce the detailed textual description.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-09,no
209,60,337,Axx Shxxx,1,"(OVERALL EVALUATION) The study investigates search behaviour within a newspaper digital library, and makes use of user interaction with facets as well as users's click data to examine their search behaviour. The literature review makes references to old literature on search behaviour within digital libraries. A majority of the references are dated before 2010. Even specifically relevant references to how users view and make use of facets in exploratory search interfaces are missing. Here are some examples: 
Kules, B., Capra, R., Banta, M., & Sierra, T. (2009, June). What do exploratory searchers look at in a faceted search interface?. In Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries (pp. 313-322). 

ACM. Kules, B., & Shneiderman, B. (2008). Users can change their web search tactics: Design guidelines for categorized overviews. Information Processing & Management, 44(2), 463-484.
 
Kules, B., & Capra, R. (2009, June). Designing exploratory search tasks for user studies of information seeking support systems. In Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries (pp. 419-420). ACM. 

Here is a recent study that should have been cited as well: Han, H., & Wolfram, D. (2016). An exploration of search session patterns in an image-based digital library. Journal of Information Science, 42(4), 477-491. 

The study is appropriately reported. However, the study does not demonstrate originality or novel methodological approach. The authors do not elaborate on what we can learn from this study or what practical or theoretical implications this study may have for conducting search behaviour studies within digital libraries or how we can inform the design and development of digital library search user interfaces or digital library systems. The findings about users&#39; search behvaiour and the sources they have consulted do not provide any specific insight. They do not shed light on how newspaper digital libraries should be designed or studies. Having read the entire paper, the question &#39;so what?&#39; comes to mind. The authors make references to the importance of long tail of search, but do not address this important aspect of the study in the literature review.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Although the paper is well organized and presented, it does not provide any novel methodology or new insight or findings that we have not previously seen in the literature of digital library search behaviour studies. There is nothing particularly interesting or useful to gain from this study. The simple question is what can we learn from this study theoretically, practically, empirically or methodologically? As a researcher who has conducted several digital library search behaviour studies as well as search log and query log analyses I have hard time answer the question.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-01-29,21:34,no,,209,60,337,Ali Shiri,1,"(OVERALL EVALUATION) The study investigates search behaviour within a newspaper digital library, and makes use of user interaction with facets as well as users's click data to examine their search behaviour. The literature review makes references to old literature on search behaviour within digital libraries. A majority of the references are dated before 2010. Even specifically relevant references to how users view and make use of facets in exploratory search interfaces are missing. Here are some examples: 
Kules, B., Capra, R., Banta, M., & Sierra, T. (2009, June). What do exploratory searchers look at in a faceted search interface?. In Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries (pp. 313-322). 

ACM. Kules, B., & Shneiderman, B. (2008). Users can change their web search tactics: Design guidelines for categorized overviews. Information Processing & Management, 44(2), 463-484.
 
Kules, B., & Capra, R. (2009, June). Designing exploratory search tasks for user studies of information seeking support systems. In Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries (pp. 419-420). ACM. 

Here is a recent study that should have been cited as well: Han, H., & Wolfram, D. (2016). An exploration of search session patterns in an image-based digital library. Journal of Information Science, 42(4), 477-491. 

The study is appropriately reported. However, the study does not demonstrate originality or novel methodological approach. The authors do not elaborate on what we can learn from this study or what practical or theoretical implications this study may have for conducting search behaviour studies within digital libraries or how we can inform the design and development of digital library search user interfaces or digital library systems. The findings about users&#39; search behvaiour and the sources they have consulted do not provide any specific insight. They do not shed light on how newspaper digital libraries should be designed or studies. Having read the entire paper, the question &#39;so what?&#39; comes to mind. The authors make references to the importance of long tail of search, but do not address this important aspect of the study in the literature review.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Although the paper is well organized and presented, it does not provide any novel methodology or new insight or findings that we have not previously seen in the literature of digital library search behaviour studies. There is nothing particularly interesting or useful to gain from this study. The simple question is what can we learn from this study theoretically, practically, empirically or methodologically? As a researcher who has conducted several digital library search behaviour studies as well as search log and query log analyses I have hard time answer the question.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-01-29,no
211,60,96,Antxxxx Doxxxx,2,"(OVERALL EVALUATION) This paper presents an investigation of the search logs of the national library of the Netherlands. The authors are performing clustering based on interactive and metadata features, with the aim to reveal usage patterns.

    Novelty:
    While this type of work is not novel in general, it is interesting it applied to digital libraries. All libraries have search logs in-house that could provide useful information for the design of their IR models and UIs.
    
    Soundness/Methodology:
    This is the biggest issue of the work presented, which is very empirical, with choices that are not always satisfactorily justified.
    The choice of k=14 is notably highly empirical, especially as Figure 1 (used to support that choice) is rather inconclusive. Given that the stability of cluster over 2 periods was the chosen evaluation measure, it would probably have made more sense to optimize the quality of the clustering by chosing a value of k that maximises that stability.
    Further, that choice of stability over 2 periods appears reasonable, but there are many other intrinsic measures of cluster quality that function without a ground truth, such a purity. These seem to have been overlooked.
    All in all, the too many empirical choices and analysis imply that reproducibility will be an issue.
    
    Results:
    The main lesson here is that there are different usage patterns. This is only conclusion upon which the paper can be firmly conclusive. Unfortunately, this is not new information.

    Presentation:
    The paper is very well written and pleasant to read, with the exception of citations: plase observe that [7], [22] and [14] are not words, and that they cannot belong to a grammatical construct such as sentence. Using them as such only makes the text harder to read. Think of citations as text in parenthesis. One good example in the text is the following, except that the citation is actually missing ([28] should occur, either right after ""Wang et al., or preferably at the end of the sentence): ""Wang et al. use unsupervised hierarchical clustering to detect patterns in user behavior from logs in social networks.""
    While Figure 3 seems to be potentially interesting, it is unfortunately too low-resolution to be readable (even its digital version). Please improve image quality, make space for it and enlarge it, or just remove it.
    Table 3, cluster 11: what is a ""home page"" in this context? Does it mean the user remains on the search page (no clicks)?
    
    Overall:
    This is promising work but seems more fit for a short paper. This investigation and its potential results are interesting, but they lack maturity. The work presented here forms the ground for very strong paper next year.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-17,06:51,no,,211,60,96,Antoine Doucet,2,"(OVERALL EVALUATION) This paper presents an investigation of the search logs of the national library of the Netherlands. The authors are performing clustering based on interactive and metadata features, with the aim to reveal usage patterns.

    Novelty:
    While this type of work is not novel in general, it is interesting it applied to digital libraries. All libraries have search logs in-house that could provide useful information for the design of their IR models and UIs.
    
    Soundness/Methodology:
    This is the biggest issue of the work presented, which is very empirical, with choices that are not always satisfactorily justified.
    The choice of k=14 is notably highly empirical, especially as Figure 1 (used to support that choice) is rather inconclusive. Given that the stability of cluster over 2 periods was the chosen evaluation measure, it would probably have made more sense to optimize the quality of the clustering by chosing a value of k that maximises that stability.
    Further, that choice of stability over 2 periods appears reasonable, but there are many other intrinsic measures of cluster quality that function without a ground truth, such a purity. These seem to have been overlooked.
    All in all, the too many empirical choices and analysis imply that reproducibility will be an issue.
    
    Results:
    The main lesson here is that there are different usage patterns. This is only conclusion upon which the paper can be firmly conclusive. Unfortunately, this is not new information.

    Presentation:
    The paper is very well written and pleasant to read, with the exception of citations: plase observe that [7], [22] and [14] are not words, and that they cannot belong to a grammatical construct such as sentence. Using them as such only makes the text harder to read. Think of citations as text in parenthesis. One good example in the text is the following, except that the citation is actually missing ([28] should occur, either right after ""Wang et al., or preferably at the end of the sentence): ""Wang et al. use unsupervised hierarchical clustering to detect patterns in user behavior from logs in social networks.""
    While Figure 3 seems to be potentially interesting, it is unfortunately too low-resolution to be readable (even its digital version). Please improve image quality, make space for it and enlarge it, or just remove it.
    Table 3, cluster 11: what is a ""home page"" in this context? Does it mean the user remains on the search page (no clicks)?
    
    Overall:
    This is promising work but seems more fit for a short paper. This investigation and its potential results are interesting, but they lack maturity. The work presented here forms the ground for very strong paper next year.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-17,no
216,62,324,Haxx Salaxxxxxxx,3,"(OVERALL EVALUATION) The authors present the problem of analyzing and remodeling digital social media platforms in the eyes of information theory. Namely, the authors propose a novel analytical framework of analyzing social media data stream (Twitter) using the entropic method of Claude Shannon's information theory.

The flow of the paper is good and it is clear to a large extend, and the authors placed considerable amount of related work throughout the paper going back to works published in 1924. There was a little bit of repetitiveness throughout while describing information theory in general and Shannon's work. The idea is rather novel and quite interesting but I have some points of concern as follows.

In their case study, the authors extracted a minute dataset totaling 300 tweets and further sub divided them into two sub datasets, for the period of just 72 hours. In the realm of twitter this is very small to extract most meaningful signals. Granted they had to manually label certain aspects of user accounts but they could have opted to create a script that mine official and non-official profiles in the field of the tweet (flu, medical) and extracted content in the size of thousands. The paper spread almost two pages to explain what is entropy and calculation matrix which is described thoroughly in previous works. Furthermore, I believe analyzing images in social media interactions merely with its RGB aspects is lacking. There are numerous precious computer vision / social media analytics papers that explain the exact opposite of this and that content is important, what is in the image is important than even its size, color scheme or filters applied. Black and white and greyscale images are proof of that. Lastly the trends they picked were fairly limited, it would have been tremendously exciting to see several other ""current"" trails and the differences between them, and analysis of the origins of the trails and how they pick up speed in retweeting or die, that would give invaluable insight with their powerful application of entropy. 

I would advise the authors to expand more on their work, there is great potential in it

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would suggest a reapplication of this same paper as a short paper due to its novelty.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-17,08:57,no,,216,62,324,Hany Salaheldeen,3,"(OVERALL EVALUATION) The authors present the problem of analyzing and remodeling digital social media platforms in the eyes of information theory. Namely, the authors propose a novel analytical framework of analyzing social media data stream (Twitter) using the entropic method of Claude Shannon's information theory.

The flow of the paper is good and it is clear to a large extend, and the authors placed considerable amount of related work throughout the paper going back to works published in 1924. There was a little bit of repetitiveness throughout while describing information theory in general and Shannon's work. The idea is rather novel and quite interesting but I have some points of concern as follows.

In their case study, the authors extracted a minute dataset totaling 300 tweets and further sub divided them into two sub datasets, for the period of just 72 hours. In the realm of twitter this is very small to extract most meaningful signals. Granted they had to manually label certain aspects of user accounts but they could have opted to create a script that mine official and non-official profiles in the field of the tweet (flu, medical) and extracted content in the size of thousands. The paper spread almost two pages to explain what is entropy and calculation matrix which is described thoroughly in previous works. Furthermore, I believe analyzing images in social media interactions merely with its RGB aspects is lacking. There are numerous precious computer vision / social media analytics papers that explain the exact opposite of this and that content is important, what is in the image is important than even its size, color scheme or filters applied. Black and white and greyscale images are proof of that. Lastly the trends they picked were fairly limited, it would have been tremendously exciting to see several other ""current"" trails and the differences between them, and analysis of the origins of the trails and how they pick up speed in retweeting or die, that would give invaluable insight with their powerful application of entropy. 

I would advise the authors to expand more on their work, there is great potential in it

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would suggest a reapplication of this same paper as a short paper due to its novelty.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-17,no
217,62,324,Haxx Salaxxxxxxx,3,"(OVERALL EVALUATION) The authors present the problem of analyzing and remodeling digital social media platforms in the eyes of information theory. Namely, the authors propose a novel analytical framework of analyzing social media data stream (Twitter) using the entropic method of Claude Shannon's information theory.

The flow of the paper is good and it is clear to a large extend, and the authors placed considerable amount of related work throughout the paper going back to works published in 1924. There was a little bit of repetitiveness throughout while describing information theory in general and Shannon's work. The idea is rather novel and quite interesting but I have some points of concern as follows.

In their case study, the authors extracted a minute dataset totaling 300 tweets and further sub divided them into two sub datasets, for the period of just 72 hours. In the realm of twitter this is very small to extract most meaningful signals. Granted they had to manually label certain aspects of user accounts but they could have opted to create a script that mine official and non-official profiles in the field of the tweet (flu, medical) and extracted content in the size of thousands. The paper spread almost two pages to explain what is entropy and calculation matrix which is described thoroughly in previous works. Furthermore, I believe analyzing images in social media interactions merely with its RGB aspects is lacking. There are numerous precious computer vision / social media analytics papers that explain the exact opposite of this and that content is important, what is in the image is important than even its size, color scheme or filters applied. Black and white and greyscale images are proof of that. Lastly the trends they picked were fairly limited, it would have been tremendously exciting to see several other ""current"" trails and the differences between them, and analysis of the origins of the trails and how they pick up speed in retweeting or die, that would give invaluable insight with their powerful application of entropy.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-17,08:59,no,,217,62,324,Hany Salaheldeen,3,"(OVERALL EVALUATION) The authors present the problem of analyzing and remodeling digital social media platforms in the eyes of information theory. Namely, the authors propose a novel analytical framework of analyzing social media data stream (Twitter) using the entropic method of Claude Shannon's information theory.

The flow of the paper is good and it is clear to a large extend, and the authors placed considerable amount of related work throughout the paper going back to works published in 1924. There was a little bit of repetitiveness throughout while describing information theory in general and Shannon's work. The idea is rather novel and quite interesting but I have some points of concern as follows.

In their case study, the authors extracted a minute dataset totaling 300 tweets and further sub divided them into two sub datasets, for the period of just 72 hours. In the realm of twitter this is very small to extract most meaningful signals. Granted they had to manually label certain aspects of user accounts but they could have opted to create a script that mine official and non-official profiles in the field of the tweet (flu, medical) and extracted content in the size of thousands. The paper spread almost two pages to explain what is entropy and calculation matrix which is described thoroughly in previous works. Furthermore, I believe analyzing images in social media interactions merely with its RGB aspects is lacking. There are numerous precious computer vision / social media analytics papers that explain the exact opposite of this and that content is important, what is in the image is important than even its size, color scheme or filters applied. Black and white and greyscale images are proof of that. Lastly the trends they picked were fairly limited, it would have been tremendously exciting to see several other ""current"" trails and the differences between them, and analysis of the origins of the trails and how they pick up speed in retweeting or die, that would give invaluable insight with their powerful application of entropy.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-17,no
218,62,324,Haxx Salaxxxxxxx,3,"(OVERALL EVALUATION) The authors present the problem of analyzing and remodeling digital social media platforms in the eyes of information theory. Namely, the authors propose a novel analytical framework of analyzing social media data stream (Twitter) using the entropic method of Claude Shannon's information theory.

The flow of the paper is good and it is clear to a large extend, and the authors placed considerable amount of related work throughout the paper going back to works published in 1924. There was a little bit of repetitiveness throughout while describing information theory in general and Shannon's work. The idea is rather novel and quite interesting but I have some points of concern as follows.

In their case study, the authors extracted a minute dataset totaling 300 tweets and further sub divided them into two sub datasets, for the period of just 72 hours. In the realm of twitter this is very small to extract most meaningful signals. Granted they had to manually label certain aspects of user accounts but they could have opted to create a script that mine official and non-official profiles in the field of the tweet (flu, medical) and extracted content in the size of thousands. The paper spread almost two pages to explain what is entropy and calculation matrix which is described thoroughly in previous works. Furthermore, I believe analyzing images in social media interactions merely with its RGB aspects is lacking. There are numerous precious computer vision / social media analytics papers that explain the exact opposite of this and that content is important, what is in the image is important than even its size, color scheme or filters applied. Black and white and greyscale images are proof of that. Lastly the trends they picked were fairly limited, it would have been tremendously exciting to see several other ""current"" trails and the differences between them, and analysis of the origins of the trails and how they pick up speed in retweeting or die, that would give invaluable insight with their powerful application of entropy.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2018-02-17,09:19,no,,218,62,324,Hany Salaheldeen,3,"(OVERALL EVALUATION) The authors present the problem of analyzing and remodeling digital social media platforms in the eyes of information theory. Namely, the authors propose a novel analytical framework of analyzing social media data stream (Twitter) using the entropic method of Claude Shannon's information theory.

The flow of the paper is good and it is clear to a large extend, and the authors placed considerable amount of related work throughout the paper going back to works published in 1924. There was a little bit of repetitiveness throughout while describing information theory in general and Shannon's work. The idea is rather novel and quite interesting but I have some points of concern as follows.

In their case study, the authors extracted a minute dataset totaling 300 tweets and further sub divided them into two sub datasets, for the period of just 72 hours. In the realm of twitter this is very small to extract most meaningful signals. Granted they had to manually label certain aspects of user accounts but they could have opted to create a script that mine official and non-official profiles in the field of the tweet (flu, medical) and extracted content in the size of thousands. The paper spread almost two pages to explain what is entropy and calculation matrix which is described thoroughly in previous works. Furthermore, I believe analyzing images in social media interactions merely with its RGB aspects is lacking. There are numerous precious computer vision / social media analytics papers that explain the exact opposite of this and that content is important, what is in the image is important than even its size, color scheme or filters applied. Black and white and greyscale images are proof of that. Lastly the trends they picked were fairly limited, it would have been tremendously exciting to see several other ""current"" trails and the differences between them, and analysis of the origins of the trails and how they pick up speed in retweeting or die, that would give invaluable insight with their powerful application of entropy.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2018-02-17,no
221,63,145,Vixxx Goxx,2,"(OVERALL EVALUATION) The paper presents a machine learning / sentiment analysis based approach of mining web pages titles to predict the closing prices of specific cryptocurrencies.

- presents an idea of filling in gaps in cases of missing data, but no discussion of other possible data imputation approaches.

- the experiments are limited in scope and i'm not confident in the generalizability of the proposed approach

Would recommend that the authors build on the proposed work.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-16,20:02,no,,221,63,145,Vinay Goel,2,"(OVERALL EVALUATION) The paper presents a machine learning / sentiment analysis based approach of mining web pages titles to predict the closing prices of specific cryptocurrencies.

- presents an idea of filling in gaps in cases of missing data, but no discussion of other possible data imputation approaches.

- the experiments are limited in scope and i'm not confident in the generalizability of the proposed approach

Would recommend that the authors build on the proposed work.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-16,no
228,65,134,Nixxx Ganxxxx,1,"(OVERALL EVALUATION) The paper is out of scope.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am not sure how Folksonomy indexing is related with digital library.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-05,04:12,no,,228,65,134,Niloy Ganguly,1,"(OVERALL EVALUATION) The paper is out of scope.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am not sure how Folksonomy indexing is related with digital library.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-05,no
237,67,7,Alxxx Abduxxxxxxxx,3,"(OVERALL EVALUATION) This paper examines the impact of pre-existing highlights on digital reading process through eye-tracking and interviews. The paper is well-written with interesting results on pre-existing highlights. There is a comprehensive coverage on prior work and a good discussion section on the results obtained.

There is a missing reference: Guidelines for Effective Usage of Text Highlighting Techniques (http://ieeexplore.ieee.org/document/7192718/).

Perhaps, because of the page limit there is a limited coverage on the study design such as the device used to conduct the study, an example of the texts used in the study, and the study set-up. This is important as it would enable future readers (for example, graduate students) to recreate the study.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-14,15:06,no,,237,67,7,Alfie Abdul-Rahman,3,"(OVERALL EVALUATION) This paper examines the impact of pre-existing highlights on digital reading process through eye-tracking and interviews. The paper is well-written with interesting results on pre-existing highlights. There is a comprehensive coverage on prior work and a good discussion section on the results obtained.

There is a missing reference: Guidelines for Effective Usage of Text Highlighting Techniques (http://ieeexplore.ieee.org/document/7192718/).

Perhaps, because of the page limit there is a limited coverage on the study design such as the device used to conduct the study, an example of the texts used in the study, and the study set-up. This is important as it would enable future readers (for example, graduate students) to recreate the study.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-14,no
239,68,28,Omxx Alxxxx,1,"(OVERALL EVALUATION) The poster describes a large census transcription project using crowdsourcing. The idea is very interesting with lots of potential for researchers and other government agencies. 

The author provides a good description of the problems, shortcomings, and overall road map. The poster is a bit weak in terms of data and other statistics, which can help assess the complexity of the problem.

The project is somewhat similar to the work by Hansen et al. on FamilySearch (record transcription on a different domain) https://dl.acm.org/citation.cfm?doid=2441776.2441848. Hansen describes arbitration and peer review as patterns for this type of work and the author should compare. That would be a great contribution or a future publication.

Overall, nice idea but lacks data and other implementation details.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Nice idea. Would like to see more data/details :)","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-01,23:23,no,,239,68,28,Omar Alonso,1,"(OVERALL EVALUATION) The poster describes a large census transcription project using crowdsourcing. The idea is very interesting with lots of potential for researchers and other government agencies. 

The author provides a good description of the problems, shortcomings, and overall road map. The poster is a bit weak in terms of data and other statistics, which can help assess the complexity of the problem.

The project is somewhat similar to the work by Hansen et al. on FamilySearch (record transcription on a different domain) https://dl.acm.org/citation.cfm?doid=2441776.2441848. Hansen describes arbitration and peer review as patterns for this type of work and the author should compare. That would be a great contribution or a future publication.

Overall, nice idea but lacks data and other implementation details.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Nice idea. Would like to see more data/details :)","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-01,no
242,68,188,Antxxxx Isxxx,3,"(OVERALL EVALUATION) This paper presents the OpenGenealogyData project, which aims at crowdsourcing trasnscriptions (first of the 1930 US census) and releasing the data openly for research. Efforts are made to make the transcribers' work easier, notably by helping them transcribe less and better (with advanced OCR and som heuristics to auto-fill data) and presenting them with parts of the census they may be better 'connected' to (by their location, family history, or search interest).

I am a bit split over whether this paper fits the JCDL standards. On the one hand, this is an intersting project, with a good motivation, a clear technology path and some choices made, which are very practical. The more 'advanced' parts of the project (recommendations for crowdsourcing) seems very reasonable, and evaluation is planned.
On the other hand, the project is in a very early stage, there is no data apparently produced at all, nor a demo. This could be ok for a short paper, but there are some areas with the project seems really pre-mature, especially:
- The 'market-focused' approach that brings some positive guidance for the design of the crowdsourcing platform backfires in the sense that there seems to be not much awareness of work done outside of the US. This reviewer is not an expert on censuses, but in the Netherlands, for example, http://www.volkstellingen.nl/ makes available OCRed tables from Dutch censuses. The CEDAR project has also ventured into further formalizing the data for exploitation by researches, and it ran into quite many problems, which could bring a valuable horizon for the OpenGenData project. 
- For all the emphasis on 'open' data, it is not clear under which conditions the data are going to be released, and whether this is really going to be open in the sense of e.g. opendefinition.org. HistoricJournals, the orgnization running the project, has a lot of 'free' services but that is not necessarily open. It took some exploration to find CC0 mentioned at https://opengendata.org/terms.html. This is great but could be more visible. It would be good to have more details on the papers and - why not - a brief discussion on the challenges for such an approach, especially in a crowdsourcing context - transcribers would be expect to contribute for really nothing, which can be envisioned but needs quite some motivation.

Maybe some of the huge space taken by the figures (Fig could be sampled, and fig 2 has a bit of repetition) could be used to add some elements long these lines.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-16,17:06,no,,242,68,188,Antoine Isaac,3,"(OVERALL EVALUATION) This paper presents the OpenGenealogyData project, which aims at crowdsourcing trasnscriptions (first of the 1930 US census) and releasing the data openly for research. Efforts are made to make the transcribers' work easier, notably by helping them transcribe less and better (with advanced OCR and som heuristics to auto-fill data) and presenting them with parts of the census they may be better 'connected' to (by their location, family history, or search interest).

I am a bit split over whether this paper fits the JCDL standards. On the one hand, this is an intersting project, with a good motivation, a clear technology path and some choices made, which are very practical. The more 'advanced' parts of the project (recommendations for crowdsourcing) seems very reasonable, and evaluation is planned.
On the other hand, the project is in a very early stage, there is no data apparently produced at all, nor a demo. This could be ok for a short paper, but there are some areas with the project seems really pre-mature, especially:
- The 'market-focused' approach that brings some positive guidance for the design of the crowdsourcing platform backfires in the sense that there seems to be not much awareness of work done outside of the US. This reviewer is not an expert on censuses, but in the Netherlands, for example, http://www.volkstellingen.nl/ makes available OCRed tables from Dutch censuses. The CEDAR project has also ventured into further formalizing the data for exploitation by researches, and it ran into quite many problems, which could bring a valuable horizon for the OpenGenData project. 
- For all the emphasis on 'open' data, it is not clear under which conditions the data are going to be released, and whether this is really going to be open in the sense of e.g. opendefinition.org. HistoricJournals, the orgnization running the project, has a lot of 'free' services but that is not necessarily open. It took some exploration to find CC0 mentioned at https://opengendata.org/terms.html. This is great but could be more visible. It would be good to have more details on the papers and - why not - a brief discussion on the challenges for such an approach, especially in a crowdsourcing context - transcribers would be expect to contribute for really nothing, which can be envisioned but needs quite some motivation.

Maybe some of the huge space taken by the figures (Fig could be sampled, and fig 2 has a bit of repetition) could be used to add some elements long these lines.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-16,no
250,71,36,Joxxxx Bexx,2,"(OVERALL EVALUATION) The authors propose a method for time-aware query suggestions, and evaluate their approach on some AOL logs from 2006. Besides the fact that I am sceptical how relevant an evaluation based on 10-year old data is (at that time, search behaviour was quite different to search behavior nowadays), the work does not relate to digital libraries. I therefore consider the paper to be out of scope of the JCDL conference, and suggest a 'reject'. 

I suggest to re-submit this paper to an IR conference, or re-do the evaluation with data relating to digital libraries, and then re-submit to JCDL next year (or to the upcoming TPDL).","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-15,09:22,no,,250,71,36,Joeran Beel,2,"(OVERALL EVALUATION) The authors propose a method for time-aware query suggestions, and evaluate their approach on some AOL logs from 2006. Besides the fact that I am sceptical how relevant an evaluation based on 10-year old data is (at that time, search behaviour was quite different to search behavior nowadays), the work does not relate to digital libraries. I therefore consider the paper to be out of scope of the JCDL conference, and suggest a 'reject'. 

I suggest to re-submit this paper to an IR conference, or re-do the evaluation with data relating to digital libraries, and then re-submit to JCDL next year (or to the upcoming TPDL).","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-15,no
254,73,178,Hexxx Hocxxxxx,1,"(OVERALL EVALUATION) This paper is relevant to a number of topics in the JCDL 2018 cfp, including e.g. data curation/stewardship, information retrieval and preservation. It is a well-structured and well–written paper, which introduces a new framework for aggregating private, personal and public web archives while addressing information privacy. It also includes detailed descriptions of how this can be achieved by amending the Memento syntax and including additional semantics. The framework allows personal web archiving effort to be ultilised to provide a more representative impression of the historical web. The methods and the discussions are both sound and comprehensive.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-14,15:17,no,,254,73,178,Helen Hockx-Yu,1,"(OVERALL EVALUATION) This paper is relevant to a number of topics in the JCDL 2018 cfp, including e.g. data curation/stewardship, information retrieval and preservation. It is a well-structured and well–written paper, which introduces a new framework for aggregating private, personal and public web archives while addressing information privacy. It also includes detailed descriptions of how this can be achieved by amending the Memento syntax and including additional semantics. The framework allows personal web archiving effort to be ultilised to provide a more representative impression of the historical web. The methods and the discussions are both sound and comprehensive.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-14,no
260,74,88,Gioxxxx Maxxx,2,"(OVERALL EVALUATION) This paper presents a study on the impact of OCR corrections on the retrievability of documents.
The paper is well written and well organised.
My only concern is about the impact of the results: the strong correlation between OCR errors and retrievability is more than predictable, the quantification of the effect is appreciable though.

Please find hereby a few suggestions to improve some parts of the paper:

In Section 4.1, I would present the original corpus (the one with errors) *before* the corrected corpus. 

Figures are nicely plotted (I appreciated the histograms at the two sides of the plot) but the shape of the marker does not help to visualise where the different categories of points are (it is difficult to detect any difference between to crosses that have a very small difference in size). I'd suggest to use different shapes, not different sizes.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-15,14:42,no,,260,74,88,Giorgio Maria Di Nunzio,2,"(OVERALL EVALUATION) This paper presents a study on the impact of OCR corrections on the retrievability of documents.
The paper is well written and well organised.
My only concern is about the impact of the results: the strong correlation between OCR errors and retrievability is more than predictable, the quantification of the effect is appreciable though.

Please find hereby a few suggestions to improve some parts of the paper:

In Section 4.1, I would present the original corpus (the one with errors) *before* the corrected corpus. 

Figures are nicely plotted (I appreciated the histograms at the two sides of the plot) but the shape of the marker does not help to visualise where the different categories of points are (it is difficult to detect any difference between to crosses that have a very small difference in size). I'd suggest to use different shapes, not different sizes.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-15,no
264,75,391,Jenxxxxxx Waxx,2,"(OVERALL EVALUATION) This paper proposes to re-rank the scores of keyphrases extracted by 
existing keyphrase extraction methods by TF-IDF weights.

In general, the method is not innovative.
Also, th experimental results cannot justify the contribution of the paper.

The major issues include the following:

1. There's a lack of novelty in the keyphrase scoring approach in sec.2.
Eq.(2) adds a penalty term in the token score, but the effects were not evaluated in the experiments.
It's not convincing to add such a term.

2. The major contribution of the paper is not clear.
Although it's identified in Fig.1, there's no corresponding evaluation in the experiments.
For example, how would you compare the different keyphrases extraction methods in this framework?

3. The experimental results in Sec.3 are not convincing.
Specifically, Figs.2 & 3 compare the results by the proposed ranking and random ranking.
Is there a ranking for Rake and other methods?
If so, why not compare with Rake and other keyphrase extraction methods?

4. It's not clear how to deal with multiple keyphrase extraction methods 
with the proposed ranking.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-19,03:27,no,,264,75,391,Jenq-Haur Wang,2,"(OVERALL EVALUATION) This paper proposes to re-rank the scores of keyphrases extracted by 
existing keyphrase extraction methods by TF-IDF weights.

In general, the method is not innovative.
Also, th experimental results cannot justify the contribution of the paper.

The major issues include the following:

1. There's a lack of novelty in the keyphrase scoring approach in sec.2.
Eq.(2) adds a penalty term in the token score, but the effects were not evaluated in the experiments.
It's not convincing to add such a term.

2. The major contribution of the paper is not clear.
Although it's identified in Fig.1, there's no corresponding evaluation in the experiments.
For example, how would you compare the different keyphrases extraction methods in this framework?

3. The experimental results in Sec.3 are not convincing.
Specifically, Figs.2 & 3 compare the results by the proposed ranking and random ranking.
Is there a ranking for Rake and other methods?
If so, why not compare with Rake and other keyphrase extraction methods?

4. It's not clear how to deal with multiple keyphrase extraction methods 
with the proposed ranking.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-19,no
274,77,234,Jxx Hx,4,"(OVERALL EVALUATION) This paper investigates online discussion forums as a potential source for extracting answers to user questions, specifically focusing on ResearchGate website. While the general idea behind the study is quite interesting, I think this paper can really benefit from authors explaining some of the details better: for instance, the authors state that KL-divergence model is the appropriate one rather than using Cosine Similarity or Query likelihood model, but why? They explain why the other two are not appropriate but does not tell the readers why the KL-divergence model is, other than stating that it is appropriate. The authors provide a fairly detailed explanation on how the answers are extracted from the questions through multiple steps but I am unclear about how how exactly they determined that the answers were “right” or “wrong”. The earlier sections talk about determining the existence of an answer and also about different aspects examined to determine the text quality, but that does not tell us if the answer is accurate or not. Could another approach (maybe a crowdsourced approach) help better determine the accuracy of the answers in the evaluation phase? 

Now the minor (but still important) points:

Really consider putting the author names before the bracketed number in the text. Simply referring to the numbered item in the sentence makes it difficult to read sentences like "" Other similar works include [12] that fused different templates using SVM
for finding answers to questions and [29] presented the combination of
structured (ontological)..."". Also please spell out any acronyms before you use it, like QB or PB in the first page. Lastly, if this paper gets accepted, it needs to be carefully proofread as there are numerous typos in the paper.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-17,00:36,no,,274,77,234,Jin Ha Lee,4,"(OVERALL EVALUATION) This paper investigates online discussion forums as a potential source for extracting answers to user questions, specifically focusing on ResearchGate website. While the general idea behind the study is quite interesting, I think this paper can really benefit from authors explaining some of the details better: for instance, the authors state that KL-divergence model is the appropriate one rather than using Cosine Similarity or Query likelihood model, but why? They explain why the other two are not appropriate but does not tell the readers why the KL-divergence model is, other than stating that it is appropriate. The authors provide a fairly detailed explanation on how the answers are extracted from the questions through multiple steps but I am unclear about how how exactly they determined that the answers were “right” or “wrong”. The earlier sections talk about determining the existence of an answer and also about different aspects examined to determine the text quality, but that does not tell us if the answer is accurate or not. Could another approach (maybe a crowdsourced approach) help better determine the accuracy of the answers in the evaluation phase? 

Now the minor (but still important) points:

Really consider putting the author names before the bracketed number in the text. Simply referring to the numbered item in the sentence makes it difficult to read sentences like "" Other similar works include [12] that fused different templates using SVM
for finding answers to questions and [29] presented the combination of
structured (ontological)..."". Also please spell out any acronyms before you use it, like QB or PB in the first page. Lastly, if this paper gets accepted, it needs to be carefully proofread as there are numerous typos in the paper.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-17,no
278,78,143,Bexx Gixx,3,"(OVERALL EVALUATION) An interesting paper with an extremely ambitious goal that is relevant to the JCDL, but the title is misleading. Instead of ""Modeling Author Contribution Rate with Blockchain"", a better fitting title would be ""Storing the individual EDITING contribution using Blockchain Technology"".

The described approach does not measure the individual contribution, but only the extent to which each author edited a paper (e.g. measured by the number of typed characters). However, writing/editing is just one form of contributing to a paper besides e.g. the data analysis, evaluation etc. This is briefly discussed, but not sufficiently considered.

The paper would benefit from discussing the research questions separately: 
1.) How can the individual author contributions be automatically measured?
2.) How can this information be stored in a tamperproof manner and linked to a given paper?
 
Regarding 1.) the authors offer an interesting discussion, but not a convincing solution. However, the authors are aware of these limitations.
Regarding 2.) the authors present a possible solution, but it remains unclear why blockchain technology is required for this. Why not just add a note to the paper itself? E.g. as percentages behind the author names: Peter Jackson (70), Julia Vogt (20), James Luck (10).

Nevertheless, I like the general idea of the paper and see the potential for this paper if the points above are more convincingly addressed.

The paper is well written and also for non-experts easy to understand.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-16,20:48,no,,278,78,143,Bela Gipp,3,"(OVERALL EVALUATION) An interesting paper with an extremely ambitious goal that is relevant to the JCDL, but the title is misleading. Instead of ""Modeling Author Contribution Rate with Blockchain"", a better fitting title would be ""Storing the individual EDITING contribution using Blockchain Technology"".

The described approach does not measure the individual contribution, but only the extent to which each author edited a paper (e.g. measured by the number of typed characters). However, writing/editing is just one form of contributing to a paper besides e.g. the data analysis, evaluation etc. This is briefly discussed, but not sufficiently considered.

The paper would benefit from discussing the research questions separately: 
1.) How can the individual author contributions be automatically measured?
2.) How can this information be stored in a tamperproof manner and linked to a given paper?
 
Regarding 1.) the authors offer an interesting discussion, but not a convincing solution. However, the authors are aware of these limitations.
Regarding 2.) the authors present a possible solution, but it remains unclear why blockchain technology is required for this. Why not just add a note to the paper itself? E.g. as percentages behind the author names: Peter Jackson (70), Julia Vogt (20), James Luck (10).

Nevertheless, I like the general idea of the paper and see the potential for this paper if the points above are more convincingly addressed.

The paper is well written and also for non-experts easy to understand.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2018-02-16,no
281,79,35,Joxxxx Bexx,2,"(OVERALL EVALUATION) The authors introduce a novel concept for tag recommendations. While this is an interesting topic for digital libraries, the authors' work does not relate to libraries. The authors focus on StackOverflow, which is a community-based question/answering platform, primarily for software developers. Consequently, i see this paper out of the scope of the JCDL. 

I suggest to re-submit this paper to an IR conference, or re-do the evaluation with data relating to digital libraries and then re-submit to JCDL next year (or to the upcoming TPDL).

Just a side note to the authors: I am not sure if ""Content-cum-Network"" is a good name for your approach. I suggest looking up the meaning of ""cum"" in the Urban dictionary.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-15,09:01,no,,281,79,35,Joeran Beel,2,"(OVERALL EVALUATION) The authors introduce a novel concept for tag recommendations. While this is an interesting topic for digital libraries, the authors' work does not relate to libraries. The authors focus on StackOverflow, which is a community-based question/answering platform, primarily for software developers. Consequently, i see this paper out of the scope of the JCDL. 

I suggest to re-submit this paper to an IR conference, or re-do the evaluation with data relating to digital libraries and then re-submit to JCDL next year (or to the upcoming TPDL).

Just a side note to the authors: I am not sure if ""Content-cum-Network"" is a good name for your approach. I suggest looking up the meaning of ""cum"" in the Urban dictionary.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-15,no
289,81,341,Giaxxxxxx Silxxxxx,3,"(OVERALL EVALUATION) This paper proposes a measure for ranking Universities based on popularity on Twitter - i.e. the UTE. 

I do not see much DL-related contribution here; this looks more like a correlation exercise between a self-defined measure and (debatable) academic rankings. 

One major concern: Ranking Universities on a popularity basis is a risky game and, even though there are some references in the related work section, the authors are not critical enough with this practice. The negative effects of such rankings have been quite recently reported in the literature and also in a best-seller book (weapons of math destruction), which effectively underlines the bias introduced by this practice. 
I do not see any value in adding such a measure even though the paper represents a well conducted exercise. 

Moreover, as future work the authors state: ""Our study is subject to a number of limitations that present opportunities for future work. Campbell’s and Goodhart's law suggest that if UTE becomes popular, institutions may seek to artificially increase their Twitter followers in order to increase their ranking.""
To me this is the core of the problem and it should be discussed right away and not as future work. If this topic is not addressed the whole UTE has very little use. 

The authors state that ""We did not address known issues with bots and spam accounts which may over in ate the stated number of Twitter followers, the primary component of our UTE score"". This is a considerable problem with this kind of study since spam accounts and bot may hugely inflate the ""popularity"" of an institution. 

I am not sure about the ""high reproducibility"" of the study since, just to mention one possibility, tweets can be deleted. It is true that if we store all the tweets in a local archive, we can reproduce the study, but this is not well-described in the paper. The Twetter API has a policy that has to be taken into account: https://developer.twitter.com/en/developer-terms/policy 

There are also issues related to the redistribution of Twitter content https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases
how do the authors deal with this and the reproducibility of the study? 

These are relevant questions for the whole community and the matter should be thoroughly discussed in general. Nevertheless, I would be more prudent about the high reproducibility of this study.

Anyway, the analysis may be of interest to investigate the effect of social media on public perception (a radically different work with a different goal).","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-19,08:40,no,,289,81,341,Gianmaria Silvello,3,"(OVERALL EVALUATION) This paper proposes a measure for ranking Universities based on popularity on Twitter - i.e. the UTE. 

I do not see much DL-related contribution here; this looks more like a correlation exercise between a self-defined measure and (debatable) academic rankings. 

One major concern: Ranking Universities on a popularity basis is a risky game and, even though there are some references in the related work section, the authors are not critical enough with this practice. The negative effects of such rankings have been quite recently reported in the literature and also in a best-seller book (weapons of math destruction), which effectively underlines the bias introduced by this practice. 
I do not see any value in adding such a measure even though the paper represents a well conducted exercise. 

Moreover, as future work the authors state: ""Our study is subject to a number of limitations that present opportunities for future work. Campbell’s and Goodhart's law suggest that if UTE becomes popular, institutions may seek to artificially increase their Twitter followers in order to increase their ranking.""
To me this is the core of the problem and it should be discussed right away and not as future work. If this topic is not addressed the whole UTE has very little use. 

The authors state that ""We did not address known issues with bots and spam accounts which may over in ate the stated number of Twitter followers, the primary component of our UTE score"". This is a considerable problem with this kind of study since spam accounts and bot may hugely inflate the ""popularity"" of an institution. 

I am not sure about the ""high reproducibility"" of the study since, just to mention one possibility, tweets can be deleted. It is true that if we store all the tweets in a local archive, we can reproduce the study, but this is not well-described in the paper. The Twetter API has a policy that has to be taken into account: https://developer.twitter.com/en/developer-terms/policy 

There are also issues related to the redistribution of Twitter content https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases
how do the authors deal with this and the reproducibility of the study? 

These are relevant questions for the whole community and the matter should be thoroughly discussed in general. Nevertheless, I would be more prudent about the high reproducibility of this study.

Anyway, the analysis may be of interest to investigate the effect of social media on public perception (a radically different work with a different goal).","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2018-02-19,no
294,82,137,Danxxxx Gixx,3,"(OVERALL EVALUATION) This article presents a method based on Open Information Extraction, a well known clustering tehnique, in order to extract automatically the authors’ roles from scientific publications using PubMed Central resources. However, this paper is already published in https://arxiv.org/pdf/1802.01174.pdf (Dominika Tkaczyk, Andrew Collins, and Joeran Beel). Consequently, I have to reject the presentation of this paper at JCDL 2018.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper is already published in https://arxiv.org/pdf/1802.01174.pdf (Dominika Tkaczyk, Andrew Collins, and Joeran Beel). The first two authors, at the most important Conference on Digital Libraries, JCDL, are at the second such deviation (paper 84), publishing twice the same paper, with different title. I wonder if this behaviour is a normal practice in that institution? However, it is inadmissible.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2018-02-20,20:10,no,,294,82,137,Daniela Gifu,3,"(OVERALL EVALUATION) This article presents a method based on Open Information Extraction, a well known clustering tehnique, in order to extract automatically the authors’ roles from scientific publications using PubMed Central resources. However, this paper is already published in https://arxiv.org/pdf/1802.01174.pdf (Dominika Tkaczyk, Andrew Collins, and Joeran Beel). Consequently, I have to reject the presentation of this paper at JCDL 2018.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper is already published in https://arxiv.org/pdf/1802.01174.pdf (Dominika Tkaczyk, Andrew Collins, and Joeran Beel). The first two authors, at the most important Conference on Digital Libraries, JCDL, are at the second such deviation (paper 84), publishing twice the same paper, with different title. I wonder if this behaviour is a normal practice in that institution? However, it is inadmissible.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2018-02-20,no
298,83,199,Jaxxx Jexx,2,"(OVERALL EVALUATION) The authors do an excellent job of describing a new method for linking contextually-sensitive role information to entity-links as means to refine and enrich them. It would be interesting to see this method integrated into existing entity-linking tools and resources for exploitation by digital libraries already deeply engaged with various entity-linking, feature extracting, and computational analytics processes.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-13,22:14,no,,298,83,199,Jacob Jett,2,"(OVERALL EVALUATION) The authors do an excellent job of describing a new method for linking contextually-sensitive role information to entity-links as means to refine and enrich them. It would be interesting to see this method integrated into existing entity-linking tools and resources for exploitation by digital libraries already deeply engaged with various entity-linking, feature extracting, and computational analytics processes.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-13,no
299,83,199,Jaxxx Jexx,2,(OVERALL EVALUATION) (This review was transferred to the metareview),"Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-23,01:46,no,,299,83,199,Jacob Jett,2,(OVERALL EVALUATION) (This review was transferred to the metareview),"Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2018-02-23,no
300,83,199,Jaxxx Jexx,2,"(OVERALL EVALUATION) This paper describes a new method for linking contextually-sensitive role information to entity-links as means to refine and enrich them. While the reviewers had some reservations regarding the level of detail the authors provide with regards to: how manual assessments were carried out and the positive/negative/false positive/false negative rates of the AL method used for the computational analysis carried out.

Overall: Accept

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Min please replace the existing metareview with this one. For some reason EasyChair is not allowing me to revise the metareviews.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-23,15:39,no,,300,83,199,Jacob Jett,2,"(OVERALL EVALUATION) This paper describes a new method for linking contextually-sensitive role information to entity-links as means to refine and enrich them. While the reviewers had some reservations regarding the level of detail the authors provide with regards to: how manual assessments were carried out and the positive/negative/false positive/false negative rates of the AL method used for the computational analysis carried out.

Overall: Accept

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Min please replace the existing metareview with this one. For some reason EasyChair is not allowing me to revise the metareviews.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-23,no
302,83,249,Byxxx Marxxxxx,3,"(OVERALL EVALUATION) This paper explores a technique for contextualizing selected named-entity references in a document intending to support better retrieval and analysis.
This is certainly an important topic and quite relevant to JCDL.
It seems that the works was done systematically using appropriate techniques.
I a few concerns that would cause me to request a revision if this were a journal article.
The dataset is not as well-described as I would like. What kinds of 'aspects' were considered in developing the gold standard? It is not clear to me whether the somewhat limited set of potential target aspects (averaging 7 per section-link as per section 4) are meaningful for this data set or how such lists would be developed for other datasets or tasks.
The narrative does not provide sufficient examples. The one politically-focused example used throughout does not provide sufficient insight into the kinds of 'aspects' that might appropriately be identified using this technique.
The testing focuses on a 'right or wrong' assignment. It is not clear to me that this is appropriate for the kinds of analytical tasks a user would have in mind. For example, a given reference could, using the author's example, be relevant to both Hillary Clinton's time as secretary of state and the subsequent election. Many such aspect differences would likely be relative. That is, a given entity reference might relate to one aspect to one degree and to several others to a lesser degree. The write up does not explain the computational outputs of the algorithm so it is not clear if the outputs are binary decisions or weights. This limits the reader’s ability to assess the value of the approach.
If I understood correctly, this binary modality is implicit in: the assessment, the gold standard, and in the target variable (P@1).
Without explaining the user task, it is hard to know whether the results are interesting or compelling.

Also, methodologically, Section 4 says ""we have manually assessed the correct relation between context and linked section."" I am not really sure what that means. If such judgments were part of creating a gold standard, the method of assignment and details about such assignments (i.e. inter-rater reliability) should be reported.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-15,19:46,no,,302,83,249,Byron Marshall,3,"(OVERALL EVALUATION) This paper explores a technique for contextualizing selected named-entity references in a document intending to support better retrieval and analysis.
This is certainly an important topic and quite relevant to JCDL.
It seems that the works was done systematically using appropriate techniques.
I a few concerns that would cause me to request a revision if this were a journal article.
The dataset is not as well-described as I would like. What kinds of 'aspects' were considered in developing the gold standard? It is not clear to me whether the somewhat limited set of potential target aspects (averaging 7 per section-link as per section 4) are meaningful for this data set or how such lists would be developed for other datasets or tasks.
The narrative does not provide sufficient examples. The one politically-focused example used throughout does not provide sufficient insight into the kinds of 'aspects' that might appropriately be identified using this technique.
The testing focuses on a 'right or wrong' assignment. It is not clear to me that this is appropriate for the kinds of analytical tasks a user would have in mind. For example, a given reference could, using the author's example, be relevant to both Hillary Clinton's time as secretary of state and the subsequent election. Many such aspect differences would likely be relative. That is, a given entity reference might relate to one aspect to one degree and to several others to a lesser degree. The write up does not explain the computational outputs of the algorithm so it is not clear if the outputs are binary decisions or weights. This limits the reader’s ability to assess the value of the approach.
If I understood correctly, this binary modality is implicit in: the assessment, the gold standard, and in the target variable (P@1).
Without explaining the user task, it is hard to know whether the results are interesting or compelling.

Also, methodologically, Section 4 says ""we have manually assessed the correct relation between context and linked section."" I am not really sure what that means. If such judgments were part of creating a gold standard, the method of assignment and details about such assignments (i.e. inter-rater reliability) should be reported.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-15,no
305,84,137,Danxxxx Gixx,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case. However, it was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library, https://arxiv.org/abs/1802.01168. Like I mentioned already, I am so disappointed to discover this practice to try to republish the same article (only the title is different) at the most important Conference on Digital Libraries...","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-14,12:57,no,,305,84,137,Daniela Gifu,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case. However, it was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library, https://arxiv.org/abs/1802.01168. Like I mentioned already, I am so disappointed to discover this practice to try to republish the same article (only the title is different) at the most important Conference on Digital Libraries...","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2018-02-14,no
306,84,137,Danxxxx Gixx,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case. However, it was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library. I recommend to the authors have to change/formulate all the content. And, it needs more clarity in the motivation of their survey and, implicitly, the business case.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library, https://arxiv.org/abs/1802.01168. Like I mentioned already, I am so disappointed to discover this practice to try to republish the same article (only the title is different) at the most important Conference on Digital Libraries...","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-23,11:48,no,,306,84,137,Daniela Gifu,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case. However, it was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library. I recommend to the authors have to change/formulate all the content. And, it needs more clarity in the motivation of their survey and, implicitly, the business case.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library, https://arxiv.org/abs/1802.01168. Like I mentioned already, I am so disappointed to discover this practice to try to republish the same article (only the title is different) at the most important Conference on Digital Libraries...","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-23,no
307,84,137,Danxxxx Gixx,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case. However, it was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library. I recommend to the authors have to change/formulate all the content. And, it needs more clarity in the motivation of their survey and, implicitly, the business case.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I revised my score on this paper from  -3 to -1 so that my review does not appear too negativ - and to contribute towards reviewer agreement. I recommend to the authors have to change/formulate all the content..","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-23,11:52,no,,307,84,137,Daniela Gifu,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case. However, it was already published in Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library. I recommend to the authors have to change/formulate all the content. And, it needs more clarity in the motivation of their survey and, implicitly, the business case.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I revised my score on this paper from  -3 to -1 so that my review does not appear too negativ - and to contribute towards reviewer agreement. I recommend to the authors have to change/formulate all the content..","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-23,no
308,84,137,Danxxxx Gixx,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case.

One concern is that it needs more clarity in the motivation of their survey and, implicitly, the business case.

Note that this has appeared as an arXiv preprint: Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I revised my score on this paper from  -3 to -1 so that my review does not appear too negativ - and to contribute towards reviewer agreement. I recommend to the authors have to change/formulate all the content..","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-03-09,04:15,no,,308,84,137,Daniela Gifu,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case.

One concern is that it needs more clarity in the motivation of their survey and, implicitly, the business case.

Note that this has appeared as an arXiv preprint: Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I revised my score on this paper from  -3 to -1 so that my review does not appear too negativ - and to contribute towards reviewer agreement. I recommend to the authors have to change/formulate all the content..","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-03-09,no
310,84,258,Phixxxx Maxx,2,"(OVERALL EVALUATION) The paper is well-written and structured; good examples.
Research questions are fine and answered.
Related work is sound and comprehensive.
Methodology is fine too. The paper presents same novel results.

The authors could write more about the training and optimization of the retrained models. How much effort was this? What kind of retraining has been done? Was is it a specific retraining for the chemistry domain? More details are need here.

My main point is the missing titles in the extraction task. The authors write: ""Unlike the typical reference parsing task, the title of the references document is not required in our project."" I miss a clear statement why this approach is chosen. 

I have some minor issues:
- What is so special about the business use case? The paper is about processing pdfs from a collection of chemistry papers. I see no need to bring ""business use case"" into the title. More argumentation is needed here. Or just change the paper title.
- The list of possible errors (page 2) is not completed. I suggest adding more cases. E.g. Copy-Paste errors, errors by human handcrafting references, ... 

The limitations of the study can be extended. 

Altogether this is a good paper with some minor issues which can easily be fixed and clarified.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I suggest that Min-Yen Kan should take a look onto the paper. He is more expert on this topic.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,20:54,no,,310,84,258,Philipp Mayr,2,"(OVERALL EVALUATION) The paper is well-written and structured; good examples.
Research questions are fine and answered.
Related work is sound and comprehensive.
Methodology is fine too. The paper presents same novel results.

The authors could write more about the training and optimization of the retrained models. How much effort was this? What kind of retraining has been done? Was is it a specific retraining for the chemistry domain? More details are need here.

My main point is the missing titles in the extraction task. The authors write: ""Unlike the typical reference parsing task, the title of the references document is not required in our project."" I miss a clear statement why this approach is chosen. 

I have some minor issues:
- What is so special about the business use case? The paper is about processing pdfs from a collection of chemistry papers. I see no need to bring ""business use case"" into the title. More argumentation is needed here. Or just change the paper title.
- The list of possible errors (page 2) is not completed. I suggest adding more cases. E.g. Copy-Paste errors, errors by human handcrafting references, ... 

The limitations of the study can be extended. 

Altogether this is a good paper with some minor issues which can easily be fixed and clarified.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I suggest that Min-Yen Kan should take a look onto the paper. He is more expert on this topic.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-16,no
312,84,94,Mixxxx Dobrevxxxxxxxxxxx,3,"(OVERALL EVALUATION) This is a useful, well structured and clearly presented study comparing the performance of ten parsing tools on automated extraction of bibliographic data. 

It needs some language revision. 

For example the first paragraph of the Introduction states: “However, such data is not always available. As a consequence, there is a demand for automated methods and tools for extracting machine-readable bibliographic metadata directly from unstructured scientific data.” – the first of the two sentences needs to be expanded – not available where? The justification of the need is not strong/clear enough.

I was also wondering what was the basis of the decision of using 67% of the dataset for training – this is quite high. It would be interesting to see the reasoning behind it.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-16,21:56,no,,312,84,94,Milena Dobreva-Mcpherson,3,"(OVERALL EVALUATION) This is a useful, well structured and clearly presented study comparing the performance of ten parsing tools on automated extraction of bibliographic data. 

It needs some language revision. 

For example the first paragraph of the Introduction states: “However, such data is not always available. As a consequence, there is a demand for automated methods and tools for extracting machine-readable bibliographic metadata directly from unstructured scientific data.” – the first of the two sentences needs to be expanded – not available where? The justification of the need is not strong/clear enough.

I was also wondering what was the basis of the decision of using 67% of the dataset for training – this is quite high. It would be interesting to see the reasoning behind it.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-16,no
323,89,265,Noxxxx Meuxxxxx,2,"(OVERALL EVALUATION) The authors present a distributed system for the semi-automated extraction and curation of citation data from printed and born-digital documents during the cataloging workflow in physical libraries. The objective of the presented project – creating accurate, openly available citation data is commendable and highly relevant for JCDL. 

The novelty of the presented system lies in the adequate combination of established technologies for the specific use case, which is undoubtedly interesting to the JCDL community. The authors make their code available as open source, which further increases the relevance of the paper.

Despite the good fit of the paper to the topics of JCDL, I have reservations to recommend acceptance of the manuscript as a full paper, i.e. mature work, because the paper and particularly the evaluation does not present enough information to assess the benefit of the presented system. The authors ask how much it would cost to curate all social science references of a given year using their system. In my view, this question is irrelevant as long as the improvement that the system offers over alternatives has not been shown. The authors should quantify the improvement in data quality (coverage and correctness) that their system achieves compared to, e.g., performing unsupervised linking of extracted references to available reference data. The paper should also try to show how certain alternative approaches affect the manual effort required of users. For instance, the authors may answer the following question: Is it indeed necessary for users to manually review every reference? How often do users need to manually correct entries? Do some fields need to be corrected more often than others? Does information from certain sources need to be corrected more often than information from other sources? Can knowledge about differences in the data quality of certain sources improve the quality of suggestions and in turn reduce the overall manual effort required, because users might be able to simply confirm entries more often? 

The paper is well structured and easy to follow. The use of references is adequate.

In its current state, the paper presents an interesting approach that has not been fully evaluated yet. In my view, the paper is more suitable to be published as a short paper.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-15,19:28,no,,323,89,265,Norman Meuschke,2,"(OVERALL EVALUATION) The authors present a distributed system for the semi-automated extraction and curation of citation data from printed and born-digital documents during the cataloging workflow in physical libraries. The objective of the presented project – creating accurate, openly available citation data is commendable and highly relevant for JCDL. 

The novelty of the presented system lies in the adequate combination of established technologies for the specific use case, which is undoubtedly interesting to the JCDL community. The authors make their code available as open source, which further increases the relevance of the paper.

Despite the good fit of the paper to the topics of JCDL, I have reservations to recommend acceptance of the manuscript as a full paper, i.e. mature work, because the paper and particularly the evaluation does not present enough information to assess the benefit of the presented system. The authors ask how much it would cost to curate all social science references of a given year using their system. In my view, this question is irrelevant as long as the improvement that the system offers over alternatives has not been shown. The authors should quantify the improvement in data quality (coverage and correctness) that their system achieves compared to, e.g., performing unsupervised linking of extracted references to available reference data. The paper should also try to show how certain alternative approaches affect the manual effort required of users. For instance, the authors may answer the following question: Is it indeed necessary for users to manually review every reference? How often do users need to manually correct entries? Do some fields need to be corrected more often than others? Does information from certain sources need to be corrected more often than information from other sources? Can knowledge about differences in the data quality of certain sources improve the quality of suggestions and in turn reduce the overall manual effort required, because users might be able to simply confirm entries more often? 

The paper is well structured and easy to follow. The use of references is adequate.

In its current state, the paper presents an interesting approach that has not been fully evaluated yet. In my view, the paper is more suitable to be published as a short paper.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-15,no
340,94,369,Praxxxx Terxxxxxx,1,"(OVERALL EVALUATION) The paper identifies a method for automatically selecting attributes for achieving the most efficient and effective de-duplication of records.
    A review of the de-duplication process and contribution of this work are identified. Various methods utilized to demonstrate the effectiveness of this method in the de-duplication process such as the block key, blocking or sorted neighborhood method, with the Jaro Winkler algorithm as a similarity measure are utilized. It identifies particular metrics of attributes in a record set, such as Duplicity, Distinctiveness, Density and Repetition to calculate relevance of an attribute. Indexing attributes identified using relevance is evaluated for both publicly available and synthetic datasets for de-duplication effectiveness and efficiency.
    While the method evaluation is provided for a variety of datasets, no comparison is provided over previous method identified in the related work particularly, Canalle. The Soundex algorithm utilized in the indexing step is particularly suited for phonetic attributes such as names, states and titles. It would be useful to examine attributes containing alphanumeric entities and or attributes generated as a result of concatenation of other attributes in demonstrating the ability of the algorithm.

    The paper needs revisions to the abstract and introduction sections to improve the readability and provide clarity to the contributions of this work.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2018-02-13,06:36,no,,340,94,369,Pradeep Teregowda,1,"(OVERALL EVALUATION) The paper identifies a method for automatically selecting attributes for achieving the most efficient and effective de-duplication of records.
    A review of the de-duplication process and contribution of this work are identified. Various methods utilized to demonstrate the effectiveness of this method in the de-duplication process such as the block key, blocking or sorted neighborhood method, with the Jaro Winkler algorithm as a similarity measure are utilized. It identifies particular metrics of attributes in a record set, such as Duplicity, Distinctiveness, Density and Repetition to calculate relevance of an attribute. Indexing attributes identified using relevance is evaluated for both publicly available and synthetic datasets for de-duplication effectiveness and efficiency.
    While the method evaluation is provided for a variety of datasets, no comparison is provided over previous method identified in the related work particularly, Canalle. The Soundex algorithm utilized in the indexing step is particularly suited for phonetic attributes such as names, states and titles. It would be useful to examine attributes containing alphanumeric entities and or attributes generated as a result of concatenation of other attributes in demonstrating the ability of the algorithm.

    The paper needs revisions to the abstract and introduction sections to improve the readability and provide clarity to the contributions of this work.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2018-02-13,no
342,94,392,Peixxxx Waxx,2,"(OVERALL EVALUATION) Database integrity is a serious real-world problem. Everyone who has a chance to work in database wether front-end or back-end, has the experience of the problem. In legacy databases that were designed before ER modeling became main approach have been migrating to new DBMS systems that are relational since the 20th. However, many such migrations have not been done systematically to identify and remove duplicates before importing data to new DBMS systems. Thus the data integrity issue remain in new systems.

This study provides experimental data to the method of automatic identification of best attributes for indexing (a key with unique value for each record). The researchers also used a computer program to create data sets as well as three real-world datasets from CORA archive and others. The researchers provided evidence whether their method worked or failed to work. Further research was proposed. 

The paper makes a valuable contribution to a real-world issue.","Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: yes",3,,,,,2018-02-19,03:22,no,,342,94,392,Peiling Wang,2,"(OVERALL EVALUATION) Database integrity is a serious real-world problem. Everyone who has a chance to work in database wether front-end or back-end, has the experience of the problem. In legacy databases that were designed before ER modeling became main approach have been migrating to new DBMS systems that are relational since the 20th. However, many such migrations have not been done systematically to identify and remove duplicates before importing data to new DBMS systems. Thus the data integrity issue remain in new systems.

This study provides experimental data to the method of automatic identification of best attributes for indexing (a key with unique value for each record). The researchers also used a computer program to create data sets as well as three real-world datasets from CORA archive and others. The researchers provided evidence whether their method worked or failed to work. Further research was proposed. 

The paper makes a valuable contribution to a real-world issue.","Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: yes",3,,,,,2018-02-19,no
352,97,282,Erxxx Neuxxxx,2,"(OVERALL EVALUATION) The paper covers an interesting subject but it will need some improvements before it can be presented in the conference.

First of all it claims that interdisciplinary cooperation improves research However it needs to present a metric and an evaluation for that or at least has to cite/evaluate a paper where such an analysis has to be shown. Claims like that have no value if not substantiated. Such a claim is not really necessary for the study. I find one aspect you do not utilize sufficiently is your mention of the Mena-Chalco study and the differences to your study. Why not also include, like they do, ‘novel’ researchers as a second example to compare the over the time development in cooperation.

The clouds you show in Figure 1 need more explanations. What reflects the distance from the center? What is the black circle in one and the gray surrounding in the other? What is the placement of the 8 areas in the circle?

Has the path length anything to do with the quality of research? Is it different for different disciplines? We know that some have many co-authors others only a few. Just giving an average without a mean and even a distribution is of limited value.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) As a short paper it is acceptable but still my rather easy improvements should be enforced before it can be published.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-12,16:13,no,,352,97,282,Erich Neuhold,2,"(OVERALL EVALUATION) The paper covers an interesting subject but it will need some improvements before it can be presented in the conference.

First of all it claims that interdisciplinary cooperation improves research However it needs to present a metric and an evaluation for that or at least has to cite/evaluate a paper where such an analysis has to be shown. Claims like that have no value if not substantiated. Such a claim is not really necessary for the study. I find one aspect you do not utilize sufficiently is your mention of the Mena-Chalco study and the differences to your study. Why not also include, like they do, ‘novel’ researchers as a second example to compare the over the time development in cooperation.

The clouds you show in Figure 1 need more explanations. What reflects the distance from the center? What is the black circle in one and the gray surrounding in the other? What is the placement of the 8 areas in the circle?

Has the path length anything to do with the quality of research? Is it different for different disciplines? We know that some have many co-authors others only a few. Just giving an average without a mean and even a distribution is of limited value.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) As a short paper it is acceptable but still my rather easy improvements should be enforced before it can be published.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-12,no
356,98,275,Fedxxxxx Naxxx,1,"(OVERALL EVALUATION) The paper presents a new system for extracting phrases answering the 5W1H questions from English news articles. The paper is very well written, the system is clearly described and the evaluation setting is well designed. 

While I have a few important remarks on the paper, which I listed below, I would start by saying that this research addresses a very interesting problem for the digital library community and therefore I encourage the authors to continue working on this, following what they state in the ""Discussion"" part of the paper.

Regarding event-detection:

There is a very large body of literature on event-detection in information retrieval (starting from the ""topic detection and tracking"" and ""first story detection"" tasks) and in NLP. While it is evident that the authors know some of these trends, I would strongly advise them to offer a more clear background on the different approaches presented in almost 20 years of research on the topic. Sprugnoli and Tonelli [1] recently published a very clear survey on event detection, that the authors could use as a starting point.

Regarding the system:

The main point of this paper is that the highly specialised systems already presented in the literature cannot be adopted in their research project as these are not publicly available. I completely agree with the authors that is important to address this problem, which is a very pressing issue involving the NLP and other application-oriented research areas, such as digital libraries. 
However, I find striking that also the authors of this paper are not directly sharing the code of their system to the reviewers. In particular, I find the sentence ""If you are interested in reviewing the code before the paper has been accepted, please send an email to felix.hamborg@uni-konstanz.de to retrieve the code and datasets."" unacceptable, given the anonymity of the peer review process. The authors should be very careful with this type of suggestions. Next time, just add a link to the code on GitHub.

Regarding the annotations:

I have found the creation of the dataset well defined and conducted. For the annotations of the output of the system in the 3-point scale, I would suggest the authors to follow the word-intrusion task, from the topic modelling literature [2], and add also some false examples, for instance answers to the ""Where"" question taken from another document, as a sanity check.

Regarding the evaluation:

While I agree with the authors that most of the previously published systems are not publicly available, I disagree on the fact that they are not described in sufficient details to re-implement them. In particular, the paper by Wang et al. (2010) clearly states how to address the ""How"" question using their approach:

""SRL parser returns all possible labeled results for each topic sentence. If a topic sentence is long enough to include a <Subject, Predicate, Object> triple, the entire clause from subject to object is selected.""

The authors could adopt SEMAFOR as a semantic role labeler: http://www.cs.cmu.edu/~ark/SEMAFOR/

In addition to this, simply reporting numbers in a table (as in Table 6) without any type of comparison with any type of baselines (even very naive ones, such as a random baseline) does not tell much to the reader. Given the fact that the authors know the literature on the topic, I would suggest them to a) re-implement an independent baseline for each step from other previous papers (e.g., a baseline for ""who"", a baseline for ""why"") or b) de-compose their system and evaluate each step; for instance what is the impact of the use of copulative conjunctions in the method extractor - does it really offer a boost in performance?

Moreover, it is not possible to compare numbers across papers, as the authors do in page 8, when the approaches are tested on different datasets or address different tasks. For example, Parton et al. (2009) not only does not address the ""how"" question, but the method is tailored for cross-lingual 5w detection.

Regarding the final output:

The authors remark that they address all 5W and 1H questions with their pipeline, however the performance on ""why"" and ""how"" are extremely poor compared to the others 4W questions. MAgP of 0.32 and 0.36 make their system not usable for any digital library or journalistic applications. I would highly recommend the authors to divide their work in two different projects:

a) make a 4W system available to the research community, which already seems to have really good performance
b) focus their effort on improving ""how"" and ""why""; results on this type of research could be very relevant for the NLP community.

--------


[1] Sprugnoli, Rachele, and Sara Tonelli. ""One, no one and one hundred thousand events: Defining and processing events in an inter-disciplinary perspective."" Natural Language Engineering 23.4 (2017): 485-506.

[2] Chang, Jonathan, et al. ""Reading tea leaves: How humans interpret topic models."" Advances in neural information processing systems. 2009.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I do not think the authors had bad intentions with the sentence:  ""If you are interested in reviewing the code before the paper has been accepted, please send an email to felix.hamborg@uni-konstanz.de to retrieve the code and datasets.""

However, suggesting the anonymous reviewer to get in touch with the authors outside easychair-JCDL2018 should be highly discouraged.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-13,13:03,no,,356,98,275,Federico Nanni,1,"(OVERALL EVALUATION) The paper presents a new system for extracting phrases answering the 5W1H questions from English news articles. The paper is very well written, the system is clearly described and the evaluation setting is well designed. 

While I have a few important remarks on the paper, which I listed below, I would start by saying that this research addresses a very interesting problem for the digital library community and therefore I encourage the authors to continue working on this, following what they state in the ""Discussion"" part of the paper.

Regarding event-detection:

There is a very large body of literature on event-detection in information retrieval (starting from the ""topic detection and tracking"" and ""first story detection"" tasks) and in NLP. While it is evident that the authors know some of these trends, I would strongly advise them to offer a more clear background on the different approaches presented in almost 20 years of research on the topic. Sprugnoli and Tonelli [1] recently published a very clear survey on event detection, that the authors could use as a starting point.

Regarding the system:

The main point of this paper is that the highly specialised systems already presented in the literature cannot be adopted in their research project as these are not publicly available. I completely agree with the authors that is important to address this problem, which is a very pressing issue involving the NLP and other application-oriented research areas, such as digital libraries. 
However, I find striking that also the authors of this paper are not directly sharing the code of their system to the reviewers. In particular, I find the sentence ""If you are interested in reviewing the code before the paper has been accepted, please send an email to felix.hamborg@uni-konstanz.de to retrieve the code and datasets."" unacceptable, given the anonymity of the peer review process. The authors should be very careful with this type of suggestions. Next time, just add a link to the code on GitHub.

Regarding the annotations:

I have found the creation of the dataset well defined and conducted. For the annotations of the output of the system in the 3-point scale, I would suggest the authors to follow the word-intrusion task, from the topic modelling literature [2], and add also some false examples, for instance answers to the ""Where"" question taken from another document, as a sanity check.

Regarding the evaluation:

While I agree with the authors that most of the previously published systems are not publicly available, I disagree on the fact that they are not described in sufficient details to re-implement them. In particular, the paper by Wang et al. (2010) clearly states how to address the ""How"" question using their approach:

""SRL parser returns all possible labeled results for each topic sentence. If a topic sentence is long enough to include a <Subject, Predicate, Object> triple, the entire clause from subject to object is selected.""

The authors could adopt SEMAFOR as a semantic role labeler: http://www.cs.cmu.edu/~ark/SEMAFOR/

In addition to this, simply reporting numbers in a table (as in Table 6) without any type of comparison with any type of baselines (even very naive ones, such as a random baseline) does not tell much to the reader. Given the fact that the authors know the literature on the topic, I would suggest them to a) re-implement an independent baseline for each step from other previous papers (e.g., a baseline for ""who"", a baseline for ""why"") or b) de-compose their system and evaluate each step; for instance what is the impact of the use of copulative conjunctions in the method extractor - does it really offer a boost in performance?

Moreover, it is not possible to compare numbers across papers, as the authors do in page 8, when the approaches are tested on different datasets or address different tasks. For example, Parton et al. (2009) not only does not address the ""how"" question, but the method is tailored for cross-lingual 5w detection.

Regarding the final output:

The authors remark that they address all 5W and 1H questions with their pipeline, however the performance on ""why"" and ""how"" are extremely poor compared to the others 4W questions. MAgP of 0.32 and 0.36 make their system not usable for any digital library or journalistic applications. I would highly recommend the authors to divide their work in two different projects:

a) make a 4W system available to the research community, which already seems to have really good performance
b) focus their effort on improving ""how"" and ""why""; results on this type of research could be very relevant for the NLP community.

--------


[1] Sprugnoli, Rachele, and Sara Tonelli. ""One, no one and one hundred thousand events: Defining and processing events in an inter-disciplinary perspective."" Natural Language Engineering 23.4 (2017): 485-506.

[2] Chang, Jonathan, et al. ""Reading tea leaves: How humans interpret topic models."" Advances in neural information processing systems. 2009.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I do not think the authors had bad intentions with the sentence:  ""If you are interested in reviewing the code before the paper has been accepted, please send an email to felix.hamborg@uni-konstanz.de to retrieve the code and datasets.""

However, suggesting the anonymous reviewer to get in touch with the authors outside easychair-JCDL2018 should be highly discouraged.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-13,no
359,98,419,Masxxxxxx Yosxxxxxx,3,"(OVERALL EVALUATION) The goal of the paper to find the skyline set is interesting. The paper employes many techniques in the literature as well as crowdsourcing.
The reviewer's main concern is that core technical contribution of the paper is unclear.
The authors model claims as a triple, namely a relation between two entities. However, this model is too naive to deal with actual scientific claims which are, in many cases, conditional ones. Examples of such conditional claims are ""X is effective for cancer Y for elder female patients"" and ""X is effective for Y if the laboratory test value of Z is under V."" In addition, relations are not simply binary but $n$-array in general. A discussion is required how the authors can extend their idea to such more general cases.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-20,00:33,no,,359,98,419,Masatoshi Yoshikawa,3,"(OVERALL EVALUATION) The goal of the paper to find the skyline set is interesting. The paper employes many techniques in the literature as well as crowdsourcing.
The reviewer's main concern is that core technical contribution of the paper is unclear.
The authors model claims as a triple, namely a relation between two entities. However, this model is too naive to deal with actual scientific claims which are, in many cases, conditional ones. Examples of such conditional claims are ""X is effective for cancer Y for elder female patients"" and ""X is effective for Y if the laboratory test value of Z is under V."" In addition, relations are not simply binary but $n$-array in general. A discussion is required how the authors can extend their idea to such more general cases.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-20,no
360,98,419,Masxxxxxx Yosxxxxxx,3,(OVERALL EVALUATION) The,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-20,11:56,no,,360,98,419,Masatoshi Yoshikawa,3,(OVERALL EVALUATION) The,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-20,no
366,100,90,Yixx Dixx,1,"(OVERALL EVALUATION) Thanks for providing me the opportunities to review this article. This paper tries to diversify the citation contexts in academic literature for knowledge recommendation. I personally pretty much like this article. Nevertheless, I here provide several minor suggestions and hope the authors can consider: (1) In Methodology section, the authors should first use natural language to provide an overview of the given section for readers, and then provide mathematical notations. Finally, some examples should be provided. (2) More details should be provided in the section of evaluation. (3) More implications (not simply methodology-related implications) could be discussed in the Conclusion section.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,Yi,Bu,buyi@iu.edu,353,2018-02-08,16:15,no,,366,100,90,Ying Ding,1,"(OVERALL EVALUATION) Thanks for providing me the opportunities to review this article. This paper tries to diversify the citation contexts in academic literature for knowledge recommendation. I personally pretty much like this article. Nevertheless, I here provide several minor suggestions and hope the authors can consider: (1) In Methodology section, the authors should first use natural language to provide an overview of the given section for readers, and then provide mathematical notations. Finally, some examples should be provided. (2) More details should be provided in the section of evaluation. (3) More implications (not simply methodology-related implications) could be discussed in the Conclusion section.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,Yi,Bu,buyi@iu.edu,353,2018-02-08,no
372,101,166,S.M.xxxxxxxx Haxxx,3,"(OVERALL EVALUATION) This paper presents a study on user behavior when associating information across physical and digital documents. *The paper is suitable for human-computer interaction (HCI) conferences.* 

The ideas presented in the paper are interesting, but the paper suffers from several weaknesses. 

1. The authors did not provide a clear description of the questionnaires they used for data collection. Hence, it is difficult to replicate the study to validate the results.

2. The authors employed the knowledge worker population (MS students, PhD students, and researchers holding PhD degrees) in their study. However, they did not provide a discussion about their research areas or domains of expertise.  Usually, computer science or engineering researchers are more proficient with advanced software features than others because of their educational training. Therefore, it was not clear whether or not the knowledge workers' areas of expertise imposed any bias in the study.

3. A limited number of document-linking software products were mentioned in the paper. However, many of these are available with advanced features. For example, the paper did not mention Mendeley. Moreover, authors may consider software such as Jupyter Notebook to expand the scope of their research.
 
4. The result section's description was too long. I think this section can be shortened because most of the results are understandable from Table 1.
  
5. Some of the design implications looked obvious (e.g., DI3: Documents side by side). Hence, they did not provide concrete research contribution. Moreover, authors did not specify any implementation approach for the design implications.

6. The references mentioned in the paper were old. I think the authors need to study recent related literature.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper may be considered for a short paper or a poster.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,2018-02-23,09:18,no,,372,101,166,S.M.Shamimul Hasan,3,"(OVERALL EVALUATION) This paper presents a study on user behavior when associating information across physical and digital documents. *The paper is suitable for human-computer interaction (HCI) conferences.* 

The ideas presented in the paper are interesting, but the paper suffers from several weaknesses. 

1. The authors did not provide a clear description of the questionnaires they used for data collection. Hence, it is difficult to replicate the study to validate the results.

2. The authors employed the knowledge worker population (MS students, PhD students, and researchers holding PhD degrees) in their study. However, they did not provide a discussion about their research areas or domains of expertise.  Usually, computer science or engineering researchers are more proficient with advanced software features than others because of their educational training. Therefore, it was not clear whether or not the knowledge workers' areas of expertise imposed any bias in the study.

3. A limited number of document-linking software products were mentioned in the paper. However, many of these are available with advanced features. For example, the paper did not mention Mendeley. Moreover, authors may consider software such as Jupyter Notebook to expand the scope of their research.
 
4. The result section's description was too long. I think this section can be shortened because most of the results are understandable from Table 1.
  
5. Some of the design implications looked obvious (e.g., DI3: Documents side by side). Hence, they did not provide concrete research contribution. Moreover, authors did not specify any implementation approach for the design implications.

6. The references mentioned in the paper were old. I think the authors need to study recent related literature.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper may be considered for a short paper or a poster.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,2018-02-23,no
375,102,23,Roxxxx Alxxx,1,"(OVERALL EVALUATION) The main point seems to be to show an example of how the “Skyline Operator” analysis can be applied to text processing of scholarly material.   The specific construct the authors propose to evaluate they call “strong witness”.  To accomplish that, there’s a complex set of descriptions relevant conceptual frameworks, of studies developing data sets, and then evaluations.  Most of this work is pretty solid and the ultimate vision of nuanced techniques for extracting the claims made by research papers is worth pursuing.  

To organize this review, I consider the paper section-by-section:

Section 2 – Related work.  Since I didn’t fully understand the notion of “strong witness”, I initially found it difficult to understand the relevance of the points in this section.  It would be helpful to give some clear examples of what is a strong witness and what is not.   Also, what do you expect readers to do with a paper that is a strong witness.  Wouldn’t a highly readable review paper be more useful to readers in most cases?

Section 3 – While Section 2 set us up to focus on text processing, here we get two dense pages that cover everything from formal definitions, to linguistics, to cognitive science.  Give the breadth of the paper, this is all relevant but I wonder if there’s some better way to help the reader see how it all hangs together.

Section 4 – This starts with the Research Questions.  I would normally expect the Research Questions to be in the Introduction.  They are too high-level to be in a section titled “Experimental Setup”.  Moreover, this section has more than the “Setup”.  It includes Results and Discussion.  At the least, it could be renamed just “Experiment” but I think it would be better to divide it into several sections – or use numbered subsections.

The main result of the paper is that 86% accuracy of the technique. (Presented just before Table 5 – by the way, Table 5 is not useful and should be dropped.)  This does sound “very promising”.  However, it seems a bit odd that the inter-rater reliability was modest (as reported right after Table 5).  

After all these machinations of this research, I would like to see a very simple description of what was really found here.  For instance, were the other (maybe simpler) correlates of “strong witness”?  What was the length of those papers compared to others?  Were the authors of those papers particularly well cited in their other work?  What about the quality of the journals in which those papers were published?
The question “Will the knowledgebase be complete if it includes strong witnesses only?” seems odd in the traditional view of libraries.  It’s not at all clear to me that a collection should “avoid redundancy”.

Section 5 – After all your consideration of the nuances of the research, I don’t understand why you think triples should be the ultimate knowledge representation.  They seem impoverished.  Much better would seem to be some sort of rich semantics and direct representation. 

Minor Points
In general, the writing is good but given the complexity, I think it would be helpful to impose a stronger top-down structure. Just adding spaces between paragraphs or indenting the paragraphs would make the conceptual structure easier to parse.

In addition, there are few terms which I could follow.  

Specifically, at the end of the first paragraph of the Introduction, what is “facettation”?  Also, in Research Question #1 (Section 6) what is “incomparable”?  Maybe it means a “distinctive dimension” but I’m not too sure.

There are also some rather casual asides which are jarring in contrast to the rest of the paper.  Is it really appropriate to say that argumentation mining “is a current trend in the natural language community” (top of Section 2)?  That point would be better made with a few citations.  Similarly, in Section 3 we are told that Skyline queries “has sparked a lot of interest for years”.  And, just following that “the field of economy” should be “economics”.  Then in Section 4 under “ Document collection” we are told that a technique is “clever”.  And, in that same sentence, it should be “MESH” rather than “Mesh”.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-10,16:01,no,,375,102,23,Robert Allen,1,"(OVERALL EVALUATION) The main point seems to be to show an example of how the “Skyline Operator” analysis can be applied to text processing of scholarly material.   The specific construct the authors propose to evaluate they call “strong witness”.  To accomplish that, there’s a complex set of descriptions relevant conceptual frameworks, of studies developing data sets, and then evaluations.  Most of this work is pretty solid and the ultimate vision of nuanced techniques for extracting the claims made by research papers is worth pursuing.  

To organize this review, I consider the paper section-by-section:

Section 2 – Related work.  Since I didn’t fully understand the notion of “strong witness”, I initially found it difficult to understand the relevance of the points in this section.  It would be helpful to give some clear examples of what is a strong witness and what is not.   Also, what do you expect readers to do with a paper that is a strong witness.  Wouldn’t a highly readable review paper be more useful to readers in most cases?

Section 3 – While Section 2 set us up to focus on text processing, here we get two dense pages that cover everything from formal definitions, to linguistics, to cognitive science.  Give the breadth of the paper, this is all relevant but I wonder if there’s some better way to help the reader see how it all hangs together.

Section 4 – This starts with the Research Questions.  I would normally expect the Research Questions to be in the Introduction.  They are too high-level to be in a section titled “Experimental Setup”.  Moreover, this section has more than the “Setup”.  It includes Results and Discussion.  At the least, it could be renamed just “Experiment” but I think it would be better to divide it into several sections – or use numbered subsections.

The main result of the paper is that 86% accuracy of the technique. (Presented just before Table 5 – by the way, Table 5 is not useful and should be dropped.)  This does sound “very promising”.  However, it seems a bit odd that the inter-rater reliability was modest (as reported right after Table 5).  

After all these machinations of this research, I would like to see a very simple description of what was really found here.  For instance, were the other (maybe simpler) correlates of “strong witness”?  What was the length of those papers compared to others?  Were the authors of those papers particularly well cited in their other work?  What about the quality of the journals in which those papers were published?
The question “Will the knowledgebase be complete if it includes strong witnesses only?” seems odd in the traditional view of libraries.  It’s not at all clear to me that a collection should “avoid redundancy”.

Section 5 – After all your consideration of the nuances of the research, I don’t understand why you think triples should be the ultimate knowledge representation.  They seem impoverished.  Much better would seem to be some sort of rich semantics and direct representation. 

Minor Points
In general, the writing is good but given the complexity, I think it would be helpful to impose a stronger top-down structure. Just adding spaces between paragraphs or indenting the paragraphs would make the conceptual structure easier to parse.

In addition, there are few terms which I could follow.  

Specifically, at the end of the first paragraph of the Introduction, what is “facettation”?  Also, in Research Question #1 (Section 6) what is “incomparable”?  Maybe it means a “distinctive dimension” but I’m not too sure.

There are also some rather casual asides which are jarring in contrast to the rest of the paper.  Is it really appropriate to say that argumentation mining “is a current trend in the natural language community” (top of Section 2)?  That point would be better made with a few citations.  Similarly, in Section 3 we are told that Skyline queries “has sparked a lot of interest for years”.  And, just following that “the field of economy” should be “economics”.  Then in Section 4 under “ Document collection” we are told that a technique is “clever”.  And, in that same sentence, it should be “MESH” rather than “Mesh”.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-10,no
376,102,23,Roxxxx Alxxx,1,"(OVERALL EVALUATION) Most of this work is technically solid and the ultimate vision of nuanced techniques for extracting the claims made by research papers is worth pursuing. However, the paper is algorithm driven and we don’t get a seat-of-the-pants sense of what was really found and why that is of value? I’d like to see some descriptive statistics comparing the documents marked as “strong witnesses”? In addition, I’d like to see some specific examples of what the programs found as results in the sub-sections. What are examples of Author Commitment, of Claims, etc

The main point seems to be to show an example of how the “Skyline Operator” analysis can be applied to text processing of scholarly material. The specific construct the authors evaluate they call “strong witness”. To accomplish that, there’s a complex set of descriptions, relevant conceptual frameworks, of studies developing data sets, and then evaluations. 

To organize this review, I consider the paper section-by-section:

Introduction/Section 2 – Could you provide a better some examples of strong witness and what do you expect readers to do with a paper that is a strong witness. Wouldn’t a highly readable review paper be more useful to readers in most cases? Wouldn’t a highly readable review paper be more useful to readers in most cases? 

The discussion of argument mining is interesting but since (as you note) much of that work doesn’t use techniques similar to yours, it 
Section 3 – While Section 2 set us up to focus on text processing, here we get two dense pages that covers everything from formal definitions, to linguistics, to cognitive science. Give the breadth of the paper, this is all relevant but I wonder if there’s some way to help the reader see how it all hangs together.
Section 4 – This starts with the Research Questions. I would normally expect the Research Questions to be in the Introduction. They are too high-level to be in a section titled “Experimental Setup”. Moreover, this section has more than the “Setup”. It includes Results and Discussion. At the least, it could be renamed just “Experiment” but I think it would be better to divide it into several sections – or use numbered subsections.

We don’t any details about how the crowd-sourcing studies were done. One of two sentences would be helpful (was it with Mechanical Turk or something else?)

The main result of the paper is that 86% accuracy of the technique. (Presented just before Table 5 – by the way, Table 5 is not useful and should be dropped.) This does sound “very promising”. However, it seems a bit odd that the inter-rater reliability was modest (as reported right after Table 5). 

The question “Will the knowledgebase be complete if it includes strong witnesses only?” seems odd in comparison to the traditional view of libraries as holding ""all the existing documents that it should hold"". To a first approximation, I don't see why that is hard. It’s not at all clear to me that a collection should “avoid redundancy” and the citation [41] doesn't seem very relevant to what question of what a library is. Rather, the authors seem to slip over into the question what makes a good knowledge base.

Section 5 – After all your consideration of the nuances of the research, I don’t understand why triples should be the ultimate knowledge representation. They seem impoverished. Much better would seem to be some sort of rich semantics and direct representation. 

Minor Points
In general, the writing is good but given the complexity, I think it would be helpful to impose a stronger top-down structure. Just adding spaces between paragraphs or indenting the paragraphs would make the conceptual structure easier to parse.

Keywords should be alphabetized.

In Section 3, the subsection headers are confusing an inconsistent: Defs 2&3 may be meaningful to you but won’t be for most readers. Why is Def 5 in parentheses? In addition, capitalization of the subsection headers is inconsistent.

In Section 4, it’s confusing to say “in the following section” after presenting the RQs. I think you mean in the remainder of this section. Also, in the “incomparable relations” section, it’s confusing to have a “summary” halfway through.

There are few terms which I couldn’t follow at all. At the end of the first paragraph of the Introduction, what is “facettation”? Also, in Research Question #1 (Section 6) what is “incomparable”? Maybe it means a “distinctive dimension” but I’m not too sure.

Given the current numbering, “Algorithm 1” should be identified as Table 1. 

There are also some rather casual asides which are jarring in contrast to the rest of the paper. Is it really appropriate to say that argumentation mining “is a current trend in the natural language community” (top of Section 2)? That point would be better made with a few citations. Similarly, in Section 3 we are told that Skyline queries “has sparked a lot of interest for years”. And, just following that “the field of economy” should be “economics”. Then in Section 4 under “ Document collection” we are told that a technique is “clever”. And, in that same sentence, it should be “MESH” rather than “Mesh”.

Throughout the document, there are many small word-choice decisions that could be could be improved. I'd suggest getting one or two friendly, native speakers of English to wordsmith the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) After all the machinations of this research, I would like to see a very simple description of what was really found here. For instance, were there other (maybe simpler) correlates of “strong witness”? What was the length of those papers compared to others? Were the authors of those papers particularly well cited in their other work? What about the quality of the journals in which those papers were published?

I gave the authors the benefit of the doubt about this because the techniques they used are plausible (though complex). But, I also won't object to rejecting the paper because their case wasn't as tight as it should have been.

(please ignore the previous version of this review)","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-11,01:41,no,,376,102,23,Robert Allen,1,"(OVERALL EVALUATION) Most of this work is technically solid and the ultimate vision of nuanced techniques for extracting the claims made by research papers is worth pursuing. However, the paper is algorithm driven and we don’t get a seat-of-the-pants sense of what was really found and why that is of value? I’d like to see some descriptive statistics comparing the documents marked as “strong witnesses”? In addition, I’d like to see some specific examples of what the programs found as results in the sub-sections. What are examples of Author Commitment, of Claims, etc

The main point seems to be to show an example of how the “Skyline Operator” analysis can be applied to text processing of scholarly material. The specific construct the authors evaluate they call “strong witness”. To accomplish that, there’s a complex set of descriptions, relevant conceptual frameworks, of studies developing data sets, and then evaluations. 

To organize this review, I consider the paper section-by-section:

Introduction/Section 2 – Could you provide a better some examples of strong witness and what do you expect readers to do with a paper that is a strong witness. Wouldn’t a highly readable review paper be more useful to readers in most cases? Wouldn’t a highly readable review paper be more useful to readers in most cases? 

The discussion of argument mining is interesting but since (as you note) much of that work doesn’t use techniques similar to yours, it 
Section 3 – While Section 2 set us up to focus on text processing, here we get two dense pages that covers everything from formal definitions, to linguistics, to cognitive science. Give the breadth of the paper, this is all relevant but I wonder if there’s some way to help the reader see how it all hangs together.
Section 4 – This starts with the Research Questions. I would normally expect the Research Questions to be in the Introduction. They are too high-level to be in a section titled “Experimental Setup”. Moreover, this section has more than the “Setup”. It includes Results and Discussion. At the least, it could be renamed just “Experiment” but I think it would be better to divide it into several sections – or use numbered subsections.

We don’t any details about how the crowd-sourcing studies were done. One of two sentences would be helpful (was it with Mechanical Turk or something else?)

The main result of the paper is that 86% accuracy of the technique. (Presented just before Table 5 – by the way, Table 5 is not useful and should be dropped.) This does sound “very promising”. However, it seems a bit odd that the inter-rater reliability was modest (as reported right after Table 5). 

The question “Will the knowledgebase be complete if it includes strong witnesses only?” seems odd in comparison to the traditional view of libraries as holding ""all the existing documents that it should hold"". To a first approximation, I don't see why that is hard. It’s not at all clear to me that a collection should “avoid redundancy” and the citation [41] doesn't seem very relevant to what question of what a library is. Rather, the authors seem to slip over into the question what makes a good knowledge base.

Section 5 – After all your consideration of the nuances of the research, I don’t understand why triples should be the ultimate knowledge representation. They seem impoverished. Much better would seem to be some sort of rich semantics and direct representation. 

Minor Points
In general, the writing is good but given the complexity, I think it would be helpful to impose a stronger top-down structure. Just adding spaces between paragraphs or indenting the paragraphs would make the conceptual structure easier to parse.

Keywords should be alphabetized.

In Section 3, the subsection headers are confusing an inconsistent: Defs 2&3 may be meaningful to you but won’t be for most readers. Why is Def 5 in parentheses? In addition, capitalization of the subsection headers is inconsistent.

In Section 4, it’s confusing to say “in the following section” after presenting the RQs. I think you mean in the remainder of this section. Also, in the “incomparable relations” section, it’s confusing to have a “summary” halfway through.

There are few terms which I couldn’t follow at all. At the end of the first paragraph of the Introduction, what is “facettation”? Also, in Research Question #1 (Section 6) what is “incomparable”? Maybe it means a “distinctive dimension” but I’m not too sure.

Given the current numbering, “Algorithm 1” should be identified as Table 1. 

There are also some rather casual asides which are jarring in contrast to the rest of the paper. Is it really appropriate to say that argumentation mining “is a current trend in the natural language community” (top of Section 2)? That point would be better made with a few citations. Similarly, in Section 3 we are told that Skyline queries “has sparked a lot of interest for years”. And, just following that “the field of economy” should be “economics”. Then in Section 4 under “ Document collection” we are told that a technique is “clever”. And, in that same sentence, it should be “MESH” rather than “Mesh”.

Throughout the document, there are many small word-choice decisions that could be could be improved. I'd suggest getting one or two friendly, native speakers of English to wordsmith the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) After all the machinations of this research, I would like to see a very simple description of what was really found here. For instance, were there other (maybe simpler) correlates of “strong witness”? What was the length of those papers compared to others? Were the authors of those papers particularly well cited in their other work? What about the quality of the journals in which those papers were published?

I gave the authors the benefit of the doubt about this because the techniques they used are plausible (though complex). But, I also won't object to rejecting the paper because their case wasn't as tight as it should have been.

(please ignore the previous version of this review)","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-11,no
383,103,308,Anixxxx Prxxxx,3,"(OVERALL EVALUATION) Summary of the review: This paper proposes use of RNN (LSTM/GRU) architecture for seizure prediction in epileptic patients. The work explores an existing problem and dataset with an existing machine learning model and shows it to beat a prior baseline. The paper can be best described as a task paper on specific kind of signal processing showing RNN architecture to beat CNN architecture.  


Relevance to JCDL: Though the CPF of JCDL calls for contributions from health/medicine community, in my opinion, the contributions are sought in the intersection area of digital libraries and health. Such work may include architectures, applications, and deployments for digitization and management of medical records, scientific medical data management, medical communities and individual behavior modeling et al. 

At best the work under consideration can be termed as an automatic classification of medical data which is more of a pattern recognition/ medical application work. A social/user/document/information management angle is needed to make it more appropriate for JCDL.  


Novelty/Originality: Since [1] as cited by authors use the same problem and the dataset with a CNN model, the novelty of the work is using an RNN model instead. 

In a broad context, it can be seen as a natural signal classification (much like speech) using a sequential model (RNN vs CNN) which is a well-explored machine learning problem. [2][3]

In a focused context, it an experimental comparison of well-known architectures on a specific dataset[1]. However, more rigor is required in experimental setup to sustain the claims. (Please see next section of review for details)   


Assessment/Evaluation/Comparison: The whole methodology and result section miss important details to confirm the findings and the claims. For example “GRU model outperforms all other models” Since the dataset is small and the number of parameters is not controlled out of LSTM, GRU, BLSTM, and BGRU, it might be an artifact that GRU does well. In other words, GRU performance might just be the result of fewer data meets fewer parameters. 

As compared to [1] the CNN results are much weaker, the author doesn’t explain this. Is this replicated CNN or authors pick it up from [1] directly (which seems less probable as [1] has a different setup)? Further [1] perform 10 fold validation, which is a better strategy in case of small dates as in the given scenario. [1] also reports Sensitivity, Specificity, and AUC which is de facto included in medical tasks results and/or dataset with high skewness. 
 
About Section Methodology:
preprocessing: Is the preprocessing applied to both the RNN and the CNN architectures? Is this preprocessing same as [1]? If No, How does this affect the baseline CNN performance as compared to the [1]? 

architecture: The implementation framework is not mentioned. The number of units in BSLTM is not mentioned.  Assuming it’s Keras-like wrapper library, one can extrapolate the hidden size of BLSTM is equal to twice of the hidden size of LSTM. This may affect the output and hence experiments with equal parameters or best parameter configuration for each model need to be done.  Claims without knowing the parameter and experimental setup cannot be validated.

Author mention using linear activation function for the FC layer. Do you mean rectilinear activation function? A linear activation function (y=alpha*x) is not a non-linearity and hence does not count as a legitimate “layer”. 

model training: Missing details include the number of epoch/ early stopping details, details of validation split (note: test split is mentioned in pre-processing but no details of validation split are given) etc. Is the test and validation split same? If Yes, then the results will not be accurate.  

About Section Results: 
As mentioned in the indicative example of GRU vs BGRU, LSTM, BLSTM, the performance lack enough details to confirm the claims. Figure 4 can be removed (since the RNN model’s training behavior is well captured and is not the focus of the work) and replaced with a more granular experimental result and discussions. 


Style/quality of writing: The writing is good and overall the work is a pleasure to read. Different portions of the paper like motivation, background, technique etc are well weighted. As mentioned earlier as well imaged take up too much of the allocated space and can be safely removed (Figure 1, 2 and 4) or decreased in size (Figure 3, 4) to free up more space to include more results.  


Replicability: The experiments are generally replicable. The dataset is public and the experimental details are adequate to replicate the setup. However, more details might be needed to reproduce the results to the decimal places. Such details include framework, system seeds, no of epochs, initialization distribution of the parameters, etc. 
Other issues (as discussed in details in the above section) about the replicability against the baseline (weakness of baseline as compared to [1]) needs to be addressed.  


Adequacy of references: The references are adequate from a focused task paper point of view.


References:

[1] Acharya, U. R., Oh, S. L., Hagiwara, Y., Tan, J. H., and Adeli, H., ""Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals,"" Comput Biol Med, Sep 27 2017.

[2] Abdel-Hamid, O., Mohamed, A. R., Jiang, H., & Penn, G. (2012, March). Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on (pp. 4277-4280). IEEE.

[3] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. ""Speech recognition with deep recurrent neural networks."" Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-15,19:21,no,,383,103,308,Animesh Prasad,3,"(OVERALL EVALUATION) Summary of the review: This paper proposes use of RNN (LSTM/GRU) architecture for seizure prediction in epileptic patients. The work explores an existing problem and dataset with an existing machine learning model and shows it to beat a prior baseline. The paper can be best described as a task paper on specific kind of signal processing showing RNN architecture to beat CNN architecture.  


Relevance to JCDL: Though the CPF of JCDL calls for contributions from health/medicine community, in my opinion, the contributions are sought in the intersection area of digital libraries and health. Such work may include architectures, applications, and deployments for digitization and management of medical records, scientific medical data management, medical communities and individual behavior modeling et al. 

At best the work under consideration can be termed as an automatic classification of medical data which is more of a pattern recognition/ medical application work. A social/user/document/information management angle is needed to make it more appropriate for JCDL.  


Novelty/Originality: Since [1] as cited by authors use the same problem and the dataset with a CNN model, the novelty of the work is using an RNN model instead. 

In a broad context, it can be seen as a natural signal classification (much like speech) using a sequential model (RNN vs CNN) which is a well-explored machine learning problem. [2][3]

In a focused context, it an experimental comparison of well-known architectures on a specific dataset[1]. However, more rigor is required in experimental setup to sustain the claims. (Please see next section of review for details)   


Assessment/Evaluation/Comparison: The whole methodology and result section miss important details to confirm the findings and the claims. For example “GRU model outperforms all other models” Since the dataset is small and the number of parameters is not controlled out of LSTM, GRU, BLSTM, and BGRU, it might be an artifact that GRU does well. In other words, GRU performance might just be the result of fewer data meets fewer parameters. 

As compared to [1] the CNN results are much weaker, the author doesn’t explain this. Is this replicated CNN or authors pick it up from [1] directly (which seems less probable as [1] has a different setup)? Further [1] perform 10 fold validation, which is a better strategy in case of small dates as in the given scenario. [1] also reports Sensitivity, Specificity, and AUC which is de facto included in medical tasks results and/or dataset with high skewness. 
 
About Section Methodology:
preprocessing: Is the preprocessing applied to both the RNN and the CNN architectures? Is this preprocessing same as [1]? If No, How does this affect the baseline CNN performance as compared to the [1]? 

architecture: The implementation framework is not mentioned. The number of units in BSLTM is not mentioned.  Assuming it’s Keras-like wrapper library, one can extrapolate the hidden size of BLSTM is equal to twice of the hidden size of LSTM. This may affect the output and hence experiments with equal parameters or best parameter configuration for each model need to be done.  Claims without knowing the parameter and experimental setup cannot be validated.

Author mention using linear activation function for the FC layer. Do you mean rectilinear activation function? A linear activation function (y=alpha*x) is not a non-linearity and hence does not count as a legitimate “layer”. 

model training: Missing details include the number of epoch/ early stopping details, details of validation split (note: test split is mentioned in pre-processing but no details of validation split are given) etc. Is the test and validation split same? If Yes, then the results will not be accurate.  

About Section Results: 
As mentioned in the indicative example of GRU vs BGRU, LSTM, BLSTM, the performance lack enough details to confirm the claims. Figure 4 can be removed (since the RNN model’s training behavior is well captured and is not the focus of the work) and replaced with a more granular experimental result and discussions. 


Style/quality of writing: The writing is good and overall the work is a pleasure to read. Different portions of the paper like motivation, background, technique etc are well weighted. As mentioned earlier as well imaged take up too much of the allocated space and can be safely removed (Figure 1, 2 and 4) or decreased in size (Figure 3, 4) to free up more space to include more results.  


Replicability: The experiments are generally replicable. The dataset is public and the experimental details are adequate to replicate the setup. However, more details might be needed to reproduce the results to the decimal places. Such details include framework, system seeds, no of epochs, initialization distribution of the parameters, etc. 
Other issues (as discussed in details in the above section) about the replicability against the baseline (weakness of baseline as compared to [1]) needs to be addressed.  


Adequacy of references: The references are adequate from a focused task paper point of view.


References:

[1] Acharya, U. R., Oh, S. L., Hagiwara, Y., Tan, J. H., and Adeli, H., ""Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals,"" Comput Biol Med, Sep 27 2017.

[2] Abdel-Hamid, O., Mohamed, A. R., Jiang, H., & Penn, G. (2012, March). Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on (pp. 4277-4280). IEEE.

[3] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. ""Speech recognition with deep recurrent neural networks."" Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-15,no
395,107,360,Kazxxxxx Sugxxxxx,1,"(OVERALL EVALUATION) The authors propose positive-unlabeled learning (PU-Learning)-based recommedation 
which improves existing supervised leaning approach to provide much more relevant 
scholarly paper recommendation. It is important to develop better recommendation system 
as the number of published papers are rapidly growing. So the topic is relevant to JCDL.   

The idea of employing PU-Learning with embedding vector in recommendation is interesting. 
But this paper has a lot of problems. The details are as follows:   

(1) Novelty/Originality
- According to Sec 2.1, the authors describe that 
""To tackle this problem, a Positive-Unlabeled Learning (PU-Learning) model is employed in this paper. 
Our model is derived from an A-EM (Augmented Expected Maximization) model [4], ...""

It seems that the authors employ one of PU-Learning approaches, ""A-EM"" as it is. If not, they need to 
clearly describe the difference between their approach and original A-EM. 


(2) Methodology
- The authors need to clearly define parameters so they do not confuse readers.  
At ""(2) Random-walk Probability"" in Sec 2.2, the authors use capital letter ""P"" to denote each paper 
(P^{a}_{1}, ..., P^{a}_{n}). But, in Sec 2.1, the authors have already use ""P"" for ""positive instance set"" in Sec 2.1. 

Similarly, in Sec 3 (just before Sec 4), ""A"" is used to denote ""an author"". But ""A"" has already been used 
as ""a set of the scholars (authors) at Sec 2.1 (right column in page 2). This should be also clearly distinguished. 

- It would be interesting to compare the author's random walk probability with PageRank-like score. 

- According to Sec 2.1, the authors address binary classification task. So the authors should also try SVM as it is often reported that SVM gives better results in binary classification. 
 
Recently, random forest, which is enhanced version of decision tree, often gives better results.  
The authors also try random forest to enrich their experimental results. 
 

(3) Assessment/Evaluation/Comparison
- In Sec 4, it's not clear that how gold-standard data is defined for each author while they describe their dataset. 
Does it depends on reliability score? 

- In Sec 4, the authors need more discussion on why proposed approach gives better results compared with 
other classifiers based on characteristics of them not just decribing ""improvement"". 

- The authors claim statistical significance of PU-larning model. So in Table 2, it is better to clearly show it by marking with ""*"" and so on. 


(4) Style/Quality of Writing
The authors need to write the paper more carefully. 

- From structure point of view, in Sec 2.1, the authors need to define P, U, and RN first, 
and then show Algorithm 1 for better readbility. In addition, while Figure 1 is shown at 
the beginning of the paper, but it is first referred to in Section 3. The authors should show 
it close to the reference, and then start to explain by using the paragraph ""Consider a conference c ..."" 
described in ""(2) Random Walk Probability"". 

- The authors need to improve expressions a lot. The reviewer shows important ones only. 
Please double-check the whole paper. 

[Sec 2.1]
- 'positive' papers have direct ... => '*P*ositive' papers have direct ...

- Consider set of the scholars ... => Consider set of the scholars (authors) ... 
(This part should be written in the same way as ""scholars (authors)"" which first appear at ""Graphical Features"" in Sec 2.2.)

[""Textual"" at Sec 2.2]
- The value of each dimension in vector w_{i} ...
(If ""w_{i}"" is a vector,  it is better to use bold font. In this paragraph, it is difficult to distinguish vector and scalar.)

- word vector w_{a_{j}} for ... => *W*ord vector w_{a_{j}} for ...

[Just before ""(3) Strength of the connectivity"" at Sec 2.2]
- Higher the score more relevant the paper.  => *The* higher the score, *the* more relevant the paper. 

[""(3) Strength of the connectivity"" at Sec 2.2]
This generate four features ... => This generate*s* four features ...

[""(1) Doc2Vec"" at Sec 2.2]
- based on it’s text context [6]. => based on *its* text context [6].

- Similar to method explained in 2.2, ... => Similar to method explained in *Section 2.2*, ... 


(5) Replicability
- If the authors much more clearly describe their approach, researchers interested in this work would 
reproduce the authors approach. 


(6) References
- It is better the authors cite the original meta-path paper: 
Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. 
""PathSim: Meta-Path-Based Top-K Similarity Search in Heterogeneous Information Networks""  
Proc. of the VLDB Endowment, 4(11):992–1003, 2011.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-07,06:01,no,,395,107,360,Kazunari Sugiyama,1,"(OVERALL EVALUATION) The authors propose positive-unlabeled learning (PU-Learning)-based recommedation 
which improves existing supervised leaning approach to provide much more relevant 
scholarly paper recommendation. It is important to develop better recommendation system 
as the number of published papers are rapidly growing. So the topic is relevant to JCDL.   

The idea of employing PU-Learning with embedding vector in recommendation is interesting. 
But this paper has a lot of problems. The details are as follows:   

(1) Novelty/Originality
- According to Sec 2.1, the authors describe that 
""To tackle this problem, a Positive-Unlabeled Learning (PU-Learning) model is employed in this paper. 
Our model is derived from an A-EM (Augmented Expected Maximization) model [4], ...""

It seems that the authors employ one of PU-Learning approaches, ""A-EM"" as it is. If not, they need to 
clearly describe the difference between their approach and original A-EM. 


(2) Methodology
- The authors need to clearly define parameters so they do not confuse readers.  
At ""(2) Random-walk Probability"" in Sec 2.2, the authors use capital letter ""P"" to denote each paper 
(P^{a}_{1}, ..., P^{a}_{n}). But, in Sec 2.1, the authors have already use ""P"" for ""positive instance set"" in Sec 2.1. 

Similarly, in Sec 3 (just before Sec 4), ""A"" is used to denote ""an author"". But ""A"" has already been used 
as ""a set of the scholars (authors) at Sec 2.1 (right column in page 2). This should be also clearly distinguished. 

- It would be interesting to compare the author's random walk probability with PageRank-like score. 

- According to Sec 2.1, the authors address binary classification task. So the authors should also try SVM as it is often reported that SVM gives better results in binary classification. 
 
Recently, random forest, which is enhanced version of decision tree, often gives better results.  
The authors also try random forest to enrich their experimental results. 
 

(3) Assessment/Evaluation/Comparison
- In Sec 4, it's not clear that how gold-standard data is defined for each author while they describe their dataset. 
Does it depends on reliability score? 

- In Sec 4, the authors need more discussion on why proposed approach gives better results compared with 
other classifiers based on characteristics of them not just decribing ""improvement"". 

- The authors claim statistical significance of PU-larning model. So in Table 2, it is better to clearly show it by marking with ""*"" and so on. 


(4) Style/Quality of Writing
The authors need to write the paper more carefully. 

- From structure point of view, in Sec 2.1, the authors need to define P, U, and RN first, 
and then show Algorithm 1 for better readbility. In addition, while Figure 1 is shown at 
the beginning of the paper, but it is first referred to in Section 3. The authors should show 
it close to the reference, and then start to explain by using the paragraph ""Consider a conference c ..."" 
described in ""(2) Random Walk Probability"". 

- The authors need to improve expressions a lot. The reviewer shows important ones only. 
Please double-check the whole paper. 

[Sec 2.1]
- 'positive' papers have direct ... => '*P*ositive' papers have direct ...

- Consider set of the scholars ... => Consider set of the scholars (authors) ... 
(This part should be written in the same way as ""scholars (authors)"" which first appear at ""Graphical Features"" in Sec 2.2.)

[""Textual"" at Sec 2.2]
- The value of each dimension in vector w_{i} ...
(If ""w_{i}"" is a vector,  it is better to use bold font. In this paragraph, it is difficult to distinguish vector and scalar.)

- word vector w_{a_{j}} for ... => *W*ord vector w_{a_{j}} for ...

[Just before ""(3) Strength of the connectivity"" at Sec 2.2]
- Higher the score more relevant the paper.  => *The* higher the score, *the* more relevant the paper. 

[""(3) Strength of the connectivity"" at Sec 2.2]
This generate four features ... => This generate*s* four features ...

[""(1) Doc2Vec"" at Sec 2.2]
- based on it’s text context [6]. => based on *its* text context [6].

- Similar to method explained in 2.2, ... => Similar to method explained in *Section 2.2*, ... 


(5) Replicability
- If the authors much more clearly describe their approach, researchers interested in this work would 
reproduce the authors approach. 


(6) References
- It is better the authors cite the original meta-path paper: 
Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. 
""PathSim: Meta-Path-Based Top-K Similarity Search in Heterogeneous Information Networks""  
Proc. of the VLDB Endowment, 4(11):992–1003, 2011.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-07,no
398,107,230,Pexx Knxxx,3,"(OVERALL EVALUATION) This paper proposes the use of an application of the Expectation Maximisation approach to the area of scholarly paper recommendations. More specifically, the paper deals with the problem of confidently identifying negative training examples in the set of unlabelled papers. The paper is clearly written. 

In terms of weaknesses. 
- I would argue that the paper is not fully aware of the state-of-the-art in academic recommender systems. Most of the successful academic recommender systems, especially those applied in production, do not use supervised machine learning approaches (as it is done in this paper), but largely benefit from algorithms such as collaborative filtering and approaches based on co-citation analysis. This is not necessarily a problem for the proposed method, as identifying negative examples is still important, but it should be reflected in the selection of evaluation algorithms presented in Table 2 (i.e. why all of the selected algorithms for evaluation are supervised when most of the systems currently in production are not?). 
Have a look at work, such as: 
a) J. D. West, I. Wesley-Smith and C. T. Bergstrom, ""A Recommendation System Based on Hierarchical Clustering of an Article-Level Citation Network,"" in IEEE Transactions on Big Data, vol. 2, no. 2, pp. 113-123, June 1 2016.
doi: 10.1109/TBDATA.2016.2541167
b) J. Beel, A. Aizawa, C. Breitinger and B. Gipp, ""Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia,"" 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL), Toronto, ON, 2017, pp. 1-2.
c) Hristakeva, Maya, et al. ""Building recommender systems for scholarly information."" Proceedings of the 1st Workshop on Scholarly Web Mining. ACM, 2017.
d) Knoth, P., Anastasiou, L., Charalampous, A., Cancellieri, M., Pearce, S., Pontika, N., & Bayer, V. (2017). Towards effective research recommender systems for repositories. arXiv preprint arXiv:1705.00578.

- ACM is not the world's largest open dataset of CS papers as claimed in Section 3. Please look at others, such as the datasets provided by CiteSeerX, Semantic Scholar and CORE. 
- This is the most important comment. The evaluation should focus on testing how well does the approach help identify Reliable Negative Instances. While this is the main/key contribution of the paper, evaluation of this aspect is missing. 
- It has been demonstrated that the results of off-line evaluation in (academic) recommender systems do not well correlate with online evaluations. Therefore, it is important that the reported recommendation methods are also tested in an online environment and tested for CTR and related measures.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-19,13:31,no,,398,107,230,Petr Knoth,3,"(OVERALL EVALUATION) This paper proposes the use of an application of the Expectation Maximisation approach to the area of scholarly paper recommendations. More specifically, the paper deals with the problem of confidently identifying negative training examples in the set of unlabelled papers. The paper is clearly written. 

In terms of weaknesses. 
- I would argue that the paper is not fully aware of the state-of-the-art in academic recommender systems. Most of the successful academic recommender systems, especially those applied in production, do not use supervised machine learning approaches (as it is done in this paper), but largely benefit from algorithms such as collaborative filtering and approaches based on co-citation analysis. This is not necessarily a problem for the proposed method, as identifying negative examples is still important, but it should be reflected in the selection of evaluation algorithms presented in Table 2 (i.e. why all of the selected algorithms for evaluation are supervised when most of the systems currently in production are not?). 
Have a look at work, such as: 
a) J. D. West, I. Wesley-Smith and C. T. Bergstrom, ""A Recommendation System Based on Hierarchical Clustering of an Article-Level Citation Network,"" in IEEE Transactions on Big Data, vol. 2, no. 2, pp. 113-123, June 1 2016.
doi: 10.1109/TBDATA.2016.2541167
b) J. Beel, A. Aizawa, C. Breitinger and B. Gipp, ""Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia,"" 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL), Toronto, ON, 2017, pp. 1-2.
c) Hristakeva, Maya, et al. ""Building recommender systems for scholarly information."" Proceedings of the 1st Workshop on Scholarly Web Mining. ACM, 2017.
d) Knoth, P., Anastasiou, L., Charalampous, A., Cancellieri, M., Pearce, S., Pontika, N., & Bayer, V. (2017). Towards effective research recommender systems for repositories. arXiv preprint arXiv:1705.00578.

- ACM is not the world's largest open dataset of CS papers as claimed in Section 3. Please look at others, such as the datasets provided by CiteSeerX, Semantic Scholar and CORE. 
- This is the most important comment. The evaluation should focus on testing how well does the approach help identify Reliable Negative Instances. While this is the main/key contribution of the paper, evaluation of this aspect is missing. 
- It has been demonstrated that the results of off-line evaluation in (academic) recommender systems do not well correlate with online evaluations. Therefore, it is important that the reported recommendation methods are also tested in an online environment and tested for CTR and related measures.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2018-02-19,no
400,108,23,Roxxxx Alxxx,1,"(OVERALL EVALUATION) The main point seems to be to show an example of how the “Skyline Operator” analysis can be applied to text processing of scholarly material.   The specific construct the authors evaluate they call “strong witness”.  To accomplish that, there’s a complex set of descriptions, relevant conceptual frameworks, of studies developing data sets, and then evaluations.  

Most of this work is technically solid and the ultimate vision of nuanced techniques for extracting the claims made by research papers is worth pursuing.  However, the paper is algorithm driven and we don’t get a seat-of-the-pants sense of what was really found and why that is of value?  I’d like to see some descriptive statistics comparing the documents marked as “strong witnesses”?  In addition, I’d  like to see some specific examples of what the programs found as results in the sub-sections.  What are examples of Author Commitment, of Claims, etc

To organize this review, I consider the paper section-by-section:

Introduction/Section 2 –   Could you provide a better some examples of strong witness and what do you expect readers to do with a paper that is a strong witness.  Wouldn’t a highly readable review paper be more useful to readers in most cases?  Wouldn’t a highly readable review paper be more useful to readers in most cases?  

The discussion of argument mining is interesting but since (as you note) much of that work doesn’t use techniques similar to yours, it 
Section 3 – While Section 2 set us up to focus on text processing, here we get two dense pages that covers everything from formal definitions, to linguistics, to cognitive science.  Give the breadth of the paper, this is all relevant but I wonder if there’s some way to help the reader see how it all hangs together.
Section 4 – This starts with the Research Questions.  I would normally expect the Research Questions to be in the Introduction.  They are too high-level to be in a section titled “Experimental Setup”.  Moreover, this section has more than the “Setup”.  It includes Results and Discussion.  At the least, it could be renamed just “Experiment” but I think it would be better to divide it into several sections – or use numbered subsections.

We don’t any details about how the crowd-sourcing studies were done. One of two sentences would be helpful (was it with Mechanical Turk or something else?)

The main result of the paper is that 86% accuracy of the technique. (Presented just before Table 5 – by the way, Table 5 is not useful and should be dropped.)  This does sound “very promising”.  However, it seems a bit odd that the inter-rater reliability was modest (as reported right after Table 5).  

The question “Will the knowledgebase be complete if it includes strong witnesses only?” seems odd in comparison to the traditional view of libraries as holding ""all the existing documents that it should hold"".  To a first approximation, I don't see why that is hard.  It’s not at all clear to me that a collection should “avoid redundancy” and the citation [41] doesn't seem very relevant to what question of what a library is.  Rather, the authors seem to slip over into the question what makes a good knowledge base.

Section 5 – After all your consideration of the nuances of the research, I don’t understand why triples should be the ultimate knowledge representation.  They seem impoverished.  Much better would seem to be some sort of rich semantics and direct representation. 

Minor Points
In general, the writing is good but given the complexity, I think it would be helpful to impose a stronger top-down structure. Just adding spaces between paragraphs or indenting the paragraphs would make the conceptual structure easier to parse.

Keywords should be alphabetized.

In Section 3, the subsection headers are confusing an inconsistent: Defs 2&3 may be meaningful to you but won’t be for most readers.  Why is Def 5 in parentheses?  In addition, capitalization of the subsection headers is inconsistent.

In Section 4, it’s confusing to say “in the following section” after presenting the RQs.  I think you mean in the remainder of this section.  Also, in the “incomparable relations” section, it’s confusing to have a “summary” halfway through.

There are few terms which I couldn’t follow at all.  At the end of the first paragraph of the Introduction, what is “facettation”?  Also, in Research Question #1 (Section 6) what is “incomparable”?  Maybe it means a “distinctive dimension” but I’m not too sure.

Given the current numbering, “Algorithm 1” should be identified as Table 1.  

There are also some rather casual asides which are jarring in contrast to the rest of the paper.  Is it really appropriate to say that argumentation mining “is a current trend in the natural language community” (top of Section 2)?  That point would be better made with a few citations.  Similarly, in Section 3 we are told that Skyline queries “has sparked a lot of interest for years”.  And, just following that “the field of economy” should be “economics”.  Then in Section 4 under “ Document collection” we are told that a technique is “clever”.  And, in that same sentence, it should be “MESH” rather than “Mesh”.

Throughout the document, there are many small word-choice decisions that could be could be improved.  I'd suggest getting one or two friendly, native speakers of English to word-smith the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) After all the machinations of this research, I would like to see a very simple description of what was really found here.  For instance, were there other (maybe simpler) correlates of “strong witness”?  What was the length of those papers compared to others?  Were the authors of those papers particularly well cited in their other work?  What about the quality of the journals in which those papers were published?

I gave the authors the benefit of the doubt about this because the techniques the used are plausible (though complex).  But, I also won't object to rejecting this paper because their case wasn't as tight as it should have been.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-11,01:29,no,,400,108,23,Robert Allen,1,"(OVERALL EVALUATION) The main point seems to be to show an example of how the “Skyline Operator” analysis can be applied to text processing of scholarly material.   The specific construct the authors evaluate they call “strong witness”.  To accomplish that, there’s a complex set of descriptions, relevant conceptual frameworks, of studies developing data sets, and then evaluations.  

Most of this work is technically solid and the ultimate vision of nuanced techniques for extracting the claims made by research papers is worth pursuing.  However, the paper is algorithm driven and we don’t get a seat-of-the-pants sense of what was really found and why that is of value?  I’d like to see some descriptive statistics comparing the documents marked as “strong witnesses”?  In addition, I’d  like to see some specific examples of what the programs found as results in the sub-sections.  What are examples of Author Commitment, of Claims, etc

To organize this review, I consider the paper section-by-section:

Introduction/Section 2 –   Could you provide a better some examples of strong witness and what do you expect readers to do with a paper that is a strong witness.  Wouldn’t a highly readable review paper be more useful to readers in most cases?  Wouldn’t a highly readable review paper be more useful to readers in most cases?  

The discussion of argument mining is interesting but since (as you note) much of that work doesn’t use techniques similar to yours, it 
Section 3 – While Section 2 set us up to focus on text processing, here we get two dense pages that covers everything from formal definitions, to linguistics, to cognitive science.  Give the breadth of the paper, this is all relevant but I wonder if there’s some way to help the reader see how it all hangs together.
Section 4 – This starts with the Research Questions.  I would normally expect the Research Questions to be in the Introduction.  They are too high-level to be in a section titled “Experimental Setup”.  Moreover, this section has more than the “Setup”.  It includes Results and Discussion.  At the least, it could be renamed just “Experiment” but I think it would be better to divide it into several sections – or use numbered subsections.

We don’t any details about how the crowd-sourcing studies were done. One of two sentences would be helpful (was it with Mechanical Turk or something else?)

The main result of the paper is that 86% accuracy of the technique. (Presented just before Table 5 – by the way, Table 5 is not useful and should be dropped.)  This does sound “very promising”.  However, it seems a bit odd that the inter-rater reliability was modest (as reported right after Table 5).  

The question “Will the knowledgebase be complete if it includes strong witnesses only?” seems odd in comparison to the traditional view of libraries as holding ""all the existing documents that it should hold"".  To a first approximation, I don't see why that is hard.  It’s not at all clear to me that a collection should “avoid redundancy” and the citation [41] doesn't seem very relevant to what question of what a library is.  Rather, the authors seem to slip over into the question what makes a good knowledge base.

Section 5 – After all your consideration of the nuances of the research, I don’t understand why triples should be the ultimate knowledge representation.  They seem impoverished.  Much better would seem to be some sort of rich semantics and direct representation. 

Minor Points
In general, the writing is good but given the complexity, I think it would be helpful to impose a stronger top-down structure. Just adding spaces between paragraphs or indenting the paragraphs would make the conceptual structure easier to parse.

Keywords should be alphabetized.

In Section 3, the subsection headers are confusing an inconsistent: Defs 2&3 may be meaningful to you but won’t be for most readers.  Why is Def 5 in parentheses?  In addition, capitalization of the subsection headers is inconsistent.

In Section 4, it’s confusing to say “in the following section” after presenting the RQs.  I think you mean in the remainder of this section.  Also, in the “incomparable relations” section, it’s confusing to have a “summary” halfway through.

There are few terms which I couldn’t follow at all.  At the end of the first paragraph of the Introduction, what is “facettation”?  Also, in Research Question #1 (Section 6) what is “incomparable”?  Maybe it means a “distinctive dimension” but I’m not too sure.

Given the current numbering, “Algorithm 1” should be identified as Table 1.  

There are also some rather casual asides which are jarring in contrast to the rest of the paper.  Is it really appropriate to say that argumentation mining “is a current trend in the natural language community” (top of Section 2)?  That point would be better made with a few citations.  Similarly, in Section 3 we are told that Skyline queries “has sparked a lot of interest for years”.  And, just following that “the field of economy” should be “economics”.  Then in Section 4 under “ Document collection” we are told that a technique is “clever”.  And, in that same sentence, it should be “MESH” rather than “Mesh”.

Throughout the document, there are many small word-choice decisions that could be could be improved.  I'd suggest getting one or two friendly, native speakers of English to word-smith the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) After all the machinations of this research, I would like to see a very simple description of what was really found here.  For instance, were there other (maybe simpler) correlates of “strong witness”?  What was the length of those papers compared to others?  Were the authors of those papers particularly well cited in their other work?  What about the quality of the journals in which those papers were published?

I gave the authors the benefit of the doubt about this because the techniques the used are plausible (though complex).  But, I also won't object to rejecting this paper because their case wasn't as tight as it should have been.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-11,no
404,109,408,Irxx Xxx,1,"(OVERALL EVALUATION) This paper examines an extension of a model for information seeking in large-scale DLs, to demonstrate how contextual browsing supports serendipitous discovery. Through the use of Compage, a prototype for contextual browsing, the researchers assess three types of contextual browsing strategies via a simulation.

The research builds on the authors’ prior model. The use of contextual browsing in understanding serendipity for large-scales DLs makes this a unique topic. While the idea of examining contextual browsing is very interesting and has great implications for the design and improvement of large-scale DLs, the simulation approach does not seem a right approach for this research topic. 

Contextual browsing is a very dynamic and complicated information seeking behavior.  Many factors including user (knowledge, motivation, interest, style, etc.) and system (organization of the information, content, layout, color, etc.) influence the way user browse information. User study is the best approach to illustrate the browsing approach and how serendipity occurs in the process. 

The authors did not clearly state their research questions, whether to validate their model or just compare the three strategies. In order to support serendipity in context, we need to know how users apply different types of browsing strategies and what the associated factors are. 

Markir et al suggested 7 recommendations for system design to support serendipity.  It would be helpful if the authors would focus on how to implement these recommendations and further test their usefulness and effectiveness. 


The main problems of the simulation design are:

1)	The simple simulation design in this study does not reflect the complexity of contextual browsing in reality.
2)	The datasets selected do not represent large-scale digital libraries.  The authors did not justify that DBPedia has the characteristics of a large-scale DL. Large-scale DLs consist of more diverse types of materials, metadata, and their associations.
3)	The researchers did not consider or describe the scenarios of contextual browsing for the three strategies that users normally go through. Instead, the simulation only specifies the linear process.
4)	 The design considers the three types of browsing strategies separately. In real life, users apply more than one strategy during one information seeking episode. 
5)	The simulation design did not take the user and interface factors that affect the contextual browsing process into consideration. 
6)	The simulation design seems cares more about the relevant results generated by the strategies than the browsing process which is less useful to support serendipity in context. 

In addition, it seems that the authors did not use a standard way to propose hypotheses and report the statistical results.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2018-02-16,22:55,no,,404,109,408,Iris Xie,1,"(OVERALL EVALUATION) This paper examines an extension of a model for information seeking in large-scale DLs, to demonstrate how contextual browsing supports serendipitous discovery. Through the use of Compage, a prototype for contextual browsing, the researchers assess three types of contextual browsing strategies via a simulation.

The research builds on the authors’ prior model. The use of contextual browsing in understanding serendipity for large-scales DLs makes this a unique topic. While the idea of examining contextual browsing is very interesting and has great implications for the design and improvement of large-scale DLs, the simulation approach does not seem a right approach for this research topic. 

Contextual browsing is a very dynamic and complicated information seeking behavior.  Many factors including user (knowledge, motivation, interest, style, etc.) and system (organization of the information, content, layout, color, etc.) influence the way user browse information. User study is the best approach to illustrate the browsing approach and how serendipity occurs in the process. 

The authors did not clearly state their research questions, whether to validate their model or just compare the three strategies. In order to support serendipity in context, we need to know how users apply different types of browsing strategies and what the associated factors are. 

Markir et al suggested 7 recommendations for system design to support serendipity.  It would be helpful if the authors would focus on how to implement these recommendations and further test their usefulness and effectiveness. 


The main problems of the simulation design are:

1)	The simple simulation design in this study does not reflect the complexity of contextual browsing in reality.
2)	The datasets selected do not represent large-scale digital libraries.  The authors did not justify that DBPedia has the characteristics of a large-scale DL. Large-scale DLs consist of more diverse types of materials, metadata, and their associations.
3)	The researchers did not consider or describe the scenarios of contextual browsing for the three strategies that users normally go through. Instead, the simulation only specifies the linear process.
4)	 The design considers the three types of browsing strategies separately. In real life, users apply more than one strategy during one information seeking episode. 
5)	The simulation design did not take the user and interface factors that affect the contextual browsing process into consideration. 
6)	The simulation design seems cares more about the relevant results generated by the strategies than the browsing process which is less useful to support serendipity in context. 

In addition, it seems that the authors did not use a standard way to propose hypotheses and report the statistical results.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2018-02-16,no
406,109,182,Xixx Hx,2,"(OVERALL EVALUATION) This paper proposed a model of strategies for contextual browsing and realized some of the strategies in a software framework. The realization was empirically verified by a set of simulated experiments. The topic is highly relevant to JCDL. In fact, this paper is a follow up of previous JCDL papers contributed by the same research group. The proposed model is well grounded on influential theories of information seeking, with valuable extensions supported by sound explanations (Tables 1, 2 and Section 3).

The simulation experiments were also well designed, with two exemplar cases rooted in practical scenarios and with worksets of different sizes. The three contexts (Reset, Unprioritized, Prioritized) were representative and the statistical tests were carefully designed. The results were carefully explained with Figure 3 being a nice presentation of the simulation results. 

In general this paper presented a nice study that is both theoretically and empirically sound. It contributes to the conceptualization of strategies of contextual browsing, proposes an implementation of these strategies based on Linked Data,and verifies both with a reasonably designed simulation experiment.

Here are some (minor) suggestions for further improvement of the paper: 

1. It could be helpful to further strengthen the connection of the simulation experiments back to the proposed model of contextual browsing strategies (Tables 1 and 2). They are connected, but it could further help the readers if the connections are explicitly stated. 

2. The columns in Table 1 were not explicitly introduced. They seemed to be different stages of contextual browsing, but a clear introduction of would be appreciated. 

3. At the end of Section 6, it would be helpful to provide a summary (in a table if possible) of the test results. As there were quite a number of combinations of contexts, worksets, replacement vs. narrowing, and tests, a summary would be helpful for the readers to follow.

4. There seems to be a incomplete sentence at end of section 5.1.2.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: yes",3,,,,,2018-02-17,12:56,no,,406,109,182,Xiao Hu,2,"(OVERALL EVALUATION) This paper proposed a model of strategies for contextual browsing and realized some of the strategies in a software framework. The realization was empirically verified by a set of simulated experiments. The topic is highly relevant to JCDL. In fact, this paper is a follow up of previous JCDL papers contributed by the same research group. The proposed model is well grounded on influential theories of information seeking, with valuable extensions supported by sound explanations (Tables 1, 2 and Section 3).

The simulation experiments were also well designed, with two exemplar cases rooted in practical scenarios and with worksets of different sizes. The three contexts (Reset, Unprioritized, Prioritized) were representative and the statistical tests were carefully designed. The results were carefully explained with Figure 3 being a nice presentation of the simulation results. 

In general this paper presented a nice study that is both theoretically and empirically sound. It contributes to the conceptualization of strategies of contextual browsing, proposes an implementation of these strategies based on Linked Data,and verifies both with a reasonably designed simulation experiment.

Here are some (minor) suggestions for further improvement of the paper: 

1. It could be helpful to further strengthen the connection of the simulation experiments back to the proposed model of contextual browsing strategies (Tables 1 and 2). They are connected, but it could further help the readers if the connections are explicitly stated. 

2. The columns in Table 1 were not explicitly introduced. They seemed to be different stages of contextual browsing, but a clear introduction of would be appreciated. 

3. At the end of Section 6, it would be helpful to provide a summary (in a table if possible) of the test results. As there were quite a number of combinations of contexts, worksets, replacement vs. narrowing, and tests, a summary would be helpful for the readers to follow.

4. There seems to be a incomplete sentence at end of section 5.1.2.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: yes",3,,,,,2018-02-17,no
408,109,187,Antxxxx Isxxx,3,"(OVERALL EVALUATION) Relevance to JCDL:
The paper is highly relevant to the the JCDL. Its general focus on navigation of large scale digital libraries will of course be of interest to the JCDL's audience; and the Compage framework in particular should excite interest for many.

Novelty/originality:
The paper is doing various things at once, not all of which harmonise entirely well with each other, so it is difficult to give a unified assessment. In addition, my confidence in most of these areas is low. In terms of the overall user framework adopted, the analysis of 'contextual browsing' is as far as I am aware novel, and potentially useful. The use of worksets as the basic unit of analysis is also interesting; if the concept itself is not new, its application is. On the other hand, the treatment of the RDF triple as a means of exploring connections felt a bit underdeveloped. The algorithm used for computing similarity in entity graphs is basic and not novel but appropriate (albeit badly explained, see later).

Methodology:
The methodology is somewhat confusing. Sections 1 through 3 concern serendipity, and how platforms might support it better; sections 4 through 6 concern workset similarity. At first glance, serendipity and similarity are distinct notions, or even in tension with each other, as the authors indeed seem to note at various points (for instance, with the occasional good hits the 'reset' strategy yields). The paper's claims that they are strongly related (for example """"We have described how contextual browsing requires a means of prioritisation to remain feasible"" in the conclusion"" is unclear. There is not much transition between section 3 and 4. Really, we seem to have two separate papers that have been zipped into one. The methodology is accordingly incoherent.

The analysis of the modes of contextual browsing is interesting, and certainly a good contribution of the paper. It is hard to follow however. Perhaps this is due to the fact that table 3 seeks to render what is essentially an assignment of behaviours that depends on 2 dimensions. But I was confused by the fact that the level that has all the target and profile specification (last line) does not enable all serendipity behaviours enabled by previous levels. 1 and 3 are missing: why?

There is little value of evaluating the approach that narrows worksets to the case of workset 2. Starting with 13 items, it is clear that the approach will have a non-standard behaviour. Well, there could be some value in evaluating this approach for small-size sets, but then the analysis should be much more granular: it cannot be at the same level as for bigger sets.

Assessment/evaluation/comparison:
This is simply inadequate. Attempting to assess serendipity effects in an automated way and without reference to actual users seems dubious at best; furthermore, it is not really clear that this is what the authors' automated tests aim to measure anyway, as they are oriented entirely towards assessing similarity. Finally, the methodology of the automated tests is, as the authors admit ('perhaps a predictable outcome given that the prioritisation is also realised via similarity calculations'), somewhat circular. No real conclusions can be drawn from it beyond very basic sanity-checking.

Style/quality of writing:
This is generally very good. There are however some strange choices being made, which do not make reading easy, at time:
- the abstract does not introduce the problem, which is quite unusual
- the categories in table 1 (column titles) are not really introduced
- the explanation of Jaccard is not good. First in the text, when it refers to binary RDF tuples (RDF is made of triples) and then in the listing, when the notation g1[p,o] is left for the reader to understand, the loop in line 5 introduces an 's' variable that is not used in the loop itself, and 'matchScores' seems like a raw list of numbers, which is not indexed by entities or couples of entities. In 5.2 there is a mention of 'directed Jaccart prioritization' that hints that Jaccard similarity is not symmetrical (which it is). If it's not what is meant here, then what is meant?
- the text in 5.1.1 hints that 'Similarity views' are distinct from 'Entity connctions' but the caption of figure 1 refers to the similarities being the content of the Entity Connctions pane.
- the choice of reflecting the scales in figures 1 and 2 is not helpful. There might be some graphical elegance, but that is offset by the reader struggling to go back to the scale on the left to figure what the similarity values are. The choive of a vertical line that corresponds to 'unprioritized' is also confusing. It would have been better to hav, for each iteration, a 'cell' that clearly groups each of the settings.
- the discussion point on dataset modeling in the conclusion is worthwhile to make, really. But such a long explanation of what was observed does not fit a conclusion, especially when there is a discussion section where it would fit perfectly.

Minor comment: a word seems to have disappeared from the last sentence of section 5.1.1.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,Timothy,Hill,timothy.hill@europeana.eu,357,2018-02-17,13:49,no,,408,109,187,Antoine Isaac,3,"(OVERALL EVALUATION) Relevance to JCDL:
The paper is highly relevant to the the JCDL. Its general focus on navigation of large scale digital libraries will of course be of interest to the JCDL's audience; and the Compage framework in particular should excite interest for many.

Novelty/originality:
The paper is doing various things at once, not all of which harmonise entirely well with each other, so it is difficult to give a unified assessment. In addition, my confidence in most of these areas is low. In terms of the overall user framework adopted, the analysis of 'contextual browsing' is as far as I am aware novel, and potentially useful. The use of worksets as the basic unit of analysis is also interesting; if the concept itself is not new, its application is. On the other hand, the treatment of the RDF triple as a means of exploring connections felt a bit underdeveloped. The algorithm used for computing similarity in entity graphs is basic and not novel but appropriate (albeit badly explained, see later).

Methodology:
The methodology is somewhat confusing. Sections 1 through 3 concern serendipity, and how platforms might support it better; sections 4 through 6 concern workset similarity. At first glance, serendipity and similarity are distinct notions, or even in tension with each other, as the authors indeed seem to note at various points (for instance, with the occasional good hits the 'reset' strategy yields). The paper's claims that they are strongly related (for example """"We have described how contextual browsing requires a means of prioritisation to remain feasible"" in the conclusion"" is unclear. There is not much transition between section 3 and 4. Really, we seem to have two separate papers that have been zipped into one. The methodology is accordingly incoherent.

The analysis of the modes of contextual browsing is interesting, and certainly a good contribution of the paper. It is hard to follow however. Perhaps this is due to the fact that table 3 seeks to render what is essentially an assignment of behaviours that depends on 2 dimensions. But I was confused by the fact that the level that has all the target and profile specification (last line) does not enable all serendipity behaviours enabled by previous levels. 1 and 3 are missing: why?

There is little value of evaluating the approach that narrows worksets to the case of workset 2. Starting with 13 items, it is clear that the approach will have a non-standard behaviour. Well, there could be some value in evaluating this approach for small-size sets, but then the analysis should be much more granular: it cannot be at the same level as for bigger sets.

Assessment/evaluation/comparison:
This is simply inadequate. Attempting to assess serendipity effects in an automated way and without reference to actual users seems dubious at best; furthermore, it is not really clear that this is what the authors' automated tests aim to measure anyway, as they are oriented entirely towards assessing similarity. Finally, the methodology of the automated tests is, as the authors admit ('perhaps a predictable outcome given that the prioritisation is also realised via similarity calculations'), somewhat circular. No real conclusions can be drawn from it beyond very basic sanity-checking.

Style/quality of writing:
This is generally very good. There are however some strange choices being made, which do not make reading easy, at time:
- the abstract does not introduce the problem, which is quite unusual
- the categories in table 1 (column titles) are not really introduced
- the explanation of Jaccard is not good. First in the text, when it refers to binary RDF tuples (RDF is made of triples) and then in the listing, when the notation g1[p,o] is left for the reader to understand, the loop in line 5 introduces an 's' variable that is not used in the loop itself, and 'matchScores' seems like a raw list of numbers, which is not indexed by entities or couples of entities. In 5.2 there is a mention of 'directed Jaccart prioritization' that hints that Jaccard similarity is not symmetrical (which it is). If it's not what is meant here, then what is meant?
- the text in 5.1.1 hints that 'Similarity views' are distinct from 'Entity connctions' but the caption of figure 1 refers to the similarities being the content of the Entity Connctions pane.
- the choice of reflecting the scales in figures 1 and 2 is not helpful. There might be some graphical elegance, but that is offset by the reader struggling to go back to the scale on the left to figure what the similarity values are. The choive of a vertical line that corresponds to 'unprioritized' is also confusing. It would have been better to hav, for each iteration, a 'cell' that clearly groups each of the settings.
- the discussion point on dataset modeling in the conclusion is worthwhile to make, really. But such a long explanation of what was observed does not fit a conclusion, especially when there is a discussion section where it would fit perfectly.

Minor comment: a word seems to have disappeared from the last sentence of section 5.1.1.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,Timothy,Hill,timothy.hill@europeana.eu,357,2018-02-17,no
410,109,172,Mauxxxx Henxxxxxx,4,"(OVERALL EVALUATION) This paper builds on an earlier piece of work is well suited to JCDL and the wider library community. The simulation of serendipity browsing in large-scale collections is very interesting.

First I must confess that my confidence in reviewing this paper is not high, and I am not familiar with the original work.
The article begins with contextual browsing and the various specifications, but there doesn't appear to be any discussion of the profiles of the users, which I think may have given a little more insight. And Table 1 requires some explanation.
More fundamentally, the paper doesn't appear to hang together, for example, the early sections on serendipity does not seem to sit well with the later worksets similarity. Perhaps this should be 2 short papers. 

The paper is well written, but I think that it suffers from a lack of coherence.

Minor problem - last sentence in 5.1.1 is not complete.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,2018-02-19,03:27,no,,410,109,172,Maureen Henninger,4,"(OVERALL EVALUATION) This paper builds on an earlier piece of work is well suited to JCDL and the wider library community. The simulation of serendipity browsing in large-scale collections is very interesting.

First I must confess that my confidence in reviewing this paper is not high, and I am not familiar with the original work.
The article begins with contextual browsing and the various specifications, but there doesn't appear to be any discussion of the profiles of the users, which I think may have given a little more insight. And Table 1 requires some explanation.
More fundamentally, the paper doesn't appear to hang together, for example, the early sections on serendipity does not seem to sit well with the later worksets similarity. Perhaps this should be 2 short papers. 

The paper is well written, but I think that it suffers from a lack of coherence.

Minor problem - last sentence in 5.1.1 is not complete.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,2018-02-19,no
413,111,323,Horxxxx Sagxxxx,1,"(OVERALL EVALUATION) The paper studies the still relevant problem of author name disambiguation in the context of digital libraries.  The novelty of the paper is on the combination of superficial and “deep” (!) features which are used in a simple neural network to discriminate pairs of papers which contain the same surface author name.  The superficial features basically refer to string similarity measures such as comparing the list of authors of two publications and lengths of the resulting list of co-authors.  Comparison of publication titles is also used. The “deeper” features make use of word embeddings trained on a DBLP dataset. These representations are    used to compare two high relevant tokens from the titles or combined using all token comparisons.  
The approach is evaluated with 5 different author names. Results appear to indicate better performance when deep features are used.
The paper is well written and relevant for the conference. The state of the art, which just concentrates on neural networks, lacks in my opinion background on the problem  and more classical works. Some aspects of the method are unclear, for example it is unclear how relevant tokens from the titles are used when top tokens are identical. It is also unclear how you compare the co-authors (token-based or named entity based).","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-15,06:44,no,,413,111,323,Horacio Saggion,1,"(OVERALL EVALUATION) The paper studies the still relevant problem of author name disambiguation in the context of digital libraries.  The novelty of the paper is on the combination of superficial and “deep” (!) features which are used in a simple neural network to discriminate pairs of papers which contain the same surface author name.  The superficial features basically refer to string similarity measures such as comparing the list of authors of two publications and lengths of the resulting list of co-authors.  Comparison of publication titles is also used. The “deeper” features make use of word embeddings trained on a DBLP dataset. These representations are    used to compare two high relevant tokens from the titles or combined using all token comparisons.  
The approach is evaluated with 5 different author names. Results appear to indicate better performance when deep features are used.
The paper is well written and relevant for the conference. The state of the art, which just concentrates on neural networks, lacks in my opinion background on the problem  and more classical works. Some aspects of the method are unclear, for example it is unclear how relevant tokens from the titles are used when top tokens are identical. It is also unclear how you compare the co-authors (token-based or named entity based).","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-15,no
422,113,426,Zhxxxx Zhxxx,2,"(OVERALL EVALUATION) This paper studied the Persistent Identifier (PID) problem, and proposed to add PID Kernel information and profile and data typing into a PID system. The overall conclusion is that this proposal can help data sharing at affordable extra cost.

I'm not very familiar with the PID system. But this paper is very clearly written with very thorough analysis and experiments. After reading I agree with the authors' proposals and conclusions. I feel like this is a clear accept although I'm not an expert in this field.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-16,19:45,no,,422,113,426,Zhiwei Zhang,2,"(OVERALL EVALUATION) This paper studied the Persistent Identifier (PID) problem, and proposed to add PID Kernel information and profile and data typing into a PID system. The overall conclusion is that this proposal can help data sharing at affordable extra cost.

I'm not very familiar with the PID system. But this paper is very clearly written with very thorough analysis and experiments. After reading I agree with the authors' proposals and conclusions. I feel like this is a clear accept although I'm not an expert in this field.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-16,no
423,113,426,Zhxxxx Zhxxx,2,"(OVERALL EVALUATION) [Updated] This paper studied the Persistent Identifier (PID) problem, and proposed to add PID Kernel information and profile and data typing into a PID system. The overall conclusion is that this proposal can help data sharing at affordable extra cost.

I'm not very familiar with the PID system. Originally I felt this paper is clearly written and thought it might be good enough. But in the discussion with other reviewers and the SPC, I agree that this paper may lack one key piece, which is the comparison with some alternative methods such as HTTP link headers and DOI as suggested by other reviewers. Otherwise it is not convincing why the proposed method is superior. Therefore I changed my rating to ""borderline"".","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,2018-02-21,07:41,no,,423,113,426,Zhiwei Zhang,2,"(OVERALL EVALUATION) [Updated] This paper studied the Persistent Identifier (PID) problem, and proposed to add PID Kernel information and profile and data typing into a PID system. The overall conclusion is that this proposal can help data sharing at affordable extra cost.

I'm not very familiar with the PID system. Originally I felt this paper is clearly written and thought it might be good enough. But in the discussion with other reviewers and the SPC, I agree that this paper may lack one key piece, which is the comparison with some alternative methods such as HTTP link headers and DOI as suggested by other reviewers. Otherwise it is not convincing why the proposed method is superior. Therefore I changed my rating to ""borderline"".","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,2018-02-21,no
428,114,259,Phixxxx Maxx,2,"(OVERALL EVALUATION) This is a strong short paper with lots of material included. The idea of the paper is excellent:  introducing novel ranking techniques for scientific papers and venues. This is very relevant for JCDL. 

The MR-Rank approach (the main algorithm) and the results look very promising.

I wonder: How are completely new venues ranked where you have no PageRank as starting point? Is the MR approach capable to solve that issue.

Conclusion is too short. The authors should say something about their approach in a more heterogeneous setting. AAN is a narrow and small community. A snapshot of Web of Science would be an interesting bigger data set.

Minor points:

- SJR and SIF are not introduced
- Abbreviations of venues are not introduced (CL, ACL, ...)
- ""In this table, 5 out of 10 top-ranked papers (in bold) are published in top 6 venues (in bold)."" -> should be ""... are published in top _4_ venues (in bold)""?
- references can be shortened to get more space which is necessary to extend conclusions

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) A very good short paper. After revision it can be considered as a best short paper candidate.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: yes",2,,,,,2018-02-16,13:41,no,,428,114,259,Philipp Mayr,2,"(OVERALL EVALUATION) This is a strong short paper with lots of material included. The idea of the paper is excellent:  introducing novel ranking techniques for scientific papers and venues. This is very relevant for JCDL. 

The MR-Rank approach (the main algorithm) and the results look very promising.

I wonder: How are completely new venues ranked where you have no PageRank as starting point? Is the MR approach capable to solve that issue.

Conclusion is too short. The authors should say something about their approach in a more heterogeneous setting. AAN is a narrow and small community. A snapshot of Web of Science would be an interesting bigger data set.

Minor points:

- SJR and SIF are not introduced
- Abbreviations of venues are not introduced (CL, ACL, ...)
- ""In this table, 5 out of 10 top-ranked papers (in bold) are published in top 6 venues (in bold)."" -> should be ""... are published in top _4_ venues (in bold)""?
- references can be shortened to get more space which is necessary to extend conclusions

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) A very good short paper. After revision it can be considered as a best short paper candidate.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: yes",2,,,,,2018-02-16,no
430,114,232,Albxxxx Laexxxx,3,"(OVERALL EVALUATION) This short paper proposes a ranking strategy specifically designed for scientific papers and venues. The proposed strategy is based on a heterogenous network that consists of three distinct sub-networks: a paper citation network, a venue citation network and a bipartite network that ties papers and venues. The paper is in general well structured, but requires a detailed proofreading to improve its text. Moreover, although the reported results seem promising, they require a more detailed discussion to support some specific claims.

Positive Aspects
- The paper addresses a topic that is relevant to JCDL.
- The paper is in general well structured.
- Results seem promising.

Negative Aspects
- The paper requires a detailed revision by a professional proofreader to improve its text.
- Even considering that this is a short paper, experimental results require a deeper discussion.
- Several important issues in the paper are left without an explanation.
- References have been misplaced in several parts of the text.
- The paper does not follow the ACM style adopted by JCDL.

Specific Comments

1. The paper title is misleading when refers to “Heterogeneous Academic Networks”.  Why do you use the term “heterogenous”? Although your network is composed of heterogeneous nodes, the term “heterogeneous academic network” seems misleading in the context of this work (see https://en.wikipedia.org/wiki/Heterogeneous_network). Maybe “multi-node” is a more appropriate term. Check this throughout the paper.

2. Some terms in the text are used interchangeably, which is not  appropriate in a scientific paper. For example, you use “measure” with the meaning of “metric”, but ""measure"" express the size, amount or degree of something  (e.g, in Sect 1, first paragraph: “However, many such classical measures are based on …”. The word “metrics” is the most appropriate in this case). It seems that you also use the words “framework”, “algorithm” and “method” interchangeably throughout the paper (see, for instance, the title and the second subtitle of Sect 3, as well as the discussions in Sect 4.3, 4.4 and 5). It is better to adopt one of these three terms and use it consistently throughout of text.

3. References have been misplaced in several parts of the paper. For instance, in the third paragraph of Sect 2 (Related Work), Meng et al. is not reference [15]; in fact there is no reference Meng et al., but Meng and Kennedy [5]. Also there is no Yan and Ding [16], but Yan, Ding and Sugimoto [15]. Also, Tri-Rank is not reference [17], but [16]. Check the references throughout the paper (or use an automatic citation tool). 

4. Still regarding references, they should be cited in the same order they appear in the list of references. Thus, [11-13, 4] should be [4, 11-13]. Also, do not use a reference number as the subject of a sentence. For example, “Among all the works …, [17] is the most relevant to our work.” Replace this by “Among all the works …, that by Hassan and Getoor [17] is the most relevant to ours.” In this same paragraph there are other examples of this kind of construction that should be rephrased. In addition, according to the ACM paper style adopted by JCDL, references should be listed in alphabetic order, not in the order they are cited in the paper. 

5. It seems that the authors have not used the proper ACM template when preparing this manuscript. The following comments should be taken into consideration: 

a) Section titles should have all its words capitalized.
1 INTRODUCTION
2 RELATED WORK
3 THE MR-RANK FRAMEWORK
4 EXPERIMENTS
5 CONCLUSIONS
 
b) Subsection titles should have the first letter of each word capitalized. 
3.1 Preliminaries 
3.2 The Ranking Algorithm
4.1 Dataset and Settings

6. At the end of Sect 1, you refer to the dataset used in the experiments as the AAN dataset. However, reference [6] refers to it as the ACL anthology corpus. Check how this dataset should be named.

7. A brief  paragraph describing the paper structure should be added to the end of Sect 1.

8. When describing your algorithm (Sect 3.2), Rule 1 refers to AVE_Vs (v_k) as “the average score that venue  v_k obtains in the last three years”. Why three years? Please explain this decision. Also, explain the role of the parameter alpha.

9. When describing equation (3) in Rule 2 (Sect 3.2), invert the order of the components (3) and (4) so that they can follow the same order they appear in (2).

10. For better reading, rephrase the last sentence of the paragraph that describes Algorithm 1 by removing the itemization as follows: “More specifically, at each iteration step, we (1) update the scores of all the papers using Rule 1 and (2) update the scores of all the venues by using Rule 2.”

11. Sect 4.1 (Dataset and Settings) should justify the choice of the baselines. Although the choice of PageRank and HITS might be obvious, you should at least comment of the other ones, which seem to be variations of the proposed algorithm. Also, the parameter setup described at the end of this section also requires some explanation. You cannot just say that it is “good for performance prediction of papers.”

12. Explanation in footnote 2 is not clear. Please, justify your choice.

13. Try to explain why HITS has obtained the smallest precision value. Just making such an observation is not enough!

14. Results expressed by the graphs in Sect. 4.2, 4.3 and 4.4 should be discussed in more details.

15. The paper requires a detailed proofreading in order to correct many typos and grammatical errors throughout the text. As a specific point, numbers from 1 to 9 should be written out. For instance,  the caption of Fig. 3 should be: “Convergence rate of the six algorithms.”

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This short paper addresses a topic that is relevant to JCDL and presents some preliminary results very promising. However, it seems it has been written in a hurry and the authors had no time to proofread it properly. The text has many typos and grammatical errors, and the results are just superficially described. Besides, the paper does not properly follow the ACM template, which shows some carelessness from the authors. In view of that, I do not recommend the acceptance of this, even considering the promising results reported.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-16,20:48,no,,430,114,232,Alberto Laender,3,"(OVERALL EVALUATION) This short paper proposes a ranking strategy specifically designed for scientific papers and venues. The proposed strategy is based on a heterogenous network that consists of three distinct sub-networks: a paper citation network, a venue citation network and a bipartite network that ties papers and venues. The paper is in general well structured, but requires a detailed proofreading to improve its text. Moreover, although the reported results seem promising, they require a more detailed discussion to support some specific claims.

Positive Aspects
- The paper addresses a topic that is relevant to JCDL.
- The paper is in general well structured.
- Results seem promising.

Negative Aspects
- The paper requires a detailed revision by a professional proofreader to improve its text.
- Even considering that this is a short paper, experimental results require a deeper discussion.
- Several important issues in the paper are left without an explanation.
- References have been misplaced in several parts of the text.
- The paper does not follow the ACM style adopted by JCDL.

Specific Comments

1. The paper title is misleading when refers to “Heterogeneous Academic Networks”.  Why do you use the term “heterogenous”? Although your network is composed of heterogeneous nodes, the term “heterogeneous academic network” seems misleading in the context of this work (see https://en.wikipedia.org/wiki/Heterogeneous_network). Maybe “multi-node” is a more appropriate term. Check this throughout the paper.

2. Some terms in the text are used interchangeably, which is not  appropriate in a scientific paper. For example, you use “measure” with the meaning of “metric”, but ""measure"" express the size, amount or degree of something  (e.g, in Sect 1, first paragraph: “However, many such classical measures are based on …”. The word “metrics” is the most appropriate in this case). It seems that you also use the words “framework”, “algorithm” and “method” interchangeably throughout the paper (see, for instance, the title and the second subtitle of Sect 3, as well as the discussions in Sect 4.3, 4.4 and 5). It is better to adopt one of these three terms and use it consistently throughout of text.

3. References have been misplaced in several parts of the paper. For instance, in the third paragraph of Sect 2 (Related Work), Meng et al. is not reference [15]; in fact there is no reference Meng et al., but Meng and Kennedy [5]. Also there is no Yan and Ding [16], but Yan, Ding and Sugimoto [15]. Also, Tri-Rank is not reference [17], but [16]. Check the references throughout the paper (or use an automatic citation tool). 

4. Still regarding references, they should be cited in the same order they appear in the list of references. Thus, [11-13, 4] should be [4, 11-13]. Also, do not use a reference number as the subject of a sentence. For example, “Among all the works …, [17] is the most relevant to our work.” Replace this by “Among all the works …, that by Hassan and Getoor [17] is the most relevant to ours.” In this same paragraph there are other examples of this kind of construction that should be rephrased. In addition, according to the ACM paper style adopted by JCDL, references should be listed in alphabetic order, not in the order they are cited in the paper. 

5. It seems that the authors have not used the proper ACM template when preparing this manuscript. The following comments should be taken into consideration: 

a) Section titles should have all its words capitalized.
1 INTRODUCTION
2 RELATED WORK
3 THE MR-RANK FRAMEWORK
4 EXPERIMENTS
5 CONCLUSIONS
 
b) Subsection titles should have the first letter of each word capitalized. 
3.1 Preliminaries 
3.2 The Ranking Algorithm
4.1 Dataset and Settings

6. At the end of Sect 1, you refer to the dataset used in the experiments as the AAN dataset. However, reference [6] refers to it as the ACL anthology corpus. Check how this dataset should be named.

7. A brief  paragraph describing the paper structure should be added to the end of Sect 1.

8. When describing your algorithm (Sect 3.2), Rule 1 refers to AVE_Vs (v_k) as “the average score that venue  v_k obtains in the last three years”. Why three years? Please explain this decision. Also, explain the role of the parameter alpha.

9. When describing equation (3) in Rule 2 (Sect 3.2), invert the order of the components (3) and (4) so that they can follow the same order they appear in (2).

10. For better reading, rephrase the last sentence of the paragraph that describes Algorithm 1 by removing the itemization as follows: “More specifically, at each iteration step, we (1) update the scores of all the papers using Rule 1 and (2) update the scores of all the venues by using Rule 2.”

11. Sect 4.1 (Dataset and Settings) should justify the choice of the baselines. Although the choice of PageRank and HITS might be obvious, you should at least comment of the other ones, which seem to be variations of the proposed algorithm. Also, the parameter setup described at the end of this section also requires some explanation. You cannot just say that it is “good for performance prediction of papers.”

12. Explanation in footnote 2 is not clear. Please, justify your choice.

13. Try to explain why HITS has obtained the smallest precision value. Just making such an observation is not enough!

14. Results expressed by the graphs in Sect. 4.2, 4.3 and 4.4 should be discussed in more details.

15. The paper requires a detailed proofreading in order to correct many typos and grammatical errors throughout the text. As a specific point, numbers from 1 to 9 should be written out. For instance,  the caption of Fig. 3 should be: “Convergence rate of the six algorithms.”

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This short paper addresses a topic that is relevant to JCDL and presents some preliminary results very promising. However, it seems it has been written in a hurry and the authors had no time to proofread it properly. The text has many typos and grammatical errors, and the results are just superficially described. Besides, the paper does not properly follow the ACM template, which shows some carelessness from the authors. In view of that, I do not recommend the acceptance of this, even considering the promising results reported.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2018-02-16,no
433,115,411,Zhxxx Xxx,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. Are these trust violations? 

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,22:44,no,,433,115,411,Zhiwu Xie,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. Are these trust violations? 

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,no
434,115,411,Zhxxx Xxx,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. Are these trust violations?

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,23:16,no,,434,115,411,Zhiwu Xie,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. Are these trust violations?

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,no
435,115,411,Zhxxx Xxx,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. It's not clear why these practices violates trust.

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,23:18,no,,435,115,411,Zhiwu Xie,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. It's not clear why these practices violates trust.

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,no
436,115,411,Zhxxx Xxx,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. It's not clear why these are trust violations.

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,23:20,no,,436,115,411,Zhiwu Xie,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct webpages from archived mementos. It would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. It's not clear why these are trust violations.

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable therefore reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,no
437,115,411,Zhxxx Xxx,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct and exhibit webpages from archived mementos. An archive is not obliged to replay webpages. If they do so, it would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. Saying these practices are trust violations is a bit shaky.

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable and reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,23:26,no,,437,115,411,Zhiwu Xie,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations:

1. Is it not clear why changes in replaying archived web pages constitute trust violations. The replay is an archive's attempt to reconstruct and exhibit webpages from archived mementos. An archive is not obliged to replay webpages. If they do so, it would be a misnomer or misrepresentation to assume the reconstruction has to be exactly what was seen before. As the results show, there are at least three types of changes: a) if the web page was dynamically constructed, reconstructing it may not exactly replicate its original state. b) the archive inserts its own dynamic content (e.g., headers, link rewrites and redirections, etc) during the reconstruction and c) the state of the archive itself changes, therefore affects the reconstruction. Analogies could be a) using an ancient musical instrument to play a modern tune; b) Arranging a Da Vinci piece to be exhibit next to a Michelangelo piece, which was not their original arrangements; c) Lending the Da Vinci piece temporarily to another archive, therefore changing the exhibit again. Saying these practices are trust violations is a bit shaky.

2. The trust of the archive should be on the authenticity of its archival process and archived mementos, not the recomposition of them. Just because a memento is authentic does not mean its replicable and reconstructable. In this sense the paper glosses over the term ""composite memento"". Is there such a thing?

3. The ""Michael's Evil Wayback"" sounds like a straw man. Even if an archive reconstructs exactly the same webpage all the time, it may still misrepresent the URI-R.

4. It's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

5. It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,no
440,115,367,Nicxxxxx Taxxxx,3,"(OVERALL EVALUATION) Authors retrieved and hashed a set of composite mementos from multiple web archives on a repeat basis to assess the stability of their fixity. They found that almost 20% of composite mementos did not have a stable hash across the successive measurements, due to changes in HTTP status code, HTTP entity, TimeMap, HTTP response headers, and dynamic values generated by JavaScript.

Authors correctly observe that the reliability of web archives is presently largely based on institutional reputation, and this is likewise the case for web archives as applied in litigation. In the legal context, this has started to give way to a concern with provenance - e.g., can Internet Archive attest to the conditions of the generation of all subsets of archived web data made accessible through the Wayback Machine? There are at least a few more web archive-specific considerations - e.g., completeness and temporal coherence - that likely post more conspicuous reliability concerns to courts than non-repeatability of hashes for composite mementos. Nonetheless, authors have highlighted a legitimate and novel reliability measure worth taking into account.

I think the article could benefit from further discussion of the importance of the findings. The results suggest the hash value for a composite memento may not be a reliable indicator of the reliability of that memento or of its parent web archive, since that value may vary due to any number of apparently common non-malicious and unpredictable changes. Considering the litigation context, the study is therefore principally useful in affirming that discrepancies in repeated hash checks of a composite memento cannot be taken for granted to mean that the information or web archive itself has been corrupted. That is a conclusion some ways away from having a new tool to detect malicious alteration of archived web resources.

It strikes me that if the devised method for hashing of composite mementos is to be useful for assessing reliability, web archive systems need to be better architected to minimize or mitigate these copious instances of false positives. Do specific recommendations for the enhancement of web archive systems follow from this research? If so, it would be great to see those unpacked.

A few additional questions for the authors:
* Could cached responses from a CDX server API serve some role in evaluating memento fixity over time? What about identifying identical mementos stored in different web archives?
* Why do you suppose there was the degree of observed variability in cumulative change across web archives? It did not seem to correlate with the web archive replay system employed.

A couple of quibbles:
1: ""We introduced some requirements to be fulfilled in order to generate repeatable hashes...(1) A generated hash must be repeatable"".
This seems like circular logic.

2: ""OpenWayback will replay the content of any selected archived web page in the browser.""
It might be worth qualifying that only those mementos in the index for that particular OpenWayback instance may be replayed.

A couple of copyedits needed:
5: ""The last column 'Transformation' in the table indicates different type of modification performed by the archive.""
7: ""This is indicted by a change in the 'Location' HTTP header.""","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-17,07:37,no,,440,115,367,Nicholas Taylor,3,"(OVERALL EVALUATION) Authors retrieved and hashed a set of composite mementos from multiple web archives on a repeat basis to assess the stability of their fixity. They found that almost 20% of composite mementos did not have a stable hash across the successive measurements, due to changes in HTTP status code, HTTP entity, TimeMap, HTTP response headers, and dynamic values generated by JavaScript.

Authors correctly observe that the reliability of web archives is presently largely based on institutional reputation, and this is likewise the case for web archives as applied in litigation. In the legal context, this has started to give way to a concern with provenance - e.g., can Internet Archive attest to the conditions of the generation of all subsets of archived web data made accessible through the Wayback Machine? There are at least a few more web archive-specific considerations - e.g., completeness and temporal coherence - that likely post more conspicuous reliability concerns to courts than non-repeatability of hashes for composite mementos. Nonetheless, authors have highlighted a legitimate and novel reliability measure worth taking into account.

I think the article could benefit from further discussion of the importance of the findings. The results suggest the hash value for a composite memento may not be a reliable indicator of the reliability of that memento or of its parent web archive, since that value may vary due to any number of apparently common non-malicious and unpredictable changes. Considering the litigation context, the study is therefore principally useful in affirming that discrepancies in repeated hash checks of a composite memento cannot be taken for granted to mean that the information or web archive itself has been corrupted. That is a conclusion some ways away from having a new tool to detect malicious alteration of archived web resources.

It strikes me that if the devised method for hashing of composite mementos is to be useful for assessing reliability, web archive systems need to be better architected to minimize or mitigate these copious instances of false positives. Do specific recommendations for the enhancement of web archive systems follow from this research? If so, it would be great to see those unpacked.

A few additional questions for the authors:
* Could cached responses from a CDX server API serve some role in evaluating memento fixity over time? What about identifying identical mementos stored in different web archives?
* Why do you suppose there was the degree of observed variability in cumulative change across web archives? It did not seem to correlate with the web archive replay system employed.

A couple of quibbles:
1: ""We introduced some requirements to be fulfilled in order to generate repeatable hashes...(1) A generated hash must be repeatable"".
This seems like circular logic.

2: ""OpenWayback will replay the content of any selected archived web page in the browser.""
It might be worth qualifying that only those mementos in the index for that particular OpenWayback instance may be replayed.

A couple of copyedits needed:
5: ""The last column 'Transformation' in the table indicates different type of modification performed by the archive.""
7: ""This is indicted by a change in the 'Location' HTTP header.""","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-17,no
444,116,372,Alxx Thuxxxx,2,"(OVERALL EVALUATION) This is a very solid research paper on the topic of the temporal variability of search engine result pages and its relation to identifying seed URLs for event-based or thematic web collections. The writing is clear and precise, the illustrative figures and tables can be dense but are understandable, and the framing of the research questions in the context of related work is very good. The work is apropos for JCDL, and the takeaway finding that SERP-based collection building should start soon after an event and be repeated persistently afterward due to the variability of top search results is useful for web curators, even if the conceit of basing thematic web collections solely on the first five pages of SERPs is less likely to be followed in practice. The future work suggestion of assessing the effectiveness of SERPs for non-news based collections is welcome.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-13,21:46,no,,444,116,372,Alex Thurman,2,"(OVERALL EVALUATION) This is a very solid research paper on the topic of the temporal variability of search engine result pages and its relation to identifying seed URLs for event-based or thematic web collections. The writing is clear and precise, the illustrative figures and tables can be dense but are understandable, and the framing of the research questions in the context of related work is very good. The work is apropos for JCDL, and the takeaway finding that SERP-based collection building should start soon after an event and be repeated persistently afterward due to the variability of top search results is useful for web curators, even if the conceit of basing thematic web collections solely on the first five pages of SERPs is less likely to be followed in practice. The future work suggestion of assessing the effectiveness of SERPs for non-news based collections is welcome.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-13,no
453,119,387,Anxxx Velxxxx,1,"(OVERALL EVALUATION) I very much enjoyed this well written article. I think it quite relevant to the JCDL community, it covers the prior art well and presents an intersting and complementary approach to the existing methods for predicting citation counts.    

There are nevertheless some improvements / modifications that I would suggest prior to final acceptance.  One - mentioned in the abstract and also later in the body of the article has to do with the classifier criterion, namely

""whether or not an article will receive at least the the median citation count of a collection of scholarly articles""

Without any further details, it isn't at all clear to which collection the authors refer nor to *when* the median citation count is evaluated.  

I assume the authors are referring to the collection of 122,653 randomly chosen articles mentioned earlier. If that is the case then the total number of citations for each article obtained from Google Scholar is going to depend on the date of publication.

Perhaps this collection should be segmented into publication periods (maybe Winter / Spring / Summer / Fall of a given year) and the median calculated for each segment of each year in which the articles were published.  Then the same would need to be done for the citation counts for each article to get a uniform measure of ""cited @ T"".

Another has to do with the concept of ""reachability"" in list of 12 features enumerated on p.2 :

""the number of unique hashtags used in tweets that are related to an article,
which indicates the reachability of an article in general terms""

The third feature listed, i.e. the

""number of different mentions used by users in their tweets""

is also unclear.  It could mean the total number of unique users which mention the article in their tweets or it could mean the number of unique tweets originated by all users, not including re-tweets by other users.  Or perhaps it means something else.

the legend, axes and title of Figure 2 is in very small font and hard to read.

A few grammatical suggestions

""Number of countries an article is mentioned from"" -> ""Number of countries from which an article is mentioned""

""Number of platforms an article is mentioned on"" -> ""Number of platforms on which an article is mentioned""

Conclusion

While I think the ability to establish early predictors based on Altmetrics data is interesting and worthy of study, I take issue with the authors' conclusion:

"".... predicting the scholarly impact of research at an early stage would save the scholarly community, research agencies, and policy makers crucial time and thereby accelerate overall research progress.""

Suppose the authors had used a dataset of features among the population of criminals in the criminal justice system that could predict the likelihood of recidivism. This would be worthy of note but not because:

""... predicting the [likelihood that a convict would re-offend] at an early stage would save the [criminal justice system], [penal] agencies, and [law] makers crucial time and thereby accelerate overall [convicts are re-arrested].""","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-13,23:09,no,,453,119,387,Andre Vellino,1,"(OVERALL EVALUATION) I very much enjoyed this well written article. I think it quite relevant to the JCDL community, it covers the prior art well and presents an intersting and complementary approach to the existing methods for predicting citation counts.    

There are nevertheless some improvements / modifications that I would suggest prior to final acceptance.  One - mentioned in the abstract and also later in the body of the article has to do with the classifier criterion, namely

""whether or not an article will receive at least the the median citation count of a collection of scholarly articles""

Without any further details, it isn't at all clear to which collection the authors refer nor to *when* the median citation count is evaluated.  

I assume the authors are referring to the collection of 122,653 randomly chosen articles mentioned earlier. If that is the case then the total number of citations for each article obtained from Google Scholar is going to depend on the date of publication.

Perhaps this collection should be segmented into publication periods (maybe Winter / Spring / Summer / Fall of a given year) and the median calculated for each segment of each year in which the articles were published.  Then the same would need to be done for the citation counts for each article to get a uniform measure of ""cited @ T"".

Another has to do with the concept of ""reachability"" in list of 12 features enumerated on p.2 :

""the number of unique hashtags used in tweets that are related to an article,
which indicates the reachability of an article in general terms""

The third feature listed, i.e. the

""number of different mentions used by users in their tweets""

is also unclear.  It could mean the total number of unique users which mention the article in their tweets or it could mean the number of unique tweets originated by all users, not including re-tweets by other users.  Or perhaps it means something else.

the legend, axes and title of Figure 2 is in very small font and hard to read.

A few grammatical suggestions

""Number of countries an article is mentioned from"" -> ""Number of countries from which an article is mentioned""

""Number of platforms an article is mentioned on"" -> ""Number of platforms on which an article is mentioned""

Conclusion

While I think the ability to establish early predictors based on Altmetrics data is interesting and worthy of study, I take issue with the authors' conclusion:

"".... predicting the scholarly impact of research at an early stage would save the scholarly community, research agencies, and policy makers crucial time and thereby accelerate overall research progress.""

Suppose the authors had used a dataset of features among the population of criminals in the criminal justice system that could predict the likelihood of recidivism. This would be worthy of note but not because:

""... predicting the [likelihood that a convict would re-offend] at an early stage would save the [criminal justice system], [penal] agencies, and [law] makers crucial time and thereby accelerate overall [convicts are re-arrested].""","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-13,no
457,120,244,Clixxxxx Lyxxx,1,"(OVERALL EVALUATION) This is an interesting exploratory paper. The deluge of data towards the end is poorly synthesized and quite hard to follow, but the high level findings are very useful (for example section 3.2). 

The discussion in section 3 and elsewhere on data preparation and cleaning is very, very important and helps to make clear that this is a somewhat artificial exploration of proof of concept, but that actual operational implementation of this at scale is very very problematic. One particular point that needs a bit of brief exploration here is whether having papers in some other format that PDF (Computer Science widely uses TeX, and in other areas Word is very commonplace) would help with the text processing flow. 

I would speculate that this works best for very focused journals; I suspect it would be ineffective for something like Science or even CACM. Perhaps worth a comment?

There are some formatting problems with the paper. The version I downloaded didn't include any author information on the front page, for example. This clearly needs cleaned up. Also, there are frequent english language problems, mostly misuses of language with are very distracting but can be figured out. I don't want to spend a great deal of time enumerating all of these here, but you should have a native english speaker read this paper and do some light copy-editing. It would make a big difference. Also there are a few places where the text speaks of a ""thesis"" rather than a paper (I realized much of it is based on a thesis). 

Finally I have a problem in that the paper totally ignores the implications of what the authors have learned, and what might happen if they succeed. For example, it seems very likely that the kinds of algorithms and criteria described here would strongly bias against really creative, ""out of the box"" papers that for example import methods or discoveries from one area into another area. Instead, they tend to promote conformity, and more articles of the sort that have been published, and hence, very safe, incremental work. This really has got to be addressed. I would happily trade off some of the very detailed experimental results for this discussion if space is an issue.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This  is a worthwile paper with some interesting results in it. However, I would particularly draw attention to my final paragraph of comments.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-14,03:15,no,,457,120,244,Clifford Lynch,1,"(OVERALL EVALUATION) This is an interesting exploratory paper. The deluge of data towards the end is poorly synthesized and quite hard to follow, but the high level findings are very useful (for example section 3.2). 

The discussion in section 3 and elsewhere on data preparation and cleaning is very, very important and helps to make clear that this is a somewhat artificial exploration of proof of concept, but that actual operational implementation of this at scale is very very problematic. One particular point that needs a bit of brief exploration here is whether having papers in some other format that PDF (Computer Science widely uses TeX, and in other areas Word is very commonplace) would help with the text processing flow. 

I would speculate that this works best for very focused journals; I suspect it would be ineffective for something like Science or even CACM. Perhaps worth a comment?

There are some formatting problems with the paper. The version I downloaded didn't include any author information on the front page, for example. This clearly needs cleaned up. Also, there are frequent english language problems, mostly misuses of language with are very distracting but can be figured out. I don't want to spend a great deal of time enumerating all of these here, but you should have a native english speaker read this paper and do some light copy-editing. It would make a big difference. Also there are a few places where the text speaks of a ""thesis"" rather than a paper (I realized much of it is based on a thesis). 

Finally I have a problem in that the paper totally ignores the implications of what the authors have learned, and what might happen if they succeed. For example, it seems very likely that the kinds of algorithms and criteria described here would strongly bias against really creative, ""out of the box"" papers that for example import methods or discoveries from one area into another area. Instead, they tend to promote conformity, and more articles of the sort that have been published, and hence, very safe, incremental work. This really has got to be addressed. I would happily trade off some of the very detailed experimental results for this discussion if space is an issue.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This  is a worthwile paper with some interesting results in it. However, I would particularly draw attention to my final paragraph of comments.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-14,no
460,120,61,Vitxxxx Casxxxxx,3,"(OVERALL EVALUATION) The paper presents an interesting AI approach to evaluate 
the chances for a paper submitted to a journal to be 
rejected right away or sent to reviewers for evaluation. 
The authors have identified and defined thirty features 
by which a supervised binary classifier will ""decide"" 
whether the submitted paper should be accepted (sent to 
reviewers) or rejected. 
The idea is a novel one, the methodology appear sound 
and well thought, the amount of data on which the 
definition of the features has been based is big enough, 
so this paper should definitely be of interest to the 
attendees of JCDL 2018. 
As a side (personal) comment, while a ""tool"" like this 
could be useful for authors as an aid to improve the 
quality of their papers, I see some danger in the 
indiscriminate use of the tool on the part of journal 
editors. Even worst, I would be afraid of the use of 
such a tool based on ""deep learning"" (as suggested 
in the conclusions) as at that point it would be 
impossible to understand why a paper would be 
accepted or rejected.
Finally, the paper is very well organized and the topics 
are well explained, but it might benefit from a review 
of the style and grammar of English.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: no",3,,,,,2018-02-16,22:43,no,,460,120,61,Vittore Casarosa,3,"(OVERALL EVALUATION) The paper presents an interesting AI approach to evaluate 
the chances for a paper submitted to a journal to be 
rejected right away or sent to reviewers for evaluation. 
The authors have identified and defined thirty features 
by which a supervised binary classifier will ""decide"" 
whether the submitted paper should be accepted (sent to 
reviewers) or rejected. 
The idea is a novel one, the methodology appear sound 
and well thought, the amount of data on which the 
definition of the features has been based is big enough, 
so this paper should definitely be of interest to the 
attendees of JCDL 2018. 
As a side (personal) comment, while a ""tool"" like this 
could be useful for authors as an aid to improve the 
quality of their papers, I see some danger in the 
indiscriminate use of the tool on the part of journal 
editors. Even worst, I would be afraid of the use of 
such a tool based on ""deep learning"" (as suggested 
in the conclusions) as at that point it would be 
impossible to understand why a paper would be 
accepted or rejected.
Finally, the paper is very well organized and the topics 
are well explained, but it might benefit from a review 
of the style and grammar of English.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: no",3,,,,,2018-02-16,no
470,123,247,Gaxx Marcxxxxxxx,1,"(OVERALL EVALUATION) This paper presents a collaborative filtering strategy that applies a neural network model to discovering relationships in biomedical literature.  The main innovation in this paper is adding an ‘uncertainty constraint’ to a multi-layer NN technique.  The idea is that this addition provides a computed assessment of the value of discovered associations to predict future associations.  The addition computes the weighted sum of errors due to classification and regression.  The two alternative NN techniques are run on Medline abstracts with typical cleaning/filtering applied.  Three metrics are used (ranking, classification, and F1) to compare the two NN techniques to rankings with 6 other systems, classification with one other system (factorization machine), and F1 scores with factorization machine runs.  The results demonstrate that the NN techniques perform slightly better than a factorization machine method and the other techniques in the case of ranking.  The NN technique with uncertainty constraint applied does a little better that the NN in all cases.  One important advantage of the NN approach over the factorization machine approach is that there is less computational cost for the NN models.  Additionally is scales well for very large collections.
A few questions/suggestions for the authors include:  How domain dependent is the uncertainty model (would training data in a financial or other space perform similarly)? 
You omitted terms that map to more than one UMLS entry.  This seems sensible but for discovering new relationships that are latent in the literature, these terms might actually act as latent ‘bridges’ rather than stop words.  It seems like this is what Swanson originally did to find paths in mutually disjoint literatures.
In section 4.5 you note that uncertainty loss plays a small role and should not be overused.  Is this an overfitting problem? 
In the conclusions, your future work might consider weighting figure/table captions more than text.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-08,20:08,no,,470,123,247,Gary Marchionini,1,"(OVERALL EVALUATION) This paper presents a collaborative filtering strategy that applies a neural network model to discovering relationships in biomedical literature.  The main innovation in this paper is adding an ‘uncertainty constraint’ to a multi-layer NN technique.  The idea is that this addition provides a computed assessment of the value of discovered associations to predict future associations.  The addition computes the weighted sum of errors due to classification and regression.  The two alternative NN techniques are run on Medline abstracts with typical cleaning/filtering applied.  Three metrics are used (ranking, classification, and F1) to compare the two NN techniques to rankings with 6 other systems, classification with one other system (factorization machine), and F1 scores with factorization machine runs.  The results demonstrate that the NN techniques perform slightly better than a factorization machine method and the other techniques in the case of ranking.  The NN technique with uncertainty constraint applied does a little better that the NN in all cases.  One important advantage of the NN approach over the factorization machine approach is that there is less computational cost for the NN models.  Additionally is scales well for very large collections.
A few questions/suggestions for the authors include:  How domain dependent is the uncertainty model (would training data in a financial or other space perform similarly)? 
You omitted terms that map to more than one UMLS entry.  This seems sensible but for discovering new relationships that are latent in the literature, these terms might actually act as latent ‘bridges’ rather than stop words.  It seems like this is what Swanson originally did to find paths in mutually disjoint literatures.
In section 4.5 you note that uncertainty loss plays a small role and should not be overused.  Is this an overfitting problem? 
In the conclusions, your future work might consider weighting figure/table captions more than text.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-08,no
472,123,388,Kaxxx Verxxxxx,2,"(OVERALL EVALUATION) This is a nice study on the topic of literature-based discovery, exploring both the contribution of ""global"" models for establishing association between entities, extensions of these using neural representations, and the incorporation of a model of uncertainty. The authors have introduced an innovative approach using neural models, and with the incorporation of the uncertainty model. The presented results are convincing.

I am not an expert on existing methods for this, although the authors seem to have done a reasonably good job of identifying and contrasting prior approaches. I note this relatively new review paper which is not cited (https://www.ncbi.nlm.nih.gov/pubmed/28838802) but reviews various methods for evaluating (the authors use the ""time-slicing"" method, predicting things one year out from a training set; the authors should relate the framing of the task in this way to previous methods) as well as detailing prior approaches; this reference should be taken into consideration. In particular, the evaluation metrics could be compared here, although I think the authors have done a good job of justifying their choices.

The authors should probably reference MetaMap as an alternative term matching approach; also see Funk et al (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-59) for evidence that exact matching might might be so good.

The authors should indicate how the basic performance of their uncertainty model compares to the state-of-the-art on the CoNLL-2010 data.

There are some minor grammatical errors in the paper, some examples:
""methods considers"" -> ""methods consider""
""if a pair .. appear"" -> "".. appears""
""few if not none"" -> ""few or even no""
""longest term to shorted term"" -> ""longest term to shortest term""
""Same as"" -> ""As for""

Some other minor issues:
- ""abstract full-text data"" -> ""abstract text data""
- I wasn't sure what ""arbitrary-order"" referred to specifically at the bottom of p 1 (right col).
- e-2 is stated to be high certainty, with an uncertainty value of 0.999; e-1 lower certainty with an uncertainty value of 0.0006: do you mean ""certainty"" value rather than ""uncertainty""? What is the range of (un)certainty values?
- there are a few citations missing (""kim citation"", ""bio citation"")","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-10,05:15,no,,472,123,388,Karin Verspoor,2,"(OVERALL EVALUATION) This is a nice study on the topic of literature-based discovery, exploring both the contribution of ""global"" models for establishing association between entities, extensions of these using neural representations, and the incorporation of a model of uncertainty. The authors have introduced an innovative approach using neural models, and with the incorporation of the uncertainty model. The presented results are convincing.

I am not an expert on existing methods for this, although the authors seem to have done a reasonably good job of identifying and contrasting prior approaches. I note this relatively new review paper which is not cited (https://www.ncbi.nlm.nih.gov/pubmed/28838802) but reviews various methods for evaluating (the authors use the ""time-slicing"" method, predicting things one year out from a training set; the authors should relate the framing of the task in this way to previous methods) as well as detailing prior approaches; this reference should be taken into consideration. In particular, the evaluation metrics could be compared here, although I think the authors have done a good job of justifying their choices.

The authors should probably reference MetaMap as an alternative term matching approach; also see Funk et al (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-59) for evidence that exact matching might might be so good.

The authors should indicate how the basic performance of their uncertainty model compares to the state-of-the-art on the CoNLL-2010 data.

There are some minor grammatical errors in the paper, some examples:
""methods considers"" -> ""methods consider""
""if a pair .. appear"" -> "".. appears""
""few if not none"" -> ""few or even no""
""longest term to shorted term"" -> ""longest term to shortest term""
""Same as"" -> ""As for""

Some other minor issues:
- ""abstract full-text data"" -> ""abstract text data""
- I wasn't sure what ""arbitrary-order"" referred to specifically at the bottom of p 1 (right col).
- e-2 is stated to be high certainty, with an uncertainty value of 0.999; e-1 lower certainty with an uncertainty value of 0.0006: do you mean ""certainty"" value rather than ""uncertainty""? What is the range of (un)certainty values?
- there are a few citations missing (""kim citation"", ""bio citation"")","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-10,no
474,123,358,Kazxxxxx Sugxxxxx,3,"(OVERALL EVALUATION) Existing works in literature based discovery use only local-information and do not take 
the effect of ""uncertainty"" into account.  To solve these problems, the authors propose 
neural collaborative filtering to discover biomedical knowledge. 

But this paper has a lot of problems. The details are as follows:


(1) Novelty/Originality
- It seems that the authors just apply neural collaborative filtering [13] 
and convolutional neural netowrk for sentence classification [18]. If not, 
the authors need to clearly claim their novelty. 


(2) Methodology
- It is nice to conduct experiments on publicly available datasets. But the authors need to optimize the number of layers in their ""MLP-CF"" and ""MLP-CF-uncertain"" approach. Whie the authors have already shown experimental results on impact of \alpha, they also need to show the results on impact of the number of layers. 

In addition, as pointed ""(1) Novelty/Originality"", the authors should develop original approach. 


(3) Assessment/Evaluation/Comparison
- It is nice that the authors compare their approach with several baselines. But they are relatively weak baselines. The authors employ neural-based collaborative filtering approach, so they need to compare their proposed approach with some variants of neural-based collaborative filtering. 

- In Sec 4.4, the authors claim that 
""we can see that our models perform better than Factorization Machine with considerable margin."" 　
regarding the F-1 scores in Table 3. 

But the authors need to perform statistical test to support the imporvement with considerable margin. 

- It is better to perform microscopic analysis as well as discussion on overall scores. 

- In abstract, the authors describe that 
""our MLP-CF model performs comparably with strong baseline Factorization Machine with much faster performance.""

But the authors do not show experimental results on procesing time. 


(4) Style/Quality of Writing
The reviewer would like the authors to correct or improve the followings. 
The reviewer only show important ones only. Please careully check your paper. 

[Sec 1]
""uncertinty"" effec => ""uncertinty"" effec*t*


[Sec 2.1]
- subsection title, ""Knowledge discovery"" => ""Knowledge Discovery""
(It is necessary to be consistent with other subsections.)

- the local-information based LBD may perform not as well, ...
=> the local-information based LBD may *not* perform not *so* well, ...


[""Multi-perceptron layers"" Sec 3.1]
If the authors use ""l_{i}"" to denote each fully-connected layer instead of just ""i"", 
Equations (3), (4), and (5) would be more intuitive.   


[""Ranking AUC"" at Sec 4.4]
- ... is show in Table 1. => ... is show*n* in Table 1.

- Second, Jaccard's coefficients and BITOLA performs ...
=> Second, Jaccard's coefficients and BITOLA *perform* ...


[""Classification AUC"" at Sec 4.4]
- compared to that in Table 1. => compared to *those* in Table 1.


[Others]
- In Tables 1, 2, and 3, the authors should use ""MLP-CF"" and ""MLP-CF-uncertain"" 
in the same phrase as the body of the paper not ""MLP-CF"" and ""MLP_Uncertain"", respectively. 


(5) Replicability
- As the reviewer pointed out in ""(1) Novelty/Originality"", the authors' approach largely 
relies on exiting approaches. In addition, the authors conduct experiments in publicly 
available datasets. From these point of view, this work has high replicability. 


(6) References
- In ""References"", the authors often skip venues ([6]), page numbers ([7], [13], [14], [15], [16]). 
While the authors cite three arXiv papers, it is highly possible that they have been published in 
official international conferences. So the authors need to cite the official published papers. 
Especially, [18] has been published in EMNLP2014.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-15,07:22,no,,474,123,358,Kazunari Sugiyama,3,"(OVERALL EVALUATION) Existing works in literature based discovery use only local-information and do not take 
the effect of ""uncertainty"" into account.  To solve these problems, the authors propose 
neural collaborative filtering to discover biomedical knowledge. 

But this paper has a lot of problems. The details are as follows:


(1) Novelty/Originality
- It seems that the authors just apply neural collaborative filtering [13] 
and convolutional neural netowrk for sentence classification [18]. If not, 
the authors need to clearly claim their novelty. 


(2) Methodology
- It is nice to conduct experiments on publicly available datasets. But the authors need to optimize the number of layers in their ""MLP-CF"" and ""MLP-CF-uncertain"" approach. Whie the authors have already shown experimental results on impact of \alpha, they also need to show the results on impact of the number of layers. 

In addition, as pointed ""(1) Novelty/Originality"", the authors should develop original approach. 


(3) Assessment/Evaluation/Comparison
- It is nice that the authors compare their approach with several baselines. But they are relatively weak baselines. The authors employ neural-based collaborative filtering approach, so they need to compare their proposed approach with some variants of neural-based collaborative filtering. 

- In Sec 4.4, the authors claim that 
""we can see that our models perform better than Factorization Machine with considerable margin."" 　
regarding the F-1 scores in Table 3. 

But the authors need to perform statistical test to support the imporvement with considerable margin. 

- It is better to perform microscopic analysis as well as discussion on overall scores. 

- In abstract, the authors describe that 
""our MLP-CF model performs comparably with strong baseline Factorization Machine with much faster performance.""

But the authors do not show experimental results on procesing time. 


(4) Style/Quality of Writing
The reviewer would like the authors to correct or improve the followings. 
The reviewer only show important ones only. Please careully check your paper. 

[Sec 1]
""uncertinty"" effec => ""uncertinty"" effec*t*


[Sec 2.1]
- subsection title, ""Knowledge discovery"" => ""Knowledge Discovery""
(It is necessary to be consistent with other subsections.)

- the local-information based LBD may perform not as well, ...
=> the local-information based LBD may *not* perform not *so* well, ...


[""Multi-perceptron layers"" Sec 3.1]
If the authors use ""l_{i}"" to denote each fully-connected layer instead of just ""i"", 
Equations (3), (4), and (5) would be more intuitive.   


[""Ranking AUC"" at Sec 4.4]
- ... is show in Table 1. => ... is show*n* in Table 1.

- Second, Jaccard's coefficients and BITOLA performs ...
=> Second, Jaccard's coefficients and BITOLA *perform* ...


[""Classification AUC"" at Sec 4.4]
- compared to that in Table 1. => compared to *those* in Table 1.


[Others]
- In Tables 1, 2, and 3, the authors should use ""MLP-CF"" and ""MLP-CF-uncertain"" 
in the same phrase as the body of the paper not ""MLP-CF"" and ""MLP_Uncertain"", respectively. 


(5) Replicability
- As the reviewer pointed out in ""(1) Novelty/Originality"", the authors' approach largely 
relies on exiting approaches. In addition, the authors conduct experiments in publicly 
available datasets. From these point of view, this work has high replicability. 


(6) References
- In ""References"", the authors often skip venues ([6]), page numbers ([7], [13], [14], [15], [16]). 
While the authors cite three arXiv papers, it is highly possible that they have been published in 
official international conferences. So the authors need to cite the official published papers. 
Especially, [18] has been published in EMNLP2014.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-15,no
482,125,47,Corxxxx Brexxxxxxx,3,"(OVERALL EVALUATION) The paper builds upon the concept of multiple diagram navigation (MDN), previously introduced by the first author. Broadly speaking, the aim of MDN is to provide users of digital libraries with a richer DL exploration experience by seamlessly supporting the navigation between diagram elements and content (i.e. the documents contained in the collection).

The authors note that a shortcoming of their previously proposed 'diagram-to-content' queries is that only a very small fraction of the full document collection is accessible from diagrams. This limitation is addressed in this paper by extending diagram-to-content queries to also reach related documents not directly connected to the diagrams. The authors use the Wikipedia hyperlink graph and internal diagram structures to realize such a 'diagram-influenced ranking' of Wikipedia pages. In an evaluation, various settings for their ranking algorithm are tested using 12 diagrams from 6 diverse domains. The paper determines a set of optimal parameters for their introduced algorithm and concludes that the diagram used in a query has a strong influence on rankings. 

One weakness of the evaluation is that it had not been made sufficiently clear according to which criteria the similarity rating (Rtng, 1-3) was determined. Since labeling was performed by a graduate student, the criteria for similarity should at least be made public to allow for some degree of reproducibility. Despite the space limitation, it would be interesting to hear what the authors propose as future work. 

The paper is well-written, and the scope of the contribution is suitable for the short paper format. I find the 3 specified RQs to be worth perusing. The evaluation methodology has been clearly presented aside from the weakness I pointed out above.
 
In summary, the paper's topic and findings will be of interest to the JCDL community, which is why I recommend it for publication.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-17,16:29,no,,482,125,47,Corinna Breitinger,3,"(OVERALL EVALUATION) The paper builds upon the concept of multiple diagram navigation (MDN), previously introduced by the first author. Broadly speaking, the aim of MDN is to provide users of digital libraries with a richer DL exploration experience by seamlessly supporting the navigation between diagram elements and content (i.e. the documents contained in the collection).

The authors note that a shortcoming of their previously proposed 'diagram-to-content' queries is that only a very small fraction of the full document collection is accessible from diagrams. This limitation is addressed in this paper by extending diagram-to-content queries to also reach related documents not directly connected to the diagrams. The authors use the Wikipedia hyperlink graph and internal diagram structures to realize such a 'diagram-influenced ranking' of Wikipedia pages. In an evaluation, various settings for their ranking algorithm are tested using 12 diagrams from 6 diverse domains. The paper determines a set of optimal parameters for their introduced algorithm and concludes that the diagram used in a query has a strong influence on rankings. 

One weakness of the evaluation is that it had not been made sufficiently clear according to which criteria the similarity rating (Rtng, 1-3) was determined. Since labeling was performed by a graduate student, the criteria for similarity should at least be made public to allow for some degree of reproducibility. Despite the space limitation, it would be interesting to hear what the authors propose as future work. 

The paper is well-written, and the scope of the contribution is suitable for the short paper format. I find the 3 specified RQs to be worth perusing. The evaluation methodology has been clearly presented aside from the weakness I pointed out above.
 
In summary, the paper's topic and findings will be of interest to the JCDL community, which is why I recommend it for publication.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-17,no
485,127,95,Mixxxx Dobrevxxxxxxxxxxx,2,"(OVERALL EVALUATION) While the paper could be of interest to the communities working on human-computer interaction with its contributions to eye movement analysis, it does not make a strong connection to the use of eye tracking within the digital library domain. There were earlier studies involving eye tracking within the evaluation of digital library interfaces (the earliest one which I am aware of is https://link.springer.com/chapter/10.1007/978-3-642-15464-5_69). 

Is there anything specific in the digital library interfaces which requires adjustments of eye tracking methodology and techniques? How exactly an improved eye tracking method is of benefit to the digital library community? What visualisations of eye tracking data are most useful within the evaluations of digital library interfaces? All these questions would be of great interest to the digital library community but may be when the authors continue with the experiments they could select suitable examples and present in some of the future digital libraries conferences.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-14,12:31,no,,485,127,95,Milena Dobreva-Mcpherson,2,"(OVERALL EVALUATION) While the paper could be of interest to the communities working on human-computer interaction with its contributions to eye movement analysis, it does not make a strong connection to the use of eye tracking within the digital library domain. There were earlier studies involving eye tracking within the evaluation of digital library interfaces (the earliest one which I am aware of is https://link.springer.com/chapter/10.1007/978-3-642-15464-5_69). 

Is there anything specific in the digital library interfaces which requires adjustments of eye tracking methodology and techniques? How exactly an improved eye tracking method is of benefit to the digital library community? What visualisations of eye tracking data are most useful within the evaluations of digital library interfaces? All these questions would be of great interest to the digital library community but may be when the authors continue with the experiments they could select suitable examples and present in some of the future digital libraries conferences.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-14,no
488,128,414,Wexxxx Xx,1,"(OVERALL EVALUATION) This paper presents preliminary works on identifying author confidence in biomedical articles. The core idea is to predict a confidence score by natural language processing and authors previous publication. The paper includes a proposed architecture and preliminary results from the sentimental analysis, average word per sentence and frequencies of medical terms are used for 10,000 articles from open archives initiative. While the topic is interesting and challenging,  the submission is not ready for publication for following reasons: 

-  Only results from a subset of proposed features are presented in the paper. With three presented features, it is not clear how they will be integrated and used together to predict confidence. 

- The paper lacks technical details. For example, it is not clear how the sentimental analysis is carried out such as language model used, the scale of scoring. It is also not clear how to convert sentiment analysis score towards confidence. If using a general model, a strong negative of certain fact may not mean lacking confidence. The context of the text must be considered. 

- A major weakness of the paper is lack of evaluation. It is not clear on how the presented results can be evaluated for their relevance and effectiveness with regard to author confidence. The presented results, therefore, don't carry many contributions beyond a basic sentimental analysis and word frequences analysis.  

- The paper also includes a number of presentation issues. For example, Figure 2 seems missing from the article. it is not clear if it is complete a missing or just a mislabeling of figures. There are also incomplete sentences.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-16,05:54,no,,488,128,414,Weijia Xu,1,"(OVERALL EVALUATION) This paper presents preliminary works on identifying author confidence in biomedical articles. The core idea is to predict a confidence score by natural language processing and authors previous publication. The paper includes a proposed architecture and preliminary results from the sentimental analysis, average word per sentence and frequencies of medical terms are used for 10,000 articles from open archives initiative. While the topic is interesting and challenging,  the submission is not ready for publication for following reasons: 

-  Only results from a subset of proposed features are presented in the paper. With three presented features, it is not clear how they will be integrated and used together to predict confidence. 

- The paper lacks technical details. For example, it is not clear how the sentimental analysis is carried out such as language model used, the scale of scoring. It is also not clear how to convert sentiment analysis score towards confidence. If using a general model, a strong negative of certain fact may not mean lacking confidence. The context of the text must be considered. 

- A major weakness of the paper is lack of evaluation. It is not clear on how the presented results can be evaluated for their relevance and effectiveness with regard to author confidence. The presented results, therefore, don't carry many contributions beyond a basic sentimental analysis and word frequences analysis.  

- The paper also includes a number of presentation issues. For example, Figure 2 seems missing from the article. it is not clear if it is complete a missing or just a mislabeling of figures. There are also incomplete sentences.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-16,no
492,130,244,Clixxxxx Lyxxx,1,"(OVERALL EVALUATION) This is an interesting paper, though the detail in the middle sections is overwhelming and I could not follow it (I am generally knowledgable on this topic but not a deep expert or carrying out active research). Most of the writing is clear, and it does a good job of framing the overall state of the art and the issues. THere are a very small number of little errors (page 8 refers to a ""thesis"" instead of a paper, for example; there's a paragraph mid-page on the right column of page 6 that seems like it got a bit mangled somehow.)

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The questions around this paper are strategic rather than topic. I am not sure that JCDL is the right place for a very detailed paper on this, though the subject is very important. There are more specialized venues. 

I wonder whether most JCDL attendees will be knowledgable or willing to wade through the very detailed material in here. There's some argument that JCDL and the authors would be better served by a short, high-level paper for the broad JCDL audience and then submission of this full paper to a more specialized venue. I would invite other reviewers to share their thoughts on this as I am not expert in this area by any means, and certainly not at this level of detail.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-14,03:25,no,,492,130,244,Clifford Lynch,1,"(OVERALL EVALUATION) This is an interesting paper, though the detail in the middle sections is overwhelming and I could not follow it (I am generally knowledgable on this topic but not a deep expert or carrying out active research). Most of the writing is clear, and it does a good job of framing the overall state of the art and the issues. THere are a very small number of little errors (page 8 refers to a ""thesis"" instead of a paper, for example; there's a paragraph mid-page on the right column of page 6 that seems like it got a bit mangled somehow.)

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The questions around this paper are strategic rather than topic. I am not sure that JCDL is the right place for a very detailed paper on this, though the subject is very important. There are more specialized venues. 

I wonder whether most JCDL attendees will be knowledgable or willing to wade through the very detailed material in here. There's some argument that JCDL and the authors would be better served by a short, high-level paper for the broad JCDL audience and then submission of this full paper to a more specialized venue. I would invite other reviewers to share their thoughts on this as I am not expert in this area by any means, and certainly not at this level of detail.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2018-02-14,no
495,130,223,Maxxxx Khxxxx,3,"(OVERALL EVALUATION) This paper tackles the problem of representing mathematical formulae, which are abundant in scientific papers and digital libraries in general. The authors motivate the work by outlining the importance of constructing powerful parser to parse and represent mathematical formulae in order to be able to perform any meaningful retrieval on them. Since labeled datasets are usually the catalyst of many research papers, the authors create a dataset of more than 300 mathematical formulae and manually annotate the literals within each formula. After that, many open source and commercial mathematical parsers are benchmarked on this golden dataset, and their performance is reported using tree edit distance from the gold parse. Finally, the authors explored incorporating the context of the formula to achieve better performance.

The motivation of this work is clear, and it’s indeed very relevant to JCDL. However, the flow of the paper is not smooth. Perhaps because the authors tried to include so much material in it, that they ended up not giving each part its fair share. Ideally, this paper could be split into 2 separate papers: one for the data collection and benchmarking, and a second paper for showing how including context is beneficial. 

Section 3.2 on building gold standard is especially not easy to follow. It could use some re-writing and explanation. Part of the issue is that the authors are explaining special cases for parsing on certain gold examples, but they refer to these formulae by their number (gold id 230) instead of listing the formula itself and explaining the issue. You might want to choose using couple of running examples to help in explaining the idea. Since constructing the gold standard is such a big deal, the authors could have dedicated the entire paper just for that, and made sure the process is very clear.

The evaluation of section 5 is not very clear to me as well. I expected it to be compared to other solutions described in Figure 3 or 4. But the authors picked 10 functions for evaluation. I’m not clear why was the evaluation was limited to this set, and how much improvement did they find in comparison to other approaches.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-16,23:44,no,,495,130,223,Madian Khabsa,3,"(OVERALL EVALUATION) This paper tackles the problem of representing mathematical formulae, which are abundant in scientific papers and digital libraries in general. The authors motivate the work by outlining the importance of constructing powerful parser to parse and represent mathematical formulae in order to be able to perform any meaningful retrieval on them. Since labeled datasets are usually the catalyst of many research papers, the authors create a dataset of more than 300 mathematical formulae and manually annotate the literals within each formula. After that, many open source and commercial mathematical parsers are benchmarked on this golden dataset, and their performance is reported using tree edit distance from the gold parse. Finally, the authors explored incorporating the context of the formula to achieve better performance.

The motivation of this work is clear, and it’s indeed very relevant to JCDL. However, the flow of the paper is not smooth. Perhaps because the authors tried to include so much material in it, that they ended up not giving each part its fair share. Ideally, this paper could be split into 2 separate papers: one for the data collection and benchmarking, and a second paper for showing how including context is beneficial. 

Section 3.2 on building gold standard is especially not easy to follow. It could use some re-writing and explanation. Part of the issue is that the authors are explaining special cases for parsing on certain gold examples, but they refer to these formulae by their number (gold id 230) instead of listing the formula itself and explaining the issue. You might want to choose using couple of running examples to help in explaining the idea. Since constructing the gold standard is such a big deal, the authors could have dedicated the entire paper just for that, and made sure the process is very clear.

The evaluation of section 5 is not very clear to me as well. I expected it to be compared to other solutions described in Figure 3 or 4. But the authors picked 10 functions for evaluation. I’m not clear why was the evaluation was limited to this set, and how much improvement did they find in comparison to other approaches.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2018-02-16,no
505,134,48,Juxxxx Bruxxxxx,1,(OVERALL EVALUATION) This paper is in the incorrect format.,"Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,2018-02-13,03:14,no,,505,134,48,Justin Brunelle,1,(OVERALL EVALUATION) This paper is in the incorrect format.,"Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,2018-02-13,no
530,142,92,Mixxxx Dobxxxx,3,"(OVERALL EVALUATION) The topic of this proposal is relevant to the conference and it presents initial results which would be interesting to discuss with the wider community. The proposals needs another language revision (e.g. there is repletion in “The remaining 302 surveys had at least at least one answer beyond the IRB agreement.”). 

The reason why I am not supporting the acceptance of this proposal is that there are three major shortcomings (the first is purely related to how this proposal is structured, the second and the third are issues with the study design): 

1) The lack of connection to any other previous work on re-use and previous results.

2) The lack of connection – and this is visible from the survey as well  - with the *rights* for the use of the collection. Different rights would influence the re-use patterns but this is a topic not addressed in the survey. 

3) In what is a short survey the focus changes – for example question  (9) “What are the two greatest barriers to assessing digital libraries?” is more generic (assessing general libraries is a huge domain) – and it is not surprising that the respondents who arrived that far in the survey are mentioning re-use as a barrier – this is what they were asked previously about and this is in a way a question which answers are heavily influenced by the topic of the survey so I personally would not trust any ranking made this way.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Great topic, poor research design.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-17,07:20,no,,530,142,92,Milena Dobreva,3,"(OVERALL EVALUATION) The topic of this proposal is relevant to the conference and it presents initial results which would be interesting to discuss with the wider community. The proposals needs another language revision (e.g. there is repletion in “The remaining 302 surveys had at least at least one answer beyond the IRB agreement.”). 

The reason why I am not supporting the acceptance of this proposal is that there are three major shortcomings (the first is purely related to how this proposal is structured, the second and the third are issues with the study design): 

1) The lack of connection to any other previous work on re-use and previous results.

2) The lack of connection – and this is visible from the survey as well  - with the *rights* for the use of the collection. Different rights would influence the re-use patterns but this is a topic not addressed in the survey. 

3) In what is a short survey the focus changes – for example question  (9) “What are the two greatest barriers to assessing digital libraries?” is more generic (assessing general libraries is a huge domain) – and it is not surprising that the respondents who arrived that far in the survey are mentioning re-use as a barrier – this is what they were asked previously about and this is in a way a question which answers are heavily influenced by the topic of the survey so I personally would not trust any ranking made this way.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Great topic, poor research design.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2018-02-17,no
533,143,3,Trxxx Aalxxxx,2,"(OVERALL EVALUATION) This poster abstract presents a log-analysis from eBook reading on mobile devices. The topic will be of interest to the conference. A main objection is the lack of comparisons that is needed to determine what is specific for mobile devices. How does this e.g. compare with reading on a large screen (at least for scientific reading that is a relevant comparison). Do screen size of device matter etc?

Another objection to this paper is the formatting. The author section does not follow the ACM format, the references are strange. On page 2, the text on the top is split over to paragraphs etc. I also find that some sentences needs to be corrected.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) NB! This is an interesting and relevant poster, but the quality of the abstract does not meet the requirements. 
The final version needs to be checked before it finally can be accepted.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-15,12:27,no,,533,143,3,Trond Aalberg,2,"(OVERALL EVALUATION) This poster abstract presents a log-analysis from eBook reading on mobile devices. The topic will be of interest to the conference. A main objection is the lack of comparisons that is needed to determine what is specific for mobile devices. How does this e.g. compare with reading on a large screen (at least for scientific reading that is a relevant comparison). Do screen size of device matter etc?

Another objection to this paper is the formatting. The author section does not follow the ACM format, the references are strange. On page 2, the text on the top is split over to paragraphs etc. I also find that some sentences needs to be corrected.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) NB! This is an interesting and relevant poster, but the quality of the abstract does not meet the requirements. 
The final version needs to be checked before it finally can be accepted.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-15,no
534,143,3,Trxxx Aalxxxx,2,"(OVERALL EVALUATION) This poster abstract presents a log-analysis from eBook reading on mobile devices. The topic will be of interest to the conference. A main objection is the lack of comparisons that is needed to determine what is specific for mobile devices. How does this e.g. compare with reading on a large screen (at least for scientific reading that is a relevant comparison). Do screen size of device matter etc?

Another objection to this paper is the formatting. The author section does not follow the ACM format, the references are strange. On page 2, the text on the top is split over to paragraphs etc. I also find that some sentences needs to be corrected.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) NB! This is an interesting and relevant poster, but the quality of the abstract does not meet the requirements. 
The final version needs to be checked before it finally can be accepted.

I revised my score to weak reject, so that I follow the other reviewers strategy on rejection because of bad formatting.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-15,12:29,no,,534,143,3,Trond Aalberg,2,"(OVERALL EVALUATION) This poster abstract presents a log-analysis from eBook reading on mobile devices. The topic will be of interest to the conference. A main objection is the lack of comparisons that is needed to determine what is specific for mobile devices. How does this e.g. compare with reading on a large screen (at least for scientific reading that is a relevant comparison). Do screen size of device matter etc?

Another objection to this paper is the formatting. The author section does not follow the ACM format, the references are strange. On page 2, the text on the top is split over to paragraphs etc. I also find that some sentences needs to be corrected.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) NB! This is an interesting and relevant poster, but the quality of the abstract does not meet the requirements. 
The final version needs to be checked before it finally can be accepted.

I revised my score to weak reject, so that I follow the other reviewers strategy on rejection because of bad formatting.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-15,no
537,143,129,Ricxxxx Fuxxxx,4,"(OVERALL EVALUATION) The paper presents large-scale data from Baidu about reading patterns with BaiduRead.  This, by itself, makes this an interesting report.  However there are a number of factors that weaken the paper:
- The study probably relates primarily to China due to its basis in Baidu.  The title/paper should indicate this rather than trying to represent a broader population.
- Are the books in Chinese or English or goth?  If both, does the language make a difference in reading patterns?
- What are the expected demographics of the readers?
- How flexible is the app in terms of page size, orientation, font size, etc., and what variation are readers using?
- The figures leave many unanswered questions.  One that struck me in particular was Figure 7.  The X axis is not labeled.  I assume that this refers to hours, but what day is the starting point (Sunday, Monday, something else?).  The weekend effect doesn't really jump out to me in this diagram.
- The formatting makes the paper hard to understand.  The paper must be reformatted into the JCDL conference format.  Items such as the distribution of text across columns at the top of page 2 are not correct for the format.  The references need to be fixed.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-26,18:41,no,,537,143,129,Richard Furuta,4,"(OVERALL EVALUATION) The paper presents large-scale data from Baidu about reading patterns with BaiduRead.  This, by itself, makes this an interesting report.  However there are a number of factors that weaken the paper:
- The study probably relates primarily to China due to its basis in Baidu.  The title/paper should indicate this rather than trying to represent a broader population.
- Are the books in Chinese or English or goth?  If both, does the language make a difference in reading patterns?
- What are the expected demographics of the readers?
- How flexible is the app in terms of page size, orientation, font size, etc., and what variation are readers using?
- The figures leave many unanswered questions.  One that struck me in particular was Figure 7.  The X axis is not labeled.  I assume that this refers to hours, but what day is the starting point (Sunday, Monday, something else?).  The weekend effect doesn't really jump out to me in this diagram.
- The formatting makes the paper hard to understand.  The paper must be reformatted into the JCDL conference format.  Items such as the distribution of text across columns at the top of page 2 are not correct for the format.  The references need to be fixed.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2018-02-26,no
550,147,139,Danxxxx Gixx,2,"(OVERALL EVALUATION) This paper presents a novel approach to word sense disambiguation based on semantic association. The methodology is effective and can improve the state of the art. The containt of this paper is clear and well described, if we take into account the reduced space allocated to writing a poster-paper. However, some comments should be considered:
-	Even that the method have various applications, it will be very useful if the author will solve the issue of proper names (see Khurana, 2017) and multi-word expressions (see Timothy Baldwin's works). 
-	Proofreading is necessary in order to correct some minor language mistakes (p. 2 “need to excluded”) and, also, the authors have to reformulate the following sentence, which is used too often: “The extended abstract introduces” (see Abstract, Introduction, Conclusion).
Anyway, the paper has good practical implications, particularly since it can use a significant number of texts.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2018-02-14,16:33,no,,550,147,139,Daniela Gifu,2,"(OVERALL EVALUATION) This paper presents a novel approach to word sense disambiguation based on semantic association. The methodology is effective and can improve the state of the art. The containt of this paper is clear and well described, if we take into account the reduced space allocated to writing a poster-paper. However, some comments should be considered:
-	Even that the method have various applications, it will be very useful if the author will solve the issue of proper names (see Khurana, 2017) and multi-word expressions (see Timothy Baldwin's works). 
-	Proofreading is necessary in order to correct some minor language mistakes (p. 2 “need to excluded”) and, also, the authors have to reformulate the following sentence, which is used too often: “The extended abstract introduces” (see Abstract, Introduction, Conclusion).
Anyway, the paper has good practical implications, particularly since it can use a significant number of texts.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2018-02-14,no
551,147,139,Danxxxx Gixx,2,"(OVERALL EVALUATION) The authors of this poster-paper present a new approach, using graphical model to detect the impact that a document have or have no after its publication. They are also concerned to examine its evolution in time. Moreover, the results are promising, reflecting how the graphical model performs on this task. We are looking forward to seeing their future results.
The structure of this paper is clear and well presented, giving information about the technologies used. 
However, a few comments should be taken into account:
-	The method is shown to have preliminary good results, but there is still much work to be done (describe it briefly).
-	Proofreading is necessary to correct some minor errors (p. 2 “Correlation between topical prevalence”, “can provides”). The paper has a good practical implication, and the authors have made available to the public their test results for a set of 100 out-of-copyright volumes from the HTDL (https://git.io/vN9lE).","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-14,16:42,no,,551,147,139,Daniela Gifu,2,"(OVERALL EVALUATION) The authors of this poster-paper present a new approach, using graphical model to detect the impact that a document have or have no after its publication. They are also concerned to examine its evolution in time. Moreover, the results are promising, reflecting how the graphical model performs on this task. We are looking forward to seeing their future results.
The structure of this paper is clear and well presented, giving information about the technologies used. 
However, a few comments should be taken into account:
-	The method is shown to have preliminary good results, but there is still much work to be done (describe it briefly).
-	Proofreading is necessary to correct some minor errors (p. 2 “Correlation between topical prevalence”, “can provides”). The paper has a good practical implication, and the authors have made available to the public their test results for a set of 100 out-of-copyright volumes from the HTDL (https://git.io/vN9lE).","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2018-02-14,no
556,151,42,Joxx Borxxxxx,1,"(OVERALL EVALUATION) This poster proposes to report the finding of a bibliographic search (that resulted in 42 articles) on the subject ""smart library"" (or that has been perceived ass related to that...).

This text gives us a sense of ""too naive / too speculative"" on the subject, as it look everything we can understand that ""has digital on it"" can be tagged of ""smart...""... this might be a first step for a future interesting work on the historical view of how this subject has been present in the literature, but what we can read in the present proposal is not very motivating... We are aware that ""the library"" as a concept is facing a huge pressure for transformation, and this kind of work could be a contribution to that, but we are not sure of that by what we can read in the present proposal... We see the potential, but... border line!","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,20:01,no,,556,151,42,Jose Borbinha,1,"(OVERALL EVALUATION) This poster proposes to report the finding of a bibliographic search (that resulted in 42 articles) on the subject ""smart library"" (or that has been perceived ass related to that...).

This text gives us a sense of ""too naive / too speculative"" on the subject, as it look everything we can understand that ""has digital on it"" can be tagged of ""smart...""... this might be a first step for a future interesting work on the historical view of how this subject has been present in the literature, but what we can read in the present proposal is not very motivating... We are aware that ""the library"" as a concept is facing a huge pressure for transformation, and this kind of work could be a contribution to that, but we are not sure of that by what we can read in the present proposal... We see the potential, but... border line!","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2018-02-13,no
566,155,42,Joxx Borxxxxx,1,"(OVERALL EVALUATION) The subject addressed by this proposal is confusing... In simple terms, and generalizing what I could perceive from the text, it seems about to understand the worldwide legal frameworks on the reuse of structured information (the authors name it ""metadata"") from third-parties...

If that is the purpose, it is interesting and relevant! But one can be sure from what can be read in the proposal...","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2018-02-13,20:12,no,,566,155,42,Jose Borbinha,1,"(OVERALL EVALUATION) The subject addressed by this proposal is confusing... In simple terms, and generalizing what I could perceive from the text, it seems about to understand the worldwide legal frameworks on the reuse of structured information (the authors name it ""metadata"") from third-parties...

If that is the purpose, it is interesting and relevant! But one can be sure from what can be read in the proposal...","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2018-02-13,no
570,156,348,Joxx Smxxx,1,"(OVERALL EVALUATION) The authors look at the differences between professional (i.e., librarians) use of video search tactics against those of regular users (undergraduate students). They found, not surprisingly, that there are several key differences particularly when a search fails to produce the desired/expected results. The authors conclude that there is considerable room for improvement in video search tools and training.

Despite the somewhat thin content, I recommend the poster be accepted even though the results are predictable, because the underlying research concept appears sound. True, the sample size is too small and likely too homogeneous (a single university's library) but there is some interesting work that can come out of their proposal potentially leading to more useful development. The area of video search and retrieval seems like an area worth deeper investigation.

I do have a few suggestions: First, the Research Method section should be expanded with more discussion about their approach. Second, a section on ""Future Work"" or ""Next Steps"" should be added to clearly lay out their plans for future research on the topic. And finally, some discussion about the limitations of the sample size ought to appear in their section on results. There should be plenty of room for these if the authors reformat using the JCDL template.

A couple of items that need correction:

1. The authors are not listed on the PDF of this paper. Actually, the submission did not have the correct JCDL publication template. The template used is single-column, larger font, with different spacing and structure which reduces available print area. The authors would have more room for discussion if the proper template is used. 

2. The authors use both written and numeric quantities. Example ""Twenty-four (24) participants "" --> It makes for awkward reading. I suggest using one or the other, not both.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-15,11:15,no,,570,156,348,Joan Smith,1,"(OVERALL EVALUATION) The authors look at the differences between professional (i.e., librarians) use of video search tactics against those of regular users (undergraduate students). They found, not surprisingly, that there are several key differences particularly when a search fails to produce the desired/expected results. The authors conclude that there is considerable room for improvement in video search tools and training.

Despite the somewhat thin content, I recommend the poster be accepted even though the results are predictable, because the underlying research concept appears sound. True, the sample size is too small and likely too homogeneous (a single university's library) but there is some interesting work that can come out of their proposal potentially leading to more useful development. The area of video search and retrieval seems like an area worth deeper investigation.

I do have a few suggestions: First, the Research Method section should be expanded with more discussion about their approach. Second, a section on ""Future Work"" or ""Next Steps"" should be added to clearly lay out their plans for future research on the topic. And finally, some discussion about the limitations of the sample size ought to appear in their section on results. There should be plenty of room for these if the authors reformat using the JCDL template.

A couple of items that need correction:

1. The authors are not listed on the PDF of this paper. Actually, the submission did not have the correct JCDL publication template. The template used is single-column, larger font, with different spacing and structure which reduces available print area. The authors would have more room for discussion if the proper template is used. 

2. The authors use both written and numeric quantities. Example ""Twenty-four (24) participants "" --> It makes for awkward reading. I suggest using one or the other, not both.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-15,no
575,157,3,Trxxx Aalxxxx,2,"(OVERALL EVALUATION) This poster abstract presents preliminary findings from a study on knowledge curation activities in Wikidata projects. Content analysis of discussions is used to identify actions. The poster topic is of interest to the conference, but contribution in this very vague. The activities that could have been of interest are the ones that are mentioned as ""mapped to the DCC occasional activities of reappraising and dispose"", but this is not further explained. The activities that are further explained are more administrative ones that do not really count as knowledge curation activities. The section on tools is also rather out of the scope indicated by the title. 

The abstract needs to be shortened because it prints as 3 pages.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Given that I find the actual contribution of this very preliminary and vague, I gave it a low score.
It is not a competitive poster as it is presented now, but I assume that an improved presentation of the findings and discussion could have saved it.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-14,15:35,no,,575,157,3,Trond Aalberg,2,"(OVERALL EVALUATION) This poster abstract presents preliminary findings from a study on knowledge curation activities in Wikidata projects. Content analysis of discussions is used to identify actions. The poster topic is of interest to the conference, but contribution in this very vague. The activities that could have been of interest are the ones that are mentioned as ""mapped to the DCC occasional activities of reappraising and dispose"", but this is not further explained. The activities that are further explained are more administrative ones that do not really count as knowledge curation activities. The section on tools is also rather out of the scope indicated by the title. 

The abstract needs to be shortened because it prints as 3 pages.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Given that I find the actual contribution of this very preliminary and vague, I gave it a low score.
It is not a competitive poster as it is presented now, but I assume that an improved presentation of the findings and discussion could have saved it.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2018-02-14,no
577,157,348,Joxx Smxxx,3,"(OVERALL EVALUATION) The authors apply Activity Theory to the actions of people involved in the curation of Wikidata WikiProjects. Given that both Activity Theory (AT) and WikiProjects are group-based and action-oriented, using AT to examine such processes seems like a natural approach. However, the explanation of the project left me somewhat confused. Three research questions were identified in the introduction, but the authors' explanation of their methodology was not clear to me. The Findings & Discussion section did attempt to answer each of the research questions but the results were not particularly compelling; I'm not sure new knowledge came out of the experiments. Perhaps the authors could develop an explanatory diagram to help clarify their process and goals. Still, I recommend accepting the poster at JCDL because it will probably help the authors solidify their goals, evaluation processes, and explanation of results. 

Grammatically and linguistically, the paper is fine. A couple of very minor edits:

Section 3.1: ""necessary to re-organizing"" --> necessary to reorganize
Multiple Sections: ""as well as, "" --> trailing comma is not needed","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-15,10:30,no,,577,157,348,Joan Smith,3,"(OVERALL EVALUATION) The authors apply Activity Theory to the actions of people involved in the curation of Wikidata WikiProjects. Given that both Activity Theory (AT) and WikiProjects are group-based and action-oriented, using AT to examine such processes seems like a natural approach. However, the explanation of the project left me somewhat confused. Three research questions were identified in the introduction, but the authors' explanation of their methodology was not clear to me. The Findings & Discussion section did attempt to answer each of the research questions but the results were not particularly compelling; I'm not sure new knowledge came out of the experiments. Perhaps the authors could develop an explanatory diagram to help clarify their process and goals. Still, I recommend accepting the poster at JCDL because it will probably help the authors solidify their goals, evaluation processes, and explanation of results. 

Grammatically and linguistically, the paper is fine. A couple of very minor edits:

Section 3.1: ""necessary to re-organizing"" --> necessary to reorganize
Multiple Sections: ""as well as, "" --> trailing comma is not needed","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2018-02-15,no
595,165,348,Joxx Smxxx,1,"(OVERALL EVALUATION) The authors use a combination of TF-IDF, burstiness (applied to terms), term growth rate, and other factors as the basis for a model for determining what areas of research areas of science are ""hot"" (growing and/or being exceptionally productive). The paper would be stronger if not for a few issues. For example, they make a couple of claims without backing them up either in the text or in the references. First, that this model is ""one of the best practices for content-based service in the library area""; and second that their approach overcomes ""disadvantages of the Fuzzy-AHP method.""  The diagrammed model is not especially informative and seems mostly to just box each of the components without really showing an extractive result. Finally, there are only 3 references cited. Others that refer directly to their claims should perhaps be listed.

Despite these and the many minor grammatical issues, this paper would be a good addition to the posters section of JCDL and the authors would doubtless find the interactions at the conference valuable for moving their model forward.

A few of the many corrections needed:

Pg. 1: ""what has led to"" --> which has led to
          ""the word frequency statistics"" --> word frequency statistics
          ""among other areas, finance,"" --> among other areas such as finance,

Pg. 2: ""TF-IDF mothod"" --> TF-IDF method

Numerous misuse of singular-plural as well as use of articles ""a"" and ""the"" where standard English grammar doesn't use them. However, the paper is perfectly readable and understandable so these are perhaps not important in this case.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-14,20:20,no,,595,165,348,Joan Smith,1,"(OVERALL EVALUATION) The authors use a combination of TF-IDF, burstiness (applied to terms), term growth rate, and other factors as the basis for a model for determining what areas of research areas of science are ""hot"" (growing and/or being exceptionally productive). The paper would be stronger if not for a few issues. For example, they make a couple of claims without backing them up either in the text or in the references. First, that this model is ""one of the best practices for content-based service in the library area""; and second that their approach overcomes ""disadvantages of the Fuzzy-AHP method.""  The diagrammed model is not especially informative and seems mostly to just box each of the components without really showing an extractive result. Finally, there are only 3 references cited. Others that refer directly to their claims should perhaps be listed.

Despite these and the many minor grammatical issues, this paper would be a good addition to the posters section of JCDL and the authors would doubtless find the interactions at the conference valuable for moving their model forward.

A few of the many corrections needed:

Pg. 1: ""what has led to"" --> which has led to
          ""the word frequency statistics"" --> word frequency statistics
          ""among other areas, finance,"" --> among other areas such as finance,

Pg. 2: ""TF-IDF mothod"" --> TF-IDF method

Numerous misuse of singular-plural as well as use of articles ""a"" and ""the"" where standard English grammar doesn't use them. However, the paper is perfectly readable and understandable so these are perhaps not important in this case.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2018-02-14,no
610,170,348,Joxx Smxxx,1,"(OVERALL EVALUATION) The authors present a proof-of-concept for enabling the extraction of linked data using machine learning techniques. The process starts with an archive, applies data-tilling-services to the content, then uses  an RDF extraction method (Brown Dog). The resulting linked data is then available for building or adding to digital collections. Their primary proposal is that local extraction of the linked data is superior to remote services. The authors have an interesting proposal, worth presenting at JCDL especially given the growth of linked data and the various approaches used in collections around the world. 

One small correction: the LaTeX for the apostrophe in the Abstract (Brown Dog's) needs to be fixed.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper has a subtitle-link to an ""Extended Abstract"". Can't recall ever seeing that before and don't know if this is kosher or needs to be removed.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-14,16:52,no,,610,170,348,Joan Smith,1,"(OVERALL EVALUATION) The authors present a proof-of-concept for enabling the extraction of linked data using machine learning techniques. The process starts with an archive, applies data-tilling-services to the content, then uses  an RDF extraction method (Brown Dog). The resulting linked data is then available for building or adding to digital collections. Their primary proposal is that local extraction of the linked data is superior to remote services. The authors have an interesting proposal, worth presenting at JCDL especially given the growth of linked data and the various approaches used in collections around the world. 

One small correction: the LaTeX for the apostrophe in the Abstract (Brown Dog's) needs to be fixed.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper has a subtitle-link to an ""Extended Abstract"". Can't recall ever seeing that before and don't know if this is kosher or needs to be removed.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2018-02-14,no
625,175,290,Pexxx Orgaxxxxxxxx,2,"(OVERALL EVALUATION) This is generally an interesting, relevant topic to the JCDL community, in an area that the panelists are qualified for. However, the proposal is underdeveloped and overly broad. I would like to see more evidence of the specific directions that the panelists hope to address, and more detail about why this discussion is needed now in the DL community. Lacking focus, detail and strong justification, it is difficult to assess the appropriateness of this panel.","Overall evaluation: -2
Reviewer's confidence: 4",-2,,,,,2018-02-20,22:44,no,,625,175,290,Peter Organisciack,2,"(OVERALL EVALUATION) This is generally an interesting, relevant topic to the JCDL community, in an area that the panelists are qualified for. However, the proposal is underdeveloped and overly broad. I would like to see more evidence of the specific directions that the panelists hope to address, and more detail about why this discussion is needed now in the DL community. Lacking focus, detail and strong justification, it is difficult to assess the appropriateness of this panel.","Overall evaluation: -2
Reviewer's confidence: 4",-2,,,,,2018-02-20,no
634,178,69,Boxxx Casxxxx,1,"(OVERALL EVALUATION) The topic of the poster is especially relevant in these days when we hear so much of the filtering of information provided to an individual based on algorithmic assessment of what the user would want to see or would react to in a predictable way.  Any effort to expand the user's range of input is interesting and worth exploring.  This project is appropriate to JCDL.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) My default on posters is ACCEPT as long as the topic is relevant and the submission is well done.  This one, I would have said accept in any case.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,2018-03-25,13:41,no,,634,178,69,Boots Cassell,1,"(OVERALL EVALUATION) The topic of the poster is especially relevant in these days when we hear so much of the filtering of information provided to an individual based on algorithmic assessment of what the user would want to see or would react to in a predictable way.  Any effort to expand the user's range of input is interesting and worth exploring.  This project is appropriate to JCDL.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) My default on posters is ACCEPT as long as the topic is relevant and the submission is well done.  This one, I would have said accept in any case.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,2018-03-25,no
661,186,43,Joxx Borxxxxx,3,"(OVERALL EVALUATION) This poster reports the findings of a project addressing the problem of cross-language recommendation, based on a semi-supervised random walk algorithm...
It is not clear what is the problem the work reported in this poster intends to address. The authors stress the purpose of ""citation"", but in fact the core of the problem is ""recommendation"", being any concern related with ""citation"" irrelevant (maybe the authors have that in mind in the objectives of the overall project, but that is not relevant for what is reported as results in this paper...).
Assuming authors can refocus this poster as a case of addressing a cross-language recommendation challenge, this can result in a relevant report of work in progress in that sense (I mean, please strip the text of repeated and unnecessary language and use the limited resource that are the two pages to provide more insight about your core work and results).","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2018-03-26,07:58,no,,661,186,43,Jose Borbinha,3,"(OVERALL EVALUATION) This poster reports the findings of a project addressing the problem of cross-language recommendation, based on a semi-supervised random walk algorithm...
It is not clear what is the problem the work reported in this poster intends to address. The authors stress the purpose of ""citation"", but in fact the core of the problem is ""recommendation"", being any concern related with ""citation"" irrelevant (maybe the authors have that in mind in the objectives of the overall project, but that is not relevant for what is reported as results in this paper...).
Assuming authors can refocus this poster as a case of addressing a cross-language recommendation challenge, this can result in a relevant report of work in progress in that sense (I mean, please strip the text of repeated and unnecessary language and use the limited resource that are the two pages to provide more insight about your core work and results).","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2018-03-26,no
664,187,222,Mxx Kexxx,2,"(OVERALL EVALUATION) This work represents a study investigating correlation between publication of research patents and Fortune 500 rank. Claiming the first study of its kind that does not have the fallacies of previous older studies, the authors identify these correlative trends as they have occurred in time.

A few stylistic errors exist in this submission:
* Section 3.2: to'steal' needs a space.
* Throughout the submission there is inconsistent use of smart quote and straight quote. For example, ""current"" in Section 3.1 uses straight-quote characters while ""corner"" in Section 2 uses smart/curly quotes. The mixed use of single and double quotes is also a little stylistically odd.

Overall, the novelty of the work would make for a worthwhile presentation as a JCDL poster.","Overall evaluation: 1
Reviewer's confidence: 3",1,,,,,2018-03-25,19:27,no,,664,187,222,Mat Kelly,2,"(OVERALL EVALUATION) This work represents a study investigating correlation between publication of research patents and Fortune 500 rank. Claiming the first study of its kind that does not have the fallacies of previous older studies, the authors identify these correlative trends as they have occurred in time.

A few stylistic errors exist in this submission:
* Section 3.2: to'steal' needs a space.
* Throughout the submission there is inconsistent use of smart quote and straight quote. For example, ""current"" in Section 3.1 uses straight-quote characters while ""corner"" in Section 2 uses smart/curly quotes. The mixed use of single and double quotes is also a little stylistically odd.

Overall, the novelty of the work would make for a worthwhile presentation as a JCDL poster.","Overall evaluation: 1
Reviewer's confidence: 3",1,,,,,2018-03-25,no
667,187,371,Praxxxx Terxxxxxx,4,"(OVERALL EVALUATION) This paper aims to identify the correlation between Fortune 500 companies ranks and their patent portfolios. It introduces the concept of temporal buckets (grouping companies by their founding years). A study of future ranking based on patent grants is provided. 

The set of Fortune 500 companies encompass different areas of industry, the paper does not make it clear of how prevalent patents and investment in research are for these companies (which could explain some of the results of both high and low correlation).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am not sure about the applicability of this paper in this forum. It seems to be linked more to econometrics or economics than digital libraries.","Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,2018-03-26,23:52,no,,667,187,371,Pradeep Teregowda,4,"(OVERALL EVALUATION) This paper aims to identify the correlation between Fortune 500 companies ranks and their patent portfolios. It introduces the concept of temporal buckets (grouping companies by their founding years). A study of future ranking based on patent grants is provided. 

The set of Fortune 500 companies encompass different areas of industry, the paper does not make it clear of how prevalent patents and investment in research are for these companies (which could explain some of the results of both high and low correlation).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am not sure about the applicability of this paper in this forum. It seems to be linked more to econometrics or economics than digital libraries.","Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,2018-03-26,no
673,189,349,Joxx Smxxx,1,"(OVERALL EVALUATION) The author presents his experience using 2 different eye-tracking products as part of a larger study on the success of digital library user tools. He
 describes his basis for selecting the two products and provides a comparative summary of his observations. Overall, the paper is reasonably well wri
teen with only a few small grammatical issues. There are a few things that could be improved and make the paper more usable by other, future research
ers:

1) Break the discussion down into 4 sections: 
    -- the Abstract (essentially just the first paragraph as currently written)
    -- Methodology section (most of the remaining text)
    -- Comparison section (using the Table as the content)
    -- References section
    
2) Rework the Table so that it appears as a single, landscape-mode table in the paper. This will make it more readable. If necessary, abbreviate the
content to fit

3) Improve the References section. Eye-tracking experiments have been going on for a long time now. It's hard to believe that this paper cites only a
 single textbook on the subject. Google alone has published lots of work in this area. At least 2 or 3 other reference works should have figured into
 this work.

4) Use the proper JCDL publication template. 

The rating of ""weak accept"" is really due to the above-mentioned weaknesses. With those improvedments, it would be rated much higher. The content is
mostly there (except for the citations) and is likely to be of significant interest to the JCDL community.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2018-03-25,19:11,no,,673,189,349,Joan Smith,1,"(OVERALL EVALUATION) The author presents his experience using 2 different eye-tracking products as part of a larger study on the success of digital library user tools. He
 describes his basis for selecting the two products and provides a comparative summary of his observations. Overall, the paper is reasonably well wri
teen with only a few small grammatical issues. There are a few things that could be improved and make the paper more usable by other, future research
ers:

1) Break the discussion down into 4 sections: 
    -- the Abstract (essentially just the first paragraph as currently written)
    -- Methodology section (most of the remaining text)
    -- Comparison section (using the Table as the content)
    -- References section
    
2) Rework the Table so that it appears as a single, landscape-mode table in the paper. This will make it more readable. If necessary, abbreviate the
content to fit

3) Improve the References section. Eye-tracking experiments have been going on for a long time now. It's hard to believe that this paper cites only a
 single textbook on the subject. Google alone has published lots of work in this area. At least 2 or 3 other reference works should have figured into
 this work.

4) Use the proper JCDL publication template. 

The rating of ""weak accept"" is really due to the above-mentioned weaknesses. With those improvedments, it would be rated much higher. The content is
mostly there (except for the citations) and is likely to be of significant interest to the JCDL community.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2018-03-25,no
686,192,85,Haxxx Daxxx,3,"(OVERALL EVALUATION) The authors presented a method and identified covering technology by using patents. They have performed an analysis of which industry of the converging technologies came from. If we consider patents as a knowledge repository, this presentation is suitable for the conference.

However, there is some presentation and typos in the paper which needs to be fixed: 
* The paper is not in the appropriated format: the text needs to be justified. In addition, spaces in some words seem to be missing.
* increase dramatically => increased dramatically 
* In Section 2, in order to inform the number of downloaded patents and classes, they expressed using the phrase ""and the number is XXX""  which is not clear. 
* The text of Figure 3 is not readable","Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,2018-03-26,16:39,no,,686,192,85,Hasan Dalip,3,"(OVERALL EVALUATION) The authors presented a method and identified covering technology by using patents. They have performed an analysis of which industry of the converging technologies came from. If we consider patents as a knowledge repository, this presentation is suitable for the conference.

However, there is some presentation and typos in the paper which needs to be fixed: 
* The paper is not in the appropriated format: the text needs to be justified. In addition, spaces in some words seem to be missing.
* increase dramatically => increased dramatically 
* In Section 2, in order to inform the number of downloaded patents and classes, they expressed using the phrase ""and the number is XXX""  which is not clear. 
* The text of Figure 3 is not readable","Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,2018-03-26,no
704,198,93,Mixxxx Dobxxxx,2,"(OVERALL EVALUATION) The submission describes work on extracting comparison points from the Wikipedia articles on pairs of entities. The motivation and contextualisation of this task are not entirely convincing - there is a statement that ""The task is challenging as comparison points in a typical pair of articles tend to be sparse."" but beyond that, what is the relevance of the proposed method to digital libraries? Who needs these comparisons and for what exactly? The current results and the various types of analysis are not very convincing.","Overall evaluation: 0
Reviewer's confidence: 2",0,,,,,2018-03-26,18:21,no,,704,198,93,Milena Dobreva,2,"(OVERALL EVALUATION) The submission describes work on extracting comparison points from the Wikipedia articles on pairs of entities. The motivation and contextualisation of this task are not entirely convincing - there is a statement that ""The task is challenging as comparison points in a typical pair of articles tend to be sparse."" but beyond that, what is the relevance of the proposed method to digital libraries? Who needs these comparisons and for what exactly? The current results and the various types of analysis are not very convincing.","Overall evaluation: 0
Reviewer's confidence: 2",0,,,,,2018-03-26,no
715,202,43,Joxx Borxxxxx,2,"(OVERALL EVALUATION) Precision medicine is a very important trend in science and technology. This poster describes an initiative to develop an ontology for that purpose, considering patient phenotype, diseases, drugs, cellular and molecular mechanisms, and sequences.
The intention for this objective might be seen as quite naive, considering the complexity of the domain, and also that the proposal does not describes the approach to build the ontology (ontology engineering techniques/method?). However, it is important to expose the JCDL community to challenges of this kind, as they might bring new relevant opportunities for the domain of digital libraries...

This submission is of only 1 page, with no details and bibliography. We believe that is the poster is accepted, the authors might be able to produce a proper 2 pages paper with these details","Overall evaluation: 1
Reviewer's confidence: 5",1,,,,,2018-03-26,08:51,no,,715,202,43,Jose Borbinha,2,"(OVERALL EVALUATION) Precision medicine is a very important trend in science and technology. This poster describes an initiative to develop an ontology for that purpose, considering patient phenotype, diseases, drugs, cellular and molecular mechanisms, and sequences.
The intention for this objective might be seen as quite naive, considering the complexity of the domain, and also that the proposal does not describes the approach to build the ontology (ontology engineering techniques/method?). However, it is important to expose the JCDL community to challenges of this kind, as they might bring new relevant opportunities for the domain of digital libraries...

This submission is of only 1 page, with no details and bibliography. We believe that is the poster is accepted, the authors might be able to produce a proper 2 pages paper with these details","Overall evaluation: 1
Reviewer's confidence: 5",1,,,,,2018-03-26,no