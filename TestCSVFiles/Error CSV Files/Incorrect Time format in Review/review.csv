a,b,c,d,e,f,g,h,i,j,k,l,m,n,o
1,2,48,Juxxxx Bruxxxx,1,(OVERALL EVALUATION) Incorrect format and unrelated content,"Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,13/22018,25:04,no
2,2,63,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) The topic is interesting, and undoubtedly touches upon very important economic aspects of the world-wide coffee trade. However, I believe that it would be of interest to a (very) small group of JCDL attendees. I would suggest to present this topic to a more appropriate event.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,15/2/2018,12:05,no
3,3,48,Juxxxx Bruxxxx,1,"(OVERALL EVALUATION) This poster analyzes responses from town hall-style meetings regarding smart communities and the implications for digital libraries. I found the methods to be confusing; the author appears to have listened to meetings and derived the opinions of the participants in the meetings and the associated implications on digital libraries in smart communities. Coupled with the double blind submission, the poorly defined methods and results leave enough doubt to prevent this from being accepted.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,13/2/2018,3:13,no
4,3,63,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) Omni-comprehensive and multi-disciplinary infrastructures and their relationship with digital libraries are indeed actual topics, but in this brief abstract the considerations provided are at such a high level that they are not able to provide further insight or actual suggestions useful for those topics. Also “human centered” seem to be more buzz-words rather than actual requirements or features. 
If this contribution will be accepted, it might be worth to better present and discuss the concept of the library as a “digital octopus”, as this is one aspect of the actual (hot) discussions whether research data should be part of the library or rather part of the “digital laboratory” (i.e. the infrastructure) supporting the research activities.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,15/2/2018,12:06,no
6,4,326,Jacxxxx Saxx,1,"(OVERALL EVALUATION) The topics of the paper is clearly appropriate for the JCDL conference.  The authors presents a study of the effectiveness of four learning schemes used to predict the quality of a citation (binary:  important vs. marginal) in an corps of scientific papers (ACL corpus on computational linguistics). The experiment is based on a set of 450 citations (not so big).  

In a second part, the authors propose two new learning strategies (one based on SVM and random forest, the second on a deep learning architecture).  The experiment shows that a better performance (precision, Recall, F1) can be achieved by the two new proposed schemes.  A statistical test is missing to confirm this findings.    

minor points
Who are the experts (Page 3, Section 3).
Is the citation collection available? (Page 3, Section 3)
Section 4.1. not fully clear:  ""Each feature is divided into four categories""  Each feature or the whole feature set?
Section ""4 Results ..."" must be ""5. Results ..""
In Fig 2 and 3 :  add a space before ""(area =""
Page 7:  ""by using the-fold"" -> ""by using the three-fold""
""6 Conclusion""  -> ""7. Conclusion""
In the conclusion, the support for your learning scheme is a single collection... maybe another scientific domain can have a different citation pattern..

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This is an interesting paper.  A few problems, but nothing that cannot be fixed in a final version.  
I don't fully understand why you reject it... but that's life...","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,23/2/2018,16:06,no
7,4,90,Yixx Dxx,2,"(OVERALL EVALUATION) The current paper explores the rhetorical context of citations in scholarly big data. I provide several suggestions and hope the authors can consider: More sentences should be provided for better elaborating the motivation of this paper. This paper needs a thorough round of English proof reading. It also needs more methodological implications in the Conclusion section. Moreover, potential pros and cons of the algorithms in the 4.1-4.4 sections should be detailed more.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,Yi,Bu,buyi@iu.edu,353,8/2/2018,16:14,no
8,4,421,Bxx Y,3,"(OVERALL EVALUATION) This paper proposed a new classifier for citation purpose classification by consolidating features from four prior models, and also a LSTM model. Overall the paper does not demonstrate significant intellectual contribution for the following reasons:

First, consolidating prior models is incremental work, unless some insights were provided regarding the strength and weakness of each model, and how the new model addresses these issues. Unfortunately no such analysis was provided. 

Second, the use of LSTM model cannot be justified because LSTM takes sequence input, but the 64 features do not form sequence. The parameters like 52 input units do not match with the 64 features. More details are needed to explain the implementation of the LSTM. Because the data set is rather small with only hundreds of examples, a 5-layer LSTM model is likely overfitting.  

Other issues:

The authors did extensive literature review on related work. It would be more helpful if the classification tasks can be specifically described along with performance comparison in that different studies used different numbers of categories and their definitions vary as well.

The research method description could also include more rationale for critical choices. For example, in 3.1. why used three different tools (OpenNLP, StanfordParser and Factorie) for pre-processing? How are features like ""Author uses tools/algorithm of cited article"" extracted?","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,17/2/2018,5:36,no
10,4,8,Ghxxxx Abxxxx,4,"(OVERALL EVALUATION) the authors apply the Extra-Trees classifier to extract 29 best new features and create a new model that they use to apply Random Forest and Support Vector Machine to classify reference as important or not important. The new model improves on the state of the art by 11.25 according to the experimental results.
The paper is technically sound, the approach is described in details and the results are reasonable. My understanding is that the paper contributed two things:
1- identified new features that will help with enhancing model accuracy
2- compared different models using two different supervised learning approaches (SVM's and RF)

The related work section is a little long for a technical paper, however, it provided a nice survey of the previous work. I wish it was shorter and the extra space was used to better explain the paper contribution and add more tables such as the confusion matrix which I like to look at since I can get more information about the model performance. 

Using the LSTM was not justified especially since LSTM is not suitable for this kind of data. The only justification given is that it is a new popular approach which is not a good justification.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,24/2/2018,7:36,no
12,5,326,Jacxxxx Saxx,1,"(OVERALL EVALUATION) Interesting paper.  The authors have analyzed 75 papers on experiments / presentation / descriptions of deploying cloud services for libraries. The main results focus on data, patrons, library staff, IT infrastructure, cloud services, costs, and policies & contracts. Besides these main aspects, the authors underline that the librarians must understand the IT processes.  In addition, the authors mention clearly the problems related to the legal aspects, the risks, and the importance for librarians to support the new web services. 

In a second part, the paper presents seven main recommendations to help librarians in selecting / developing a set of cloud services (the service scope, managing the time, the costs, the quality, the human resource, the communication and the risks).   

The style and writing of the paper is clear and easy to follow.  The organisation is good and examples are used to help the reader to understand the underlying problems / issues / possible solutions. 

This paper might generate questions and discussions during the conference.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,8/3/2018,21:18,no
14,5,411,Zhxxx Xx,2,"(OVERALL EVALUATION) The paper summarizes challenges reported in papers related to library cloud adaptions, then offers some general guidelines to address these challenges. While the topic is relevant and of interest to JCDL audiences, the paper does not go deep enough to reveal much new information not already well known. The methods section lacks details, making it difficult to evaluate the validity of the results. The recommendations section makes all identified challenges a project management issue, which lacks support from the literature and experience. A few other issues:

1. Figure 1 is too small. The clustering and the linking could be a major contribution of the paper but are not sufficiently addressed in the paper.
2. It is not clear how many of the identfied challenges are library specific. It feels like that they are in general limitations of any SaaS. These issues are being addressed by the industry, e.g., via govcloud, better tooled management interfaces, and better cost estimation, etc. It’s not clear if the challenges remain unresolved or have been alleviated by these new developments.
3. Are library journals the best place to mine these challenges? It may be the case that libraries lag behind the IT industry in cloud adoptions such that library SaaS is implemented poorer than industry average and then library personnel is poorer prepared for the transition? Or the quality of some papers can be lacking? For example, the UWHS example cited in the paper drawing from a single failure of adopting free cloud-based videoconferencing its conclusion that ‘UWHS is less likely to rely on any free, cloud application for any “critical project again”’. This clearly ignores the fact that a significant number of universities have already outsourced their email services to free google or Microsoft cloud services.
4. The focus on SaaS, in particular ILS, does not take into considerations of many other library cloud adoptions. Many institutional repository software, including Dspace and fedora, have developed cloud images and are widely used.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Could still be accepted as a poster.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,17/2/2018,0:33,no
15,5,301,Bexx Plxx,3,"(OVERALL EVALUATION) The manuscript addresses the role of cloud computing in library services.  Is there a net gain to incorporating cloud services? What are they? What are the potential problems?   While cloud computing as a formalized concept has been in the lexicon of IT for over a decade, with recent growing awareness of data sensitivities and vulnerabilities, the topic remains timely.  The objective of the manuscript is strong:  illuminate the benefits and concerns to incorporating cloud computing in library services, and offer recommendations.  

The manner in which the objective is achieved is deeply weak however. A fundamental weakness is an ill defined concept of cloud computing.  For instance, the authors imply that email is a cloud service. It is not, and shows confusion over the difference between distributed local area services provided by an institution and commercial cloud providers.  The authors bring some clarity to the question by naming Software As A Service (SAAS) in the abstract, but this is only mentioned in the abstract and the SAAS concept is then not carried through the body of the work.   The manuscript could be improved by carrying the concept of SAAS through the manuscript, and within that, carefully distinguishing local area services (such as those supported by the institution), private cloud services, and public cloud services.   Use a similar conceptual frame to identify library services too - those are similarly not well defined.   

The two major topics of the manuscript: a challenges of cloud computing (Sections 3, 4) and a recommendation section (5). The two topics are disjoint, and appear to have been glued together given that they do not support one another, do not flow from one to the other, and have opposing assumptions.  The challenges (Sections 3, 4) appears to be a student literature survey; it is written in a halting, disconnected style that speaks to a master's student summarizing papers and hooking them together in a barely present narrative.  This incoherence between topics and a glaring lack of technical knowledge on the part of the authors results in rather absurd inferred conclusions (i.e., 4.1 para 3): Since anyone can set up a cloud account, it is advisable to not share patron data over the web.   (4.1 para 7): Data loss is a glaring threat because (Amazon) hosts can crash. The reviewer refrains from attempting to explain why these are absurd conclusions, and instead suggests that the authors can improve the manuscript by vetting the conclusions with someone with deeper technical knowledge than the authors appear to have. 

The authors raise good questions about the location, rights, and protections of their major data assets.   The institution as a stakeholder has data resource management policies that affect the decision to move core data assets to a commercial cloud.  The manuscript would be strengthened by a focus on data assets of the libraries. The reviewer recommends this focus.  

For this or any topic to be effective in the context of this manuscript, however, the challenges need to be crisply identified in a more coherent narrative in Sec 4, and then addressed in an integrated fashion in Sec 5.   This requires bringing together the highly disparate Sec's 4 and 5, but that is required anyway for the manuscript to approach being in a form that contributes to a reader's knowledge.  

In summary, while the manuscript is far too weak for inclusion in this venue, the authors are encouraged to continue to work on the topic especially in handling the data assets of a library. It is recommended that the authors additionally engage a technical data scientist who can help crystallize the true challenges of moving to a commercial cloud.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is heavily flawed;  it's two relatively unconnected topics glued together.  The first topic shows glaring weakness of knowledge in computing, resulting in absurd conclusions.  The second topic does not reflect any conclusions from the first topic; it is completely standalone.  I will strongly argue against this manuscript being accepted in the form that it is in.  Perhaps next year we'll see a better version.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,18/2/2018,16:26,no
17,6,2,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This short paper presents a mobile device application that supports the exploration of the physical collection in a library building by recommending relevant subject areas in other areas. Log file analysis is used to answer some specific research questions such as what trends can be identified for this kind of browsing.

I find this research is interesting and relevant for the conference, but although the experiment is somewhat easy to understand, the presentation is unfocused and not yet at the level that is required for a conference publication. 

In the abstract and the beginning of the paper, the paper claims to present the development and evaluation of the topic space recommendation model. I do not find that this model is presented properly anywhere in the paper, and do not see any references to other papers presenting it. It is partially described in the background section, but difficult to identify what this model actually does. In the conclusion, the name of the app that implements this module is mentioned but it is unclear if this app module is the same as the model initially indicated as the topic of the paper. 

The introduction of the paper (as well as the abstract) lacks the contextualization and motivation for this experiment. I also find it inappropriate to include authors contention and personal position, as it only weakens the contribution.  In general, the paper lacks a good description with examples on how the system works and what the actual contribution is.

A main contribution of this paper seems to be the log analysis that is used to answer 2 research questions. The first RQ is well formulated, but the second is hard to figure out and the author can improve the contribution from this research by more carefully designing a set of well-defined research questions. 

The graphs do not render well on my print, and I suspect that this also will be a problem in the final print. Even in the pdf on screen, the images do not render well because of low resolution. 

The analysis in the finding seems to be focused on what recommendations that have been made - or followed by end users. Given that the log is covering 2 years, I find the number of users and records recommended surprisingly low and indicates a mobile app that is rather infrequently used? It is somewhat unclear if they at all investigated how successful the recommendations where. Phrases like ""checkout records"" and ""strong enough for circulation"" may make sense for a librarian but is not a precise way of describing the limitations of the research. The recommendations are described as having a ""long tail power law distribution"" but the research would have been much more interesting if the authors succeeded in explaining this phenomena in terms of user experience. All in all, I find it hard to figure out what the actual findings are.

The paper has a good list of publications, although the formatting needs to be improved.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,23/2/2018,8:53,no
19,6,138,Danxxxx Gxx,2,"(OVERALL EVALUATION) In the digital era, the huge number of books gets harder and harder to manage over time. In order to solve this problem, the author of this paper presents a new application in order to develop and evaluate the topic space recommendation model. 
Also, this study presents some interesting results about how this application increases awareness and access to library services. 
The structure of this paper is clear and  well proportioned. Also, the method is presented in detail, giving enough information about the methods used. 
However, a few comments should be taken into account by the author:
- Figure 1 should be placed in the paper near where it is first discussed, rather than at the end of the Introduction, if possible.
- Proofreading is necessary to correct some minor errors (pg. 1 “to scholarly inquiry”, “has been renewed interest on” – the use of the article is recommended; “a collections”).
The paper describe how to develop and evaluate the topic space recommendation model as an alternative to the personalization algorithm. Anyway, in order to develop this model by using a mobile application, this research has a good practical implication.
With suggested minor improvements along these lines, this paper can make a good case for digital library development.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,8/3/2018,15:29,no
20,6,427,Zhxxxx Zhxx,3,"(OVERALL EVALUATION) This paper did an analysis of how library users navigate different categories of books by using a mobile app. To be more specific, the authors analyzed the log of 18 unique mobile app users over 2 years, and analyzed how's the recommendation quality after users have scanned some book barcodes. The conclusions are that: (1) some book classes have more subsequent recommendations than others. (2) the outlier analysis shows how topic expansions are related with some attributes, and resulted in long-tail phenomenon. 

My general feeling about this paper is borderline. One major issue that I'm concerned most is that there are too few users involved in this study (i.e. only 18 unique users). This might cause high variance in the subsequent analysis. Moreover, I strongly suggest the authors combine more closely Sec. 4 and Sec. 5 to show how the log analysis of the apps and Fig.2&3 lead to the conclusions in Sec.5. In its current writing, my feeling is that Sec.5 is very loosely connected with Sec.4. 

My past experience on Internet-related analysis papers is that, a study like this shall be done on at least hundreds of users, and use statistics or data analysis tools to reveal some key metrics (e.g. number of expanded topics after starting item, the plot of long-tail distribution, some more quantitative analysis) which lead to the claimed conclusions. But I do acknowledge this may not apply to library science, and it's possibly wrong in this paper's case, so please use your discretion to ignore this paragraph.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,16/2/2018,19:35,no
22,7,49,Juxxxx Fx,1,"(OVERALL EVALUATION) This paper presents a method of generating a representative document with a canonical timeline of events that are cataloged on Twitter by users sharing URIs of articles. In other words, the authors are using sharing activity on social media to generate a timeline for events being discussed at high volumes.

The authors use hashtags, n-grams, and their associated shared URIs to generate topical indexes, and use a retweet and sharing count to identify popular topics under the assumption that popular means the event should be cataloged. 

The approaches to identifying topical events is very intriguing. However, the fine-grained intent of the authors is unclear. one sentence in particular in the introduction (""While one could argue that editorial content and wikipedia pages contain the best information, many other perspectives are left behind due to attention, bias, and human scalability."") suggests that this approach is designed to incorporate multiple biases or viewpoints. However, the approach used by the authors has the potential to introduce bias. One can imagine that tweets using hashtags regarding #benghazi or #clintonemails would center around content vastly different than #trumptaxreturns or #mexicowall. Rather, timelines regarding #rogueone would be much more comprehensive. In other words, I didn't see any evaluations regarding the completeness of the timelines using this selection method to discredit any bias introduced into the resulting documents. I would have preferred to see examples of bias being removed or countered to ease my discomfort with the potential for this phenomenon to occur.

My concerns about bias regardless of popularity of content is amplified in the evaluation section. The authors aim to generate a timeline of events, but only use one wikipedia article (out of 9!) that has a comparable timeline. When comparing against the wikipedia articles, the inferred timelines have a very broad set of precision and recall measures (e.g., 0.07 vs 0.80) based on the topics. It seems difficult to draw conclusions about algorithmic effectiveness given this breadth.

Further, the authors use humans to identify defects in the constructed timelines. This seems to excuse low recall of events in the timeline. In other words, their evaluation identifies the number of irrelevant events in a timeline; if the recall of the timeline is low, the algorithm may perform better. However, f-measure is more indicative of the algorithm's success in this case. I would have preferred to see a canonical reference timeline (potentially generated by human experts) and the precision *and* recall and associated defects measured. 

Figures 6 and 7 were not fully understandable; what is ""T"" and ""W"" on the X-axis?

In summary, this paper is extremely interesting, but the evaluation and test dataset evaluation falls short.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I hate to be the ""non-committal reviewer"", but I truly am on the proverbial ""fence"" about this paper. I see a high value in the work (it's really neat that they are working on automatic story generation given the shutdown of storify), but think there are moderately concerning shortcomings with the evaluation. I look forward to my peers' input and will revise my review if others' have compelling arguments that I have not considered for either acceptance or rejection.

UPDATE -- given my peers' comfort with this paper and my reverence for both of their high levels of expertise in the area, I have changed my review to ""Accept"";","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,19/2/2018,17:54,no
23,7,18,Haxxx Alxxxx,2,"(OVERALL EVALUATION) This paper proposes an interesting idea for automatically generating a story in the form of a wiki using tweets and news articles. It introduces the concept of social pseudo relevance feedback and social query expansion. 

The paper offers a clear explanation of the methods used. 

Here are some questions and suggestions: 

- Figure 1 is mentioned in the text on page 1. However, the figure does not appear until page 3. The figure should be moved to page 2 where it would be most helpful to readers.
- Page 2: Change “Sharif et al.” to “Sharifi et al.”
- Page 2: “Integer LP approach.” Please state what LP stands for.
- Page 2: A shortcoming of the paper is that it does not offer an in-depth discussion of related work. For example, “Other work includes linking online news and social media [28]” and “generating event storylines from microblogs [15]” are just the titles of the cited articles. This is not sufficient to explain anything about these studies, the literature in general, or how the current paper extends, differs from, or contributes to the research landscape. 
-Moreover, how does the current paper differ from the following related work that was not cited: 
- Hua, T., Zhang, X., Wang, W., Lu, C. T., & Ramakrishnan, N. (2016, October). Automatical Storyline Generation with Help from Twitter. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 2383-2388). ACM. 
- Zhou, D., Xu, H., & He, Y. (2015). An Unsupervised Bayesian Modelling Approach for Storyline Detection on News Articles. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 1943-1948).
- Wang, Z., Shou, L., Chen, K., Chen, G., & Mehrotra, S. (2015). On summarization and timeline generation for evolutionary tweet streams. IEEE Transactions on Knowledge and Data Engineering, 27(5), 1301-1315.
- Page 8: Section 6.2. How many articles were collected?
- Pages 8-9: In Figures 6, 7, and 8. What do the x-axis and y-axis represent?
- Page 8: Section 6.2. The authors compare their timeline with references from Wikipedia given that many articles on Wikipedia don’t have a timeline. It would be interesting to compare the proposed timeline with a Wikipedia timeline. Several events on Wikipedia do have a timeline, however: https://en.wikipedia.org/w/index.php?search=Timeline+&title=Special:Search&fulltext=1&searchToken=3cc4yhepmcrukrd4z9jne76m1.  
- It is not clear what percentage of links are from spammers or advertisers.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,16/2/2018,0:55,no
25,7,25,Yaxxxx Alnxxxxx,3,"(OVERALL EVALUATION) The paper presents a framework that automatically generates a timeline for an event or an evolution of a story from the online stream of social media. The product of this research is similar to a wiki page in a couple of aspects: table of content, story evolution or timeline, references, related work. As the authors mentioned, a Wikipedia page usually is more than this; it usually has a history of the events, causes, and consequences.  The authors performed three evaluation methods on the page they generate an offline evaluation, a Wikipedia evaluation, and a diversity evaluation.   


The paper is well written and the methodology was clear. However, some parts of the evaluations and the results need more clarification. For example, section 6.1 that explains the offline evaluation doesn’t show enough details about table 3, especially the length. I couldn’t get exactly how the evaluation was done, who are the workers who evaluated D2 and what are their background, etc.?","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,16/2/2018,19:56,no
27,8,327,Jacxxxx Saxx,1,"(OVERALL EVALUATION) The objective of this paper is to present a multi-disciplinary methodological framework.
The current version of the paper is a description of a experimental design of a digital libraries (cultural heritage) for a Inuit community (North of Canada).  The author must clearly justify the choice and explain some of the used term (e.g., multi method) and how this can be integrated into a framework.  I'm expecting reading the advantages and limitations of the proposed new methodical considerations.  But many interesting questions can be found from this experiment (what are the specific cultural heritage objects that must be preserved? in which form and why?, etc.).

The writing is clear and organisation of the paper is good.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very limited interest, and the title does not correspond to the content","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,19/2/2018,15:27,no
29,8,400,Mixx Wrxxx,2,"(OVERALL EVALUATION) The paper outlines the methodological framework used to develop a community digital library (Digital Library North - DLN)  for an indigenous community in the north of Canada. The introduction, context and related work sections provide decent background. The core is section 4 which outlines the framework key elements; environmental scan, formative usability study, surveys , community leader engagement, information audit, community workshops and photography, What has already been reported in other references are the development of the model for the environmental scan and results of a community survey.

In the introduction and in the beginning of the conclusion, the author (project team) asserts that to develop a community DL for indigenous communities, you need a diverse methodological framework, and that is what the paper lays as the framework for the DLN.

The conclusions, however,  jump the reader from a review of the framework (with, what I'd expect as limited evaluation of its parts), to asserting three key lessons for developing a cultural digital heritage library without presenting or referring to specific results analysis - e.g. the final conclusion is the “DLN framework allows for a deeper and a more accurate perspective of how to develop community DLs … for indigenous and aboriginal communities”. It seems a leap from the framework discussion and note of some initial results to these assertions for what is a work in progress - I can understand these key issues, but it seems to get to assertion based on evaluation would be future work (or work that’s been done, but not reported yet).

That said, I’m intrigued to hear more of the project, and I think this project is definitely within the audience interest of JCDL. As this short paper only outlines the framework of community engagement, I hope we could look forward to reports on the environmental scan and how well it functions for a DL, more analysis on the usability data (and the comment of “less formal ways to collect usability data” would be interesting to develop), and what was learned from the leader engagement as a long paper. The information audit appears to have identified quite a large corpus of material, and it would seem an archival digitization task is in the offing.

Section 4.3 - end of section states survey data were analyzed and presented in another paper, but that paper is not in the references.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,19/2/2018,14:55,no
30,8,287,Daxxx Nixxxx,3,"(OVERALL EVALUATION) This paper describes methods used to engage with a community for the construction of heritage collections.

The paper is well-written and it clearly describes the methods chosen for the project. The review of related work is an appropriate size for a short paper but there is no clear linkage between this work and the framework in Section 4. Figure 2 just seems to be the union of all methods previously mentioned. The methods are described well but in a more narrative format than in a critical reflection - which is what I would hope for.

At the end of Section 3 there is mention of another paper but no citation. This is odd unless it is also submitted to the conference.

The “key lessons” listed in the Conclusion don’t appear to be be specifically based on the experiences of this project: can they just be restated as ‘we used the methods we had selected earlier’? There doesn’t appear to anything specific that links the methods chosen and the community-based collection development: I don’t see a clear contribution beyond reporting project activity.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,17/2/2018,3:22,no
31,9,370,Praxxxx Terxxxxx,1,"(OVERALL EVALUATION) The paper provides a method for combining knowledge captured from practitioners and from documents into semantically meaningful concepts. 
  The paper utilizes the lettrines labelled by historians utilizing relevance to propogate labels across the database. The motivation behind the paper and a brief
  background into challenges and related work is provided. The relationship between keyword visual representation, concepts in the context of system design, the learning process, 
  the algorithm are discussed. Experimental results for a set of 910 lettrines across different letters, patterns, background and sizes from Virtual Humanistic Libraries
  was used to study propogation. Results are discussed and future work is identified.

  The paper is innovative and applicable to digital libraries and image analysis. There are minor issues with clarity which should be addressed. Particularly
  the last line of the first paragraph of the Introduction, spacing through out the introduction (manually_done, a a_conclusion). Acronyms (CIBR - Content based IR ?) need
  expansion, spelling for words such as precized (3.2) needs correction.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,14/2/2018,3:22,no
32,9,62,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) Lettrines are the big decorated letters that often appears as the first letter of a word in ancient books. They are very useful for historians to distinguish the works, the printers and the date of printing. The paper presents a system for the interactive propagation of annotations made by a historian to other lettrines in a data base of lettrines’ images. 
The methodology is well described and the results seem quite promising. However, the topic is very specialized and would be of interest to a small minority of attendees. Also from the description it does not appear that the described methodology could be used in other applications. The contribution might be more appropriate if presented as a poster.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,16/2/2018,23:53,no
33,9,13,Marxxxxxxx Agxxx,3,"(OVERALL EVALUATION) The authors in the abstract specify that the goal of the paper is: ""... we propose an approach to interactively propagate annotations representing the historians’ knowledge on a database of lettrines images manually populated by historians (with annotations). ""
The concept of annotation is then central to the work, but in the related works section they do not refer to seminal works of the extensive bibliography on annotations, digital annotations and systems to support the creation and management of annotations. Why? Are the authors aware of all the work done in the sector?
Much important activity has been carried out within the DELOS network of excellence and the Open Annotation Collaboration.
Subsequently, many results of interest were published in the proceedings of the JCDL, ECDL, TPDL, and ACM DogEng conferences, and in the relevant scientific journals such as, for example, ACM TOIS and IJDL.

The presentation of what has been done by the authors is presented at very different levels of study:
- The authors use keywords that are typical of the field of information retrieval and that refer to general concepts, but the authors never contextualise in depth each topic with respect to what actually done. 
- Instead, section 3.3 shows a ""learning algorithm"" for a model. This algorithm refers to a specific implementation, so is more low level. It would be useful to the reader to understand what the authors propose, if the model the algorithm refers to was presented. But the presentation of the model is not introduced and explained in the paper.

In section 4.1 the process of indexing of the documents is illustrated in a simplistic way, as if the authors did not really know the methods of indexing the text that have always been based on the knowledge of the research results of George Kingsley Zipf.

Probably the authors were influenced by the choice to present their results through a short paper, so they oscillate between a presentation of general concepts in a generic way, and then with some attempt to provide details of some aspects that would be of interest. It also seems that the paper was written hastily, because there are many errors in writing: repetitive words, words that lack a letter, unnecessary white space, etc.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Unfortunately in the current presentation the work can not be accepted.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,18/2/2018,16:23,no
34,9,30,Daxxx Baixxxxxx,4,"(OVERALL EVALUATION) This submission details an adaptive pattern recognition technique the authors have devised to support scholars without a programming background to study visual artifacts within documents, such as lettrines.  Indeed, lettrines is the focus of this article, although the authors point out that -- given the adaptive nature of the approach which starts with no a priori knowledge -- the approach is also applicable to other visual artifacts.

The paper is reasonably structured, however the number of grammatical errors and spelling mistakes makes some parts hard to follow.  Any spell checker would flag ‘achine’ ‘manuallydone’ and ‘precized’ as not recognized words: the first looks like it was meant to be ‘machine’, the second ‘manually done’, but I could not work out what the latter was meant to be.  

The paper uses the term ‘semantic concepts’ numerous times without defining (or giving a reference to) what is meant by this.  This is more problematic than the grammatical and typing errors in being able to follow the work that is being described.  I wonder if all that is being described in this case are manually assigned text labels? It is another detail that could be addressed with some editing work, but continues the trend of a lack of care and attention to detail.  

The description at the very end of Section 2 is rather nebulous as to what it means.  I wonder, for example, if the fundamentals of the Gamera software suite by the DDMAL group at the McGill University (Canada) (which incidentally would count as prior work) doesn’t meet the requirements set out by the authors.

In terms of the technical work, it was troubling to read that their processing of the image maps it to be a gray-scale image when color is given as one of the four principal elements to lettrines that are studied (“the letter, the color of the letter, the pattern and the background”). The use of 3x3 cells also seems somewhat arbitrary, and not justified.  It begs the question what would happen to the accuracy of the technique reported if lettrines varied considerably in size, or if the digitized images the scholar is interested in comes from disparate sources where scan resolution is not uniformly controlled.  This then links to the wider context of just how exactly the document recognition system being described -- which requires it to be under the control of the scholar -- would operate in practice in a digital library -- this is not addressed in the paper, but is clearly an important issue given the conference topic.

The reported test set size is not very large (910 lettrine images covering the 26 letters of the alphabet), which was then further split in two to produce training and testing data.  I found it unusual that precision results were given without being accompanied with recall rates.

These issues make the paper too problematic in its current form for me to recommend it be accepted as a short paper to JCDL.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,20/2/2018,9:39,no
36,10,326,Jacxxxx Saxx,1,"(OVERALL EVALUATION) This paper focuses on the problem of identifying temporal street names (a street name with a date reference) in various languages: to automatically determine whether a street name is a temporal street name.  The paper is well structured and clearly written.   The description of the proposed system is well presented.  Various analyses are provided: Are some dates more frequent than others? Are some months more frequently used than others? ...  The precision achieved by the system is rather high (97%), and 62% when providing an explanation. 

The main drawback of this paper is the topic.  It is very specific and not directly connected with DL.   

Minor comments.
Beginning of Section 2.1:  The duration and set type are not explained.
Last paragrph in Section 2.5.  We could have a street and avenue with the same date, but both are distinct streets (the same in German with Strasse, Weg, Gasse).
Beginning of Section 5.1.  It seems that the street names appearing in the test set were already used when generating the system.  Need to confirm (or not) this point.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very specific topic, and if you have room, why not.  The paper is clearly written with an evaluation","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,8/3/2018,23:31,no
37,10,163,Fexxx Haxxxx,2,"(OVERALL EVALUATION) The paper proposes an analysis process that automatically extracts ""temporal"" street names from OSM. Afterward, the process seeks for each extracted street name Wikipedia pages that explain the cause or origin of the respective street name. Afterward, the authors conduct a manual analysis of the street names and explanation.

While the paper presents an interesting idea, it is unclear to me how this would be of interest to the JCDL community, mainly for two reasons First, the computer science contribution is rather low, since the described process simply combined well-established NLP methods to extract street name and find relevant Wikipedia pages. Second, the analysis of temporal streets (Sec 4) is likely more of interest to the social sciences or geography that to a computer science audience. Moreover, the usecase of the research project is not sufficiently described by the authors: whom would the proposed project and results be beneficial to?

In Section 5, the authors evaluate the two processing steps of their automated analysis: extraction and seeking of Wikipedia pages to explain temporal street names. As the authors note in Sec 1 and Sec 7, comparing the workflow with other methods is difficult or even not possible, since no approach so far has aimed to address this issue. While the evaluation of both processing steps is technically sound, a comparison with a baseline method would be great, specifically for the second step, which links Wikipedia pages to extracted street names. Also, it is unclear how many annotators participated in the study (Sec 5.2), only one or were those the same as mentioned in Sec 5.1. In both Sections, the authors should give information on the background of the participants. Even more important, they should caluclate the ICR and only accept the results if the ICR is sufficiently high between all involved participants.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,31/1/2018,14:02,no
38,10,57,PĂxxxx Caxxx,3,"(OVERALL EVALUATION) This paper presents a study on street names mentioning dates. The authors show how the data is obtained and perform a study on the distribution of dates in several countries.

The paper is very appropriate for this venue. Although, technically it is not very original, it does show novel exploratory results. The method is well described and is potentially useful to many different areas.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,16/2/2018,15:05,no
39,12,341,Giaxxxxxx Silxxxx,1,"(OVERALL EVALUATION) The paper presents a quite interesting insight into the training text used to determine word embeddings. On the other hand, the paper could present better motivations for the work and clearer take-home messages. 
In general, the paper does not read very well and it is not always easy to follow. The presentation of the work could be largely improved in clarity and organization in order to make it more readable and comprehensible. Moreover, there are some problems with the evaluation which uses a ""quality"" measure not better specified and do not report any statistical study of significance which is required for such analyses. 


More in details:
The first sentence is not grounded in the literature: ""Word embedding approaches like Word2Vec [21] or Glove [25] are powerful tools facilitating better search results and data analysis in digital libraries."" Word embeddings are used in many contexts especially in NLP; in IR they are used within neural networks in particular, but they are more means to represent documents in order to employ neural networks models for search rather than retrieval methods themselves. Moreover, within DLs I do not know if they have been employed to improve search results or to analyse data; it could be, but references are needed or this sentence should be revised accordingly.
As a consequence, the very motivation for this work is not well grounded in the DL area. 

The description of word embeddings is not very clear and quite cumbersome. I mean, I know how word embeddings are determined but I had some troubles understanding section 2.1.1 An example would have better served the purpose. 

Figure 1 in Section 3.2 is baffling. What is average quality? How is ""quality"" defined? Usually, the effectiveness of a model is evaluated by using proper metrics (e.g. DCG or AP or ...) and each metric has a proper meaning measuring a different angle of the model. Is quality based on how close the predicted embedding is with the ""correct"" one? 

Quality is not defined also afterward. This affects all the results. I do not know what I am looking at. 
Anyway, are the differences between the similarity measures of statistic significance? 

The comparison with the Google dataset is rather speculative and this is comprehensible since the underlying text is not available.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,26/1/2018,9:50,no
41,12,330,Chrxxxxx Sexxxx,2,"(OVERALL EVALUATION) The paper investigates the influence of corpus fragmentation on the quality of text embeddings using standard evaluation sets. 
This topic is highly interesting, and obtained results would help researchers in choosing good parameters for corpus construction and model training. Word embeddings for encoding text semantics is of interest to the community. 
The chosen methodology is generally sound, the paper nicely written and the steps are understandable. However, I do have some question about details/choices and are not convinced about some conclusions the authors made. Especially, the conclusion of ""3-grams are best"" and the deviation from the notion of window size of Mikolovs paper might cause confusion and hinder correct uptake of results from the community. 
Publishing source code and data sets is definitely a plus and increases reproducibility of the study.
 
Detailed comments can be found below.


Background
==========
- Formula (1): I think this is a simplification. I would add a parameter \theta for model-specific hyperparameters (e.g. negative sampling in word2ve). Among the hyperparameters are then d, epoch_nr, win). C and dict_size are parameters for the training data and not the model itself.
Also the fragmentation (k in k-gram) would be such a parameter.
- 2.1.2. I was wondering why the authors chose CBOW. since already in Mikolov's original paper [citation 21 in the paper] skip-gram outperforms cbow.
- Footnote 2: For me, it is not clear how you treat sentences. For sentence ""A b c d. E"" and 3-grams would you generate abc, bcd or anything else? An Example would have helped.
- Section 2.3. 
 - It seems that you use a different notion of window size and context than word2vec. A window size of 2 would generate 4 context words in word2vec/cbow. For a 5-gram a b c d e, the training example generated would be a b d e -> c (only the middle word is predicted). In you example you 1) consider a different notion of window size and 2) also create training examples a b c d -> e.
First, I would like to know the reasoning for these choices and second it has to be made clear in the paper that this differs from word2vec notion of window-size, otherwise readers might misinterpret the results. Your window size of 2 is Mikolov's window size of 1.

Experiment Setup
=================
- Figure 1 interpretation: It seems from figure 1 that - other than described in the text - the model is best at 7 epochs, and after getting worse, accuracy seems to increase again after 10 epochs. So, I am not convinced of your choice of 5 epochs.
- I wondered for the analogy tasks how you counted test cases for which a, b, and/or c were not part of the vocabulary. Did this happen?
- It would be helpful for result interpretation to report the values achieved in other studies on the different corpora. Just to see, whether your corpus+model+fragmentation-methods are way off or very close to what has been achieved elsewhere.


Experiment Questions
====================
- experiment question 1: 
 - ""..size of any n-gram corpus increases exponentially with large n"". I am not convinced of that. The number of tokens is nearly the same. Of we move a sliding window over a text of size s, we get s-n+1 tokens (if we ignore sentence boundaries). The corpus size is then the number of distinct tokens. As I agree, that the number of bigrams is larger than the number of unigrams, I am not convinced that this relation also holds for 10-grams and 11-grams, for instance. Seeing, that we stop to generate the n-gram at sentence boundaries. (**)
 - Justification of the question: I think this question needs to be limited to corpora of specific sizes. For a small corpus, but large n, we will have a lot of n-grams with a very low match count. Thus, the question (and your answers) need to be constrained to ""sufficiently large corpora"" (as you also argue in section 3.1.)
- experiment question 2:
 - the same argument for the size of the corpus as above holds here

Experiment Results
==================
- Section 5.2.1. n-gram size, referring to table 5:
 - I would disagree with the interpretation that 3-gram is the best. The total loss for 2-grams compared to 3-grams is 50% (17% to 6.7%). From 3-grams to 5-grams we again half the loss (6.7% to 2.3%). For the analogy tasks, results will even be better than full-text with 5-grams.  Only the difference from 5-grams to 8-grams is nearly neglect-able on average. Thus I would go for 5-grams. However, if a second parameter (e.g. the corpus size and thereby the training efficiency) needs to be considered, this choice might be different (see also my other comment marked with **)
  - I would use a similar argument w.r.t results in table 6.
- Section 5.3. 
  - It would be helpful to get a more detailed knowledge why min-count is so important. Which examples (or how many) could not be solved in the analogy task because a word from the test set was not part of the training set due to this threshold? Could you give examples?
  - Further, it would be interesting to see the different final corpus sizes with a specific miscount parameter (how many n-grams will be excluded?)
- Section 5.3.3.
  - what is an existing match count compared to the actual value for the match count? Do you mean theoretical value an actual value? Like x-axis are all natural numbers between 1 and max-match-count and y-axis is the observed frequency? Please clarify, to help interpret figure 2



Language/Format/Structure
=========================
- It took me some time to relate the results in Table 1-4 to the benchmark set described in section 3.3. It would be helpful to have the names you used for the columns of the table in section 3.3 (some are there but not all), and also have an additional heading in the tables saying which are similarity and which are analogy test sets. Further, it would also have been helpful to read the evaluation metric (Spearman vs. accuracy) in the table (either in the caption or in the column heading. 
- The same (heading, aggregation) applies for tables 7 and 8.
- I suggest to aggregate Tables 1-4 were aggregated, having an additional column/indicator for the respective window size. This would allow to better compare the results w.r.t. window sizes.
- above table 5: ""as explained Example 3"" -> as explained in Example
- Section 5.2.: You already introduced the parameter \emp{win} as window size. Please use the same parameter in section 5.2. (instead of $j$).
- Table 5: I was confused that the bold values (largest) ones actually encode the second-to-worst values. This does not make sense to me (apart that in the interpretation that this is the method you would chose, but I would disagree). Please reconsider the coding. 
- Table 10: please include the distance measure (cosine) in the table header
- Figure 2 should be placed on the page before, at least before table 10
- Figures 1 and 2 have some artefacts in print-out. Could you include a vector graphics version instead?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,9/2/2018,13:28,no
42,12,31,Daxxx Baxxx,3,"(OVERALL EVALUATION) This paper presents work analyzing the comparative accuracy of word embeddings trained on Google ngram-style corpora (i.e., as counts of ngrams rather than full, continuous text) under a several different experimental designs (varying the minimum count threshold and essentially the ngram/window size).  While most work using word embeddings tends to either work directly with embedding pre-trained on another continuous corpus (e.g., word2vec, Glove), some important work does use embeddings trained on Google ngrams (e.g., Hamilton et al. 2014), so it's a useful exercise to examine how the embedding quality might degrade as a function of reducing the amount of information in aggregating it.

Strengths:

-- Overall this is a nice experimental design, and I appreciate the in-depth explanation of the causes behind the degration in quality for the different factors.

-- The use of two corpora (Wikipedia, 1B word dataset of news) is great, and it's heartening to know the results are similar between the two, which speaks to the robustness of the results

-- I appreciate the clear recommendations (e.g., setting a minimum count threshold no less than 1/1,000,000).

Weaknesses.

-- I have strong concerns that the results presented here are not due to the factors examined (window size, min count), but rather to hyperparameter choices in word2vec (or other artefacts of the learning algorithm) such as the learning rate or the order in which the data is presented.  This is most salient in the example from table 1; with a window size of 1, word2vec trained on full, continuous text sees exactly the same information as every model with a fragmentation level > 2 (i.e., wiki_3_1, wiki_5_1 , wiki_8_1) -- only (as the authors point out), half as many times as any fragmented model.  If a full model and the wiki_3_1 model were initialized at exactly the same place and the wiki_3_1 was run for half as many iterations as the full model, so that it observed exactly the same *amount* of training data, would we not expect to see exactly the same representations in both models? 

-- This may be what section 5.4 is getting at, but since word2vec is trained using SGD, the order in which it is presented information matters for the embeddings that are learned.  Is it possible that the deficient behavior is observed here with the fragmented models because it's essentially taking a steps that's twice as big for each update (compared to the full model), since it's seeing exactly the same data twice?

-- Some of the results show the fragmented models performing worse than the full model, which I suspect is a result of just statistical error.  Can you present confidence intervals for the results (using the bootstrap, for example)?

-- I think some of the citation practices here could be improved; for example, instead of saying ""the famous example"" of man/women = king/queen, just cite Mikolov 2013; the citation provided for sentiment analysis on Twitter (Brody and Diakopoulos) doesn't actually use word embeddings at all.  

Minor

-- The initial discussion of fragmentation is a little confusing; why are 2grams more fragmented than 5grams?  I think this is conflating fragmentation with the minimum count parameter.

-- I disagree with the rhetoric in section 1 that one can come to ""general conclusions"" about the quality of embeddings with intrinsic evaluation.  Those metrics assess fitness for those specific tasks, but not a ""general"" fitness across a wide range of tasks.

-- in 2.1.2., yes word2vec and Glove have been shown to share general properties, but it's too strong to state ""that there are no fundamental theoretical differences"" between them.

-- I'm quite confused by what figure 2/section 5.3.3 is meant to communicate.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,16/2/2018,23:57,no
43,13,369,Praxxxx Terxxxxx,1,"(OVERALL EVALUATION) This paper introduces a data extraction system for acquiring affiliation information from web pages across domains incorporation conditional token probabilities, enriched using structural features (DOM) of HTML documents. A good background for methods used in extraction of entities and challenges in extraction of entities using a single methodology across domains is provided. The idea underlying the paper is unique, and is appropriate to digital libraries.

    There are however serious concerns in the discussion of the methodology, and evaluation of the methodology. When discussing the mathematical foundations of how the conditional probabilities are calculated the paper seems to make use of the equality operator, when an approximation operator should be utilized, since the
LHS of the equation is no longer equal to the RHS when simplifications are applied to one side of the equation. The evaluation of the paper does not state if the same set was used for building the conditional token probabilities and for testing. Assuming that the same set was utilized, the paper does not discuss the fit of the model (over fitting is possible when the size of sample is small (total of 11782 entities) even with cross validation). It would be useful to have any of the methods discussed in related work be evaluated on the same set for comparison.

    There are minor spelling and grammatical errors (Introduction Line 2: Tipically, Introduction Paragraph 4: To acknowledge (estimate?)","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,13/2/2018,6:36,no
46,13,149,Swxxxx Gotxxxxx,2,"(OVERALL EVALUATION) 1. An interesting task as current NERs are not such elaborative in terms of languages and html content as targeted in this paper. The paper focus on extracting names from faculty websites. The traditional NER taggers are more focused on the grammatical structure and are not suitable for faculty websites. The authors should emphasize on this. 
2. The paper uses a methodology of the textual content and structural content to achieve the precision on the task. 	
3. This is a useful problem and can be applied in many education application. The authors should also talk about how this can be useful in the education environment applications. 
4. The current models fail in such tasks. So authors should provide a comparison experiments of using standard NER techniques on this tasks. 
5. More details about labeling approach, and statistics of the languages and faculty counts will be useful. It is unclear from the paper the details of the dataset for training and testing. Also, it would be good to provide some details of how the model performed on various languages. On which languages it performed better vs which languages it failed? What is the analysis?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have seen that standard NERs models have many limitations and had to be tweeked for problems like this. The authors used text and structural features for better accuracy and on multiple languages. I would say that more details on the experiments should be sufficient to accept this paper.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,24/2/2018,0:43,no
48,13,324,Haxx Salxxxxxxx,3,"(OVERALL EVALUATION) In this work, the authors present one of the problems of web data extraction, namely entity name extraction of authors in faculty directories with the purpose of enriching public bibliographic databases. They propose a statistical approach that combines textual and structural features of HTML web pages which produced high precision and recall upon testing it against a dataset they created of +11000 researchers.

The flow of the paper is good, with high clarity and organization. The authors covered the prior contributions extensively and with breadth. They also devote a good amount of the paper to explaining the problem, and their approach and methodology to solve it. I was most impressed with the dataset that they collected and manually labelled. I hope the authors will publish it as well as this work so that the scientific community could benefit from it. Finally the experiments were sufficient and well documented. All in all this is a good balanced paper in my opinion. My only suggestion is that it would have been much more impactful if they ran their experiments on the same testing dataset but using a couple of the other prior approaches from their related works section. Their 0.95 F-score is good but would have been more impactful if it was compared with the other methods' results.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: yes",2,,,,,9/3/2018,0:18,no
49,14,48,Juxxxx Bruxxxx,1,(OVERALL EVALUATION) No paper submitted.,"Overall evaluation: -3
Reviewer's confidence: 1
Recommend for best paper: no",-3,,,,,12/2/2018,10:34,no
50,14,63,Vitxxxx Casxxxx,2,(OVERALL EVALUATION) The few lines of abstract provided seem to suggest that the topics of the poster will be the digitization of donor files (to be used in the future for sociological research) and the use of SobekCM (an open source system for digital libraries) to create collections of historical materials. Both topics are not new and might not be of interest to the attendees of JCDL 2018.,"Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,15/2/2018,12:07,no
52,14,339,Sanxxxxx Sixxx,3,(OVERALL EVALUATION) No document available for review,"Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,16/2/2018,5:50,no
53,15,389,Kaxxx Verxxxx,1,"(OVERALL EVALUATION) This paper proposes a method for ontology fusion that is based on a set of heuristics for handling local ontological relationships that exist for a node/entity in each of two ontologies, and prioritising specific relationships during the fusion process. Overall I found the paper difficult to follow and the intuitions underlying the approach not well justified.

The authors talk about ""binary similarity"" but this entire concept seems poorly articulated to me. This is simply ""strict matching"" rather than ""similarity"", is it not? Binary: either something matches or it doesn't. There is no ""similarity"".

Methodology:
The authors present their method in a collection of semi-formal definitions and rules. However, these aren't entirely formalized and do not clearly express constraints. Consider Rule II: if there are no nodes in O_1 that match o_j, then o_j is added to O_1. In that case o_j is inserted as a child of o_i -- which o_i given that o_j matches none of the nodes? Why is it safe to assume that o_i has the same type as this node o_j that doesn't match anything in O_1? This does not appear to be well-defined.

In general several of the definitions and rules seem to be defined in terms of the example in Figure 1 but it is not entirely clear how it can be applied in general.

The algorithm pseudocode is likewise not general (rather it is defined in terms of the layers in the example; does the notion of layers generalize?). Furthermore it contains errors:, n, I, etc. are defined to be integers but the pseudocode refers to null (implying a set) and in any case n is never updated and I is incremented as a counter, not an item removed from a set.

Why are all relations other than subordination lumped together as ""Correlation""? Why is this okay to do?

3.2.2 refers to stability of results in terms of ""mapping size""/large datasets. I don't see how this follows from the results in Figure 3 -- is this inferred from contrasting Figure 3 with Figure 2? In that case, putting the results in the same graph would make it more clear. This could be tested more systematically by considering different-sized subsets of the larger ontologies. Further, it is important to explore what is the impact of having many entities that cannot be directly mapped between two ontologies, i.e. for which Sim(o_i,o_j)=0? I suppose the reason for many other algorithms allowing for (partial) similarity rather than binary matching (Rule I isn't about similarity at all -- it requires strict matching) is to allow more nodes/entities to be aligned. What is the intuition that supports hardening this requirement? This very likely explains the higher precision -- strict matching would result in many false negatives (nodes unmatched when they should be).

It is not entirely clear what the P/R/F numbers mean -- correctness of nodes, relations, both? What exactly is a TP?

How can the authors claim 100% accuracy in the Conclusion?

Quality of writing:
The language in this paper is hard to follow in places; word choice and grammar are both not fully fluent. Some of these are easy fixes (""on the base of"" -> ""on the basis of""), others are a little more complicated (""the algorithm of main traverse procedure"" -> ""an algorithm for traversal of ontologies"") and others are just unclear (""timeless efficiency""). These are all examples from the abstract, but the main text suffers from this as well (""All entities ... are operated by layered traversal"" -- how can ""entities"" be ""operated""?) What are ""deformation methods""? I'm not sure what ""the missing open access of experiments environment"" means precisely. What is a ""loop algorithm"" and why does it lead to lower recall? Not following the logic.

The authors also mention that ontology fusion is distinct from ontology mapping/alignment in 3.1.1 but they do not elaborate on what this means. In what way are these tasks different?

References:
The referencing has some gaps. First, a sentence on the use of lexical semantic similarity _in ontology fusion_ cites a paper which is only about lexical similarity, not about ontology fusion (i.e. not really supporting the point that ""most studies"" focus on this but rather using the reference to indirectly define lexical semantic similarity). In fact there are several references which only consider semantic similarity but don't seem to contribute to the issue of ontology fusion. At the same time, the notion of lexical semantic similarity is not directly defined.

The authors might be interested in:
Joslyn, Cliff, Patrick Paulson, and Karin Verspoor. ""Exploiting term relations for semantic hierarchy construction."" Semantic Computing, 2008 IEEE International Conference on. IEEE, 2008.
Gessler, Damian DG, Cliff Joslyn, and Karin Verspoor. ""A posteriori ontology engineering for data-driven science."" Data Intensive Science (2013).","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,29/1/2018,7:21,no
54,15,97,Antxxxx Doxxx,3,"(OVERALL EVALUATION) This paper introduces BS Onto, a system for ontology fusion that relies on binary similarity (that is, BS). The motivation of this contribution is to tackle the issue of the performance of ontology fusion.
  Experiments show that BSOnto can perform in line with most existing systems (that is, slightly below the state of the art), but is computed faster.
  Relevance is a potential concern. A real use case analysis is lacking, and it is neither clear in the paper why ontology fusion is an important issue, nor why efficiency of ontology fusion is especially important to the JCDL community. This seems to be an offline process, for which efficiency is not so much of a concern.
  Soundness is another concern. The paper introduces challenges in terms of space- and time-complexity, but the core complexity of the algorithms is not discussed. This is only addressed by run time observations.
  Presentation: The paper is essentially well-written although a small number of formulations could be improved thanks to careful proof-reading.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,17/2/2018,7:27,no
55,15,345,Daxxx Smxx,4,"(OVERALL EVALUATION) The authors describe a method for ontology fusion and evaluate it on tasks from the OAEI Instance Matching track. The proposed method (BSOnto) is shown to have higher precision, and slightly lower recall, than several baselines. This seems like a good, focused short paper contribution; however, the writing is at times very obscure. For example, the abstract refers to ""main traverse procedure"" and ""timeless efficiency"". The last paragraph of the introduction seems to be saying that it's very hard to evaluate or compare systems at all, but that doesn't seem right given the straightforward evaluation. The mapping between ontology tasks in the Evaluation section (#3) is unclear. The runtime comparisons could analyze the significance of the results, given the much lower variance of the proposed system. All in all, a little work on clarifying this paper's contributions would be useful.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,20/2/2018,14:03,no
56,16,247,Gaxx Marxxxxxxx,1,"(OVERALL EVALUATION) This paper presents a strategy for name disambiguation (only the homonym problem) that does not require supervised learning and thus may be less costly to apply.  Rather than presenting the method as solution and comparing it to other solutions, the novelty in the paper is its focus on the different features (sources of evidence) and how results evolve over iterations of the clustering algorithm used.  The eight features are those typically used in similar work and the one key issue is defining a measure of similarity that drives whether the algorithm merges clusters or not.  Assessing the veracity of this measure with different selected combinations of features and investigating a limiting factor for convergence are the two issues of substance in the paper.  The evaluation and results are presented in a set of figures that demand careful reading and viewing on the part of the reader but demonstrate how results vary not only depending on what features are included but also how the clustering process proceeds over time.  It would be helpful to the reader to do a bit more explanation of each figure in the text and put them closer to text in the final version.  The finding that co-author and author references are helpful under different clustering constraints (similarity, limiting factor) confirms common sense about disambiguating authors with the same tokenized name.  
Overall, this is a useful paper because it demonstrates a method that could be applied in different use cases, for example, those where precision is valued over recall, or vice versa.  In effect, digital librarians could tune the similarity and limiting parameters to the needs of their users and collection.  Additionally, the costs of implementation could be considered in implementing the unsupervised approach with tunable parameters.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,8/2/2018,20:06,no
57,16,65,Lilxxxx Caxxx,2,"(OVERALL EVALUATION) The paper presents an alternative approach to name disambiguation.  The theme of the proposed approach is to reduce the complexity of the process.  The approach is tested on Web of Science data in order to have good name sets for testing, using the author-id of WoS. 

The authors describe a good selection of relevant work, acknowledging that some approaches achieve good results.  Their focus is on reducing the complexity of the approach.  No direct comparison of the results of their approach to other approaches is available.  Instead, the authors rely on specific results of their approach over a variety of sizes of data.

The paper is clearly written and the results documented in a large set of graphs showing performance.  

The authors several times use the word intend instead of intent
The frequent use of w.r.t is a bit distracting.  No other abbreviations are used and this comes up rather often.
In the final paragraph of Section 4.2, there is an instance of his that should be this.

Figure references are not always in order of the figure numbers.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,15/2/2018,20:01,no
58,16,142,Bexx Gxx,3,"(OVERALL EVALUATION) The paper addresses the well-researched problem of author name homonymy. Various approaches have been proposed to address this problem such as analyzing co-author networks.

The author presents a very simple, yet novel probabilistic similarity measure that delivers state-of-the-art results although being conceptionally simple.

What I like:
- interesting idea
- well presented
- good analysis and data visualization
- good discussion of results
- figures in the appendix
- the surprisingly good results

What could be improved:
- It would be great if the code and the data would be made available to allow reproducibility. This way others can compare their own results with the results achieved. E.g. using https://dataverse.org/","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,16/2/2018,21:22,no
60,16,386,Anxxx Vexxxx,4,"(OVERALL EVALUATION) This paper should be of considerable interest to the JCDL community and I was compelled by the claims made for this work: a simple, easy to understand and effective author-name disambiguation.  

Although the overall method is pretty clear and offers a promising strategy to tackle this problem, its claims to greater simplicity and greater accuracy than other methods are not solidly substantiated by evidence. Other claims such as the discussion of quality limits for clustering moves in section 3.6 sounds like the author(s) are convincing themselves that their formula for ""l"" just works and that the reader should just take it on faith that, for example ""the exact values [for alpha and beta] are not particularly important - only the order of magnitude"" and the Implementation details section is sparse.  The discussion of the experiments in section 4.4 also leaves the impression that the authors have not had much time to systematically analyze the result to understand why their choices of alpha, beta and epsilon are effective. The experimental maximization of these parameters may be evident to the authors from an examination of the results graphs but it would be helpful to more clearly guide the reader through that reasoning.

In short - I think this approach to the problem is very promising and I trust that the author(s) are onto an effective strategy. For this conference, I would have preferred that this paper might summarized as a short paper (without all the graphs etc.) or else that the claims of effectiveness and simplicity be better substantiated, even with just a table of F1 scores comparing the agglomerative clustering approach with other approaches, even if they were arrived at on different corpora. 

Below are a few suggestions for improvement for the next version of this paper.

P. 1 Col 2. ""We note that when disambiguating a name {\it name}, we do not need to consider any other names {\it name'} (in) R""

I am not sure (a) I understand what that means or (b) why this is worthy of note.  Is it because other approaches (e.g. ones that use pair-wise comparisons of names) do not consider each name one at a time?

Since the authors of this paper seem to put some weight on their (simpler) approach.

P.2 Col 1. ""She shows that the extend of ambiguity has a direct influence on the scientific performance"" - typo with ""extend""

P.2 Col 1. ""They show among others that the top-ranked researchers are so high up the ranking due to a lack of author name disambiguation."" 

Suggest a rephrasing ""They show, among other things, that one reason that the top-ranked
researchers are so high up the ranking is due to the lack of author name disambiguation.""

P.2 contains four occurrences of the phrase ""very good results"" or ""very good results"" to describe other researchers work but with no quantitative measures to back it up (except for one reference to an F1 measure).

Suggest an comparison of F1 measures of various methods (if available), e.g. in the discussion section - to substantiate the ""very good"" / ""better"" claims

P.2 Col 2. ""Depending on supervision and what they call ’rules’ renders their method rather complicated"".

What aspect of supervision makes their method complicated?  The fact that it *depends* on supervision or that their supervisory training method depends on 'rules'... I am willing to accept that the method is complicated but it isn't clear why.

P.2. Col 2. ""there is exactly one document d(x) that this mention appears on."" Suggest:
""there is exactly one document d(x) in which this mention appears.""

P.3. Col 1. ""All categories assigned to d(x)""... by whom? Elsevier's subject categories? It isn't clear from the description of Fcat(X) whether or not the authors of this article decided on the categories.  Same comment about keywords (although it seems likely that these are they keyword chosen by the articles' authors.

P.3. Col 1. ""All names given as authors of all documents d' referenced by d"".  Does this mean the same thing as ""All the names of the authors in the list of references in d"" or does it mean ""All the names of the authors in the list of references in d that are also authors in the collection.""?  In other words, are you considering author names for works referenced by d but which (the works) are not also in the collection you were studying?

P.3. Col 1. The sentence structure for Feature 8 is unclear.

Section 3.2 describing the Agglomerative clustering algorithm seems could be clarified to substantiate the claim in section 2 that ""blocking"" is being used. If, as the authors state in 3.2 ""the initial state where each mention x is in its own cluster C = {x}"" and ""Then, pairs (C,C') of clusters are merged"", surely this amounts to pairwise comparisons of {x} / {x'} in the limiting case where each cluster is a singleton of one.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I didn't want to say it point-blank to the author(s) but it seems to me that there is a lot of ""hand-waving"" in this paper.  It also reads like ""notes to myself about why I did what I did"" rather than a research paper.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,18/2/2018,18:35,no
61,17,376,Ricxxxx Toxxx,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the proposal and use of a framework for the validation of conformance checkers. Three dimensions are considered in the framework: correctness, usability, and usefulness.

Motivation and objectives are started clearly. Furthermore, the evaluation framework is sound. Its phases and components are in general explained properly and their use is illustrated in such a way that may serve as a guide for others in the community interested in performing similar evaluation protocols. 
Experimental results validate the use of the proposed framework and shared learned lessons are of wide interest.

Presentation needs to be improved, although. Some issues include:

1.	It is not clear how many tools compose the conformance checker. An architectural overview of the system should be included in the paper.
2.	It is not clear the rationale for the media types considered in the study.  Part of the discussion present in reference [10] should be brought to this paper.
3.	A more detailed overview regarding the profiles of participants could be incorporated with the paper. For example, it is not clear how familiar they are with similar tools (or technologies) to those used in the usability studies.
4.	There is no clear discussion about used classifiers in the correctness evaluation phase.
5.	Tables 2 and 3 should be replaced by graphs (e.g., boxplots).

Some minor issues include:

6.	Some overview about achieved results could be included in the abstract.
7.	Provide dates for the last access to the listed links.
8.	The use of quotes in the description of classes should be revised.
9.	It is not clear what authors mean by “Section 2.2 IDs”. Does this section refer to the PDF reference?","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,10/2/2018,18:49,no
62,17,356,Shxxxx Sugxxxx,2,"(OVERALL EVALUATION) This paper presents an experimental evaluation under the project named PREFORMA to evaluate conformance checkers for preservation of digital resources in three aspects - correctness, usability and usefulness. This paper describes the evaluation methodology in each of these aspects and shows the evaluation results followed by discussions. 

The project presented in this paper is interesting and practical. The methodology taken in this project seems reasonable and sound as a practical project. However, unfortunately, this paper is weak as a scholarly paper because it lacks descriptions about the innovative features of the evaluation methodology and/or results obtained from the project. 

Proof-reading by a third person is recommended for this paper because there are unclear descriptions/sentences.
- Correctness, Usability, Usefulness need definitions as criteria for evaluation
- Unclear English phrases, e.g., singular and plural forms of ""the conformance checker(s)""
- N in the sentence ""where N is the total number..."" below formula (3) is not used in none of the formulas
- It would be better to include richer related works

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper is acceptable as a poster.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,17/2/2018,16:00,no
63,17,202,Jaxx Kaxx,3,"(OVERALL EVALUATION) The paper investigates ""conformance checkers"" for three file formats.

There are some key strengths:

- The general topic of quality control of file ingestion at scale, is of key importance to the practical application of advanced DL solutions in heritage and memory institutions.

- The attention to the professional aspects, rather than the pure technical aspects, is refreshing and long overdue -- these tend to be the key barrier to realworld application.

- The proposal is a very simple and straightforward approach with limited novelty -- which is a key strength as this will be crucial for practical uptake.

There are some limitations:

- The results are interesting, and promising, but a more crisp cost/benefit analysis would be welcome.

- A broader discussion of the embedding in (current) curatorial practices would be welcome.

- The paper at times reads too much like an EU project report, and less as a scientific paper on the topic -- although this is perhaps a matter of style and preferences.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Interesting -- and important new aspect to discuss at JCDL -- but more a project report than having earthshattering results or insights.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,23/2/2018,11:43,no
64,17,415,Seuxxxxx Yxx,4,"(OVERALL EVALUATION) * Strengths
	- Good amount of details provided for a conformance checker developed from the project PREFORMA
	- Opensource tool and training/test corpus publicly shared
	- Thorough evaluation of the prototype tool involving both users and experts

* Notes
	- Figures 3 and 4 unnecessary (taking up too much space without much information, font in these figures is too small to read)","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,24/2/2018,16:11,no
65,18,55,Klexxxx BĂxx,1,"(OVERALL EVALUATION) IMHO, the contribution of this paper is incremental/marginal and does not warrant publication at this conference. It is too specifically tailored to one paper, which is somewhat extended and compared against.

More specifically:
- ""uncovered by the inventory"" -- What does this mean?
- ""we take another random sample ... on the set taken by the authors"" -- Why do you say this here?
- ""... in improving the precision of sense detection."" -- Upto here, everything sounds incremental, and the contribution has not become clear yet.
- ""... of novel word sense detection."" -- The text until here (the previous paragraph in particular) are too verbose.
- ""In particular, if a target word qualifies ...of the two time points."" -- I do not sufficiently understand this.
- ""Manual evaluation of the results ... by the original method."" -- Why are there no comparisons with other approaches, such as the ones from Kulkarni et al. or Hamilton et al.?
- ""The proposed method can therefore ... of novel word sense detection."" -- This is not precise enough. What exactly can be combined with what?
- As mentioned, related work is mentioned, but the relationship remains unclear, in terms of performance in particular. There need to be experimental comparisons.
- ""their bigram distribution"" -- What is it? This submitted paper should be self-contained.
- lexicographer's mutual information -- What is it? This submitted paper should be self-contained.
- ""we are concerned with only 'birth' cases for our study"" -- Why?
- Hierarchical Dirichlet Process -- What is it? This submitted paper should be self-contained. 
- ""and are expressed by the top-N words ... probability"" -- An example would be helpful (if this plays a role). All in all, this description is not understandable. The relationship to the new method proposed later is not sufficiently clear to me.
- ""The authors treated the ... in a sense repository."" -- I do not understand this.

English:
- as per the users' needs
- with 'attractive personality' related sense
- Attempt has also been made
- detect novel sense of a word","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,29/1/2018,8:24,no
66,18,350,Mxx Sxx,2,"(OVERALL EVALUATION) The present paper proposes a new technique based on network features to improve the precision of new word sense detection. The paper is particularly interested in detecting continuous changes of word meanings over time.
Although the paper provides important research problems and the interesting apporach to word sense detection, the paper suffers from the following several issues.


Major problems
The authors claim that ""In this paper, we showed how complex network theory can help improving the performance of otherwise challenging task of novel sense detection."" However, I am not convinced how the present paper address complex network theory to tackle the word sense detection problem. If the authors want to make such strong argument, they need to provide engough evidence for it.


I am confused about the following sentence. What do you refer to by ""In this work"" Mitra et al. or your work? It seems to me that you refer to your work. But it is vert unclear what you exactly refer to.
""In this work, authors consider multiple time points and not only detect new senses (i.e., ‘birth’), but also identify cases where (i) two senses become indistinguishable (‘join’), or (ii) one sense splits into multiple senses, or (iii) a sense falls out of the vocabulary (‘death’).""

Tense of verb must be consistent. I found that there are a number of places that tense of verb is inconsistent.

The authors mentioned that ""We perform the evaluations manually and each of the candidate word is judged by 3 evaluators."" But they did not mention about the agreement rate among three evaluators. Since the author used the manual evaluation, it is critical to show the agreement of judgements among evaluators.

Minor problems
like machine translation, semantic search, disambiguation, Q&A etc. --> like machine translation, semantic search, disambiguation, Q&A, etc.

by Mitra et al. -> by Mitra et al. [27]","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,14/2/2018,17:14,no
67,18,176,Anxxxx Hixx,3,"(OVERALL EVALUATION) The authors introduce their NLP approach to disambiguation and language dynamics, in particular the detection of new meanings and word meaning shifts. 

The paper has a very strong focus on the baseline an evaluation, with the novelty of the approach being merely sketched. Unfortunately, significant related work on disambiguation is missing from the non-NLP field, such as ""An open-source toolkit for mining Wikipedia"", ""Improving access to large-scale Digital libraries through Semantic-enhanced Search and Disambiguation"", as well as work on Gerbil- the General Entity Annotation Benchmark Framework, to name just a few. 

The evaluation is described in detail, especially the detection of novel senses and the detection of known shifts. However, the authors describe their results in detail (for some concrete examples) but do not compare with any other approaches (beyond their two baseline algorithms), which makes evaluating the impact of their results difficult. We suggest shortening abstract and introduction, as well as Section 3, to make room for more details on comparison to other approaches in their experiments.  

The paper appears as if mere lip-service is paid to Digital Libraries by mentioning them just once in the introduction. Clearer discussion in how this research would benefit digital libraries (beyond generic 'large data') could make the paper more relevant to the JCDL audience.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,18/2/2018,19:00,no
68,20,74,Hunxxxxxxx Cxx,1,"(OVERALL EVALUATION) This workshop proposes to gather the studies on the knowledge discovery from digital libraries.  My main concern of this proposal is that such a scope is probably too large for a workshop and very likely to be highly overlapped with the main conference. The proposed topics of interest include very broad topics (e.g., artificial intelligence, big data analytics, information retrieval, etc.); some of which I believe even larger than the topics of interest for the main conference.  As a result, it is perhaps inappropriate to include this proposal as a workshop for JCDL.

The organizers are suggested to list some prospective program committee members to show that the organizers are well prepared.","Overall evaluation: -1
Reviewer's confidence: 4",-1,,,,,24/1/2018,2:47,no
69,20,352,Laxxx Soxxxx,2,"(OVERALL EVALUATION) The workshop aims at promoting knowledge discovery for the digital library, which might be a potential research topic. However, I think the topic of knowledge discovery is too broad. Organizers should at least mention examples of possible tasks willing to be solved by knowledge discovery technics. What are the downstream tasks and the long-term objectives in the field? I have the feeling that any work feeling with data mining/machine learning/big data/… might fit with the topic. In my sense, the « topics of interest » item might be more particularly addressed to research challenges.

Otherwise, important dates and submission details sound good. Organizers have publications in related fields but nothing is said about their experience in organizing workshops.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,27/1/2018,14:18,no
70,20,402,Dxx W,3,"(OVERALL EVALUATION) Knowledge discovering is related to digital libraries but not quite new. The topics listed in the proposal seem a bit decentralized and covers everything including big data analytics, artificial intelligence, information retrieval, etc. As a workshop, I think it should be focused on a specific field. In addition, the proposal is simple, not considering some detailed factors like the acceptance rate, the audience, how to call for papers, etc. It even does not tell the brief bio about organizers. I think it should be further carefully planned.","Overall evaluation: 0
Reviewer's confidence: 4",0,,,,,28/1/2018,18:44,no
72,21,20,Suxxx Alxxx,1,"(OVERALL EVALUATION) relevance to JCDL: This study explores an important topic -- what metric companies can use as a guide for strategic decisions about research investments. Linking research and outcomes is important for all sectors however a clearer linkage of the relevancy to JCDL is needed. 

 novelty/originality: The approach used by the researchers -- linking a comprehensive review of patents with revenue generation as reported in the Fortune 500 rankings is sound and extends work undertaken in the past. It does offer a novel approach adding the concept of temporal buckets.

 methodology: The methodology is well thought out and has checks in place to achieve rigor. The study works with a substantial dataset (2.6 million full txt articles and 93 million patent citations) and identifies rigorous processes including a full suite of preprocessing steps.

 style/quality of writing: This paper is very well written both in terms of its organization and style.  The argument is clearly stated following a logical progression that provides the reader with scaffolding for understanding the impetus for the work, the design of the research, the experimental setup and the results.  In addition the writing is easy to read with terms being defined and concepts being described in easy-to-understand language. 

evaluation: the study results are interesting and well presented. The possible explanations offer interesting analyses of the results and insights that are useful. The addition of case studies offers further insight and suggests the authors were working to triangulate results which adds further credibility to their work.

 replicability: The description of the study  appears to be thorough and explicit which suggests it would be replicable. For example the description of bucket construction, which is a unique piece of this study, appears to be sufficiently detailed for replication. I use the word ""appears"" since my own experience is that until replication is tried it is not possible to imagine every condition that may diminish replicability.

 adequacy of references: References are acceptable for this paper which wisely uses much of its specified space on this study after providing an adequate lit review to set context","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,24/2/2018,20:07,no
73,21,238,Jonxxxxx Lexxx,2,"(OVERALL EVALUATION) This paper details an interesting study into ranking trends of Fortune 500 in terms of revenue and innovation (measured via patents). The authors propose temporal buckets for aggregation and analysis. The authors present several breakdowns and analyses for sets/buckets of Fortune 500 companies. Analysis of content found within patent data also provides an interesting source of network science based insights into the interworkings of Fortune 500 companies. The concept of temporal ranked shifts itself is an interesting and relevant topic to retrieval. The paper itself is well written. 

However, this paper does not propose new insights, approaches, techniques, or theory that are revelant to the JCDL community at large. While this paper presents interesting insights on several aspects of their dataset and case studies, the proposed analysis appears more ancedotal than exhaustive. The references are suitable for the topic but indicate that an enterprise, economics, or data mining conference would be more suitable for this submission. It is not clear where this paper could possibly fit within the JCDL agenda.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,16/2/2018,3:52,no
74,21,288,Jeaxxxxxx Ogxx,3,"(OVERALL EVALUATION) This paper proposes a  study concerning the relationship between innovation and revenue generation for several Fortune 500 companies over a period of time. This kind of study is quite important since it permits to have a view on the amount to be invested for future research, according to the correlation between scientific production and economic performance. This study relies on two important parameters to evaluate research outputs which are respectively the quantity and quality of scientific papers and patents. 


Relying on a robust study of the patent citation dataset available in the Reed Technology Index collected from the US Patent Office, authors highlight some relations between parameters such as number of (i) patent applications, (ii) patent grants, (iii) patent citations and Fortune 500 ranks of companies. 
This analysis is crossed with a temporal analysis showing the trends over the time, and try to highlight causal explanation concerning the influence of these parameters.. Two use cases of industry giants illustrating•fierce technology competition and its effect on overall ranks is also discussed through a nice graph based analysis


The paper is very well written are argued, the data are perfectly presented, and the experimental conditions are correctly discussed…. However, the paper  raises several questions and comments that follow …

First of all, the analysis proposes a study concerning several Fortune 500 companies, but doesn’t really make any disctinction between the nature of their activities while these nature could strongly infuuence the results of the analysis… For instance, the production industry may behave differently from the service industry. 
Regarding this aspect, the classification doesn’t really distinguish the enterprises issuing of the digital economy, while their business model is very different from “historical” industry, especially in terms of patent management… These points should be discussed in a final version of the paper, in order to objectively assess the situation..
Another point deal with the classification in 3 buckets that conditions many conclusions of the paper. Even if this classification permits to structure the reasoning process, it would be interesting to add some comments about the influence of this classification on the global conclusions of the paper… What would have happened if a company had been classified in another bucket ? what would have been the conclusions if 4 or 5 buckets had been used…In reality, what is the impact of the choice of this classification in 3 buckets
Another more theoretical point point which raises a question is the correlation tool, that is used in many places in the paper. The correlation is an excellent analysis tool for some parts of the criteria which are studied in the paper, but I think that the paper could be enhanced by using more “powerful” mathematical tools for such studies. This point is particularily true for the temporal analysis (trends in the time) for which some inter-correlation tools (coming from signal processing) or distances between distributions could also be interesting (Kulback distance or word mover’s distance for instance)… In order to cross more parameters, some techniques such as Principal component analysis could also be very useful …
In the 6th part, concerning the characteristic of temporal rank shift, the classification in 4 categories is discussable. It is discussable because it impacts the conclusion. Even if we can consider that the authors made a choice (which is “their choice”), the way the classification is done could be improved… The thresholds which are retained for the classification (80%) and the distribution around these 80% (+/- 10 or +/-2 std deviation rely on theoretical assumptions which have nont been proved (normal distribution ? ) 
Concerning th e 7th part, which is very interesting, the construction of the graph should be more clearly explained … The edges are explained, but nothing about the node is given… Are the nodes attributed ? 
Without these information it is quite difficult for the reader to extract the meaning of the graph. 
Furthermore the drawing of the figure 13 (graphs) permits to highlight some kind of “patterns” in the graph.. It would nbe interesting to try to interpretate these patterns, as well explaining a little bit the global shape of the observed graph.

Besides these comments this paper is very interesting and shows how the crossing of different disciplines can help to analyze a societal question.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I hesitaed between weak accept and borderline. The topic is interesting. The way the problem is addressed is correct, but could be improved with more powerful theoretical tools. However, it is a nice transdisciplinary subject.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,16/2/2018,18:50,no
76,21,99,Jx Stexxxx,4,"(OVERALL EVALUATION) This is an interesting bibliometric analysis of the relationships among large corporations and patent production and use. As such, it definitely has many merits and has an audience that would appreciate it. However, after three readings of the paper, I cannot find myself making a case that this paper is in scope for JCDL. This paper is better suited for a informetrics conference or a business conference innovation conference.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,18/2/2018,2:25,no
77,22,49,Juxxxx Fx,1,"(OVERALL EVALUATION) This paper uses health rumor data to evaluate users' trust in the material being rumored.

Several red-flags exist with this submission:
-- Wrong format (JCDL uses ACM/IEEE)
-- Submitted as double blind (JCDL uses single blind)
-- Poor use of space (3/4 of page 10 are white space)
-- No ties to JCDL topics (digital libraries, archiving, institutional knowledge)

Beyond this, the authors also utilize a flawed methodology by only using verified rumors to measure user trust rather than compare user trust in verified rumors vs non-rumors. Further, the authors rely heavily on the work by Zhang (as cited), but make no effort to compare their results or differentiate their work from the work of Zhang. It seems that explicit supporting or rejecting the proposed hypotheses would allow the authors to either support Zhang's findings (i.e., health rumors follow the same trust patterns as non-health rumors) or demonstrate that health rumors have special properties that make them different from non-health rumors.

This paper unfortunately is not suitable on any of its topic, presentation, or methods to be accepted to JCDL2018.","Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,5/2/2018,19:40,no
80,22,10,Maxx Agxx,2,"(OVERALL EVALUATION) The research topic is relevant to JCDL and literature is well reviewed. 

Research hypotheses look reasonable and interesting, but the survey design doesn’t look fit very well for the research questions. The fact that all of the 30 participants are graduate students (22 master degree holder and 8 PhD candidate, and 14 participants major in social science) means that they are likely to have higher information literacy than the general public and to be critical to the rumors, thus they are unlikely to share general attitude toward online health rumors, as the authors briefly mentioned. Therefore, a direct comparison between precedent studies is difficult. A sugges for further research: making a similar survey with younger students with presumably lower information literacy and comparing the result with that of the present study would give insight into a role of information literacy education. 

The selection of the rumors used in the survey requires a more detailed explanation, especially because the rumors database (reference 33) is not accessible. Authors used such a broad word “health” as a keyword and“randomly” selected rumors “which include the proposed rumor presentation (picture, verification or hyperlink)”, but a criteria for selection, for example, the number of comments/retweets, classification of dread rumors and wish rumors (which are common classification for online health rumors), or categories of the health topic (such as dentists, beauty care, child care, diseases of aging people, etc.) could have improved the survey plan and been useful for data analysis. 

A little work for the presentation of the article would be required; tables shouldn’t divided into different pages/columns (tables 1 and 2), figures in table 3 should be right-aligned, the caption for the figure 3 should be beneath the figure rather than in the next column, and contents of sections 3.1 and 3.2 are partially redundant.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I changed my evaluation from ""1"" to ""-1"", though I didn't virtually change my comments (just added one phrase). Re-consideration of the problems of the paper which other reviews had pointed out, with which I agree, and I myself had wrote in my reivew made me change my evaluation.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,22/2/2018,0:49,no
81,22,335,Frxxx Shxxxx,3,"(OVERALL EVALUATION) The study of what components make rumors trusted is important and particularly so with respect to health information.  The authors appear to use the term rumor to be misinformation although the term rumor could also be used to refer to correct information from a non-direct source.

While the study has numerous issues (small sample size of microblog posts, inappropriate study population), the results of the paper substantiate/reinforce prior results.   The authors acknowledge the issues with the study in the limitations section -- which is refreshing, but this seems really to be a decent pilot study for a more rigorous examination of the issues using a more appropriate study population and a larger set of microblog posts and/or variations of the same microblog post with/without the features being explored.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,18/2/2018,19:11,no
82,22,390,Jenxxxxxx Wxx,4,"(OVERALL EVALUATION) This paper investigates the effect of rumor presentation on user trust 
in online health rumor.

In general, the papaer is clearly written.
However, in addition to the ANOVA test of the presence of pictures, verification, and hyperlinks,
there's no proposed method in distinguishing between rumors.

The contribution of the paper is not clear.

Also, the sample size in the experiment is too small with 30 students in the survey,
and 10 students in the interview.
The data size of 24 rumors is also not acceptable.
The experimental results are not convincing.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,19/2/2018,3:28,no
83,23,375,Domxxxxx Tkxxxx,1,"(OVERALL EVALUATION) The paper is about semantic modelling, which is an important topic. The paper, however, is very chaotic and difficult to follow. The state of the art, the novel contributions, and the boundaries between the two are not clearly stated. It is therefore difficult to judge the novelty or relevance.

The paper is very abstract and lacks examples or diagrams that would help the reader to understand it. A lot of names appear without any definition or reference.

In addition, I think the abstract does not match the paper itself. For example, the authors write in the abstract: ""We explore the similarity of our approach to object-oriented analysis and modeling."" I don't see such a comparison in the paper.

For this reasons, I believe it should be rejected.","Overall evaluation: -2
Reviewer's confidence: 2
Recommend for best paper: no",-2,,,,,12/2/2018,15:07,no
85,23,60,Joxx Hx,3,"(OVERALL EVALUATION) The paper describes a mapping between ontological concepts and the object-oriented model. The goal of the authors is to provide rich descriptions of real life objects, based on the Human Activities and Infrastructures Foundry of ontologies. There is a good intellectual work behind the paper, that follows previous works of the authors. However, the paper includes several inconsistencies from the object-oriented perspective that need to be pointed out.

First of all, the paper is confusing and merges concepts of very different levels of abstraction without providing a clear rationale. 
The goal of the work is very ambitious since the authors try to build structured models of complex historical situations. They propose to use the conceptualization made in the HAIF, which seems to be a good option. However, at the same time, they put the focus on how its concepts map to Object-oriented concepts, making assumptions that are not exact. For instance, the authors claim that the goal of encapsulation in object-oriented programming is pairing of objects with methods, which is too simplistic and out of focus. Encapsulation is a programming technique used to ensure information hiding as a way of reducing coupling between modules, and later classes, in classical programming languages. Pairing objects woth methods would be a mean to ensure encapsulation, but not a goal on itself.

Quallities are clearly properties of objects that, surprisingly, are not mapped to attributes in object-orientes languages. Relational qualities, in turn, could be seen as associations (in UML terminology). Finally, there is another mistake when the authors associate UML with BPMN. There are completely different notations. In fact, UML has a much wider scope than BPMN, to the point that the so-called Activity Diagram in UML covers the same domain as BPMN, namely process modelling.

There is extensive literature on formal and object oriented modelling languages proposed in the early 1990s that explored the view of objects as observable processes. H. D. Erich, A. Sernadas, G. Saake and others developed families of languages that covered this view). Also, Yair Wand published a famous paper where he defined a direct mappingbetween the Mario BUnge’s ontology and the OO model:

An ontological model of an information system
Y Wand, R Weber - IEEE transactions on software engineering, 1990

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is confusing, with notorius inconsistencies regarding the Object oriented model.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,15/2/2018,9:42,no
86,23,141,Cx Lxx,4,"(OVERALL EVALUATION) Based on previous work that proposed a Human Activities and Infrastructures Foundry (HAIF), this paper considers issues in the implementation of that Foundry. Previously they proposed a basic formal ontology BFO to describe their foundry.  This paper explore additions to the foundry ontologies to support formal modeling on Korean ceramic water droppers.

They then investigate two museum objects to identify some of the differences between descriptions of Universals and Particulars in the BFO, and explore the possibility of using their techniques to supplement traditional metadata.

There are several issues not discussed.

There were no actual examples given of the derived ontologies nor was there any discussion of the size of the ontology and how it would scale.

There is no discussion of how such an ontology would be evaluated.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,16/2/2018,17:06,no
88,24,148,Sujxxxx Dxx,1,"(OVERALL EVALUATION) The proposed topic is of interest to the JCDL community and as the authors point out, the number of images being made available online and part of digital library collections is increasing.
Consequently, it is good to have a venue where people working on related topics and listed problems can gather to discuss status of research, practical aspects, and commercial solutions in these areas.

However, I am unable to judge if the listed authors were involved in similar workshop organization and have enough background and expertise on the topics mentioned in the proposal (based on their homepages) to be able to review and judge papers in the area.
The call for workshop specifically asks for the following information which is missing in the proposal.

·         identification of the expected audience and expected number of attendees
·         contact and biographical information about the organizers
·         if a workshop or closely related workshop has been held previously, information about the earlier sessions should be provided dates, locations, outcomes, attendance, etc.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The workshop sounds interesting and will be of value to the JCDL community.
However, I could not figure out the proposers' background and expertise on these specific topics and if they have organized similar workshops previously.","Overall evaluation: 0
Reviewer's confidence: 4",0,,,,,25/1/2018,5:34,no
89,24,381,Supxxxxxx Tuxxx,2,"(OVERALL EVALUATION) This workshop aims to bring together researchers and practitioners who deal with digital libraries of images and image retrieval. Image fields have played a big role in digital libraries recently, it would be nice to have a workshop that keep participants abreast of challenges and solutions in handling image collections.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,26/1/2018,13:36,no
90,24,352,Laxxx Soxxxx,3,"(OVERALL EVALUATION) The workshop addresses a very interesting and relevant topic dealing with image collection. Recent advances in deep learning have increased the need for large-scale image collections, opening several challenges in terms of creation, organization, access, and use. 
The workshop format (one day) is accurate and allows enough time for discussion. The possible topics are interesting. 

Suggestion: I would add another one related to the creation/use of multi-modal collections (e.g., text/image) since both evidence sources are complementary and there are several contributions in terms of multi-modal retrieval, captioning, ....","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,27/1/2018,14:08,no
91,25,168,Daxxxx H,1,"(OVERALL EVALUATION) The paper proposed an entity relationship model and a set of visualization schemes.
Node-link graph, force-oriented layout and bubble map and etc. are used in the schemes.
The entity model include three entity sets: the main-entity set, the child-entity set and the secondary-entity set.
Two examples of applying the visualization scheme have been presented.

Strength and contributions:
An entity relationship model is proposed and a set of visualization schemes are proposed.
The model has high universality and enables quick search of related entities.
Detailed requirement, rendering method and exhibition schemes have been discussed.
Two visualization systems have been made for two datasets to show the applicability of the scheme.

Weakness and questions:
The novelty of the proposed scheme is limited or at least not well illustrated.
When first mention some algorithm or terms, it’s better to add the corresponding citations. For example, when force-oriented layout is mentioned in introduction, citations should be added. Otherwise it is not easy to follow.
In Chinese Medicine, how about two prescriptions with same medicine but different amount? They must be the same prescription?
In related work or introduction, there should be more clearer discussion on how this work differ from previous work.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,11/2/2018,20:09,no
92,25,1,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This full paper presents a query/resultset model and visualization scheme for data that consist of entities of multiple types that are interrelated. The initial contribution is a generalized model of main entity, with related child entities and secondary entity(ies), which is applied in the construction of search results and the visualization of the results. The method and visualization are explored two different use cases, which serve as a proof of concept, but otherwise there is no evaluation to validate the generic nature, applicability or usefulness of the solution. 

The contribution is relevant for the conference, but there are different elements of this paper that can be improved. I find that the maturity of the research is a bit below what we should expect for a full paper. 

I find the proposed generalized model interesting, but the paper lacks proper evidence that this is a common pattern. In knowledge bases with multiple entities, there are potentially many different entities that can serve as the main entity - depending on the user´s interest. This sort of implies to me, that such a model is more related to the presentation of results. The two use cases that utilizes this model, may at best serve as an indication that this is a reasonable generic design assumption. The paper has an emphasis on the visualizations, but only presents the solution and a major critique to the paper is the lack of evaluation of the results in terms of usability. A user study - or other form of evaluation - will significantly increase the value of this.

The paper is reasonably well structured, with relevant figures and illustrations. The language generally needs to be improved, and the numbered lists should have been properly formatted as lists. The space before the citations is systematically left out, but the list of references is relevant. In general, I find this contribution interesting and relevant, but will encourage the users to broaden the scope and maybe contextualize and relate to the field of entity search. An more systematic evaluation of either the usability of the  visualization or the general applicability of the proposed model, is needed.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,14/2/2018,11:34,no
93,25,6,Alxxx Abduxxxxxxx,3,"(OVERALL EVALUATION) This paper presents a visualization scheme for displaying multi-entity relationship using the similarity correlation among the entities. The scheme was then demonstrated through the use of two datasets: a Chinese herbal medicine prescription dataset and a paper dataset.

This paper is interesting as it provides users with the capability of visualizing the multi-entity relationship within a search result. Within the visualizations, users are able to explore the similarity between the entities. However, the colors that are used in graphics make the visualizations hard to read. I would suggest the use of either ColorCAT (http://colorcat.org/) or ColorBrewer (http://colorbrewer2.org/) in the justification of the color usage. Although there is a section on the sample analysis, it would be nice to have a user evaluation to demonstrate the usability of the system (besides the authors of the paper). Perhaps a case study with domain experts, e.g., a pharmacist using the Chinese herbal medicine prescription dataset completing some defined tasks. I can understand the usage of the bubble graph in providing a more detailed class classification chart, but I was wondering whether the paper can also provide an explanation how it plans to address the issue of JND in using a bubble graph.

Some minor comments:
- Missing references:
* NEREx: Named-Entity Relationship Exploration in Multi-Party Conversations (https://dx.doi.org/10.1111/cgf.13181)
* WebVOWL: Web-based Visualization of Ontologies (https://link.springer.com/chapter/10.1007%2F978-3-319-17966-7_21)
- Some typos:
* Section 3.2: and and association -> and association
* Reference to the Chinese herbal medicine system database","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,14/2/2018,14:17,no
94,26,181,Nixx Hoxxxx,1,"(OVERALL EVALUATION) I think this paper has merit for a number of reasons, although some modification to its methodology would yield more rich results. It is relevant to the JCDL since it highlights practical and social issues related to digital libraries through its investigation of the scholarly literature about digital scholarship centers in academic libraries worldwide, with particular focus on China. They explained that China is in an early, exploratory stage regarding digital scholarship, so this preliminary study is novel and produces the beginnings of a framework for establishing these centers and services in their country.

I thought the methodology of searching academic databases and conducting content analysis on the results was well-intentioned, however they only searched Web of Science, ScienceDirect and Emerald (plus some professional reports) for English-language sources and three Chinese sources. If they intend to learn about digital scholarship services offered by university libraries, the academic literature will likely only provide evidence from other overview studies or individual institutions that have happened to publish about their services, which might not be representative of what's actually offered. Many digital scholarship centers offer information about their services on their websites, in blogs, on social media, etc. and further study of these sources might yield more reliable results about what institutions are actually offering. 

That being said, the authors did a good job analyzing what results they did get, and I liked their strategy of grouping digital scholarship services into broader categories for the purpose of constructing a framework. Groupings like this are interesting and needed within digital scholarship in order to better organize the wide array of services associated with the term. If they find additional services offered through more data/literature collection, they might consider revising or expanding them. The writing style was clear and understandable with a few small stylistic and grammatical errors that a quick copyedit could fix.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,9/2/2018,17:05,no
95,26,39,Toxxxx Blxxx,2,"(OVERALL EVALUATION) The paper provides an overview of digital scholarship activities in order to support further development in China. While it largely is an overview of existing literature, it could be interesting to the conference to understand some recent developments there. It is a detailed literature review but lacks a strong theoretical framework to bring together the various elements they identify as digital scholarship. It would have been nice to see some critical evaluation whether the hype around DS is justified.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would think this works better as poster - if accepted.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,14/2/2018,13:15,no
97,26,397,Nicxxxxx Woxx,3,"(OVERALL EVALUATION) The authors are reporting on a national initiative to establish a theoretical framework for developing digital scholarship services within Chinese university libraries. The initiative itself could be relevant to JCDL audiences; however, the content of this paper is mostly a preliminary literature review. There are distracting grammatical errors and the discussion is fairly short. Table 1 wastes space that could be used for more detailed discussion. There is nothing novel or original in the conclusion. There is little regarding the unique or interesting factors affecting providing digital scholarship services in China. As the authors note, further empirical work needs to be done to support their framework.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,15/2/2018,19:41,no
98,27,307,Anixxxx Prxxx,1,"(OVERALL EVALUATION) Summary of the review: 


Relevance to JCDL: The given work explicitly deals with the problem of domain specific linguistic variations in a big collection documents. 
The work present a cross-collection topic model combined with automatic domain term and phrase selection. The model achieves this by introducing different distributions (parameter) governing by the entropy for the collection-specific and collection-independent words.  


Novelty/Originality: The main novelty is treating domain specific text as something being generated from a distribution for common terms and another distribution for domain dependent terms (per domain) and training for the distributions jointly to maximise entropy. 

Assessment/Evaluation/Comparison: The proposed model is compared in details with the closest cross collection topics model. The authors perform explicit evaluation on perplexity and topic coherence as well as implicit evaluation on document classification to distinguish the words in two groups as mentioned. 


Style/quality of writing: The writing is good and overall the work is a pleasure to read. Different portions of the paper like motivation, background, technique etc are well weighted. 


Replicability: The experiments are generally replicable. The dataset is public and the experimental details are adequate to replicate the setup.   


Adequacy of references: The references are adequate from a focused task paper point of view.


General Queries:
1) recently there has been a lot of interest in using word embedding for compensating for the lack of semantic information in statistical LDA model? How do you think it compares with [1] (or many similar works)? Basically can word embedding compensate for facts that topic and apparatus mean the same thing (and hence maybe separate distributions are not needed)  

2) How does this model compare with hLDA where we can have hierarchy for domains and topics?


Minor:
line 29/1: “Upto 4% better perplexity” use lower instead of better? since other two comparisons are higher and all three are better in some way.

line 22/2:Linguistic contrasts, such as domain-specific vocabulary, complicate topic modeling. Maybe the example paragraph follow directly after this line.


References:

[1] Das, Rajarshi, Manzil Zaheer, and Chris Dyer. ""Gaussian lda for topic models with word embeddings."" Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vol. 1. 2015.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,15/2/2018,19:51,no
99,27,107,Andxxxxx Ferxxxx,2,"(OVERALL EVALUATION) In the paper, the authors propose combine ccLDA with a measure of termhood for modeling multiple domain-specific text collections, tokenizing by phrase segmentation. They propose a novel topic model that splits the vocabulary into collection-specific and collection-independent words, according to entropy-based termhood measure.

They uses the entropy of hapax legonema for determining the threshold value to split the vocabulary into domain-specific and -independent words.

The paper is well written and organized, the references are suitable.

The paper is original and relevant to JCDL.

The authors compare their proposal with ccLDA under accuracy, topic coherence and perplexity measures. 

Please, put the equation of termhood in the text body.

At the link provided in the paper, I was unable to access the code of the work. This makes it difficult to replicate the work results. 

In the Datasets section, you must discuss how the articles from English, French and German Wikipedia are reduced to the minimum number of words.

In the Document Classification section, it is not clear what the values presented on Table 3 are. Are they average values from 10-fold cross validation?

A bracket is missing in the first equation at the Perplexity section.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: yes",3,,,,,16/2/2018,18:35,no
100,27,140,Cx Lxx,3,"(OVERALL EVALUATION) The authors present a cross-collection topic model combined with automatic domain term extraction and phrase segmentation. Their model distinguishes collection-specfic and collection-independent words based on information entropy and reveals commonalities and differences of multiple text collections. They evaluate their model on patents, scientific papers, newspaper articles, forum posts, and Wikipedia articles. In comparison with state-of-the-art cross-collection topic modeling, their model achieves higher topic coherence, better perplexity, and higher document classification accuracy. They claim there model results in very clear topic representations.

These are interesting results but on small data sets that are very disparate. There are several issues.

How will the models scale? Not well I suppose.

How does one measure ""clear-cut topic representations."" Just by looking? This claim is over stated and not evident from the tables.

It would have been more interesting to compare fields within computer science or within science. The areas compared are somewhat disjoint.

There are many missing relevant citations; here are a few.

Jo, Yookyung, John E. Hopcroft, and Carl Lagoze. ""The web of topics: discovering the topology of topic evolution in a corpus."" Proceedings of the 20th international conference on World wide web. ACM, 2011.

Du, Lan, Wray Lindsay Buntine, and Huidong Jin. ""Sequential latent dirichlet allocation: Discover underlying topic structures within a document."" Data Mining (ICDM), 2010 IEEE 10th International Conference on. IEEE, 2010.

He, Qi, et al. ""Detecting topic evolution in scientific literature: how can citations help?."" Proceedings of the 18th ACM conference on Information and knowledge management. ACM, 2009.

Wang, Xiaolong, Chengxiang Zhai, and Dan Roth. ""Understanding evolution of research themes: a probabilistic generative model for citations."" Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2013.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,18/2/2018,15:17,no
101,28,399,Mixx Wrxxx,1,"(OVERALL EVALUATION) Paper describes investigation of some possible approaches to ranking papers by citation. Common method is through number of citations. Here, they do some sentiment analysis of the sentence in which the citation occurs, determine if it’s a “positive”, “neutral” or “negative” citation, and use that to modify the citation count.  They use some NLP approaches to develop the sentiment - they have developed a classifier to give the positive, neutral (objective) and negative.

They then use this to compare rankings by two general approaches; one based on citation count, and a second based on pagerank-like method that weights citation edges.  In each case, they run over a corpus from the computational linguistics literature (ACL anthology network) twice, once with just the count, or pagerank, the second incorporating sentiment. Results in each pair indicate changes in rank for a given paper. 

I have no problem with the basic method used to get a sentiment measure, and then define the ranking approaches and do a comparison, but what’s the value of this beyond a nice NLP and ranking exercise?

Adding in sentiment has changed the ranking, but what does that mean? In each example case, the papers moved down the ranking when sentiment was included (but then didn’t some go up then too?) Let’s say I have a paper, could I think that neutral citations are in some ways just “dotting the i’s” of recognizing the literature (e.g. when referencing JCDL literature, something this paper doesn’t do by the way), positive cites are those building on, or using, what my paper has suggested, and negative could well be others looking to differentiate their work.  Hence, a negative sentiment could be an indicator of really new work in the field.

I think to make this a more solid contribution to thinking about how to develop automated citation analysis tools that would help others find relevant material in the field, then more thought into what the data means is needed - something the authors begin to hint at in the future work section. At the core of this is to figure out what those sentiment measures can actually tell us.  This is definitely dangling an interesting prospect in thinking of bibliometrics or scientometrics.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The core experiment is actually quite nice and would have been better as a short paper as it’s really not yet formed in how their ranking approaches utlizing sentiment would help a researcher in the field. Mainly as they haven’t really figured out what their sentiment measures actually mean.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,10/2/2018,22:07,no
103,28,413,Wexxxx X,2,"(OVERALL EVALUATION) This paper presented an investigation of introducing sentiment of a citation into the ranking of scientific papers. The paper first investigated methods of assessing a sentimental score for each citation sentences. Then the authors proposed two basic ranking schema and compared ranking results with/without considering sentiment score. Finally, the impact of the sentiment influence in the index is closely inspected with one paper as an example. 

The paper is generally well written with experiments towards sentiment analysis accuracy and differences of ranking results. But the overall contribution and conclusion are not clearly presented due to the list of concerns listed as follows. 

- The sentiment analysis accuracy is about 80.61% which doesn't seem very high. The author also did not compare their results with other approaches. As an active research field, many open source packages and pre-trained models are available to support sentiment analysis. The author should consider adding comparisons of their proposed method with some established one, such as Stanford coreNLP for sentiment analysis.  This can help readers better understand their accuracy results and contributions. 

-  The author trained and tuned based on a smaller curated dataset (corpus 1) and applied it to a larger corpus. As shown in table 1, the distribution of positive/negative instances is quite different between two corpora. Additional explanation and analysis may be needed. One potential reason is due to the bias introduced by oversampling of minority classes. 

- The comparison results of different ranking results may need more discussion. What is the goal of such comparisons?  It is not clear what conclusion authors trying to draw besides the four indexes are different. On the other hand, how the sentiment scores are introduced to build indices are not clearly presented. Different weighting schema used may change the ranking results greatly. 

- The analysis of example paper also seems inconclusive. Introducing the sentiment score displayed different impacts in different indexing schema. The research questions 3 need further studies.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,25/2/2018,11:29,no
104,28,258,Phixxxx Mxx,3,"(OVERALL EVALUATION) The paper introduces different ranking options for scientific papers. The novelty is taking sentiments of citation contexts/instances into account.

The introduction in well written and motivates the project of utilizing citation instances. I would recommend to use citation context as a term. This seems to be the more established term. 

In related work the authors miss the research around citation contexts a bit. There are many relevant papers. They also miss to mention some relevant workshops and shared tasks. But this is not a huge problem.

The problem starts with the research questions which are not well formulated and clear. Esp. RQ 1 and 3. 

The corpora are fine and adequate.

Chapter 6 is detailed but I miss the connection to the RQs.

The ranking indexes should have proper names not just numbers.
The interpretation of the evaluation of the indexes is not well explained and discussed.

Citation contexts/instances can play a major role in DL and can improve IR, but I can’t see how the approach in this paper can improve the current situation in DLs.

I like the general idea a lot but the set-up of the paper (RQs, evaluation and discussion of the results) is not proper enough.

Perhaps the paper can be downsized to a short paper. Effectively it is not a complete full paper (10pages) yet.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Perhaps the paper can be downsized to a short paper. Effectively it is not a complete full paper (10pages) yet.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,16/2/2018,11:51,no
105,28,421,Bxx Y,4,"(OVERALL EVALUATION) This manuscript aims for incorporating citation sentiment into ranking index of scientific publications. While this is an interesting idea, the work described in this manuscript seems in the early stage of research, which is not yet ready for publication due to a number of methodological issues.

First, citation sentiment does not indicate target. A positive sentiment could mean good or bad to a cited paper in a comparison like ""[1] is better than [2]"". Some research is needed to investigate how to identify the target of sentiment.

Second, the details of the citation sentiment classification were not reported. Also, since the goal is to rank scientific publications, how would the classification accuracy affect the ranking?

Third, some caution is needed when using the SMOTE method for oversampling language data. Different from image data, which can be easily altered slightly to generate a new example, language data cannot be easily altered without distorting the semantic meaning. There have been some creative approaches for augmenting language data, e.g. Zhang et al. (2015) 

Zhang, X., Zhao, J., & LeCun, Y. (2015). Character-level convolutional networks for text classification. In Advances in neural information processing systems (pp. 649-657).

Lastly, the evaluation strategy is confusing. Several ranking index methods were compared based on the similarity between the ranking result, and then a particular paper was picked to investigate the influence of citation sentiment on the ranking. Neither step provides strong evidence for the validity of the ranking methods.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,17/2/2018,3:33,no
106,29,417,Xixx Yxx,1,"(OVERALL EVALUATION) This paper proposed to solve multi-label document classification based on titles only, since the availability of titles is much higher than that of full-text, therefore we could have more training data for a machine learning model. This idea is innovative and has its value in practical usage.

However, there are several problems in the comparison results. Multiple factors (training set size, models, feature representations) are entangled, making the comparisons less convincing.

First, the author claimed that MLP is better than CNN and LSTM in certain cases. However, MLP used the TFIDF representation of *full* text and CNN and LSTM methods use the pre-trained embeddings only for the *first* k words. This is an unfair comparison since first k words are a biased subsampling of the full-text (e.g. on average full-text has 2.5k-6.7k words but the author only used first 250 words). MLP method can utilized all information in the full-text.

Second, the author used different hyper-parameters (e.g. # hidden units) for full-text and titles, and the model for titles has more #hidden units (therefore larger capacities). For comparison purpose, the author should use the same hyper-parameters. Only in that case can we conclude that the benefits are coming from the more available titles, not the model itself. I guess the author's rationale was trying to get the best performance. However this doesn't control the factors.

The claim that deep learning models outperform traditional methods when training set is larger that 65k is not rigorous. This is not what [44] suggested. [44] suggested that generally speaking there is a dichotomy, but the exact number should be case-by-case.

The author claimed that there are very few papers on classification with large-scale label spaces. However nearly all neural machine translation models are dealing with large-scale label spaces.

A citation to AdaGrad is missing.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,16/2/2018,18:21,no
107,29,31,Daxxx Baxxx,2,"(OVERALL EVALUATION) This paper presents work investigating the performance of several neural models (CNN, LSTM, MLP) at the task of multi-label subject classification for two publication datasets (EconBiz and PubMed), where the primary question is assessing the relative performance of models trained only on the full text of the articles compared to models that have increasing access to additional training data in the form of article titles.  This work is very clearly described, well-grounded in important literature on neural approaches to text classification and presents an interesting, focused contribution.  A few comments:

-- The main contribution of this paper is presenting experimental evidence of the relative performance of training on just full text compared to full text + many more titles.  To trust these results, we need to be sure that all models are given their best chance of succeeding (in terms of hyperparameter optimization etc.).  Section 4.3 notes how hyperparameter choices were made for full text and titles separately (which is great!) but I'm unclear concerning the discussion throughout of different architectures chosen for different models/training sets (e.g., one-layer MLP with 2K hidden size presumably for just full text, and a different model for full text + titles?  PubMed titles has its own model?), and it would be helpful for the reader to understand exactly where those design choices were made.  Additionally, while the use of deep-ish neural networks is motivated here by the size of the *titles*, that assumption is not applicable for the smaller full-text dataset.  Including models that are designed to give the full text their best fighting chance would be more convincing than being hamstrung by the constraint of a neural model.

-- What exactly is a ""sample-based F1"" (and how is it different from standard F1)?

-- More details on exactly how hyperparameter optimization was carried out would also be useful (e.g., held-out dev data)?

-- The results in table 2 have enough variability that it would be helpful to quantify your uncertain about the performance with confidence intervals.

-- It's not clear to me exactly how the MLP baseline is different from other MLP whose results are reported here.  From the description in 3.2, it seems to changing the activation function in the base MLP to a ReLU and adding bigrams as features?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,17/2/2018,1:03,no
108,29,174,Draxxxxxx Herxxxxxxx,3,"(OVERALL EVALUATION) This paper looks at subject classification of scholarly documents. It compares the performance of classification models trained on full-text vs solely on titles. The motivation for exploring title-based classification is the wide availability of titles for training (since full-text is freely and easily available only for open access documents). Given the amount of available training data, the paper looks at whether increasing the number of training examples when using titles can achieve comparable performance to full-text.

For this task, the authors explore deep learning, which has been shown to work especially well and significantly better than traditional models when large amounts of training data are available. Furthermore, the authors look at this task as the case of extreme multi-label classification as both datasets contain thousands of labels. Specifically, the authors compare the performance of multi-layer perceptron (MLP), convolutional neural network (CNN) and recurrent neural network (RNN) for subject classification of papers from the biomedical database PubMed and from the economy and business database EconBiz. 

This is both a well written paper and an interesting experiment. The authors show that given enough training samples, title based classification can reach a comparative or even better performance than full-text based classification. This is a useful finding which shows large databases of publication titles such as PubMed can be used to create models for automated subject classification. I also think it is interesting the MLP and the LSTM performed so well (compared to the CNN).

I only have a few minor questions. When describing the MLP model, the authors state only 25k most common unigrams (and additional 25k bigrams in case of the second MLP model) were used. How did the authors determine 25 thousand unigrams to be sufficient for the task?

I would appreciate if the authors could talk in more detail about their chosen evaluation metric (F1 score). Was it calculated as mean F score per label or in a different way?

Page 7, beginning of discussion (I think this may be a typo) — the authors state the performance of the best performing title model trained on all available EconBiz titles is nearly 10% better than the best performing full-text model, however, table 2 and the previous text say this improvement is less than 5%. Maybe a typo?

I would also be interested in how long did the models train for each training set.

Overall this research seems well done and within a scope relevant to the conference.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,17/2/2018,3:45,no
109,31,168,Daxxxx H,1,"(OVERALL EVALUATION) Generally I think it's good paper. Especially for the way it propose to generate synthetic data. 

Strength:
1. A novel way of generating labeled data. It’s really useful for data-driven approaches. Compared with several previous way of generating labeled page data, the one proposed here is more “general”. Both latex and xml documents are considered. According to the evaluation, the generated data has high precision and recall. It could also be used for generating other elements too.

2. Learning based model could be improved by feeding more data. I think it is the good direction to go and the performance showed it outperforms rule-based method in PubMed dataset.

3. The paper is clearly written and easy to follow.


Weakness&Question:
1. Overfeat is not state-of-the-art framework any more. Have u tried Faster-RCNN or SSD? The caption detection problem might be solved by using a better architecture. For example, change the different anchor setting in Faster-RCNN.

2. why the dataset is called distantly supervised dataset? Because it’s synthetic?

3. How to deal with sub-figure? Figure that contains several sub-figures seems not easy to deal with a pure visual model.

4. There are two recent paper “Learning to Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks” and “Multi-scale, Mutli-task FCN for semantic page segmentation and table detection”. It’s good to see some discussion in the related work.

5. More ablation study could be done. For e.g, changing the CNN architecture.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,4/2/2018,17:07,no
110,31,374,Domxxxxx Tkxxxx,2,"(OVERALL EVALUATION) The paper describes a method for generating a ""gold standard"" dataset useful for extracting figures from scientific papers, as well as a neural networks-based algorithm for this task. In general, this is an important task and very relevant to digital libraries. The dataset the authors published will definitely be useful for the future research. The research described in the paper is novel. The paper is well written and easy to understand. In my opinion, it is a good contribution to JCDL.

The biggest issue I see is that the authors work mostly with images of the documents' pages, for example, they detect figures in the images of the pages using neural networks. At the same time for many many years scientific papers have been published as born-digital PDFs, which already contain the images along with the direct information about their location on the pages, so there is no need to ""guess"" this using machine learning. I wonder why the author do not extract this information from the PDF content and use it for figure extraction. I would strongly suggest the authors to at least comment on this issue in the paper.

Of course, this issue is not relevant for tables or image/table captions, which are present as fragments of the PDF text stream and are not easily distinguishable from other fragments.

The authors also use the PDF text stream for some tasks, such as finding image captions and table regions. The problem here is that the order of the text in the PDF stream in general is meaningless and has nothing to do with the order of the text on the page. So for example, it is entirely possible that the image caption text is split and fragments are present in different locations within the PDF stream. Standard way to analyze PDF text is to extract text fragments from the stream along with their coordinates, and concatenate them using only the coordinates. Libraries such as PDFBox or iText can be helpful for this.

Minor comments:

1. Several times the authors write the phrase ""labels for figure extraction"", or similar. In fact, it appears in the second sentence of the abstract. It is not obvious what are those ""labels"". I suggest to provide more detailed description and/or examples, ideally the first time it is mentioned.

2. Please specify for all the tasks what is their input (images/PDF/...). It is not clear.

3. There are some interesting differences in the distributions in Figure 2. Is this due to the different research areas of the papers, or the described method for building the dataset?

4. Please use labels for axes in figures (eg. figure 2).

5. Evaluation: it would be nice to see the results of PDFFigures/DeepFigures separately for figures and their captions (table 3). Also, have the authors evaluated matching figures with the captions, that is, how accurate is the described distance-based approach?

6. In line 956 the authors write: ""Additionally, our data generation approaches could be extended to other information about papers such as title, authors, and sections; while systems for extracting this information have traditionally relied on purely textual features"". This sentence is not true; for many years now the state of the art metadata extraction systems (GROBID, CERMINE, PDFX, Team-Beam and others) use extensively visual/location/formatting/size features to locate various information within the documents.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,12/2/2018,14:51,no
111,31,417,Xixx Yxx,3,"(OVERALL EVALUATION) This paper proposed using deep object detection model from computer vision community to extract figure and tables in PDF. They leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate  gures with no human intervention. The large-scale annotations will be released and the performance is on par with a rule-based method on CS-Large and significantly better on PubMed.

The novelty is limited, since the similar object detection model is widely used in computer vision. Also, the author should cite several papers that use deep learning models (detection models and segmentation models) to extract figures/tables, such as [1,2.3].

[1] Chen et al. Page segmentation of historical document images with convolutional autoencoders. ICDAR2015.

[2] Yang et al. Learning to Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks. CVPR2017.

[3] He et al. Multi-Scale Multi-Task FCN for Semantic Page Segmentation and Table Detection. ICDAR2017.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,16/2/2018,18:44,no
112,32,331,Chrxxxxx Sexxxx,1,"(OVERALL EVALUATION) This paper investigates how to rank formulas hypothesising that different formulas within one (Wikipedia) are not of the same importance. 
The work is interesting for the community and well motivated. 
Compared to previous work the authors propose an extended features set (including graph-based features) and evaluate the contribution of each feature type on the final performance. 
While the work is generally sound and mostly convincing, the paper lacks details for fully understanding the approach. The same lack of details hinders the judgement of the validity of the authors' evaluation. Both does not allow to reproduce the experiments (which would be feasible if the source code would have been published). Further, the authors refer to their most related work (which is from the same group [1]), but decide to generate a new data set for evaluation, and I am not sure why (is the data set easier?).  Related work needs to be rewritten.
See detailed comments below.

[1]https://dl.acm.org/citation.cfm?id=2756918

DETAILED COMMENTS

Related Work
- What does "" use the manual run to derive insights for refining the query formula to improve the low precision"" mean? For me, this does not summarize the work of Schubotz et al.
- ""through LDA and doc2vector method"" - I suppose ""doc2vec"" is meant here. 
- W.r.t. the work most similar to the paper, Wang et al., it would help to be clear which features they employed, to judge how large the extension was. 

Method
- ""citations (hyperlinks) which exists in the description of the formula"" -> I am not clear how you identify the description of the formula. For some examples (as in Figure 2) it might be obvious, but what about the formulas in page https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms for example?
- 3.2.2. details about how the word embedding model was generated are missing (e.g. stopwords removed?, hierachical sampling used?)
- formula-based features: how is the length of a formula calculated? e.g. x^2 is it of length 2 or 1?, similarly, is the numerical position counted in characters?, is position feature binary (i suppose so)?
- inner-article features: 
  - semantic relevance to article ""Then we respectively calculate the semantic relevance among the title, lead paragraph of article and textual descriptions of the formula as two metrics"" - I am little confused what is compared to what? 
  - semantic relevance to keywords: here tf-idf+cosine is used. Why? Or why did you chose a different similarity as before (similarity to article)?
- inter-article features:
  - as I read in 3.2.1. the graph is constructed with wikipedia pages as nodes. I suppose ""in-formula"" is the in-degree of the corresponding node in the graph? But what about formulas that do not have an own wikipedia article? As https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms? Or do you omit those?  The same question is open for me for the feature ""out-formula""

Evaluation
- Details about how the arcticles are selected are missing. This is crucial, since it would allow to judge the complexity of the task (and maybe answer the question about formulas in articles like https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms I mentioned before)
- Figure 3 - how many articles do have exactly 1 formula? Ranking would be easy for that subset of the data set.
- If you use the average values of the annotated importance (which is a ordinal value between 0 and 4), how do you deal with e.g. with an average of 2.3? Is this rounded?
- Table 4: Value ""0.0.93""?
- Table 4: score > 0, P5, w/o For value 0.9347 is greater than the (bold printed) value in the ""All"" column ""0.938""
- The most related work stems from the same group, so I am wondering why the evaluation was not done on the data set used before? 

Language, form, structure
- ""previous work has achieved well performance"" -> good performance
- ""just play the supportive roles"" -> omit the ""the""
- Omit heading 3.1. 
- Figure 2 is not readable in print-out. There is some horizontal space left, with some rearrangement the figure should fit without adding vertical space.
- RankSVM[], ListNet[], same for 3.3.2 ""surrounding text[]"" -> spaces are missing 
- footnote 2 -> url no longer valid, should it be https://sourceforge.net/p/lemur/wiki/RankLib/?
- case-sensitivity in sec 3.3.1. and 3.3.2: inconsistency in feature naming, e.g. ""Semantic Relevance to article"" vs. ""Semantic relevance to Keywords""
- ""The articles of Wikipedia is well-structured"" - singular/plural
- Table 2: round values to the same precison 
- ""they are only consider"" -> they only consider","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,7/2/2018,11:07,no
113,32,19,Haxxx Alxxxx,2,"(OVERALL EVALUATION) - This paper addresses the idea of ranking formulae within an article based on the relative importance of each. Using articles and formulae from Wikipedia, the authors build a formula citation graph and use a word-embedding model to store the relations between the formulae and articles (11 features). Their ranking achieved better results than other baseline methods have.  
- With only 332 articles that present 2,384 formulae, the dataset is not large.
- Six undergraduates labeled the formulae based on their importance (0–4), and the average was used. However, it is unclear as to whether the undergraduates had any issues in regard to understanding any of the articles or formulae. Further, the authors do not state whether the ratings were distributed or skewed. 
- The problem is not well motivated. The fundamental question of how ranking formulae will benefit researchers is not clear. 
- There are situations where in order to assert or stress a point, an author can explain concepts using an example, and in this paper the authors do exactly this using Bayes’s Theorem. However, the authors do not state how the model would behave or how to rank the formula.
- It is not clear how the paper would deal with similar formulae that use different symbols. 
- Additionally, most related papers refer to at least one common formula. However, it is not clear how this paper would deal with such cases. 
- Page 2: Footnote #2 https://sourceforge.net/lemur/wiki/RankLib returns the following statement: “Whoops, we can't find that page.”
- Page 3: The description of the “link” feature is not very clear. 
- Page 4: The following incorrect citation is given: “[14] Kushal Singla. 2016. A Document Retrieval System for Math Queries. In NTCIR.” The citation should read as follows: “Thanda, Abhinav, et al. …” 
- Some related works that are not cited: 
- Sun, B., Tan, Q., Mitra, P., & Giles, C. L. (2007, May). Extraction and search of chemical formulae in text documents on the web. In Proceedings of the 16th international conference on World Wide Web (pp. 251-260). ACM.
- Kacem, A., Belaid, A., & Ahmed, M. B. (1999, September). EXTRAFOR: automatic EXTRAction of mathematical FORmulas. In Document Analysis and Recognition, 1999. ICDAR'99. Proceedings of the Fifth International Conference on (pp. 527-530). IEEE.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,16/2/2018,1:18,no
114,32,313,Andxxxx Raxxx,3,"(OVERALL EVALUATION) The paper presents an approach to rank equations within an article with respect to their assumed importance, with the importance being computed via factors such as citations to an equation, level of detail, etc.

The paper is generally well written, providing a convincing story line and solid analyses. One limiting factor might be the reliance on Wikipedia articles both for the establishment of the citation graph as well as for the source article to be ranked. However, this seems to be rather a practicality of parsing equations rather than a fundamental issue.
Concerning the study, the authors report on the gender of the annotators – is this information relevant in any way? More interesting in this context would have been to report the inter-indexer consistency to determine how solid any ranking numbers obtained are.
Minor aspect: figure 1 has printing problems, resulting in a massive black box obfuscating the figure.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,17/2/2018,10:26,no
115,35,272,Anixxxx Mukxxxxx,1,"(OVERALL EVALUATION) This paper  presents an analysis of what scholars ask about, what features the questions and answers convey, and what socio-emotional reactions feature during the interaction on ResearchGate Q&A?
One of the strong point of this study is annotation of 371 questions and 7530 answers extracted from RG's Q&A into several categories mentioned in the paper. 

The paper is fairly well written and the methods are fairly well explained. Some of the answers for question 3 (Section 3), I did not find particularly useful or compelling. The differences across questions seems to be the most useful of the analyses.

Some issues:
1. I could not find definition of non-standard terms like ""AVE"", ""PCT"" presented in Table 4.
2. Many grammatical mistakes like ""What characteristics of different types of questions?"", ""What characteristics of the answers of different questions?"", ""Measures of Answers under Questions""etc.

I strongly suggest that the annotation be made public for future use. This will be of particular interest to the JCDL community to present some use-case of this study.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,27/1/2018,7:12,no
116,35,365,Taxxxx Suxxx,2,"(OVERALL EVALUATION) This article describes an analysis of question - answer pairs on Library and Information Science drawn for ResearchGate. The authors briefly review the rise of academic social networking sites and prior studies. The majority of the paper describes the results: characteristics of the questions being asked, time until they were first answered, nature of the answers and how these vary by type of question, etc. They then discuss the results in terms of three research questions: types of questions, types of answers by question type, and how these vary based on ""socio-emotional"" reactions. 

The strength of this paper is that it has all the pieces one would expect in a scientific paper and is reasonably well-organized and written, within the page limit constraints. More examples of what their coding framework items really mean would have been helpful, particularly for the socio-emotional analysis which seems like a misnomer relative to the data being reported. 

The weakness of the paper is that there is really no motivation for the research - who will use this information in what way? There is no contribution to theory, and no stated contribution to practice. The justification is the academic social networks exist and therefore we should study them. But, the authors provide no ""towards what end"" rationale. It is not clear what other researchers should do with the information that most questions posted on Research Gate are seeking information, rather than opinions, for instance. The key finding is that if you ask a question on Research Gate you had better be prepared to wait a long time for an answer (Table 4)!

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This could be a good POSTER.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,12/2/2018,23:52,no
117,35,26,Yaxxxx Alnxxxxx,3,"(OVERALL EVALUATION) The paper is a quantitative study based on analyzing 371 questions related Library and Information Science (LIS) on Research Gate. The main questions that the authors investigated were: What kinds of questions do scholars ask on RG Q&A on the topic of LIS?, What characteristics of the answers of different questions?, and What socio-emotional reactions do scholars express during the interaction?

The findings of the paper are interesting and draw a good picture for the Q&A in RG in LIS. I couldn't see why these findings are useful or what is next? 

These are some suggestions/comments:
* The methodology of collecting the data is not clear. How the categorization of questions was done?
* Bales’ interaction process analysis (IPA) - the authors mentioned the method they used in identifying the socio-emotional reactions of the answers and questions, but they didn’t clarify the process. Was it the process of identifying the reactions manual or there was any type of extraction? 
* The first paragraph of section 3.2 is not clear.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would recommend to re-submit it as a poster.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,16/2/2018,19:15,no
118,36,80,Faxxx Crexxxx,1,"(OVERALL EVALUATION) It is an interesting paper presenting a methodology that combines the analysis of content read by searchers while performing some predefined controversial search task. While it does not manage to find an answer to its original research 
question, it still describes an interesting way to combine different kind of data when running IIR studies. Overall, it needs some rewriting and refocusing and its current form is not ready to be published but it is a promising piece of research and authors should be encouraged to continue and frame their contribution in the definition of innovative  methodologies.
Cons: 
- students as surrogate searchers cause a serious bias to study
- tasks are not real and thus motivation is an issue too
- authors are aware of the limitation caused by ""the sample and information seeking tasks information"" but they should go further and argue that they could not come to any conclusion in respect to their research questions. Instead they should describe in more details their methodology and how it can be used in IIR studies.
- lack of definitions: some very crucial concepts are left without proper description or even reference, e.g., Comprehensive Model of Information Seeking or the Big Five Model, even if authors expect these to be well known stil they should at elate provide a literature reference
- English needs revision

Pro:
- the use of a combined method of content and statistical analysis provides some very interesting insights
- pilot study paving the way for its redesign","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,Monica,Landoni,monica.landoni@usi.ch,317,14/2/2018,17:14,no
119,36,5,Juxx Abxx,2,"(OVERALL EVALUATION) The paper is not in scope of JCDL. While JCDL does include papers on information seeking and information behavior they are always tied to other relevant topics related to digital libraries. There is no evidence of this in the paper. It presents a study on variables that affect cognitive dissonance within biased information seeking. The variables explored include: gender, personality type, and health info literacy. There is no evident application of the conceptual model or limited findings to digital libraries or other systems.

Further, the writing is problematic and suffers from ESL issues that affect readability at times. The tables are not useful and do not present the findings in a readable manner. The patterns exhibited by the participants (S and O) are not explained in the narrative but appear as one line under the table. The authors, while saying they measured the effect of gender, do not present findings on this variable.

To make this paper of publishable quality for JCDL Proceedings would require significant revision by the authors.

I would reject/strongly reject this paper based on these observations.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,15/2/2018,17:19,no
120,36,183,Xixx H,3,"(OVERALL EVALUATION) This paper presents a study exploring how individual differences could possibly influence cognitive dissonance reduction in biased information seeking. The topic is interesting and relevant to JCDL. The authors did a good job in explaining what biased information seeking was, and grounding the study on the theory of cognitive dissonance. 

The use of eye tracking is relatively novel in information seeking studies, which shows good potential of this study.  However, this paper in its current form lacks many technical details and justifications, and thus may not be suitable for publication.

Although biased information seeking is an interesting and important question, it was not very clear why the two patterns (S and O) were important or why the research question was important. In other words, what are the implications of the findings? Would the findings help improve information seeking or information system design? 

The statistics of the S and P patterns shown in Table 2 were not defined, leaving readers wonder what those numbers represent. The ANOVA test also lacked explanation: what were the different groups being tested with regard to each variable? For gender variable, as there were only two genders, shouldn't t-test be used instead of ANOVA? 

In summary, the study is interesting and potentially publishable, but this paper is not sufficient to publish in its current form. The authors are encouraged to modify the paper and resubmit to JCDL next year.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,17/2/2018,9:01,no
121,37,59,Joxx Hx,1,"(OVERALL EVALUATION) The paper presente Microarchives, a proposal of framework for the management of complex Web resources as cohesive entities. The topic is definitely within  the scope of the Conference, and the motivation for the work is clearly stated.

Dealing with complex digital objects has been a key challenge since the early architectural models like the Kahn and Wilensky Framework. Obviously, the advances in the forms of Web content left some issues out of the framework that need to be addressed.

The initial study of the management of dynamic Web content is interesting. The different dimensions studied have not been, however, explicitly addressed in the presentation of the proposed solution.  

While the idea seems promising, the paper style is narrative, lacking technical detail that could have been provided in the extension of a full research paper. Some more formal detail could have been provided to e.g. metadata schemas, or the different services provided, which are only described with text accompanied by some screenshots. No details about the APIs  of the different components, the metadata schemas or the exchange formats are provided.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The idea of the paper is nice, as the prototype looks, too. However, there is very little technical detail included in the paper. the level of detail is rather suitable for a poster presentation.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,12/2/2018,18:18,no
122,37,27,Omxx Alxxx,2,"(OVERALL EVALUATION) The authors describe Microcrawler, a web application that allows users to create Micro Archives. Micro Archives are digital object representations.

The idea is very promising, and the concept is neat, with lots of practical applications. 

The paper is mostly about the implementation of the proposed approach using two use cases: blogs and software. It is not very clear how one should interpret the results of both use cases. Are there any recommendations?

On the system side, the components are described at a very high level. That said, the source is available via github.

Minor item: Figure 6 is too big.; figure 1 is unnecessary.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I'm not really an expert on this topic. From the systems side, the paper is weak (not a lot of detail). That said, they are trying to solve a good problem.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,14/2/2018,22:43,no
124,37,189,Nabxxxx Jaxxx,3,(OVERALL EVALUATION) The paper introduces the concept of Micrawler and how can it be beneficial to Micro Archives. It gives a good overview of the architecture. The methodology is well defined. The author talks about the opportunities and ends the paper with a nice conclusion.,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,20/2/2018,14:52,no
126,38,271,Anixxxx Mukxxxxx,1,"(OVERALL EVALUATION) I liked the overall idea of the paper. The experiments have also been done quite rigorously. However, the major problem of this approach is that the authors approximate all their quantities, using the nodes only in set C. For instance they assume that the fidelty of all the nodes in T can be approximated by the fidelty of the nodes in C. Similarly, they extrapolate the impact of the ghost vertices through the impact of the vertices C. These approximations could have serious repercussions and an exhaustive error analysis experiment is needed to support such choices for real networks. For instance, the error for various sub-cases need to be studied |T|~|C|, |T|>|C|, |T|>>|C| etc. If the error values diverge then that is fatal for the system. In case the paper is accepted, I would like to see a complete treatment of errors in the camera ready version of the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am giving a weak accept because the identification of the problem is good and the experimentation is exhaustive. However, the error due to their assumptions might make their system unusable. In case of acceptance, the authors should be advised to report error analysis experiments.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,24/2/2018,5:42,no
127,38,55,Klexxxx BĂxx,2,"(OVERALL EVALUATION) The paper targets at the difference between ideal crawling results on a 'complete' web graph and the crawling results on an, as they call it, incomplete graph. In particular, their concern is that values of so-called centrality indices may be different on incomplete graphs, and they try to quantify this difference.

I have some severe concerns regarding this submission, which -- in my opinion -- deem it unsuitable for publication at this point.
- Motivation. Their motivating example is that the NASA homepage may not be available, while other pages on this same site are. I am not convinced. Why should exactly the homepage be unavailable, while other pages from the same organization are? IMHO, a real crawler would just try to access these pages another time later.
- Undue simplifications. Your model relies on a, as you can it, simple crawling strategy ""where it can be assumed that each vertex is part of the crawl independently from all other vertices with some sampling probability p_s"". Why is this a _simple_ strategy? I totally do not see how one could implement this.
- Unclear reference point. You ""focus on the most impartial strategy, wich is vanilla BPS, but explicitely produce partial crawls by dropping x% of the vertices of the input graph"". If I know that I will compute centrality measures later, why should I pursue such a crawling strategy? Wouldn't it be more reasonable to try to identify the central nodes as early as possible, during the crawl, and to keep these? It seems to me that the problem is somewhat constructed: First you throw away nodes in a random manner. Then you observe big, 'remarkable' deviations between the centrality values in your sample and in the original graph. But I do not see why one should throw away nodes in this way in the first place. (I might have missed/misunderstood something here, but then it is the responsibility of the authors to present things in a way that does not give way to false conclusions.)

Detailed comments:
- ""exhibit deviations from their original values"" -- This is not a fact, but the hypothesis behind your work. You should formulate it like this.
- ""Vertices in our input crawls are either completely crawled ... as ghost vertices."" -- I do not really understand why we have this sentence. Isn't this completely clear?
- ""We present ranking correlations ... in the above mentioned .gov graph."" -- This is not sufficiently intuitive in my opinion.
- ""In addition the results from [32] imply ... "" -- I do not see how this sentence is related to the previous one.
-""... and can be easily computed in a distributed manner"" -- How can this function? You should provide some intuition here. -- More importantly, I do not really see what I would do next once I know this number.
- nDCG -- Please provide a reference.
- ""impact"" is mentioned in the first paragraph of Section 4, but without proper introduction. There is a definition later, but this is too late, since you are using the term earlier.
- What is a 'discordant pair'?
- You are overloading the abbreviation d(v). Once it is a characteristic of the target graph, another time (one column later) of the crawl. This should be avoided.
- ""Moreover, for synthetic target graphs ... and constant p."" -- I do not understand this.
- ""\Sum_{v.u\in N(v)} ..."" -- I do not understand this notation.
- ""The impact on any vertex ..."" -- of v, or what do you mean?
- ""assuming that each ghost vertex exerts on average an impact of Im(C) on its neighbors"" -- Why is this assumption justified? It seems to me that it is not.
- ""To understand ... acts as the seed set. -- All in all, this is not understandable.

- English:
* Especially if not crawled for ... such as Web archives.
* Complicating matters further.
* suitable sized
* a ranking deviations
* explicitely","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,19/2/2018,9:46,no
128,38,53,Roxxx Buxx,3,"(OVERALL EVALUATION) This paper describes a method for estimating the non-fidelity of a sampled graph in terms of the ability to rank nodes correctly using PageRank. This metric is then used to extract subgraphs from large graphs that are likely to be high fidelity.

I am concerned about the technical soundness of the paper. Some assumptions were insufficiently supported. 

- In 4.1, the authors assume that nodes are sampled independently and uniformly at random. Of course, this is not how crawls are typically constructed, which the authors well know since they model web crawls later in the paper. The assumption of independent sampling is not realistic and could have a big impact on their key result, the fidelity measure. The authors do not address the limitations implied by this assumption, even in the concluding discussion. Possibly the reason that the HAK measure works well on random graphs is because the assumption is closer to reality here, rather than anything structural about the graphs themselves. More attention needs to be paid to the impact of this assumption, or possibly a more realistic assumption can be made in its place. (For example, the probability of sampling a node could be made proportional to its degree since high degree nodes are more likely to be encountered in crawls.)

- At the end of 6.1, the authors state that they used personalized PageRank when computing the ranking deviations for their target graphs. The reason for doing this is extremely unclear. Personalized PageRank and regular PageRank are quite different and serve different purposes. Seed nodes would, of course, be more likely to be internal to the sampled networks and have more reliable PageRank values. So, it is not at all clear that the results relative to the personalized version generalize to non-personalized PR. This limitation is not mentioned in the abstract, which discusses only PageRank and makes the work seem to solve a different problem than it does.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,20/2/2018,22:21,no
129,39,149,Swxxxx Gotxxxxx,1,"(OVERALL EVALUATION) 1.	The paper focus is not clear and the author digressed with many objectives. This is analytics paper but lacks the motivation for analytics. The goals defined as similar to the design of the dashboard. There is nothing new in this paper
2.	The author also talk about various papers in library research but the linkage to the paper focus is missing. Is this paper relevant to recommendations or space utilization or resources (both physical and electronic) management or user profiling or user behaviour analysis or subjects/topics analysis or Technology analysis? I see everything combined in the write up but the flow is tangled and complex
3.	Overall, what is the takeaway from the paper? What is that someone can learn from this paper? How can they benefit in applying what they learn from this paper? All these points are unanswered.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,16/2/2018,0:07,no
130,39,401,Dxx W,2,"(OVERALL EVALUATION) Internet of Things (IoT) infrastructure within the physical library environment is popular, which includes features for location-based recommendations. The topic of this paper is to analyze the users' requests and behaviors, which is very interesting. The results indicated that users of IoT-based recommendation are interested in a broad distribution of subjects, with a short-head distribution from this collection in American and English Literature. A long-tail finding showed a diversity of topics that are recommended to users in the library book stacks with IoT-powered recommendations.
  However, the paper can be improved in the following aspects:
  1) I suggest the literature review focus on the location-based recommendations of Internet of Things infrastructure within libraires. However, right now it includes too many topics as Information Organization Foundations and Emerging Technology, Information Seeking Research Needs, Monitoring and Evaluating Space with IoT Hardware.
  2) Before results analysis, it should introduce the data set that was used in this study and the Internet of Things infrastructure in the author's library. So that we can know how it works.
  3) The biggest problem of this paper is that most of the results analysis is descriptive statistic, like API hit by month, location distribution, subject distribution, etc, and the results are not surprising. I would like to suggest more deeper analysis methods to be used to find more interesting results. 
  4) Some suggestions and implications should be addressed to the Internet of Things infrastructure within the physical library and its location-based recommendation services.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,16/2/2018,16:49,no
131,39,196,Samxxxx Jayxxxxxx,3,"(OVERALL EVALUATION) This paper presents IoT infrastructure within a physical library environment for mobile, dynamic wayfinding support and recommendations for items in the library stacks based on the location-based service. The authors presents the evaluation and analysis of the user requests for recommendations based on their location and the subject area of the library stack which the user request recommendations. The results suggest the users of this particular study are interested in a broad distribution of subjects with diverse topics from the library book stacks. 

It is not clear from the section 3 how the WayFinder app is enhanced with the IoT location-based recommendations. The Figure 1 depicts some of these ideas but I believe some technical details of the architecture would greatly improve the readability. Does the WAyfinde provide recommendations based on the currently searched items and then locate the similar items and populate it based on the location? The example only describes a situation where the popular items (popular is loosely defined here!) near the user. Does the popularity is based on the searchers from other users or based on the prior searches made by the same user? Based on the statement “ similar needs can be analyzed to create a hybrid collaborative filtering with the content-based filtering from the app search modules, which may be relevant in providing recommendations here” I assume the recommendations are only based on the classification associated with stack, location.  

On the positive side, I believe the approach is well-motivated and related to the digital library. The basic idea seems natural, and thus worth for an investigation. On the other hand, I cannot judge the novelty of the paper beyond what is stated therein, namely it lies on the proposed idea of creating simple recommendations from the library stack locations and user navigation.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Seems like a similar version of the paper exist elsewhere,
https://arxiv.org/ftp/arxiv/papers/1801/1801.06552.pdf","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,16/2/2018,19:17,no
133,40,124,Nuxx Frxxx,1,"(OVERALL EVALUATION) This paper still requires much effort from the authors to be ready for publication. Interpreting the description of the research is extremely hard due to the immature state of the text. Thus, due to the problems with the writing, it is difficult to provide detailed comments.

The authors should think more deeply about the right terms to use to describe the concepts involved in their work, and also the English language usage, which needs considerable improvement.

The paper also does not strictly follow the template and is not well formatted in many places.

One comment on the title, is that the use of unexplained acronyms (SQA) should be avoided.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,9/3/2018,1:49,no
134,40,204,Minxxxx Kx,2,"(OVERALL EVALUATION) This paper examines the behavior of 4M+ users on the Zhihu community
question answer site.  The authors report a long-tailed distribution
of user's activity and categorize the users into several different
user types.

While such a study would be of great interest to the social media and
social community areas of JCDL, I cannot currently recommend this
paper for publication due to many problems with the current work.

First, glaringly, the format of the paper is incorrect.  This is
unfortunately, a telling sign that the authors did not bother to
invest much into their submission quality.  There are many problems
with whitespace, incorrect spacing and poor figures that give real
difficulties in the readability of the work.  Literal variable names
are undifferentiated from the normal text making it difficult to
follow.  Table 2's multi-page span is unwieldy and not helpful and
needs to be better communicated to be helpful to anyone.

Second, more importantly, methodologically the work needs improvement.
The role classification (Table 1) is simplistic, belying the
complexity of the mixed_behavior role.  The core work on page 5-6
seems to just lie with the execution of some MATLAB routines and to
report these findings.  The ""entropy method"" needs to be better
grounded and disclosed.

I would encourage the authors to work towards more sophisticated
methods for analysis, as it is clear such a dataset would be useful to
analyze and compared for user behavior differences to contrast with
other social network and CQA sites elsewhere (e.g., Baidu Zhidao,
StackExchange/Overflow, and Yahoo! Answers).

Details:

- I think the common term is community question answering (CQA) rather
than SQA.

- (Python-based) crawling is a technical detail and does not merit
  more than one or two lines of discussion in a scientific paper,
  unless there is significant changes or particularities of the work.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,21/2/2018,1:00,no
136,40,314,Saxxxx Rxx,3,"(OVERALL EVALUATION) This paper analyzes the Chinese social network Zhihu. The users are classified into four types: knowledge seeker, contributor, browser and mixed behavior user. The category of a user is found deterministically depending on how many posts they write or read. Finally, the ""score"" for a user is calculated using an entropy-based method. 

The paper is readable but has many formatting issues. There are many spaces and I am certain that the ACM format is not followed here. Some information is irrelevant such as: a. ""Python's pandas object is used to group large sample data"" or ""The Python pandas object was used to separate the sample data and grouped by using the grouped"". Figure 3 should be a histogram instead of a scatter plot. Figure 4 is not at all clear: the proportion data should have been explained in other ways.  

But I have a major concern with the way the ""user performance quality assessment indicator system"" is created, which is the actual contribution of the paper. Table 2 possibly lists the features used to calculate the user performance. But it is not well explained what the ""indicator weights"" for these features are (other than the statement ""The entropy method uses the concept of information entropy to calculate the weight of the indicator system, that is, the greater the difference between each values of an index is, the smaller the entropy is, and the greater the weight of the index is"").  This should have been explained in greater detail. Also, it is unclear what the last column in table 2 is (""0.36"", page 5, last paragraph). Finally, the statement ""Due to the large number of users, we use the average score of the four categories users to represents the user's level of performance quality"" isn't very clear to me. Does this mean all user's in the same group have the same level of performance quality? The granularity is fairly shallow in that case. For figure 5, there's no X-axis or Y-axis label, so hard to understand what it represents. 

The contributions listed by the authors in the conclusion are as follows:

1. ""This paper uses Python to crawl the behavior data of more than 4 million users on Zhihu platform""
-   This is not really a research contribution. 

2. ""Classifies the users into four categories based on
different Q&A behavior: Knowledge Seeker, Knowledge Contributor, Mixed Behavior User, Browser.""
-  Knowledge seeker and knowledge contributors are well-known categories reported in prev. literature.  However, I am not sure how the authors differentiate between the browser and the inactive users since no browsing data is analyzed. More confusing is the mixed behavior user: wouldn't it be more intuitive to consider a user with more answers than questions to be ""knowledge contributor""? This classification scheme artificially boosts the performance score of one class: the mixed behavior user because from table 3 it is clear that they have higher content contribution score than even the ""knowledge contributor"" class. 

3. ""We constructed a user performance quality assessment indicator system, using entropy method to calculate the weight and find out the user's comprehensive performance quality score, we analyzed the scores based on the user-layer.""
- I consider this to be the most important contribution of the paper, but as mentioned before, it is not very well explained. Also, is the goal here to say which type of user is better than the other? That is obviously going to be the mixed behavior class because they are more active, therefore have more content contribution. 

Considering these issues I would suggest rejection.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would suggest a rejection, or, in the best case acceptance as a short paper.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,9/3/2018,1:53,no
137,40,94,Mixxxx Dobrexxxxxxxxxxx,4,"(OVERALL EVALUATION) This paper addresses a topic which is relevant to JCDL. However it does not follow the ACM template and needs a very serious language revisions - refining the use of terms too (e.g. refining phrases like ""data crawled by Python language"" and ""This paper uses Python to crawl...""). There are some missing components from the legends of the diagrams. 

Besides that, the paper needs to introduce better how the research questions had been selected exactly - and the first one needs to be revised (they are called ""problems"" in the paper). In addition, there is a lot of context from research done in Asia, this does not represent a balanced view on the work on user assessment in other continents.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,21/2/2018,20:18,no
139,41,190,Nabxxxx Jaxxx,1,"(OVERALL EVALUATION) This paper is relevant to one of the JCDL topics: Personal Digital Information Management. It gives a good overview of how personal digital information management tools can be used but it lacks the comparison of the tools in depth.There are many tools, which the author listed but did not talk about how and when those could be effective. It is focused on only Evernote as digital information management tool. The abstract says the paper is about how and to what extent digital tools can be used. Although the author has described the Evernote tool in detail, the aspect of how it has made a significant improvement in data management and how it has transformed the way data is managed is not well addressed. The methodology used is case studies research which is all based on one tool, Evernote, and does not address the user’s experience with other tools, as suggested in the topic.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,23/2/2018,15:02,no
140,41,317,Brxxxx Rexx,2,"(OVERALL EVALUATION) This paper is not appropriate for JCDL because it is actually not a research paper. The author does not begin with a research problem, present a research question, or even describe which methodology was used to conduct the research. It is more a quick summary of the usefulness of PDIMT, as well as an opinion piece. 

The paper is inconsistent. It begins with a table comparing several PDIMT tools, their features, and prices; however it proceeds to only cover Evernote. Much of the paper is dedicated to describing various Evernote features, and these descriptions at times seem to have been lifted directly from the Evernote website. It quotes glowing reviews of Evernote and ends with the suspicious sentence: ""Probably it is time to pick up few PDIMT tools especially Evernote and start using it."" The paper shows no balanced or objective approaches, and at times sound like a shill for a commercial product. 

In addition to the afore-mentioned issues, this paper is poorly written. It contains many statements that are either ungrammatical or simply awkwardly phrased. Some examples:

""but they are finding difficult"" should be ""but they are finding it difficult""
""Evernote is even search for handwritten words"" should be ""Evernote even searches for handwritten words""

I do not recommend this paper for inclusion in JCDL.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,13/2/2018,15:46,no
141,41,226,Clauxxxxxxx Kxx,3,"(OVERALL EVALUATION) The paper aims at fostering to use a personal management system for digital library 
search tasks, but it is a commercial for Evernote. It does not motivate 
why a user should use such a system nor does it include any research question.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,16/2/2018,6:16,no
145,42,377,Ricxxxx Toxxx,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the implementation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to “encyclopedic image database” and “deep learning”. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of “SRU”.
3.	Provide a reference to “Tesseract”.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.


In summary, this paper lacks organisation. Presentation is not clear and contributions are not properly discussed.
However, if authors address these presentation issues, the paper may become acceptable.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,22/2/2018,9:13,no
146,42,239,Jonxxxxx Lexxx,2,"(OVERALL EVALUATION) This submission presents an interesting approach in the domain of image retrieval, as applied to historical image collections. The paper serves as an interesting case study that demonstrates the applicability of several very well known, existing deep learning techniques to CBIR. 

relevance to JCDL: This paper provides little to no new theoretical contributions to the JCDL community. However, it demonstrates a generally useful integration of deep learning with CBIR.

novelty/originality: The submission makes use of existing techniques and APIs, especially in the deep learning domain. While the techniques do not appear to be novel, their application in this case study and type of collection is of interest to JCDL. 

methodology: The approach utilizes typical CBIR techniques (segmentation, extracting metadata, extracting content, etc.)

assessment/evaluation/comparison: The application of recognition techniques (from the deep learning community) is interesting, as is the limitations of applying these approaches (which are largely trained on contemporary collections) to historical collections. 

style/quality of writing: The submission is passable, but requires another pass through the document for editoral corrections.
	e.g., punctuation: The system returns pairs of estimated “class/confidence”

replicability: Certainly, the directness of the practical approach, minimal new theoretical contributions, the listed APIs/software, and listed collections led to much better than typical replicability. 

adequacy of references: The CBIR and DL papers provide reasonable support for the approach.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,16/2/2018,6:53,no
147,42,418,Xixx Yxx,3,"(OVERALL EVALUATION) The author proposed a way to unify access to all illustrations in an encyclopedic digital collection. In order to do this, the author combine visual features from existing API (IBM Visual Perception) and text to improve the performance. Evaluation on a test set is conducted and the mixed MD produced best performance.

The novelty of the method is limited, since the author combine existing vision APIs.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,16/2/2018,19:42,no
148,42,173,Mauxxxx Henxxxxx,4,"(OVERALL EVALUATION) This short paper is certainly within the interests of the JCDL community and is well written, albeit with some punctuation errors.

While it does not add a great deal of new theoretical knowledge, it is an interesting demonstration of the challenges and potential solutions of deep learning of image description and application.

I would have liked some details of possible new work the authors will do on the combining of metadata housed in documentary silos, as noted in their introduction - perhaps this could be the future of a long paper.

A minor point, acronyms need to be spelt out in first use, e.g. GAGA and CBIR","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,23/2/2018,23:10,no
149,43,180,Nixx Hoxxxx,1,"(OVERALL EVALUATION) This paper addresses practical issues with digital libraries related to perceptions of Linked Data and the Semantic Web among survey respondents in libraries, archives, and museums worldwide. Linked Data and the Semantic Web are still emerging technologies gaining adoption in the digital library community and this study captures professionals' desires to participate in Linked Data efforts at their institutions but regularly encounter barriers due to lack of usable tools, infrastructure, and training.

This paper used a survey to collect data- it would be nice to attach that survey to this paper so I could see what was asked precisely. A footnote on page 3 indicates it would be ""made publicaly [sic] available' in the Trinity College institutional repository but I can't find it in there. The approach for gathering this information was commonly used and two similar linked data survey studies were described in the literature review.

I thought the study would benefit from more specific findings and more information about tools that the study investigated for working with linked data. I also would have appreciated more detail about how people use tools to create, share, and consume linked data to complement their findings with what people would like a future, hypothetical tool to do. The tool rating results from the CSUQ were a bit general and it would be nice to know more precisely how tools fell short when working with Linked Data rather than an overall usability score. Moreover, since this paper seeks to establish how library information professionals could create linked data for the semantic web to facilitate information sharing, it might benefit from describing how these professionals currently share information about their digital and physical collections using other services (OAI-PMH, etc.), the tools used for that, and how linked data would build upon more recent efforts to share and aggregate this metadata.

The style of writing is adequate, however a few assorted stylistic and typographical errors suggests that additional copyediting would help it. The conference is in Fort Worth, TX, not ""Forth Worth"". These things can be easily changed and would help give this paper additional polish. I thought the references and cited literature were adequate and I appreciate the in-depth discussion of similar studies surveying Linked Data perceptions in the digital library domain. This paper could do a little more to distinguish itself from those studies, which seem quite similar in how they are currently described.

I gave this paper a ""weak accept"" for the reasons stated above- it is a well-executed and timely study that needs a bit of copyediting and further explanation to have a greater impact on the broader digital library/archives/museum field.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,8/2/2018,21:01,no
151,43,61,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) The paper presents the results of a survey (based on 185 valid questionnaires) addressing Information Professionals (IPs) in the LAM sector (Libraries, Archives, Museums) to understand their knowledge, use and requirements about Linked Data (LD) and Semantic web (SW). 
The topic and the approach are not new and the results bring little additional knowledge in this field. However, the topics of LD and SW are clearly relevant, especially for the Library domain, and it might be worth to present the results of the survey as a short paper, to try and stir some discussions in the audience and renew the interest in these topics. 
Some editorial notes: 
- ""IPs are well positioned to play a leading role
in the development of the SW"" 
The acronym SW is defined half a page later
-""data stored in RDF format vie a SPARQL endpoint""
what is ""vie"" ?
- ""System Knowledge Organisation System (SKOS)""
The first S stands for Simple","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,16/2/2018,23:52,no
152,43,153,Juxxx Grxxxx,3,"(OVERALL EVALUATION) Authors describe a study of LAM perspectives on linked data tools and practice. Understanding barriers to engagement in linked data practice continues to be of interest to libraries, particularly as libraries begin to incorporate new LD tools such as VIVO into their workflows, and I appreciate the focus on LAMs and not just libraries. As authors note, prior survey-based research has been conducted in the area of linked data. Sharing a list of questions and response options would help to reinforce the original contributions mentioned on pg. 3.

In “Related Work”, authors describe a similar survey study conducted by OCLC Research. While I know of OCLC and many in JCDL and library communities will be familiar with the organization, I would suggest spelling out the OCLC acronym. More citations to sections that talk about the OCLC Research study are necessary. The first “Related Work” sentence cites both the original study and a follow up study and it isn’t clear in subsequent paragraphs whether authors are referencing insights gained from the original OCLC study or the one conducted by OCLC’s Yoshimura. In “References” the citation for the Yoshimura article ends with what I believe is a reference to page “6” but there are no page numbers in the D-Lib article. Should this be changed to “1-6” or should the “6” be removed? Authors shift in paragraph 5 of “Related Work” to another study by LaPolla, but then it isn’t clear to me whether references to earlier studies in paragraphs 5-8 refer to the original OCLC Research study, Yoshimura’s study, or the LaPolla study.

In terms of methodology, authors interview two IPs with a lot of LD experience and working in a university library (possibly the same library), which contradicts the assertion made that the study “takes into account the views of IPs, both with and without LD experience, and from a variety of LAM domains.” Authors also don’t provide a list of which institutions participated in the survey, how they were selected, and how (and why) “Researchers and Academics” were selected to participate. Authors recognize that the goal to reach a variety of LAM institutions was not achieved; this may have been in part due to the approach to survey invitation (a methodology section which could be described in more detail).

In the conclusion, authors suggest the study points to the need for developing LD interlinking tools as a next step and that requirements were gathered, but I’m not sure what the requirements are (is it the list in section 5.6?) or which interlinking tools participants were asked to rate in Table 7 (is it the list in Table 5?). A reference in the conclusion to the requirements list would help, as well as including the list of questions and response options.

In terms of other changes recommended, authors should spell out the semantic web acronym SW when it first appears in the second “Introduction” paragraph. “Vie” in pg. 2 paragraph 2 should be changed to “via”. In the last sentence in paragraph 5, “of” should be changed to “on”. The semicolons on page 5 that precede lists of responses should be changed to colons. In the “Introduction” section, paragraph 6 should be moved to “Results” and paragraph 7 to “Conclusion”.

Overall, the paper is relevant to the JCDL community.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) It is difficult to determine with any great authority the originality of the paper because I am unable to compare the assertions made by the authors of the contributions to new knowledge (on pg. 3) to a list of survey questions (which could then be compared to the list of questions from previous studies). Authors also state explicitly (pg. 4, methodology) that they pull heavily from questions and answer options from earlier studies. There still may be much to learn from the new questions asked and from answers from different types of LAMs to the same or similar set of questions.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,17/2/2018,3:48,no
154,44,77,Timxxxx Cxx,1,"(OVERALL EVALUATION) This submission is well-written and strongly in scope for JCDL. However, especially given the long paper and poster about Quill presented at last year's JCDL, this submission is weak in terms of novelty / originality and at best a weak accept. The incremental work reported is really not enough for a new long paper. Sections 1, 2, and 3 (and multiple of the figures) largely reiterate and recap the 2017 jcdl paper and poster and do so in greater depth than necessary for a follow-up. The rest of the submission focuses on an ad hoc assessment of Quill over the last year and on changes made in response to this assessment, but the assessment reported is immature, incomplete and weak methodologically (not unrelated). While section 4 provides some new insights gleaned from observations of users wanting to use Quill to better understand 19th century negotiations, the discussion provides insufficient context about these users and the conventions or other negotiations they were seeking to analyze (the introduction mentions Utah's constitutional convention, but this use case is never mentioned again in the body of the paper). This is a problem that recurs throughout the remaining sections of the paper. Clearly the workshops were a source of inspiration for the updates made to Quill over the last year, but the observations reported from the workshop seem anecdotal and potentially random. It does not appear that user feedback was gathered in a structured manner, such that feedback could have been coded and analyzed systematically. At the very least it would be better to see the observations reported as case studies. Better yet would be if the workshops had led to formal surveys, focus groups or user studies, the results of which could then have been coded, analyzed and reported systematically using established methodologies. The overall impression is that the testing, iterative improvements, and gathering of user feedback is very much a work in progress, more suitable for reporting at this stage in a short paper. This impression is reinforced by how sparsely some of the 'enhancements' of Quill are reported, e.g, the implementation of 'quick jump' codes instead of reliance on standard URLs as a solution to a perceived weakness in using URLs for certain classroom settings.  This is reported in the paper as an assertion with no documentation as to the magnitude of problem, nor follow-up as to whether quick jumps actually solve the problem. The intuitions and changes to Quill discussed in sections 5, 6, and 7 are certainly plausible, but in a full paper for JCDL further evidence of confirmation is the expectation. A plausible, better (in my opinion) short paper could be distilled out of this submission (as a follow-up to last year's long paper and poster), but in this reviewer's opinion there may not be enough here to warrant acceptance as a full paper for this year's conference.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,16/2/2018,23:31,no
158,44,4,Juxx Abxx,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. However what is missing from the paper is any discussion on how the data was gathered during the workshops or how it was analyzed. This is definitely a weakness of the paper. While the authors do describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system, there is no connection between what they learned from the user study data and how it led to specific modifications of the system.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study nor does it present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collections and this perhaps would make for an interesting panel presentation.

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.

As suggested above this paper could be re-worked as a panel about user studies and how they are used in system design in this and other digital humanities projects.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have edited my review as requested at the program committee meeting last Friday. Please let me know if it needs further revisions.
J.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,26/2/2018,22:06,no
160,44,260,Roxxxx Hx,3,"(OVERALL EVALUATION) This paper is a continuation of the JCDL award winning Quill paper contribution from JCDL 2017. It focuses on the following extensions of the Quill  platform to assist researchers to collaborate with Quill on the study of a wider range of negotiated discourse material. This will further enable use of Quill in the classroom and in a wider range of academic pursuits that can utilize third-party digital archives as source material for Quill analysis. Furthermore the articles discusses community development of the Quill platform through the creation of the Negotiated Text Network.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very effective non-technical paper that further refines the Quill framework and offers opportunity for expanded Quill community building. This should allow for improved overall functionality of the platform along with opportunities for sustainability of the application among the negotiated texts research community. I suggest we try to get them in as a poster to discuss their progress.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,21/2/2018,23:49,no
161,44,99,Jx Stexxxx,4,"(OVERALL EVALUATION) This is a follow-on paper from the Quill project that was honoured at JCDL 2017 in Toronto. Like the earlier paper, it is very well written and clearly presented. It also continues to represent a project very much worthy of our attention as digital library developers/researcher and digital humanities scholars. Notwithstanding my generally strongly positive appreciation of the paper, I am struggling to give the paper more than a weak accept this time. For me, the issues at play include:

1. The paper is a bit incremental (i.e., not quite enough has really evolved from last year's great paper).

2. The nominal ""user feedback"" that the team uses as its justification for design changes and system decisions is reported upon in an anecdotal manner. This anecdotal presentation makes very it difficult for this reviewer to be convinced of the connection between the user input and the implementations justified in the paper by their input. Simply put, there is not enough information about the users and their feedback given in the paper to make any kind of judgment.  

Again, I am strongly positive on the project overall. I would suggest that poster presentation (or maybe short paper) might be the better mode of presentation this year.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,18/2/2018,2:38,no
162,45,53,Roxxx Buxx,1,"(OVERALL EVALUATION) This paper describes a content-based book recommendation system using neural embedding of authorship as a data source.

I think the results are interesting and the paper is well-written. I have the following concerns that the authors should address:

1. Full-content-based book recommendation is only possible when the content is available. That implied books that are out of copyright, as is the case with Project Gutenberg texts. However, that severely limits the set of volumes that can be considered. This is the primary reason for the fact that ""only a few"" book recommenders use this information. This point should be noted in the introduction because it is a major limitation of the method -- it requires resources that are unavailable for the most part -- although the HathiTrust Digital Library is trying to get around this problem.

2. There is an implicit assumption that authors are consistent in their stylistic aims and genres. You are training the network to map all of the works of a given other to a single output. This assumption seems quite weak. Consider someone like (the late) Ursula LeGuin, who wrote essays, poetry, science fiction, fantasy, and other works. Someone who likes ""The Left Hand of Darkness"" might not be at all interested in ""The Wizard of Earthsea"". It might be true of the selective digitization within the Gutenberg Project, which is more likely to select the most popular / famous works of authors. It also means that this method will work best on authors who write only a single kind of work. Such limitations and assumptions need to be acknowledged in the paper.

3. As the authors note in Section 5, precision and recall are not normalized for the length of the user's profile. A better metric for ranking tasks is NDCG@10, which has built-in normalization and can be compared across users and across systems. I would recommend that the authors use this instead and drop the recall/precision metrics.

4. The authors include only content-based baselines in their experiments. This is good at showing the advantages of the author-oriented representation, but it does not convince this reader that the proposed system is really an improvement on the state-of-the-art. Perhaps a collaborative recommender would work better and all of this content processing is unnecessary. A collaborative method has the additional advantage of working on any book and not needing full-text access.

5. A obvious direction to take this work is to consider hybrid models in which authorship is consider one signal to be combined with others. For example, a separate classifier could be trained for identifying book genres such as those found in the NoveList metadata. Recommendation based on this data could be be combined with the authorship info. In addition, a recommendation hybrid would be a natural way to incorporate the collaborative data that this system ignores. The authors do not seem to be aware of this possibility.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,7/2/2018,1:41,no
164,45,291,Denxxxxx Pexxxx,2,"(OVERALL EVALUATION) The paper proposes a system that recommends books based on their authors’ writing style. It focuses on the analysis of the textual content of books to improve their recommendations. The system transfers information learned by an authorship identification (AuthId) classifier to a book recommendation module by using a neural network.

The subject is relevant to digital libraries. The paper is well written and with a good bibliographical review.

An issue not evaluated in the work was the efficiency of the approach. Processing the whole text of books using neural networks may not be efficient, and infeasible to big data.
Is it really necessary to analyze the whole text of books to identify the authors' writing style?
Maybe less than 100,000 words are sufficient for identification. It would be interesting to add experiments with fewer words per book in order to verify this assumption.

The baselines used for comparison are general information retrieval approaches. It would be interesting also to make a comparison of the proposed approach with other specific baselines to recommend books found in the literature. This would be important to ensure that writing style recommendation is actually the best recommendation strategy.

According to Figure 2, although the proposed approach achieves better results than the baselines, the results for precision and recall are very low. Does this occur in other works of the literature? Or is it a feature of the dataset and the way the experiments were done?
Although the authors have done a good analysis of the results, they should better justify the low values.

The Conclusions section is very simple. There is plenty of room for increments.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,21/2/2018,14:25,no
165,45,246,Mirxxxx Mx,3,"(OVERALL EVALUATION) This paper introduces a book recommendation method based on the content of the book. The paper is very short (with a 10 page limit, it has practically just six). Overall, it is contribution is not novel enough, as explained next. 

One motivation for the work is ""It is quite suprising [sic], then, that the analysis of the textual content of books to improve their recommendations is still very limited."" Well, truly analyzing the content of a whole book in order to improve recommendations by just a fraction may not be worthy at all, given that good recommendation results are achieved with much simpler, quicker, practical methods. Likewise, a founding stone of the method seems flawed: Given the text of a book as input, the AuthId classifier predicts its author""; why does a recommendation algorithm needs to predict the book's author if such an information is usually available at the book's metadata?

The main part of the solution is an adaptation of an existing one (published at reference #28). Then, the recommendation function is actually Support Vector Regression. Therefore, the novelty of the work is very narrow. Among 39 references, there is only one to JCDL (year 2012), which makes me wonder if the audience of JCDL would be actually interested in the topic of the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper has practically six pages (the 7th has only two references, which would fit in 6 pages by excluding unnecessary urls within the references). I think this should have been submitted to the short papers track.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,16/2/2018,12:28,no
166,46,86,Pexxx Daxx,1,"(OVERALL EVALUATION) This paper presents findings from a study of the role and significance of scientific meetings (e.g. conferences, workshops) in a range of physical and computer science disciplines. The study comprised analysis of a range of “metadata” (e.g. h5-index, acceptance rates, whether the meeting has been held regularly without interruption, the extent to which a meeting’s geographic location by edition) about these meetings to arrive at a range of conclusions, for instance, prestigious conferences typically have a high level of continuity. The study involved collection of an impressive range of data, which have been carefully processed and analyzed. 

However, the Discussion and Conclusions are unsatisfying and underdeveloped. The main conclusions are presented as a series of bullet points, with a sentence for each. The authors have presented a very large range of data in their findings: for a paper of this length, the authors would be better advised to focus on a subset of their data so that they can develop their conclusions in depth. 

Additionally, I am not convinced that the selected measures necessarily reflect the prestige of meetings. For instance, I can think of examples of prestigious conferences in other scientific disciplines where acceptance rates are high, or where the conference is held in the same geographic venue annually (e.g. the American Geophysical Union conference is a leading geosciences conference, but has been held for over 40 years in San Francisco). It would be interesting for the authors to critically reflect on whether the selected measures (e.g. h-5 index, acceptance rate, geographic variation of venue) truly reflect the prestige or significance of a meeting.

I also believe this paper would be more appropriate for a conference or journal about scholarly communications: the topic of the paper is not about digital libraries, and the authors do not attempt to link their findings to digital libraries. 

Finally, a few other points that could be addressed in a revision:
•	The abstract doesn’t do justice to the paper, and would benefit from more specificity about the paper’s main findings;
•	A clear definition of what is meant by “metadata of events” is needed much earlier on in the paper, for instance in the final paragraph of the Introduction;
•	“Data gathering is the process of collecting data from a variety of sources in an objective and unbiased manner.” (p. 2) Is this sentence really necessary?","Overall evaluation: -2
Reviewer's confidence: 2
Recommend for best paper: no",-2,,,,,16/2/2018,21:38,no
167,46,312,Andxxxx Raxxx,2,"(OVERALL EVALUATION) The paper reports on an extension of a preceding study (published in TPDL2017) to analyze publication venue metrics, adding math, physics and engineering to the computer science venues already studied. While the numbers reported and the study design are basically fine, it is unclear what the lessons learned from the numbers reported are – or which of those actually come as a surprise or novel knowledge. Specifically, it is unclear what the motivation for the study is in the first place. The introduction states that the goal of the study is to “gain more insights on the significance of the mentioned problems in order to ultimately devise novel means for scholarly communication.” Unfortunately, the remainder of the paper never touches on any of these novel means of scholarly communication, nor how they might be linked to the numbers reported. The end of the introduction finally lists two research questions (how important are events for scholarly communication, and what makes an event high-ranked in a community), but the conclusions fail to give solid answers beyond the fact that some disciplines rely more on conferences than on journals. The fact that high continuity leads to higher appreciation of an event also does not really come as a surprise. 
Having these seemingly well-known facts supported by solid numbers may, however, constitute valuable knowledge by some.
Some of the insights gained may be influenced also by the design decision (such as using an h5 index, i.e. impact over 5 years – the time span and value life cycle of references is known to be highly domain-specific, with humanities having way longer citation cycles than the fast-paced computer sciences. Additionally (and maybe most surprisingly from a CS perspective) is the fact that other disciplines provide less numeric information on their publication venues, rendering many of the detailed analyses that were performed 8previously) on the CS domain inapplicable for the new domains added.
The paper might benefit from more clarity in the feature description (e.g. geographic spread is stated to be calculated based on continent, state or city – would it not make sense to compute it across all three? Similarly, how is time variance represented? It is not stated in the methods part of the paper – experiment descriptions seem to hint at quarterly granularity, noting that shifts between neighboring quarters should not be considered shifts at all. Why not measure variance / use days of shift around the median?
The geographic analysis states that roughly 50% of all events were held in the US. While this is obviously a true statement it ignores size aspects: the US is by far the biggest country in the context of the analysis. 

Minor aspects:
- the legend of fig 3 is way to small rendering it illegible.
- equations should be labeled

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Reporting some interesting numbers confirming intuitively well-known facts with few lessons learned.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,17/2/2018,10:37,no
168,46,231,Albxxxx Laxxxx,3,"(OVERALL EVALUATION) This paper presents an interesting analysis of the characteristics of scientific events from four distinct areas: Computer Science, Physics, Engineering and Mathematics. The paper is in general well organized and written, and presents results that are of interest of the JCDL community. However, a preliminary version of this paper, addressing only Computer Science events, was presented at TPDL 2017, which is clearly acknowledged by the authors. Despite that, the additional analyses of the CS events added to this paper do not expand much the previous results reported in the TPDL 2017 paper, which means that only the overall analysis of the non-CS events (particularly Sect. 5.1) can be seen as new. However, this analysis is not deep enough and does not address several aspects that differentiate CS and non-CS events (e.g., the length of the papers).  Thus, in this reviewer’s opinion this paper does not present enough additional material in comparison with its TPDL version that justifies its acceptance for presentation at JCDL 2018. In view of that the authors should expand their analysis of the non-CS events and prepare a more comprehensive version of the current manuscript for being submitted to a relevant journal of the area such as the International Journal on Digital Libraries.

Positive Aspects
- The paper addresses a topic that is relevant to JCDL.
- The paper is well structured and written.
- Reported results are very interesting.

Negative Aspects
- As acknowledged by the authors, this paper is an extended version of a paper recently presented at TPDL 2017.
- Extensions added to the paper address events from other areas (Physics, Engineering and Mathematics), but are not detailed enough to reveal new relevant findings. 
- Particularly, the analysis of the non-CS events added to the paper are superficial and do not go into specific aspects that differentiate these events from the CS ones (such a kind of analysis would be a relevant contribution). 

Specific Comments

1. Some claims in Sect 1 (Introduction) should be supported by specific references, for instance when you mention questions regarding proliferation of scientific literature and reproducibility, as well as when you mention the importance of events to the CS area.

2. When mentioning OpenResearch in Sect 4.1, you comment on the use of SPARQL. As a first impression, this comment seemed out of the scope in that paragraph, but having looked at your TPDL paper it seems that some facilities for more complex queries in OpenResearch requires the use of SPARQL. Thus, this must become clear when discussing this data source.

3. Table 3 is not that relevant. Name unification in this work is not an issue, so this space could have been used to address more relevant aspects of your analysis.

4. The analysis of the non-CS events (Sect. 5.1) covers much less aspects than the analysis of the CS ones. This is quite comprehensible since obtaining data about CS events is much easier. Despite that, you should have tried to present some specific results that covered both CS and non-CS events, before going deeper into the analysis of CS events. For example, when comparing the average STJ indicator for CS events and non-CS events, nothing has been said about the fact that outside the CS area event papers are rarely cited (particularly in PHY and MATH areas) and, for this reason, their STJ indicators cannot be compared with those of CS events. Besides, you have commented on the submission and acceptance rates for CS events, but have not commented on this same issue for non-CS events. This seems a relevant issue when discussing the importance of event papers for the CS area.

5. Graphs in Fig. 3 and 6 are too small and, therefore, very difficult to read.

6. As a final comment, the title of the paper gives the wrong impression that the reported analysis equally covers all four scientific areas: CS, PHY, ENG and MATH. Since such an analysis is not possible, due to the lack of data to equally cover different aspects of these areas, the results should be reported in two distinct sections: one addressing a deeper comparative analysis of the four areas covering common issues and one presenting more specific results covering only the CS area.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper is an extended version of a paper presented at TPDL 2017 (this is clearly acknowledged by the authors). Although the TPDL 2017 paper only addressed CS events, this JCDL version does not present substantial new results with respect to the CS events, basically including a general comparison involving the non-CS events. A more elaborated version of this paper, without page limit restrictions, could be submitted to IJDL.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,17/2/2018,19:21,no
169,47,428,Maxx Ĺ˝xxx,1,"(OVERALL EVALUATION) 'Publish or perish' is the reality we are all facing. Any study of the review process is therefore very interesting for a broad audience. 
In this submission the authors use the data from two journals in physics and compare the outcomes of two approaches in reviewing: single reviewer and multiple review process. In addition to the reviews themselves, other data sources were used, such as citation data. Several additional studies were performed, focusing on the performance of different reviewers and characteristics of reviewer groups. The results are presented in several figures.

The problem I see is that the authors are building their research on questionable assumptions. For example they do not take into account a very probable scenario that editors choose one reviewer for papers they consider of good quality as opposed to the papers they find more questionable. The authors also take the citation count as measure of quality. While citations are a proof of visibility, absolute numbers should not be compared directly. We all know that papers on very specialised topics do not receive many citations, as opposed to methodological papers, for example. Finally, even in a set of higly cited papers some will be cited more - not proving that others are of lower quality. 
There is also no need for reviews to be of comparable length or style, even if they convey similar opinions. Reviewers are invited to review papers based on their particular expertise and availability and the number of reviews does not necesarily reflect the quality of the reviewer. Reviewer groups do not have to be in complete agreement - an editor may choose reviewers with different expertise in order to receive an analysis of different aspects. The authors even assume that a reviewer group has to be compatible, similar to collaborative learning - there is absolutely no need for that; each reviewer needs to be competent, ethical and motivated on his/her own.
I am therefore not surprised that contradictory results were obtained - maybe it was the right answers to wrong questions.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,12/2/2018,14:14,no
171,47,399,Mixx Wrxxx,2,"(OVERALL EVALUATION) The core of the paper is reporting a technique to potentially help editors assemble a good set of reviewers for a submission based on reviewers’ past review history. They utilize a genetic algorithm to perform analysis and present possible sets of reviewers based on use of GA’s in putting together teams in other fields.  They have access to the corpus of reviews of both JHEP and JSTAT, two leading journals in physics. Using these corpus, they do analysis of reviews and accept/reject results for both single reviews and multi-reviewed cases (apparently both journals do have single-review submissions), and use this to build a hypothesis for their algorithm. They use part of the corpus for training, and test over the larger corpus. The goal is to select review teams to reduce the number of discordant sets of reviewers (i.e. review ends with no resolution on disagreements) after observing what they believe are issues of some less impactful papers being accepted, and some more impactful being rejected (but then published elsewhere) as measured in long term citation count. 

I do wonder about the authors synonymous use of high quality with high impact through use of long term citation count - in my own thinking, it’s the issue of just what a citation count really means even though we use this as the core measure of long term impact.  In thinking this, I wonder if the approach to reducing discordance in reviews is necessarily the ultimate goal as doesn’t discordance at times highlight where there may be still unsettled approaches?  Yet striving to get consensus is part of reviewing, and helping with that is a useful goal.  While the paper elicited this tension for me, and it may be interesting to hear how the authors think about this, I don’t think it undermines their current work.

This work builds on their previous work - one referenced from the ACM conference on Information and Knowledge Management, and another, surprisingly not referenced in related work, from last year’s JCDL “Influence of Reviewer Interaction Network on Long-term Citations: A Case Study of the Scientific Peer-Review System of the Journal of High Energy Physics” which looked at a variety of features that may impact acceptance/rejection, with particular attention to the reviewer network, and just focused on JHEP.  I would like to see this reflected in related work.

It would also help to have some discussion of how JHEP interacts with ArXive with respect to papers that are considered for JHEP. This may help in understanding the review process and the decisions the authors made in approaching this work. That will allow readers to contrast with other approaches.

Section 5.2, part way through paragraph (LL, MM, HH, LH, MH, LH) should read (LL, MM, HH, LH, MH, LM).

Section 6, last paragraph before section 6.1, state “we argue that assigning multiple referees to a submission is similar to forming compatible referee groups” - that “forming compatible referee groups” would seem to be wrong as the comparison was to a collaborative learning setting, so what was the desired collaborative learning group you’re comparing to?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,19/2/2018,15:02,no
173,47,86,Pexxx Daxx,3,"(OVERALL EVALUATION) This paper presents findings from a study of peer-review practices in two physics journals, in particular focusing on relationships between referees’ evaluations of articles and these articles’ subsequent success (measured through number of citations). The study finds that accepted articles that were reviewed by a single referee were subsequently more highly-cited than accepted articles reviewed by multiple referees, and that articles where there was a higher level of discordance between referees tend to have lower citation rates. Based on this study, the paper then outlines and evaluates an approach for allocating sets of reviewers to articles to improve review processes. 

While the dataset used in the analysis is fairly large (approx. 36,000 papers), it is unclear whether the findings (based only on two journals in a highly-specialized domain of physics) are generalizable across a broad range of disciplines. 

There are a few other points that the authors could clarify in a revision:
•	What is meant by the term “compatible referee groups” (p. 7 and elsewhere)?;
•	What basis do the authors have for assuming that referees with similar opinions of an article will tend to produce reviews of similar length (p. 4)? Surely there are many other factors that affect the length of a review?;
•	When a referee is classified as anomalous, how many reviews must a referee have performed to be classified with any degree of significance? Do referees, in reality, actually perform sufficient reviews such that the system of allocating referees to articles is practical?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I've revised the review to remove the comment about the paper not being relevant, and have amended my score accordingly","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,24/2/2018,18:20,no
174,47,406,Jixx W,4,"(OVERALL EVALUATION) In this paper, the authors observe that in a review system papers with single reviewers seem to get more citations than papers that are reviewed by multiple reviewers. They then build a framework using GA to kind of predict what types of reviewers should be used. 

However, the authors do not investigate the reasons why a single author was assigned. In fact, there could be lots of legitimate reasons that the editor made the choice of assigning only 1 reviewer rather than multiple. 

Another strong assumption is to use citations as a measure of impact. This has been known to cause flaws because citations accumulate. The number of citations depends on the total number of years and the popularity of the particular field. 

But this topic is in general interesting. Balancing strengths and weaknesses, I voted it as a borderline paper.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,24/2/2018,20:15,no
175,48,70,Sauxxxx Chaxxxxxxx,1,"(OVERALL EVALUATION) This paper introduces an algorithm named xFactor to generate aspects out of annotated documents. Using this technique, the documents can be modeled as combination of aspects of different kind such as temporal, geographical and entity based. The aspect generation and factor functions are very novel. 

The authors, in their research and literature review, did cover a lot of breadth in the area in terms of capturing the different aspects of the problem.

From the experimentation perspective, it would have been nice to show the effectiveness of the technique described in the paper on actual document retrieval and compare it with the state of the art methods. As a reader, I would want to know whether using these techniques would provide any improvement in retrieving documents, given a query. Some experiments on that front would tell the reader where this technique stands in terms of document retrieval performance.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper felt very dense in terms of the complexity of the content presented. The ideas expressed were not easy to grasp in the first reading. It took me multiple readings through various sections to understand the work.

Also, as mentioned in the main review, I want to know how the techniques discussed in this paper would help in document retrieval. Though the paper does talk about retrieving aspects in annotated archives, I want to know whether I can use this technique for actual document retrieval.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,14/2/2018,4:00,no
176,48,246,Mirxxxx Mx,2,"(OVERALL EVALUATION) This paper the xFactor as an algorithm to  automatically generate semantic aspects for resolving ambiguous queries over born digital files. It also has a second contribution in the form of a testbed with more than 5000 aspects assimilated from Wikipedia (kudos to the authors for providing it). The paper uses Olympic terms as example, which is both time effective (as the Winter games are occurring right now) and timeless (as many people recognize the names and cities involved in the games). 

The problem is not novel (solving queries is necessary since DL conception) but its solution is very interesting and relevant to JCDL. The paper also provides good examples that make the methodology very clear. The solution is tested over two very different datasets: news and webpages. The ground truth was built from scratch (based on wikipedia) and will be made publicly available (as it is not replicable) once the paper is published.

As for improvements needed, I didn't understand the correctness/novelty association with precision/recall, having the latter as a ""probabilistic interpretation"" of the former. The results show precision/recall of 0.134, 0.264, etc; aren't those values way too low? There is improvement over the compared methods (sure), but still... Related work is way too long. I mean, with so much work done, shouldn't the experimental evaluation be more complete by considering more existing methods in the comparison?

Some style suggestions. Webpages are better cited as footnotes instead of references (as the latter should be for more scientific work). Also, references should all follow a pattern; most of conferences are cited in abbreviated for, so double check that all are like that (#32). Likewise, proofreading may be necessary mostly to improve word diversity (e.g., ""the user about quick facts about"", three close ""there exist/s"" in the introduction) and simplify some parts (""this is due to the fact that the factors"" -> this happens because the factors) . For Equation 5, there should be a \noindent before the following ""where""; same in the similarity computation.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,16/2/2018,11:55,no
177,48,277,Wolxxxxx Nexx,3,"(OVERALL EVALUATION) While the paper addresses a relevant task, it is written in an overly formalistic way, without clearly spelling out advantages and usage scenarios of the proposed approach, and without discussing possible limitations (other than providing numbers). I think the paper is still worthwile to publish, but to increase its impact, the authors should definitively add some more examples and discussion, otherwise the results achieved in this work will not be taken up by others.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,18/2/2018,19:39,no
178,48,354,Vexxxx Srixxxxxx,4,"(OVERALL EVALUATION) The idea of extracting semantic information from text documents and using them for retrieval is certainly well known. The authors attempt to add a new dimension to this approach (via 'aspects'). My concern is regarding the evaluation - one of the baselines chosen by the authors (BM25 based) to evaluate their xFactor algorithm is a fairly weak one in this context. The other baselines are based on LDA, although it is not clear why it needs to be so. Authors also don't discuss any plans for user studies, which one would expect would be vital for this particular task. There's also limited discussion in terms of results and lessons learned (for ex., why is this better than vanilla relevance feedback based approaches, beyond improvements in 'correctness' and 'novelty'). The authors would do well to provide insights along these lines as well.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is methodologically sound, and provides some useful insights. If others think the authors have evaluated their methods reasonably well, I don't mind accepting this paper.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,24/2/2018,17:28,no
179,49,148,Sujxxxx Dxx,1,(OVERALL EVALUATION) The workshop topics are of continuing interest to the JCDL community and the workshop has been conducted previously at JCDL. The authors are well-established researchers on the proposed topics and the workshop can be expected to be successful based on previous meetings.,"Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,25/1/2018,5:37,no
180,49,194,Adxx Jaxxx,2,"(OVERALL EVALUATION) This is ongoing workshop on general topics related to web archiving and DLs.

The team is well-experienced and have run the workshop with good success for several years already. I think JCDL2018 should keep tradition of having a smaller dedicated event to web archiving, especially, giving growing interest in related topics, and I would be glad to see WADL be organized again in 2018.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,28/1/2018,10:36,no
181,52,74,Hunxxxxxxx Cxx,1,"(OVERALL EVALUATION) This workshop attempts to gather the innovative approaches to exploring and mining the medical and biomedical text information, such as the related academic literature, patient data, and health records.  I think this topic deserves to be a workshop since the medical and biomedical texts have several features that are different from other texts.  As a result, it is beneficial to discuss domain-specific research issues and design domain-specific algorithms for mining such datasets.

The prospective program committee members seem mostly specialized in computer science.  I would suggest adding a couple of experts in the biomedical domain to help identify the essential needs in this field.

The list of the topics can be improved.  Some of the listed topics seem redundant.  For example, “biomedical corpus” looks very close to “biomedical digital libraries” to me.  Some topics seem too large.  For example, “textual big data techniques” seems to contain the textual techniques beyond the medical and biomedical domain.

Finally, it seems that the JCDL does not request double-blind reviews.  If this is the case, it is probably unnecessary to add such a constraint for the workshop.  I would suggest most of the policy be consistent with the main conference to prevent confusion.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,24/1/2018,2:50,no
182,52,154,Juxxx Grxxxx,2,"(OVERALL EVALUATION) The proposal builds on a previous iteration of the workshop and represents the convergence of diverse medical data perspectives. The expertise of the organizers is evident, and a list of citations to related papers would help to demonstrate depth of understanding of current issues.

All of the program topics in section 2 are exciting and the curation of health and medical data is undoubtedly a critically important issue (and one that would be of interest to the JCDL community), but I have concerns that the organizers are trying to cover too much in a half-day workshop. It would make sense to either make it a full day workshop or to focus a half-day on only the topics of highest relevance to the JCDL community. Either way, I would suggest asking prospective authors to speak to how their research/practice in the areas mentioned relates (or could apply) to advances in the development of digital libraries to support the curation of health/medical data. 

Overall, the digital library connection to the proposed workshop needs to be more explicitly stated. The objective stated in overview section 1 lacks a high degree of clarity, as the areas of emphasis do not map clearly to the program topics in section 2. The connections may be there, but points of intersection need to be noted. And more emphasis should be placed on ethics/privacy/security issues, as these are not just data management concerns. 

It would also help to include specific goals and expected outcomes of the workshop, and to reference how the 2018 workshop is informed by outcomes of the 2017 workshop. Also, what is the connection to evidence synthesis and are there ways the health science library community would benefit from workshop participation or from broader workshop outcomes?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Dr. Edward Fox is listed as a prospective program committee member and is a colleague of mine at Virginia Tech; I do not believe this presents a conflict.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,26/1/2018,19:34,no
183,52,402,Dxx W,3,"(OVERALL EVALUATION) The topic of this workshop proposal focuses on the curation of medical data which is the hot topic and related to digital libraries. The proposal is carefully written and considers some detailed issues like the review process, the estimated submissions and accept rate, the audiences, even issues about the rule of multiple contributions with one manuscript. The most important thing of the workshop is that it has good foundation. It has been held once in 2017, therefore, it should have experiences about how to organize the workshop and it has potential submissions and attendances which can help to extend the participant of JCDL conference. Finally, the organizers are from different universities of Europe and Asia, that can help to increase the international participants of JCDL conference. The organizers are experts of medical data analysis with good research basis that can guarantee the quality of submissions.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,28/1/2018,17:53,no
184,53,429,Maxx Ĺ˝xxx,1,"(OVERALL EVALUATION) This (very short) paper reports on a usability study of the Saudi Digital Library (SDL). An online questionnaire, based on Oxford Digital Library survey, was designed to capture the opinions of Saudi students studying it the US. The questions, at least as apparent from the Results and Discussion section, were rather general. Exactly half of 44 participants had used SDL and 14 have never heard of it. 
User studies are extremely important to help improve the servisec offered. Yet this study, admittedly referred to as 'initial' in the conclusion, suffers from numerous problems. First the participants were not really the intended audience of SDL, which is aimed at students of Saudi universities. Students in the US, regardless of nationality, have other resources provided for them. The sample was very small and in addition the author makes hasty conclusions such as 'perceived satisfaction was high (n=18'), 'results were easy to understand (n=15'), 'about (!) 16 users reported feeling confident using SDL in English'. The author also claims that 'users were challemged when attempting to locate Arabic language materials', yet 21 reported no opinion, 5 found it easy and 6 found it difficult. Different numbers are listed in the next paragraph, though.
Unfortunately this is a poorly designed and very superficial study.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,6/2/2018,10:07,no
185,53,409,Irxx Xx,2,"(OVERALL EVALUATION) The study has good aims, but fails to follow through. It is a pilot study on Saudi DL, to investigate whether the DL meets the needs of Saudi students in the U.S. 

There is clearly a gap in the research methodology. The researcher uses a questionnaire to test the satisfaction of a usability study s/he conducted, but is never clear regarding the process for the usability study. The data would be skewed as 14 participants never heard of SDL, so would have never used it. Apparently, the author did not conduct a usability test. The design is not transparent or justified and the results are fairly simplistic, descriptive results which don’t provide an in depth explanation of the findings. 
Descriptive data reported; however, what scale was used, Likert and what was the rating? 

Some of the results could be better presented in a table or figure. 

More references would be beneficial, specifically for justification of the research design and analysis

Unfortunately, due to the simplicity of the approach, the study is replicable. But it doesn’t really take a sophisticated approach. It is very basic.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,12/2/2018,22:59,no
186,53,100,Jx Stexxxx,3,"(OVERALL EVALUATION) This is an little paper outlining some early work on understanding the users associated with the Saudi Digital Library. As such, this paper is definitely in scope. There are a couple of worthwhile aspects of the paper, such as mechanism for reaching out to find survey responds. Notwithstanding the good aspect the paper does have several shortcomings the might prevent it from being accepted as is. 

First, the paper is only two pages. Using the full allotment of available pages would have allowed the author to provide more analysis and discussion.

Second, and related to the first point, there is very little actual information about the findings. Most of the paper is setting up the study and then it ends with a few conclusions. 

I could see this paper being accepted as poster presentation if the author could provide some more discussion and analysis of the survey data.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very light on content but germane to the conference.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,18/2/2018,2:16,no
187,54,271,Anixxxx Mukxxxxx,1,"(OVERALL EVALUATION) In this long paper the authors compare existing graph embedding and feature engineering methods, presenting combined approach for constructing co-author recommender system formulated as link prediction problem. 

The authors have conducted a quite elaborate background research however they fail to present ANY evaluation experiments.  I found it as a half-baked paper. Therefore, without comparison against state-of-the-art baselines and elaborate discussion section, I strongly recommend rejection of this paper. For instance, how does your work compare with: https://arxiv.org/abs/1505.04560 ... I can cite many other similar papers and a thorough evaluation is seriously in the lacking for this paper.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,27/1/2018,7:08,no
189,54,358,Kazxxxxx Sugxxxx,2,"(OVERALL EVALUATION) The authors propose a method for recommending co-authors or collaborators by taking this task 
as link prediction problem. The authors plan to use machine machine learning to predict new edges 
in temporal structure of co-authorship network. 

While this work is well-motivated by the current situation in the National Research University 
Higher School of Economics (HSE) as described in Section 3.3, the authors just describe the plan 
for experiments. To make this paper publishable, the authors should propose a novel approach, 
evaluate it with a relevant measure, and then discuss the obtained results. 

(1) Novelty/Originality
- The authors try to apply just regression or classification models as described in Sec 4. 
So if the authors propose a novel approach, for example, neural network-based model, this paper 
would be intersting. 


(2) Methodology
- The authors should detail more about their proposed approach not just listing features, 
similarity scores, and symmetric binary functions. 


(3) Assessment/Evaluation/Comparison
- As pointed out above, the authors need to quantitatively evaluate their proposed approach. 
It is also important to compare their proposed approach with some state-of-the-arts. 


(4) Style/Quality of Writing
This paper has some wrong expressions as follows: 

[Sec 3.1]
- infromation on ... => in*for*mation on ... 

- Figure ?? => Figure 1
(Need to specify Figure ID) 

[Sec 3.2]
- In certain cases when the author is ... => In certain cases *where* the author is ... 

- Last name and initials => *l*ast name and initials 


(5) Replicability
- The authors plan to use classical regression or classification models as described in Sec 4. 
So if the authors clearly describe their features, it is eacy to reproduce their experiments. 


(6) References
- The authors need to survey related work that is more specific to ""identifying collaborators."" 
For example, they need to cite the following papers: 

H.-H. Chen,  L. Gou, X. Zhang, and C. Lee Giles: 
""CollabSeer: A Search Engine for Collaboration Discovery"" (JCDL2011)

H. Deng, J. Han, M. R. Lyu, and I. King: 
""Modeling and Exploiting Heterogeneous Bibliographic Networks for Expertise Ranking"" (JCDL2012)

J. Tang, S. Wu, J. Sun, and Hang Su: 
""Cross-domain Collaboration Recommendation"" (KDD2012)

S. S. Rangapuram, T. Bühler, M. Hein
""Towards realistic team formation in social networks based on densest subgraphs"" (WWW2013)

S. H. Hashemi, M. Neshati, and H. Beigy: 
""Expertise Retrieval in Bibliographic Network: A Topic Dominance Learning Approach"" (CIKM2013)

M. Y. Allaho, W.-C. Lee: 
""Increasing the Responsiveness of Recommended Expert Collaborators for Online Open Projects"" (CIKM2014)","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,5/3/2018,18:26,no
191,54,174,Draxxxxxx Herxxxxxxx,3,"(OVERALL EVALUATION) This paper presents an approach for predicting and recommending co-authors based on various network-based features. The problem is formulated as link prediction task and is tackled by combining approaches — author similarity measures and more traditional graph-based recommendations with network embeddings. I think this is a very interesting problem and potentially a useful solution, however, the paper seems incomplete, which is why I cannot recommend it for acceptance. In particular, in section 4 the authors mention they measure AUC, however, the results are not presented in the paper and it seems the entire results section is missing. Unfortunately, without any results, it is hard assessing the method. I have some general comments and questions regarding the remainder of the text.

First, in the introduction, the authors seem to jump around different topics, which made the text harder to follow. In the related work section the authors mention several terms without explaining them (e.g. attribute-based vs. self-organizing networks). While these terms may be well known in graph analysis, I think it may still be worth providing a brief explanation to help the reader.

There are a number of statements in the data collection section which may need more explanation. In particular, the authors state that they “manually input at the personal web-page of researcher research interest list according to RSCI categorization”. This sounds very time consuming. For how many records was the manual processing done? 

The authors state duplicate records were merged, how was this done? Was this done based on title match, similarity, or other criteria? 

Next, the authors state that “All the missing fields were omitted during computational part of filed with median over respective category of articles and authors.” — I’m not sure I understand this sentence, does it say some fields were omitted if empty and some filled with median values? It would be useful to state which fields were omitted and which were replaced with median values. 

It would also be useful to provide some statistics for the author disambiguation results to give the reader a sense of how well did the disambiguation work. 

Regarding the quality metrics, it would be helpful to state which specific metrics were imported (was it JIF, eigenfactor, or other metrics?) and again provide some basic statistics for these metrics. Finally, I would appreciate if the authors could share some overview statistics of the dataset. I assume this was partially done in figure 1, however, the figure is quite difficult to read. I thought the commas are used as decimal separators instead of colons, however, in that case, some of the values don’t make sense to me (e.g. I would expect the number of authors to always be a natural number).

Regarding the choice of the dataset, I think it would be particularly interesting to perform the experiment on a dataset which includes authors from different institutes rather than authors from a single institute only. I would expect the authors within the institute to more likely form connections, since there is a higher likelihood that they already know each other. From this perspective I find the choice of the dataset somewhat limiting. Forming connections across institutes and maybe even across countries is much harder and is therefore a task which may much more benefit from recommendations.

Finally, the paper would greatly benefit from proofreading.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,17/2/2018,6:12,no
192,56,112,Schxxxxx Fx,1,"(OVERALL EVALUATION) An overall interesting and novel piece of work and approach to detect plagiarised content using a series of image similarities algorithms and computations.  The paper is structured well and written clearly with a set of accompanying evaluations. The following are some coments/questions:

Section 3.1: VroniPlag collection – As this might be new to many readers, provide more information of how the information in the collection is structured, particularly the “fragments” among dissertations that are deemed plagiarised in some form.  How did you decide which test fragments to use for your evaluation? Were these 1:1 or 1:many instances (spread across documents) in the collection.  If applicable, how was this handled by your system? How is the “original” content defined?

Figure 3: The outlier detection reports that a final suspicious score is calculated (Section 3.11). The combination and computation of this score can be incorporated in the Figure for clarity and completeness.

Section 4: The selection of the 15 image pairs suggest a 1:1 relationship (see earlier point).  The random 4500 images might be skew in terms of the ratios of the level of disguise between and test and random set of images although this might not be a problem in view of the large number of random images. 

In Tables 1 and 2, in addition to the 4 distance calculations, should not the final suspicious score be shown in the table? Was this score being used?  If so, how?   For cases 5 and 11, which random image came out top – what are the characteristics of these images?  It also appears that (manual?) tweaking of threshold values are necessary to identify the outliers. 

What are the limitations of this work, and challenges faced in making this an automated production detection system?","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,7/2/2018,4:06,no
193,56,401,Dxx W,2,"(OVERALL EVALUATION) This paper proposes an adaptive image-based plagiarism detection approach suitable for analyzing a wide range of image similarities. The topic is very interesting and consistent with the conference. The results indicate that the proposed adaptive image similarity assessment is an effective approach. However, the paper can be improved from the following aspects:
  1) The adaptive image-based plagiarism detection process includes several key components as shown in Figure 3 and the following subsections. It is not clear which part is the novel and proposed by the authors.
  2) In the evaluation part, only selecting 15 image pairs as the sample is too small. 
  3) ""Given the moderate size of our test collection, we assume images that were ranked at ranks beyond 10 as unsuccessful retrievals"". The authors should give some references for the unsuccessful retrieval to make 10 as more reasonable.
  4) To demonstrate the proposed approach, I suggest the authors to use other related approaches as baselines to compare. 
  5) I am wondering whether the image retrieval related metrics can be used to evaluate the retrieval results for image-based plagiarism detection.
  6) As a full paper, it should have discussion section to analyze the interesting findings and implications of this work.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,16/2/2018,19:52,no
196,56,103,Patxxxx Fx,3,"(OVERALL EVALUATION) The paper studies an interesting problem.

The approach they proposed is rather piecemeal.
Many components are used, yet not all of them are fully validated. 

The evaluation data sets used for evaluation is rather small.

It is hard to know the novelty of the proposed approach.The authors need to make this more salient in the paper.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,23/2/2018,14:42,no
198,56,204,Minxxxx Kx,4,"(OVERALL EVALUATION) This paper examines the problem of detecting academic plagiarism using image based methodologies for plagiarism of figures/charts and other graphical contents, using both ratio and perceptual based hashing, involving the repurposing of the AlexNet 2012 architectured convolutional network net (CNN) for one of the 4 methods ensembled to make the system.

The image-based approach to plagiarism is a good idea that has been raised but little practical studies have gone onto working on this area to my knowledge.  It is heartening to know that this work has been executed.  The paper is well structured and argued, and the model and architecture is sound and reasonable.

Minor points.  I'm not sure that I would agree with ""Strongly disguised images"" as plagiarism.  Certainly photographs of different landmarks don't constitute plagiarism and I think it would be good for the authors to state why the VroniPlag collection deems such cases as plagiarism.

Figure 3 can be better drawn to be more vertically compact and use space more efficiently.  The aspect ratio looks off.

I'm not sure that the implementation details (in Py 2.7) would be necessary details to discuss.  The algorithmic steps in 3.4 however, are nicely described and reduced to a practical description, which is helpful.

It would be great if the authors could address how their system would scale and the constants modified when dealing with a larger corpus.  For example, in S 3.11 there are some constants (for #m of strongly related images) that would seem to be quite corpus and scale specific.  How to select these well would be a useful side discussion if there is sufficient room for discussion.

The approach is also scalable as the system uses parallel processes to execute different forms of plagiarism detection.  

There are two concerns that I hope the authors can address:

- The evaluation.  I'm pretty skeptic that the results are actually as strong as they report them to be.  13 of 15 is ok, but how about false positives (e.g., problems with precision low)?  Given that there are potentially may legitimate pairs of images that are incorrectly flagged as potentially plagiarized, this problem needs to be discuss in much more detail.  The authors seem to focus only on recalling the 15 images.  13 of 15 isn't particularly convincing, and it would have been good to attempt manufacture some cases synthetically to see whether the approaches can detect them (and accordingly distinguish them) from the others.

- The word ""adaptive"".  I don't see how the approach is being adaptive.  The system parameters need to be set appropriately for the system to work well, and couldn't find how these parameters are set.  It seems the ""adaptive"" part might actually a significant downside (if I am interpreting correctly).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Overall, a paper with some issues, but overall, nicely structured, well-written and timely.  Will be a good basis for others to improve upon later, especially if the authors could share their dataset.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,23/2/2018,15:25,no
199,57,163,Fexxx Haxxxx,1,"(OVERALL EVALUATION) The paper describes a very promising approach, coined WELDA, that enhances topic models, specifically LDA, by adding/removing topic words using information from related word embeddings. Topic words are an important part when working with topics, e.g., they are usually used in further processing steps of an NLP analysis pipeline, e.g., to summarize documents or find related documents. Hence, the quality of these topic words is a critical factor of the workflow. The proposed approach shows a novel way of addressing the issue of low quality topic words by combining two state of the art approaches: LDA and word embeddings.

relevance to JCDL: highly relevant since the approach addresses a very common issue in digital libraries and handling NL.

novelty/originality: the approach combines two state of the art techniques/concepts in a novel, non-trivial way. 

methodology: the development of the approach and important design decision are all well understandable and sufficiently discussed in the paper. 

assessment/evaluation/comparison: in their evaluation, they authors analyze the runtime performance and topic quality of their approach, and compare the quality with state of the art approaches on two datasets. WELDA seems to be superior in most the the cases, indicating that the approach is an improvement over the state of the art. however, the study described in sec 5.4 misses crucial information on the number and age of participants.

style/quality of writing: The paper is written well understandable, technically sound, and very well structured.

replicability: since the approach is currently (see also my note below) not publicly available, reproducing the results would be difficult!

I want to emphasize that it will be great for the research community if the authors decide to publish their code/software under an open-source license, such as Apache v2. Unfortunately, the authors currently do not mention whether they plan to make their code publicly available, so I assume that this is currently not the case.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The authors should be asked to publish their code/software under an open-source license.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,31/1/2018,13:23,no
200,57,70,Sauxxxx Chaxxxxxxx,2,"(OVERALL EVALUATION) This paper uses the combination of topic modeling and word embedding to improve topic quality. The novel part about this work is it uses an effective sampling mechanism to replace a word identified via topic modeling and replace it with a word based on nearest neighbor embedding space. This lets it select a word that is more relevant to the topic. This also provides it an an ability to account for out-of-vocabulary words. The approximation based technique used for nearest neighbor search in the embedding space was a good approach to improve the runtime performance and the authors understood the importance of it and addressed it. The qualitative evaluation methodology was sound and focussed on the important aspects of topic modeling.


Recommendations for the authors : I would have liked the authors to evaluate their techniques on more datasets. As a reader I want to gauge the ability of this technique on modern datasets than span beyond the obvious ones like 20 NEWS. I would want to know how this technique would work on short and unstructured data like tweets.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Recommendation for Authors : I would want the authors to show the break up of the individual times for the runtime evaluation in Table 7. As a reader I want to understand the runtime at the individual corpus level. Also, I would want the authors to make the figures more accessible to users that might be reading a black and white copy of the paper. It relies too much on color coded graphs and points.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,14/2/2018,4:00,no
201,57,65,Lilxxxx Caxxx,3,"(OVERALL EVALUATION) The paper provides a detailed description of a new approach to combining word embeddings with latent Dirichlet allocation to improve a topic model.  The paper gives the background, reviews pertinent prior work, illustrates the new approach  and describes the data sets used and the results.  There are sufficient figures to describe the strengths and weaknesses of both the techniques that are combined.  The handling of out-of-vocabulary words is a strength.  The topic modeling approach handles previously unseen words well because of the scale of the data on which it is built.  The LDA approach ignores words not previously seen.  Merging the two provides a definite improvement for the LDA work.  

The paper is well written, very clear, and includes all the information needed for others to evaluate the work and to replicate the results.  

Line 241 (thanks for the line numbers) refers to a ""method by Mikolov et al."" but does not include a citation.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: no",3,,,,,14/2/2018,22:29,no
